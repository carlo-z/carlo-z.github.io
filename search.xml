<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[高并发 —— 限流算法]]></title>
    <url>%2Falgorithm%2F%E9%AB%98%E5%B9%B6%E5%8F%91-%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、限流的作用由于API接口无法控制调用方的行为，因此当遇到瞬时请求量激增时，会导致接口占用过多服务器资源，使得其他请求响应速度降低或是超时，更有甚者可能导致服务器宕机。 限流(Rate limiting)指对应用服务的请求进行限制，例如某一接口的请求限制为100个每秒,对超过限制的请求则进行快速失败或丢弃。 限流可以应对： 热点业务带来的突发请求； 调用方bug导致的突发请求； 恶意攻击请求。 因此，对于公开的接口最好采取限流措施。 ​ 二、限流算法实现限流有很多办法，在程序中时通常是根据每秒处理的事务数(Transaction per second)来衡量接口的流量。 本文介绍几种最常用的限流算法： 固定窗口计数器； 滑动窗口计数器； 漏桶； 令牌桶； 1、 固定窗口计数器算法 固定窗口计数器算法概念如下： 将时间划分为多个窗口； 在每个窗口内每有一次请求就将计数器加一； 如果计数器超过了限制数量，则本窗口内所有的请求都被丢弃当时间到达下一个窗口时，计数器重置。 固定窗口计数器是最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。考虑如下情况：限制1秒内最多通过5个请求，在第一个窗口的最后半秒内通过了5个请求，第二个窗口的前半秒内又通过了5个请求。这样看来就是在1秒内通过了10个请求。 ​ 2、滑动窗口计数器算法 滑动窗口计数器算法概念如下： 将时间划分为多个区间； 在每个区间内每有一次请求就将计数器加一维持一个时间窗口，占据多个区间； 每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间； 如果当前窗口内区间的请求计数总和超过了限制数量，则本窗口内所有的请求都被丢弃。 滑动窗口计数器是通过将窗口再细分，并且按照时间”滑动”，这种算法避免了固定窗口计数器带来的双倍突发请求，但时间区间的精度越高，算法所需的空间容量就越大。 3、漏桶算法 漏桶算法概念如下： 将每个请求视作”水滴”放入”漏桶”进行存储； “漏桶”以固定速率向外”漏”出请求来执行如果”漏桶”空了则停止”漏水”； 如果”漏桶”满了则多余的”水滴”会被直接丢弃。 漏桶算法多使用队列实现，服务的请求会存到队列中，服务的提供方则按照固定的速率从队列中取出请求并执行，过多的请求则放在队列中排队或直接拒绝。 漏桶算法的缺陷也很明显，当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也都得在队列中等待一段时间才能被响应。 4、令牌桶算法 令牌桶算法概念如下： 令牌以固定速率生成； 生成的令牌放入令牌桶中存放，如果令牌桶满了则多余的令牌会直接丢弃，当请求到达时，会尝试从令牌桶中取令牌，取到了令牌的请求可以执行； 如果桶空了，那么尝试取令牌的请求会被直接丢弃。 令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。 ​ 三、单体应用限流1、信号量Semaphore限流private final Semaphore permit = new Semaphore(10, true); 2、Guava的RateLimiter实现限流RateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个 3、利用Atomic类自己实现限流算法​ 四、接入层限流1、nginx限流1、自带模块 limit_req_zone and limit_reqhttps://www.nginx.com/blog/rate-limiting-nginx/ 2、nginx+lua+redis 实现复杂的限流算法openresty了解一下 ​ 五、分布式限流1、为什么要分布式限流 当应用为单点应用时，只要应用进行了限流，那么应用所依赖的各种服务也都得到了保护。 但线上业务出于各种原因考虑，多是分布式系统，单节点的限流仅能保护自身节点，但无法保护应用依赖的各种服务，并且在进行节点扩容、缩容时也无法准确控制整个服务的请求限制。 而如果实现了分布式限流，那么就可以方便地控制整个服务集群的请求限制，且由于整个集群的请求数量得到了限制，因此服务依赖的各种资源也得到了限流的保护。 2、现有方案而分布式限流常用的则有Hystrix、resilience4j、Sentinel等框架，但这些框架都需引入第三方的类库，对于国企等一些保守的企业，引入外部类库都需要经过层层审批，较为麻烦。 3、代码+redis+lua实现分布式限流本质上是一个集群并发问题，而Redis作为一个应用广泛的中间件，又拥有单进程单线程的特性，天然可以解决分布式集群的并发问题。本文简单介绍一个通过Redis实现单次请求判断限流的功能。 1、脚本编写经过上面的对比，最适合的限流算法就是令牌桶算法。而为实现限流算法，需要反复调用Redis查询与计算，一次限流判断需要多次请求较为耗时。因此我们采用编写Lua脚本运行的方式，将运算过程放在Redis端，使得对Redis进行一次请求就能完成限流的判断。 令牌桶算法需要在Redis中存储桶的大小、当前令牌数量，并且实现每隔一段时间添加新的令牌。最简单的办法当然是每隔一段时间请求一次Redis，将存储的令牌数量递增。 但实际上我们可以通过对限流两次请求之间的时间和令牌添加速度来计算得出上次请求之后到本次请求时，令牌桶应添加的令牌数量。因此我们在Redis中只需要存储上次请求的时间和令牌桶中的令牌数量，而桶的大小和令牌的添加速度可以通过参数传入实现动态修改。 由于第一次运行脚本时默认令牌桶是满的，因此可以将数据的过期时间设置为令牌桶恢复到满所需的时间，及时释放资源。 编写完成的Lua脚本如下： 123456789101112131415161718192021222324252627local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token')local last_time = ratelimit_info[1]local current_token = tonumber(ratelimit_info[2])local max_token = tonumber(ARGV[1])local token_rate = tonumber(ARGV[2])local current_time = tonumber(ARGV[3])local reverse_time = 1000/token_rateif current_token == nil then current_token = max_token last_time = current_timeelse local past_time = current_time-last_time local reverse_token = math.floor(past_time/reverse_time) current_token = current_token+reverse_token last_time = reverse_time*reverse_token+last_time if current_token&gt;max_token then current_token = max_token endendlocal result = 0if(current_token&gt;0) then result = 1 current_token = current_token-1end redis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_token)redis.call('pexpire',KEYS[1],math.ceil(reverse_time*(max_token-current_token)+(current_time-last_time)))return result 2、执行限流这里使用Spring Data Redis来进行Redis脚本的调用。 编写Redis脚本类: 1234567891011121314151617public class RedisReteLimitScript implements RedisScript&lt;String&gt; &#123; private static final String SCRIPT = "local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token') local last_time = ratelimit_info[1] local current_token = tonumber(ratelimit_info[2]) local max_token = tonumber(ARGV[1]) local token_rate = tonumber(ARGV[2]) local current_time = tonumber(ARGV[3]) local reverse_time = 1000/token_rate if current_token == nil then current_token = max_token last_time = current_time else local past_time = current_time-last_time; local reverse_token = math.floor(past_time/reverse_time) current_token = current_token+reverse_token; last_time = reverse_time*reverse_token+last_time if current_token&gt;max_token then current_token = max_token end end local result = '0' if(current_token&gt;0) then result = '1' current_token = current_token-1 end redis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_toke redis.call('pexpire',KEYS[1],math.ceil(reverse_time*(max_tokencurrent_token)+(current_time-last_time))) return result"; @Override public String getSha1() &#123; return DigestUtils.sha1Hex(SCRIPT); &#125; @Override public Class&lt;String&gt; getResultType() &#123; return String.class; &#125; @Override public String getScriptAsString() &#123; return SCRIPT; &#125;&#125; 通过RedisTemplate对象执行脚本： 1234567public boolean rateLimit(String key, int max, int rate) &#123; List&lt;String&gt; keyList = new ArrayList&lt;&gt;(1); keyList.add(key); return "1".equals(stringRedisTemplate .execute(new RedisReteLimitScript(), keyList, Integer.toString(max), Integer.toString(rate), Long.toString(System.currentTimeMillis()))); &#125; rateLimit方法传入的key为限流接口的ID，max为令牌桶的最大大小，rate为每秒钟恢复的令牌数量，返回的boolean即为此次请求是否通过了限流。为了测试Redis脚本限流是否可以正常工作，我们编写一个单元测试进行测试看看。 123456789101112131415161718192021222324@Autowiredprivate RedisManager redisManager;@Testpublic void rateLimitTest() throws InterruptedException &#123; String key = "test_rateLimit_key"; int max = 10; //令牌桶大小 int rate = 10; //令牌每秒恢复速度 AtomicInteger successCount = new AtomicInteger(0); Executor executor = Executors.newFixedThreadPool(10); CountDownLatch countDownLatch = new CountDownLatch(30); for (int i = 0; i &lt; 30; i++) &#123; executor.execute(() -&gt; &#123; boolean isAllow = redisManager.rateLimit(key, max, rate); if (isAllow) &#123; successCount.addAndGet(1); &#125; log.info(Boolean.toString(isAllow)); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); log.info("请求成功&#123;&#125;次", successCount.get());&#125; 设置令牌桶大小为10，令牌桶每秒恢复10个，启动10个线程在短时间内进行30次请求，并输出每次限流查询的结果。日志输出： 12345678910111213141516171819202122232425262728293031[19:12:50,283]true [19:12:50,284]true [19:12:50,284]true [19:12:50,291]true [19:12:50,291]true [19:12:50,291]true [19:12:50,297]true [19:12:50,297]true [19:12:50,298]true [19:12:50,305]true [19:12:50,305]false [19:12:50,305]true [19:12:50,312]false [19:12:50,312]false [19:12:50,312]false [19:12:50,319]false [19:12:50,319]false [19:12:50,319]false [19:12:50,325]false [19:12:50,325]false [19:12:50,326]false [19:12:50,380]false [19:12:50,380]false [19:12:50,380]false [19:12:50,387]false [19:12:50,387]false [19:12:50,387]false [19:12:50,392]false [19:12:50,392]false [19:12:50,392]false [19:12:50,393]请求成功11次 可以看到，在0.1秒内请求的30次请求中，除了初始的10个令牌以及随时间恢复的1个令牌外，剩下19个没有取得令牌的请求均返回了false，限流脚本正确的将超过限制的请求给判断出来了，业务中此时就可以直接返回系统繁忙或接口请求太过频繁等提示。 3、开发中遇到的问题1）Lua变量格式 Lua中的String和Number需要通过tonumber()和tostring()进行转换。 2）Redis入参 Redis的pexpire等命令不支持小数，但Lua的Number类型可以存放小数，因此Number类型传递给 Redis时最好通过math.ceil()等方式转换以避免存在小数导致命令失败。 3）Time命令 由于Redis在集群下是通过复制脚本及参数到所有节点上，因此无法在具有不确定性的命令后面执行写入命令，因此只能请求时传入时间而无法使用Redis的Time命令获取时间。 3.2版本之后的Redis脚本支持redis.replicate_commands()，可以改为使用Time命令获取当前时间。 4）潜在的隐患 由于此Lua脚本是通过请求时传入的时间做计算，因此务必保证分布式节点上获取的时间同步，如果时间不同步会导致限流无法正常运作。]]></content>
      <categories>
        <category>高并发</category>
      </categories>
      <tags>
        <tag>高并发</tag>
        <tag>限流</tag>
        <tag>nginx</tag>
        <tag>openresty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcpdump 捕获中文传输数据包，并手动解析]]></title>
    <url>%2Fos%2Fcomputer-network%2Ftcpdump%E6%8D%95%E8%8E%B7%E4%B8%AD%E6%96%87%E4%BC%A0%E8%BE%93%E6%95%B0%E6%8D%AE%E5%8C%85%E5%B9%B6%E6%89%8B%E5%8A%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[开启tcpdump抓包centos7 下 ，只抓有数据的包： tcpdump -i eth0 ‘((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0’ and src 10.10.139.42 and dst 10.10.128.57 and port 21000 -XX ​ 示例（1）输入“中国a”centos7 下 netcat 输入： 抓包结果放在 WireShark中分析 tcpdump抓包： wireshark分析抓到的文件： 可以看到 中文的16进制编码为 d6 d0 b9 fa 61 0a， 查看 控制台的编码： 可知，中文经过GBK编码以后的 16进制表示为： d6 d0 b9 fa 61 0a； GBK的编码规则如下： d6d0 b9fa 610a 按双字节表示转换为：D6D0 B9FA 610A GBK对照表：http://ff.163.com/newflyff/gbk-list/ D6D0 对应的中文为： B9FA 对应的中文为： 610A 对应的中文为： 搜索没有对应的GBK编码 ASCII码对照表：http://ascii.911cha.com/ 0A 为换行符； ​ 示例（2）输入“asdf123”输入： 抓包： 解析：]]></content>
      <tags>
        <tag>tcpdump</tag>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcpdump抓包后，wireshark分析]]></title>
    <url>%2Fos%2Fcomputer-network%2Ftcpdump%E6%8A%93%E5%8C%85%E5%90%8E-wireshark%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[来源：https://segmentfault.com/a/1190000015044878?utm_source=tag-newest 因为最近要解析 TCP 报文中 option 段的一块数据，所以不得不详细了解下 TCP/IP 报文。虽然之前看过，很长时间没这么细致地用过，导致了健忘，借着这个机会，通过 tcpdump 抓包分析，详细捋一遍 TCP/IP 报文。 报文获取如果那样干巴巴地讲这个东西比较晕，而且网上的文章一大堆，没有什么创新。我选择换一个角度来切入 TCP/IP 协议。首先通过 tcpdump 准备报文。【1】我在 192.168.1.22 这台机器的 10000 端口启一个 redis 服务。【2】通过 tcpdump 这个工具来抓取数据包，命令如下： 1tcpdump -w /tmp/logs -i eth0 port 10000 -s0 【3】在 192.168.1.26 这台机器上访问 192.168.1.22:10000 这个 redis 实例，可以用 redis-cli 客户端，也可以用 telnet，发送一个 ping, 得到对端回复 pong。【4】停止抓包，用 tcpdump 读取这个数据包（-x 以16进制形式展示，便于后面分析） 1tcpdump -r /tmp/logs -n -nn -A -x| vim - 其中有一个数据包是这样的，这也是这篇文章要分析的: 123410:54:54.270967 IP 192.168.1.26.61096 &gt; 192.168.1.22.10000: Flags [P.], seq 1041414875:1041414889, ack 658186233, win 115, options [nop,nop,TS val 2377448931 ecr 2741547141], length 140x0000: [4560 0042 7567 0000 3d06 6F3C C0A8 011A 0x0010: C0A8 0116] &#123;eea8 2710 3e12 badb 273b 1ff9 0x0020: 8018 0073 64b0 0000 0101 080a 8db4 fde3 0x0030: a368 b085&#125; 2a31 0d0a 2434 0d0a 7069 6e670x0040: 0d0a 注意：【1】之前在文章常用 shell 中介绍过抓包神器 tcpdump，还不会的小伙伴可以偷瞄一眼。【2】上面报文数据中的 [、]、{ 和 } 是为了方便区分数据，我自己加上的。[]包围的部分为本报文中的 IP 头，{}包围的部分为本报文中的 TCP 头。 报文分析IP 报文整体结构如下，因为抓到的数据包是 redis 服务，因此在传输层为 TCP 协议。 IP 层解析解析数据包之前，先把 IP 协议拿出来，如下： 可以看到，IP 报文头部采用固定长度(20B) + 可变长度构成，下面的 TCP 头部也是这样。然后下面对着抓到的数据包进行分析：【1】0x4 4bit， ip 协议版本0x4 表示 IPv4。【2】0x5 4bit，ip首部长度该字段表示单位是32bits(4字节) ，所以这个 ip 包的头部有 5*4=20B，这就可以推出，该 IP 报文头没有可选字段。4bit 可以表示最大的数为 0xF，因此，IP 头部的最大长度为 15*4=60B。该报文的 IP 头部我已经在报文中标注出来了。【3】0x60 8bit，服务类型 TOS该段数据组成为 3bit 优先权字段(现已被忽略) + 4bit TOS 字段 + 1bit 保留字段(须为0)。4bit TOS 字段分别表示自小时延、最大吞吐量、最高可用性和最小费用。只能置其中 1bit，全为 0 表示一般服务。现在大多数的TCP/IP实现都不支持TOS特性 。可以看到，本报文 TOS 字段为全 0。【4】0x0042 16bit， IP 报文总长度单位字节，换算下来，该数据报的长度为 66 字节，数一下上面的报文，恰好 66B。从占位数来算， IP 数据报最长为 2^16=65535B，但大部分网络的链路层 MTU（最大传输单元）没有这么大，一些上层协议或主机也不会接受这么大的，故超长 IP 数据报在传输时会被分片。【5】0x7567 16bit，标识唯一的标识主机发送的每一个数据报。通常每发送一个报文，它的值+1。当 IP 报文分片时，该标识字段值被复制到所有数据分片的标识字段中，使得这些分片在达到最终目的地时可以依照标识字段的内容重新组成原先的数据。【6】0x0000 3bit 标志 + 13bit 片偏移3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8）【7】0x3d 8bit 生存时间TTLIP 报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。本报文该值为 61。【8】0x06 8bit 协议指出 IP 报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。【9】0x6F3C 16bit IP 首部校验和由发送端填充。以本报文为例，先说这个值是怎么计算出来的。 123# 将校验和字段 16bit 值抹去变为 `0x0000`，然后将首部 20字节值相加0x4560 + 0x0042 + 0x7567 + 0x0000 + 0x3d06 + 0x0000 + 0xC0A8 + 0x011A + 0xC0A8 +0x0116 = 0x27B95# 将上述结果的进位 2 与低 16bit 相加0x7B95 + 0x2 = 0x7B97# 0x7B97 按位取反~(0x7B97) = 0x8468 【10】0xC0A8011A 32bit 源地址可以通过一下 python 程序将 hex 转换成我们熟悉的点分 IP 表示法 12345&gt;&gt;&gt; import socket&gt;&gt;&gt; import struct&gt;&gt;&gt; int_ip=int("0xC0A8011A",16)&gt;&gt;&gt; socket.inet_ntoa(struct.pack('I',socket.htonl(int_ip)))'192.168.1.26' 本报文中的 src addr 为 192.168.1.26，恰好就是发起请求的 IP。【11】0xC0A80116 32bit 目的地址经过计算为 192.168.1.22，恰好就是启 redis 服务那台机器的 IP。 由于该报文首部长度为 20B，因此没有可变长部分。 传输层解析本报文携带的数据使用的 TCP 协议，因此下面开始分析 TCP 协议。与上面的 IP 报文一样， TCP 报文头也才用采用固定长度(20B) + 可变长度的形式。首先还是看 TCP 协议的格式，从网上找了一张图，如下： 注： TCP 的头部必须是 4字节的倍数,而大多数选项不是4字节倍数,不足的用 NOP 填充。【1】0xeea8 16bit，源端口解析得到 61096，这与 tcpdump 读包显示的是一致的。16bit 决定了端口号的最大值为 65535.【2】0x2710 16bit，目的端口解析得到 10000。【3】0x273b1ff9 32bit，序号解析得到 1041414875，这与上面 tcpdump 显示的 seq 段是一致的。【4】0x273b1ff9 32bit，确认号解析得到 658186233，这与上面 tcpdump 显示的 ack 段是一致的。【5】0x8 4bit，TCP 报文首部长度也叫 offset，其实也就是数据从哪里开始。8 * 4 = 32B,因此该 TCP 报文的可选部分长度为 32 - 20 = 12B，这个资源还是很紧张的！ 同 IP 头部类似，最大长度为 60B。【6】0b000000 6bit, 保留位保留为今后使用，但目前应置为 0。【7】0b011000 6bit，TCP 标志位上图可以看到，从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。从抓包可以看出，该报文是带了 ack 的，所以 ACK 标志位置为 1。关于标志位的知识这里就不展开了。【8】0x0073 16bit，滑动窗口大小解析得到十进制 115，跟 tcpdump 解析的 win 字段一致。【9】0x64b0 16bit，校验和由发送端填充，接收端对 TCP 报文段执行 CRC 算法，以检验 TCP 报文段在传输过程中是否损坏，如果损坏这丢弃。检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。【10】0x0000 16bit，紧急指针仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 下面是 TCP 可选项，其格式如下： 常见的可选项如下图： 【11】0x01NOP 填充，没有 Length 和 Value 字段， 用于将TCP Header的长度补齐至 32bit 的倍数。【12】0x01同上。【13】0x080a可选项类型为时间戳，len为 10B，value 为0x8db4 0xfde3 0xa368 0xb085，加上 0x080a，恰好 10B!启用 Timestamp Option后，该字段包含2 个 32bit 的Timestamp（TSval 和 TSecr）。【14】0x8db4 0xfde3解析后得到 2377448931，恰好与 tcpdump 解析到的 TS 字段的 val一致！【15】0xa368 0xb085解析后得到 2741547141，恰好与 tcpdump 解析到的 TS 字段的 ecr一致！ 数据部分解析上面分析得知，该 IP 报文长度为 66B，IP 头长度为 20B，TCP 头部长度为 32B，因此得到数据的长度为 66 - 20 - 32 = 14B，这与 tcpdump 解析到的 len 字段一致！下面来分析这个具体的数据。这里涉及到 redis 协议，不知道的小伙伴可以查看这篇文档redis 协议说明。在抓包时，用客户端向 redis 服务端发送了一个 ping 命令，转换成 redis 协议如下： 123*1\r\n$4\r\nping\r\n 下面看抓包数据解析，这需要对照 ascii 码表来看，在 linux 下可以用 man 7 ascii 这个命令来获得，或者在这里查看ascii码表。 12340x2a31 -&gt; *10x0d0a -&gt; \r\n0x2434 -&gt; $40x0d0a -&gt; \r\n0x7069 0x6e67 -&gt; ping0x0d0a -&gt; \r\n 既然详细说到 TCP/IP 协议，那补充一下 tcpdump filter 的几点用法。filter可以简单地分为三类：type, dir 和 proto。 type 区分报文的类型，主要由 host（主机）, net（网络，支持 CIDR） 和 port(支持范围，如 portrange 21-23) 组成。dir 区分方向，主要由 src 和 dst 组成。proto 区分协议支持 tcp、udp 、icmp 等。 下面说几个 filter 表达式。proto[x:y] start at offset x into the proto header and read y bytes[x] abbreviation for [x:1]注意：单位是字节，不是位！举几个栗子：【1】打印 80 端口，有数据的 tcp 包 1tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)' ip[2:2] 从 ip 报文的第3个字节开始读2个字节，这个恰好就是 ip 包的总长度，单位是字节ip[0]&amp;0xf 取的是 ip 报文第 1 个字节的低 4 位，&lt;&lt; 2（乘以 4），为 ip 头部长度，单位是字节tcp[12]&amp;0xf0 取的是 tcp 报文第 13 个字节的高 4 位，&gt;&gt; 2 其实等价于 &gt;&gt; 4 然后 &lt;&lt; 2，为 tcp 头部长度，单位是字节。所以 ((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) 表示的数据长度。【2】打印 80 端口，长度超过 576 的 ip 包 1tcpdump 'port 80 and ip[2:2] &gt; 576' 【3】打印特定 TCP Flag 的数据包TCP Flags 在 tcpdump 抓取的报文中的体现：[S]：SYN（开始连接）[.]: 没有 Flag[P]: PSH（推送数据）[F]: FIN （结束连接）[R]: RST（重置连接）[S.] SYN-ACK，就是 SYN 报文的应答报文。 12tcpdump 'tcp[13] &amp; 16!=0'# 等价于tcpdump 'tcp[tcpflags] == tcp-ack' 打印出所有的 ACK 包. 12tcpdump 'tcp[13] &amp; 4!=0'# 等价于tcpdump 'tcp[tcpflags] == tcp-rst' 打印出所有的 RST 包，即包含 [R] 标志的包。 更多 tcpdump filter 可以查看 PCAP-FILTER 或者 man tcpdump！ 好了，这个 IP 包的解析就到此为止了，照着 TCP/IP 协议分析了一遍, 发现协议也就那么回事儿，没有想象的那么难，不要害怕协议！ 具体抓包示例 IP协议： tcp协议：]]></content>
      <tags>
        <tag>tcpdump</tag>
        <tag>tcp</tag>
        <tag>wireshark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 模拟 网络延迟、网络丢包、网络中断]]></title>
    <url>%2Fos%2Fcomputer-network%2Flinux%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E5%BB%B6%E8%BF%9F-%E7%BD%91%E7%BB%9C%E4%B8%A2%E5%8C%85-%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%96%AD%2F</url>
    <content type="text"><![CDATA[昨天，笔者讲述了如何将CPU和IO撑满，这个其实很好理解，写个CPU密集型的程序让CPU忙个不停就可以撑满CPU；弄个程序一直写就可以让IO也撑满。有兴趣的同学可以看下昨天的这篇文章《看我如何作死 | 将CPU、IO撑满》，不过里面的做法分别是使用openssl speed和linux dd工具来实现这两个功能。 面对CPU和IO时，相信大家都能很快的反应出如何实现，那么面对网络问题时，大家的反应又是如何呢？不会是拔网线吧。。。 在故障注入，或者说故障演练，甚至说混沌工程中，可以设计很多类型的故障，今天要介绍的就是网络故障。 混沌系统是在分布式系统上进行实验的学科，目的是建立对系统抵御生产环境中失控条件的能力以及信心。 在复杂的网络环境下，数据包发送和接收的时间间隔或长或短。在网络状况较差时，调用下游服务时可能要过很久才能收到返回，这时服务的反应如何，直接关系到稳定性与高可用。 我们这里索要模拟的网络故障有三类，分别是：网络延时、网络中断以及网络丢包。 ​ 一、tc工具介绍笔者也不卖关子，本文模拟的网络故障是通过linux的tc工具来实现的。Linux内核网络协议栈从2.2.x开始，就实现了对服务质量的支持模块。具体的代码位于net/sched/目录。在Linux里面，对这个功能模块的称呼是Traffic Control ,简称TC。TC是一个在上层协议处添加Qos功能的工具，原理上看，它实质是专门供用户利用内核Qos调度模块去定制Qos的中间件。 Linux操作系统中的流量控制器TC（Traffic Control）用于Linux内核的流量控制，主要是通过在输出端口处建立一个队列来实现流量控制。 接收包从输入接口（Input Interface）进来后，经过流量限制（Ingress Policing）丢弃不符合规定的数据包，由输入多路分配器（Input De-Multiplexing）进行判断选择：如果接收包的目的是本主机，那么将该包送给上层处理；否则需要进行转发，将接收包交到转发块（Forwarding Block）处理。转发块同时也接收本主机上层（TCP、UDP等）产生的包。转发块通过查看路由表，决定所处理包的下一跳。然后，对包进行排列以便将它们传送到输出接口（Output Interface）。一般我们只能限制网卡发送的数据包，不能限制网卡接收的数据包，所以我们可以通过改变发送次序来控制传输速率。Linux流量控制主要是在输出接口排列时进行处理和实现的。 tc工具的语法还是很复杂的，笔者（微信公众号：朱小厮的博客）试图想要在本文中详细的讲解一下tc的用法，最后还是放弃了，篇幅太长，难以穷尽。所以本文中只是针对前面说的三种故障简单的演示一下tc的用法以及对应故障的实现方式，希望能够能大家有个小小的印象。如果以后遇到类似问题，或者说对这个东西感兴趣，可以再深度的学习一下。 ​ 二、paping工具介绍在正式介绍如何模拟网络故障之前，还要介绍一个工具来查看模拟的效果如何。 通常我们测试数据包能否通过IP协议到达特定主机，都习惯使用Ping命令，工作时发送一个ICMP Echo，等待接受Echo响应，但是Ping使用的是ICMP协议，如果防火墙放通了此协议，依旧能够ping通，但是无法确定通过tcp传送的数据包是否正常到达对端。 而paping可以在Linux平台上测试网络的连通性及网络延时等。它的用法很简单: 1234-p, --port N 指定被测试服务的 TCP 端口（必须）；--nocolor 屏蔽彩色输出；-t, --timeout 指定超时时长，单位为毫秒，默认值为 1000；-c, --count N 指定测试次数。 比如下面的示例（记得先要开启一个以80为端口的服务, 示例中的xxx.xxx.xxx.xxx代表ip地址）: 123456789101112131415hidden@hidden$ ./paping -p 80 -c 5 xxx.xxx.xxx.xxxpaping v1.5.5 - Copyright (c) 2011 Mike LovellConnecting to xxx.xxx.xxx.xxx on TCP 80:Connected to xxx.xxx.xxx.xxx: time=27.47ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=97.83ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=37.38ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=57.62ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=71.87ms protocol=TCP port=80Connection statistics: Attempted = 5, Connected = 5, Failed = 0 (0.00%)Approximate connection times: Minimum = 27.47ms, Maximum = 97.83ms, Average = 58.43ms 可以看到平均链接时间为58.43ms。 如果你的机器上没有安装paping，那么可以采用如下的方式安装： 123wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/paping/paping_1.5.5_x86_linux.tar.gztar -zvxf paping_1.5.5_x86_linux.tar.gz ./paping -p 80 -c 5000 www.baidu.com 如果有以下的错误： ./paping: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory 可以先安装对应的库来解决： 12sudo apt-get install libstdc++6sudo apt-get install lib32stdc++6 ​ 三、模拟网络延时使用tc命令模拟延迟300ms（对应的删除命令为tc qdisc del dev eth0 root netem）： 12 tc qdisc add dev eth0 root netem delay 300ms// 该命令将网卡eth0的传输设置为延迟300ms发送 此时再次执行paping命令: 123456789101112131415hidden@hidden$ ./paping -p 80 -c 5 xxx.xxx.xxx.xxxpaping v1.5.5 - Copyright (c) 2011 Mike LovellConnecting to xxx.xxx.xxx.xxx on TCP 80:Connected to xxx.xxx.xxx.xxx : time=326.11ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=417.02ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=326.94ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=326.19ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=353.51ms protocol=TCP port=80Connection statistics: Attempted = 5, Connected = 5, Failed = 0 (0.00%)Approximate connection times: Minimum = 326.11ms, Maximum = 417.02ms, Average = 349.95ms 与之前的58.43ms相比相差了291.52ms ≈ 300ms。 更真实的情况下，延迟值不会这么精确，会有一定的波动，我们可以用下面的情况来模拟出带有波动性的延迟值： 12tc qdisc add dev eth0 root netem delay 300ms 50ms//该命令将 eth0 网卡的传输设置为延迟 300ms ± 50ms (250 ~ 350 ms 之间的任意值)发送 ​ 四、模拟网络中断这次使用如下的命令： 12tc qdisc add dev eth0 root netem corrupt 10%//该命令将 eth0 网卡的传输设置为随机产生 10% 的损坏的数据包 此时再次执行paping命令: 1./paping -p 80 -c 100 xxx.xxx.xxx.xxx 注意这里的次数改成了100，为了更能清楚的看到中断的实际效果。 运行这个命令的过程中，会有“Connection timed out”字样报出，类似： 123456789&lt;snip&gt;Connected to xxx.xxx.xxx.xxx: time=26.26ms protocol=TCP port=80Connection timed outConnected to xxx.xxx.xxx.xxx: time=65.10ms protocol=TCP port=80Connection timed outConnected to xxx.xxx.xxx.xxx: time=26.50ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=25.93ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=27.71ms protocol=TCP port=80&lt;snip&gt; 最终的统计结果如下： 1234Connection statistics: Attempted = 100, Connected = 90, Failed = 10 (10.00%)Approximate connection times: Minimum = 25.67ms, Maximum = 133.77ms, Average = 51.98ms 结果显而易见，验证了此次故障模拟所对应的效果。 ​ 五、模拟网络丢包使用如下命令： 123tc qdisc add dev eth0 root netem loss 7% 25%//该命令将 eth0 网卡的传输设置为随机丢掉 7% 的数据包, 成功率为 25% //如果不加上后面的25%，那么一丝就是随机丢掉7%的数据包 再次执行paping命令时，也会有Connection timed out报出，最终的统计结果如下： 1234Connection statistics: Attempted = 100, Connected = 99, Failed = 1 (1.00%)Approximate connection times: Minimum = 25.80ms, Maximum = 133.87ms, Average = 60.94ms 7%*25%的值在1%-2%之间，符合测试的结果预期。 tc还可以模拟一些其它的网络故障，比如网络包重复、网络包错序等等，有兴趣的同学可以继续深入了解一下。 ​ 六、引申混沌工程和传统测试之间的区别很多同学在进行一些故障测试的时候，会认为其正在进行混沌实验，其实混沌工程和传统的测试之间是有区别的。 混沌工程和传统测试（故障注入FIT、故障测试）在关注点和工具集上都有很大的重叠。譬如，在Netflix（如果还不知道Netflix是谁，可以先看看这篇《明星公司之Netflix》了解一下）的很多混沌工程实验研究的对象都是基于故障注入来引入的。混沌工程和这些传统测试方法的主要区别在于：混沌工程是发现新信息的实践过程，而故障注入则是对一个特定的条件、变量的验证方法。 当你希望探究复杂系统如何应对异常时，对系统中的服务注入通信故障（如超时、错误等）不失为一种很好的方法。但有时我们希望探究更多其他的非故障类的场景，如流量激增、资源竞争条件、拜占庭故障（例如性能差或有异常的节点发出有错误的响应、异常的行为、对调用者随机性的返回不同的响应，等等）、非计划中的或非正常组合的消息处理等等。因为如果一个面向公众用户的网站突然收到激增的流量，从而产生更多的收入时我们很难称之为故障，但我们仍然需要探究清楚系统在这种情况下的影响。 和故障注入类似，故障测试方法通过对预先设想到的可以破坏系统的点进行测试，但是并没能去探究上述这类更广阔领域里的、不可预知的、但很可能发生的事情。 在传统测试中，我们可以写一个断言（assertion），即我们给定一个特定的条件，产生一个特定的输出。测试一般来说只会产生二元的结果，验证一个结果是真还是假，从而判定测试是否通过。严格意义上来说，这个过程并不能让我们发掘出对于系统未知的、尚不明确的认知，它仅仅是对我们已知的系统属性可能的取值进行测验。而实验可以产生新的认知，而且通常还能开辟出一个更广袤的对复杂系统的认知空间。 混沌工程是一种帮助我们获得更多的关于系统的新认知的实验方法。它和已有的功能测试、集成测试等以测试已知属性的方法有本质上的区别。 ​ 七、后续后面还会有几篇相同主题的文章发出，不出意外，下一篇应该是《怎么让进程假死》，如果有兴趣的话，可以持续关注本公众号（朱小厮的博客）。如果你还有有什么需要进一步了解的可以在下方留言，或者也聊聊你对这一块的认知和想法。]]></content>
      <categories>
        <category>linux</category>
        <category>centos7</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>混沌工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11、Elasticsearch For Hadoop]]></title>
    <url>%2Felasticsearch%2F11-Elasticsearch-For-Hadoop%2F</url>
    <content type="text"><![CDATA[11.1、单机版Hadoop安装 11.2、ES-Hadoop安装 11.3、从 HDFS 到 Elasticsearch 11.4、从Elasticsearch 到 HDFS -—————————————————– 11.1、单机版Hadoop安装hadoop分为 单机模式、伪分布式模式、完全分布式模式 1、ssh免密登录 vi /etc/ssh/ssh_config 文件尾添加： 12StrictHostKeyChecking noUserKnownHostsFile /dev/null 2、hadoop下载安装[root@localhost data]# mkdir -p /data/hadoop [root@localhost data]# cd /data/hadoop [root@localhost hadoop]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz 3、haoop 单机模式[root@localhost hadoop]# tar -zxf hadoop-2.7.7.tar.gz [root@localhost hadoop]# mkdir input [root@localhost hadoop]# echo “hello world” &gt; input/file1.txt [root@localhost hadoop]# echo “hello hadoop” &gt; input/file2.txt [root@localhost hadoop]# ./hadoop-2.7.7/bin/hadoop jar ./hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /data/hadoop/input/ /data/hadoop/output [root@localhost hadoop]# cat output/part-r-00000 4、hadoop 伪分布式模式[root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/hadoop-env.sh 123456修改JAVA_HOME 为：export JAVA_HOME=/data/tools/jdk1.8.0_65export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"改为：export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=" [root@localhost hadoop]# mkdir -p /data/hadoop/hdfs/tmp [root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop.tmp.dir 可以自定义； fs.default.name 保存了NameNode的位置，HDFS和MapReduce组件都需要用到它； [root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/mapred-site.xml.template 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;tconfiguration&gt; mapred.job.tracker 保存JobTracker的位置，只有MapReduce需要知道； [root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/hdfs-site.xml 配置HDFS数据库的复制次数： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 以上就配置完了，接下来格式化 namenode： [root@localhost hadoop]# ./hadoop-2.7.7/bin/hadoop namenode -format 启动hadoop： [root@localhost hadoop]# ./hadoop-2.7.7/sbin/start-all.sh 启动成功； 添加hadoop到环境变量： [root@localhost hadoop-2.7.7]# vi /etc/profile 12export HADOOP_HOME=/data/hadoop/hadoop-2.7.7export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin [root@localhost hadoop-2.7.7]# source /etc/profile 5、HDFS常用操作开放 50070 端口 [root@localhost hadoop-2.7.7]# firewall-cmd –permanent –add-port=50070/tcp success [root@localhost hadoop-2.7.7]# firewall-cmd –reload success [root@localhost hadoop-2.7.7]# firewall-cmd –query-port=50070/tcp yes 访问hdfs：http://172.18.1.51:50070/ [root@localhost hadoop]# hadoop fs -ls / [root@localhost hadoop]# hadoop fs -mkdir /work [root@localhost hadoop]# touch aa.txt [root@localhost hadoop]# hadoop fs -put aa.txt /work [root@localhost hadoop]# hadoop fs -test -e /work/aa.txt [root@localhost hadoop]# echo $? 0 [root@localhost hadoop]# echo “hello hdfs” &gt; aa.txt [root@localhost hadoop]# hadoop fs -appendToFile aa.txt /work/aa.txt [root@localhost hadoop]# hadoop fs -cat /work/aa.txt hello hdfs [root@localhost hadoop]# hadoop fs -rm /work/aa.txt 19/06/10 05:04:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes. Deleted /work/aa.txt [root@localhost hadoop]# hadoop fs -rmr /work/a rmr: DEPRECATED: Please use ‘rm -r’ instead. rmr: `/work/a’: No such file or directory [root@localhost hadoop]# hadoop dfsadmin -report [root@localhost hadoop]# ​ 11.2、引入ES-Hadoop依赖 12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-hadoop&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt; &lt;version&gt;2.7.7&lt;/version&gt; &lt;/dependency&gt; ​ 11.3、从 HDFS 到 Elasticsearch准备json文档，上传到HDFS中 vi blog.json 123&#123;"id":5,"title":"JavaScript高级程序设计","language":"javascript","author":"NicholasC.Zakas","price":66.4,"publish_time":"2012-03-02","desc":"JavaScript技术经典名著。"&#125;&#123;"id":3,"title":"Python科学计算","language":"python","author":"张若愚","price":81.4,"publish_time":"2014-01-02","desc":"零基础学python，光盘中坐着度假开发winPython运行环境，涵盖了Python各个扩展库。"&#125;&#123;"id":4,"title":"Python基础教程","language":"python","author":"Helant","price":54.5,"publish_time":"2014-03-02","desc":"经典的python入门教程，层次鲜明，结构严谨，内容详实。"&#125; [root@localhost hadoop]# hadoop fs -put blog.json /work 编写代码，从 hdfs 读取数据，写入到 Elasticsearch： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.learn.eshdoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.elasticsearch.hadoop.mr.EsOutputFormat;import java.io.IOException;/** * 读取 HDFS 上的内容然后写入 Elasticsearch */public class HdfsToEs &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); conf.setBoolean("mapred.map.tasks.speculative.execution", false); conf.setBoolean("mapred.reduce.tasks.speculative.execution", false); conf.set("es.nodes", "10.10.139.42"); conf.set("es.port", "9200"); conf.set("es.nodes.wan.only", "true"); conf.set("es.resource", "blog/_doc"); conf.set("es.mapping.id", "id"); conf.set("es.input.json", "yes"); Job job = Job.getInstance(conf, "EmrToES"); job.setJarByClass(HdfsToEs.class); job.setMapperClass(MyMapper.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(EsOutputFormat.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(otherArgs[0])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125; public static class MyMapper extends Mapper&lt;Object, Text, NullWritable, Text&gt; &#123; private Text line = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; if (value.getLength() &gt; 0) &#123; line.set(value); context.write(NullWritable.get(), line); &#125; &#125; &#125;&#125; pom中指定主类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.learn.eshdoop.HdfsToEs&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt;&lt;goal&gt;shade&lt;/goal&gt;&lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;com.learn.eshdoop.HdfsToEs&lt;/mainClass&gt; &lt;/transformer&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer"&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 运行：mvn clean package 生成对应的jar 包 [root@localhost hadoop]# mv es-hadoop-learn-1.0-SNAPSHOT-jar-with-dependencies.jar HdfsToEs.jar [root@localhost hadoop]# hadoop jar HdfsToEs.jar /work/blog.json 在Kibana 中 查看 GET blog/_mapping GET blog/_search 11.4、从Elasticsearch 到 HDFS1、将索引 blog 保存到 hdfs添加Java类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.eshdoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.Writable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.elasticsearch.hadoop.mr.EsInputFormat;import java.io.IOException;/** * 查询 Elasticsearch 索引然后将结果写入 HDFS */public class EsToHdfs &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); conf.set("es.nodes", "10.10.139.42"); conf.set("es.port", "9200"); conf.set("es.resource", "blog/_doc"); conf.set("es.output.json", "true"); Job job = Job.getInstance(conf, "hadoop es write test"); job.setMapperClass(MyMapper.class); job.setNumReduceTasks(1); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(EsInputFormat.class); FileOutputFormat.setOutputPath(job, new Path(otherArgs[0])); job.waitForCompletion(true); &#125; public static class MyMapper extends Mapper&lt;Writable, Writable, NullWritable, Text&gt; &#123; @Override protected void map(Writable key, Writable value, Context context) throws IOException, InterruptedException &#123; Text text = new Text(); text.set(value.toString()); context.write(NullWritable.get(), text); &#125; &#125;&#125; 修改pom.xml中的主类为：com.learn.eshdoop.EsToHdfs。 mvn clean package 重新打包 上传以后执行： [root@localhost hadoop]# mv es-hadoop-learn-1.0-SNAPSHOT-jar-with-dependencies.jar EsIndexToHdfs.jar [root@localhost hadoop]# hadoop jar EsIndexToHdfs.jar /work/blog_mapping [root@localhost hadoop]# hadoop fs -cat /work/blog_mapping/part-r-00000 2、将带条件查询 blog 的 数据 保存到 hdfs1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.eshdoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.Writable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.elasticsearch.hadoop.mr.EsInputFormat;import java.io.IOException;/** * 查询 Elasticsearch 数据 然后将结果写入 HDFS */public class EsQueryToHdfs &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); conf.set("es.nodes", "10.10.139.42"); conf.set("es.port", "9200"); conf.set("es.resource", "blog/_doc"); conf.set("es.output.json", "true"); conf.set("es.query", "?q=title:python"); Job job = Job.getInstance(conf, "query es to HDFS"); job.setMapperClass(EsToHdfs.MyMapper.class); job.setNumReduceTasks(1); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(EsInputFormat.class); FileOutputFormat.setOutputPath(job, new Path(otherArgs[0])); job.waitForCompletion(true); &#125; public static class MyMapper extends Mapper&lt;Writable, Writable, NullWritable, Text&gt; &#123; @Override protected void map(Writable key, Writable value, Context context) throws IOException, InterruptedException &#123; Text text = new Text(); text.set(value.toString()); context.write(NullWritable.get(), text); &#125; &#125;&#125; 在pom.xml中修改主类名称为：com.learn.eshdoop.EsQueryToHdfs； mvn clean package 重新打包，然后上传； [root@localhost hadoop]# mv es-hadoop-learn-1.0-SNAPSHOT-jar-with-dependencies.jar EsQueryToHdfs.jar [root@localhost hadoop]# hadoop jar EsQueryToHdfs.jar /work/EsQueryToHdfs [root@localhost hadoop]# hadoop fs -cat /work/EsQueryToHdfs/part-r-00000]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK单节点系统构建]]></title>
    <url>%2Felasticsearch%2FELK%E5%8D%95%E8%8A%82%E7%82%B9%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[安装 jdk（jdk7+）tar -zxf jdk-8u65-linux-x64.tar.gz -C /data/carloz/tools/设置java环境变量vim /etc/profile 1234export JAVA_HOME=/carloz/tools/jdk1.8.0_65export JRE_HOME=$JAVA_HOME/jreexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=$CLASSPATH:.:/JAVA_HOME/lib:$JAVA_HOME/jre/lib source /etc/profilejava -version 安装 Elasticsearchtar -zxf elasticsearch-7.0.0-linux-x86_64.tar.gz -C /data/carloz/tools/cd elasticsearch-7.0.0/ ./bin/elasticsearch 这是出于系统安全考虑设置的条件。由于ElasticSearch可以接收用户输入的脚本并且执行，为了系统安全考虑。创建elsearch用户组及elsearch用户groupadd elsearchuseradd elsearch -g elsearch -p elasticsearchcd /data/carloz/tools/chown -R elsearch:elsearch elasticsearch-7.0.0/su elsearchcd elasticsearch-7.0.0/./bin/elasticsearch -d 一个警告，操作系统内核版本太低，忽略即可 netstat -nltp | grep 9200 ps aux | grep elastic* curl ‘http://127.0.0.1:9200&#39;如下代表启动成功： 配置Elasticsearchsu elsearchcd /data/carloz/tools/elasticsearch-7.0.0vi config/elasticsearch.ymlgrep -v ^# config/elasticsearch.yml 集群监控curl -XGET ‘http://localhost:9200/_cluster/health?pretty=true&#39; 安装 Logstashsu rootcd /carloz/downloadtar -zxf logstash-7.0.0.tar.gz -C /data/carloz/tools/cd /data/carloz/tools/logstash-7.0.0/ 定义 一个叫 stdin 的数据源，和一个叫stdout的输出目标，无论我们输入什么，都会输出到标准命令行：./bin/logstash -e ‘input { stdin { } } output { stdout {} }’等待启动成功之后，输入 hello logastsh 1、Logstash 输入源2、Logstash 输出目标3、Logstash 过滤器 定义输入源，输出目标： mkdir -p myconfvi myconf/jyweb-nginx.conf 1234567891011input&#123; file&#123; path =&gt; ["/opt/software/nginx/logs/*.log"] &#125;&#125;output&#123; elasticsearch&#123; hosts =&gt; ["localhost:9200"] index =&gt; "jyweb_nginx_log" &#125;&#125; 启动logstash./bin/logstash -f myconf/jyweb-nginx.conf &amp; 安装 Kibanatar -zxf /carloz/download/kibana-7.0.0-linux-x86_64.tar.gz -C /data/carloz/tools/cd /data/carloz/tools/kibana-7.0.0-linux-x86_64 修改配置文件，指向Elasticsearch服务器vi config/kibana.yml 1234server.port: 5601server.host: "0.0.0.0"server.name: "logs-server"elasticsearch.hosts: ["http://localhost:9200"] 启动./bin/kibana 访问：http://10.10.139.42:5601]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10、ES新闻搜索实战]]></title>
    <url>%2Felasticsearch%2F10-Elasticsearch%E6%96%B0%E9%97%BB%E6%90%9C%E7%B4%A2%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[10.0、本文前提 10.1、从新浪采集新闻存储到mysql 10.2、将mysql数据 同步到 Elasticsearch 10.3、从Elasticsearch中搜索数据，返回给前端 10.4、ES新闻搜索实战 —— 查询界面 10.5、ES新闻搜索实战 —— 结果展示 -————————————————– 项目仓库：https://gitee.com/carloz/elastic-learn.git 具体地址：https://gitee.com/carloz/elastic-learn/tree/master/elasticsearch-news -————————————————– 本文前提已经部署好了ELK系统 10.1、从新浪采集新闻存储到mysqlhttp://finance.sina.com.cn/7x24/ 12345678910111213141516171819202122232425262728# 新浪新闻采集脚本### 1、使用python 2.7 解析器配置环境变量，path下添加：D:\python\tools\Python27;D:\python\tools\Python27\Scripts;安装依赖模块：pip install requestshttps://sourceforge.net/projects/mysql-python/下载64位版本的mysql: http://www.codegood.com/download/11/### 2、新建数据库CREATE DATABASE `sina_news`;CREATE TABLE `news` ( `id` int(11) NOT NULL, `news_type` enum('其他','央行','观点','市场','数据','公司','行业','宏观','A股') NOT NULL, `create_time` datetime DEFAULT NULL, `rich_text` text, PRIMARY KEY (`id`));### 3、运行，即可在数据库里看到数据 python数据采集脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding: UTF-8 -*-import stringimport jsonimport timeimport requestsimport MySQLdbimport randomimport reconn = MySQLdb.connect("10.10.87.38", "crm", "crm@2015", "sina_news", charset='utf8')cursor = conn.cursor()template_url = string.Template( 'http://zhibo.sina.com.cn/api/zhibo/feed?callback=jQuery$jQueryId&amp;page=1&amp;page_size=$page_size&amp;zhibo_id=152&amp;tag_id=$tag_id&amp;dire=f&amp;dpc=1&amp;pagesize=$page_size&amp;_=$datetime')tag_ids = &#123;u'A股': 10, u'宏观': 1, u'行业': 2, u'公司': 3, u'数据': 4, u'市场': 5, u'观点': 6, u'央行': 7, u'其他': 8,&#125;headers = &#123; 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'zh-CN,zh;q=0.9', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36', 'Accept': '*/*', # 'Referer' : 'http://finance.sina.com.cn/7x24/?tag=10', 'Connection': 'keep-alive', 'Cookie': 'U_TRS1=000000d5.5a546170.5cb7de02.83b7c4e0; U_TRS2=000000d5.5a616170.5cb7de02.256bb0da; UOR=www.baidu.com,blog.sina.com.cn,; SINAGLOBAL=114.114.114.114_1555553794.454804; Apache=114.114.114.213_1555553794.454805; ULV=1555553794485:1:1:1:114.114.114.114_1555553794.454805:; SCF=AhOLahPmRlTviyZ4YQHaxRNdunCqZL3kO2SBnELkwjeVg8ZMdSXgud0IsBd4CaJIt5s-9YmaaRxgNVK4w6koPXE.; ULOGIN_IMG=gz-d89f6db983d2c25da42c59504991a4867f53; sso_info=v02m6alo5qztLSNk4S5jJOQs46TnKadlqWkj5OEuI6DnLCOg4y1jbOMwA==; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhM3uQ2UWBVDQgNwIoS4aG35NHD95Qp1hnNehn0SKM0Ws4Dqcjci--Xi-zRiKn7i--fiKysi-8Wi--fi-2Xi-2Ni--RiK.7iKyhi--fiKnfiK.Xi--fi-82iK.7; _s_upa=5; lxlrttp=1556243090; NEWSCENTER=522b9f1b6a2f61766931ac50242bed94; SUB=_2A25x5Y0UDeRhGedG41UR-C3JzD-IHXVSkvncrDV_PUNbm9BeLWitkW9NUOtWwD6pEwnHVFGGf0Y42aAcKr49dHwM; ALF=1589850308',&#125;for news_type, tag_id in tag_ids.items(): headers['Referer'] = 'http://finance.sina.com.cn/7x24/?tag=%s' % random.choice(tag_ids.values()) datetime = int(1000 * time.time()) crawlurl = template_url.substitute(datetime=datetime, jQueryId="111207214587420816325_%s" % datetime, tag_id=tag_id, page_size=20) try: text = requests.get(crawlurl, timeout=2, headers=headers).text news = json.loads(re.sub('^try[^\(]*\(|\);&#125;catch\(e\)&#123;&#125;;$', '', text))['result']['data']['feed']['list'] except Exception, e: print str(e) continue for data in news: unique_id = data['id'] rich_text = data['rich_text'] create_time = data['create_time'] try: mysql_command = u"insert into sina_news.news (id,news_type,create_time,rich_text) values ('%s','%s','%s','%s')" % (unique_id, news_type, create_time, rich_text) mysql_command += u" on duplicate key update news_type='%s', create_time='%s',rich_text='%s';" % (news_type, create_time, rich_text) cursor.execute(mysql_command) conn.commit() except Exception, e: print mysql_command print str(e) pass 采集到的数据： ​ 10.2、将mysql数据 同步到 Elasticsearch在Elasticsearch中新建索引： 1234567891011121314151617181920PUT sina_news&#123; "mappings": &#123; "properties": &#123; "id": &#123; "type": "integer" &#125;, "news_type": &#123; "type": "keyword" &#125;, "create_time": &#123; "type": "date" &#125;, "rich_text": &#123; "type": "text", "analyzer": "ik_smart" &#125; &#125; &#125;&#125; 同步可选技术： https://github.com/siddontang/go-mysql-elasticsearch —— 没有实际用于生产的例子，不太懂go； https://www.elastic.co/blog/logstash-jdbc-input-plugin —— 全量同步、增量同步，不支持删除；定时任务执行；文档《https://www.cnblogs.com/mignet/p/MySQL_ElasticSearch_sync_By_logstash_jdbc.html》； https://github.com/jprante/elasticsearch-jdbc/tree/master https://github.com/m358807551/mysqlsmom —— 从binlog同步，支持全量、增量同步，支持删除操作；文档《https://elasticsearch.cn/article/756》； https://github.com/mardambey/mypipe —— 写到kafka，需要自己从kafka消费，再处理成json以后写入Elasticsearch； 本次只是一个示例，而且需求里也没有实时同步删除数据，采用官方软件：logstash-input-jdbc； https://github.com/logstash-plugins/logstash-input-jdbc https://github.com/logstash-plugins/logstash-input-jdbc/releases 给Logstash安装jdbc插件： cd /data/carloz/tools/logstash-7.0.0/ ./bin/logstash-plugin install logstash-input-jdbc 上传jdbc包： [root@10-10-139-42 logstash-7.0.0]# mkdir -p mylib [root@10-10-139-42 mylib]# cd /data/carloz/tools/logstash-7.0.0/mylib 上传mysql-connector-java-8.0.15.jar到该目录： 设置配置文件： [root@10-10-139-42 myconf]# vi sina_news.conf 12345678910111213141516171819input &#123; jdbc &#123; jdbc_driver_library =&gt; "/data/carloz/tools/logstash-7.0.0/mylib/mysql-connector-java-8.0.15.jar" jdbc_driver_class =&gt; "com.mysql.jdbc.Driver" jdbc_connection_string =&gt; "jdbc:mysql://10.10.87.38:3306/sina_news" jdbc_user =&gt; "crm" jdbc_password =&gt; "crm@2015" schedule =&gt; "* * * * *" statement =&gt; "select * from news where id &gt; :sql_last_value" use_column_value =&gt; true tracking_column =&gt; "id" &#125;&#125;output &#123; elasticsearch &#123; index =&gt; "sina_news" hosts =&gt; "localhost:9200" &#125;&#125; 开始同步： [root@10-10-139-42 logstash-7.0.0]# ./bin/logstash -f myconf/sina_news.conf &amp; GET sina_news/_search 至此，数据上传成功，并且程序持续监听中。 查看数据： 123456789101112131415161718192021222324GET sina_news/_search&#123; "size": 0, "aggs": &#123; "news_stats": &#123; "stats": &#123; "field": "id" &#125; &#125; &#125;&#125;GET sina_news/_search&#123; "sort": [ &#123; "id": &#123; "order": "desc" &#125; &#125; ], "from": 0, "size": 20&#125; ​ 10.3、从Elasticsearch中搜索数据，返回给前端工程结构如图： 使用springboot开发，在pom.xml中键入依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.learn&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-news&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;elasticsearch-news&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 搜索入口 12345678910111213141516171819/** * @param keywords * @return 搜索结果页面 */@RequestMapping("/search")public ModelAndView searchFile(String keywords, @Nullable Integer from, @Nullable Integer size) &#123; ArrayList&lt;NewsModel&gt; hitsList = null; try &#123; hitsList = mySearchService.searchSinaNews(keywords, from, size); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; log.info(keywords + "：共搜到：" + hitsList.size() + " 条数据！"); ModelAndView mv = new ModelAndView("result.html"); mv.addObject("keywords", keywords); mv.addObject("resultList", hitsList); return mv;&#125; 搜索核心代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.learn.elasticsearchnews.service;import com.learn.elasticsearchnews.model.NewsModel;import com.learn.elasticsearchnews.utils.EsUtils;import org.elasticsearch.action.search.SearchRequest;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.common.unit.Fuzziness;import org.elasticsearch.common.unit.TimeValue;import org.elasticsearch.index.query.MultiMatchQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.rest.RestStatus;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHits;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;import org.elasticsearch.search.sort.FieldSortBuilder;import org.elasticsearch.search.sort.SortOrder;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;@Servicepublic class MySearchService &#123; public ArrayList&lt;NewsModel&gt; searchSinaNews(String keywords, Integer from, Integer size) throws Exception &#123; if (null == from) from = 0; if (null == size) size = 10; RestHighLevelClient client = EsUtils.getClient(); String index = "sina_news"; String field1 = "news_type"; String field2 = "rich_text"; MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(keywords, field1, field2) .fuzziness(Fuzziness.AUTO); HighlightBuilder highlightBuilder = new HighlightBuilder() .preTags("&lt;span style=\"color:red\"&gt;") .postTags("&lt;/span&gt;") .field(field1) .field(field2); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(multiMatchQueryBuilder); searchSourceBuilder.sort(new FieldSortBuilder("id").order(SortOrder.DESC)); searchSourceBuilder.from(from); searchSourceBuilder.size(size); searchSourceBuilder.timeout(new TimeValue(5, TimeUnit.SECONDS)); searchSourceBuilder.highlighter(highlightBuilder); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(index); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); ArrayList&lt;NewsModel&gt; resultList = new ArrayList&lt;&gt;(); if(searchResponse.status() == RestStatus.OK) &#123; SearchHits searchHits = searchResponse.getHits(); for (SearchHit hit : searchHits) &#123; Map&lt;String, Object&gt; resMap =hit.getSourceAsMap(); Map&lt;String, NewsModel&gt; newsMap = new HashMap&lt;&gt;(); NewsModel news = new NewsModel( Integer.valueOf(resMap.get("id").toString()), resMap.get("news_type").toString(), resMap.get("rich_text").toString(), resMap.get("create_time").toString()); resultList.add(news); &#125; &#125; return resultList; &#125;&#125; ​ 10.4、ES新闻搜索实战 —— 查询界面在浏览器中访问：http://localhost:18080/ ​ 10.5、ES新闻搜索实战 —— 结果展示在搜索框中搜索“美国”， 因为只是demo，就不做分页了，大家需要自己做即可]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.1、ES搜索详解 —— 搜索机制]]></title>
    <url>%2Felasticsearch%2F6-1-Elasticsearch%E6%90%9C%E7%B4%A2%E8%AF%A6%E8%A7%A3-%E7%B4%A2%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本章参考文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html注意文档对应的Elasticsearch版本 索引过程： 第2象限 有 原始文档； Elasticsearch 保存 文档的原始内容 和 对应的倒排序索引文件； 搜索过程： 在 倒排索引文件 维护的 倒排记录表 找 关键词 对应的 文档集合，然后做评分、排序、高亮，将结果返回给用户； 过滤机制： 根据条件对文档进行过滤，不计算评分； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899DELETE booksPUT books&#123; "settings": &#123; "number_of_replicas": 1, "number_of_shards": 3 &#125;, "mappings": &#123; "properties": &#123; "id": &#123; "type": "long" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125;, "language": &#123; "type": "keyword" &#125;, "author": &#123; "type": "keyword" &#125;, "price": &#123; "type": "double" &#125;, "publish_time": &#123; "type": "date", "format": "yyyy-MM-dd" &#125;, "desc": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125;&#125;PUT books/_doc/1&#123; "id":1, "title":"Java编程思想", "language":"Java", "author": "Bruce Eckel", "price":70.20, "publish_time":"2017-01-02", "desc":"Java学习必读经典，殿堂级著作！赢得全球程序员的广泛赞誉。"&#125;PUT books/_doc/2&#123; "id":2, "title":"Java程序性能优化", "language":"Java", "author": "葛一鸣", "price":46.50, "publish_time":"2012-08-02", "desc":"Java程序更快，更稳定。深入剖析软件设计层面、代码层面、JVM层面的优化方法。"&#125;PUT books/_doc/3&#123; "id":3, "title":"Python科学计算", "language":"python", "author": "张若愚", "price":81.40, "publish_time":"2014-01-02", "desc":"零基础学python， 光盘中坐着度假开发winPython运行环境，涵盖了Python各个扩展库。"&#125;PUT books/_doc/4&#123; "id":4, "title":"Python基础教程", "language":"python", "author": "Helant", "price":54.50, "publish_time":"2014-03-02", "desc":"经典的python入门教程，层次鲜明，结构严谨，内容详实。"&#125;PUT books/_doc/5&#123; "id":5, "title":"JavaScript高级程序设计", "language":"javascript", "author": "Nicholas C. Zakas", "price":66.40, "publish_time":"2012-03-02", "desc":"JavaScript 技术经典名著。"&#125;GET books/_mappingmatch_all搜索,简化语法：GET books/_searchterm query 搜索：GET books/_search&#123; "query": &#123; "term": &#123; "title": "思想" &#125; &#125;&#125; 数据量很大的情况下，需要分页： from：开始位置 size：返回文档最大数量 12345678910GET books/_search&#123; "from": 1, "size": 2, "query": &#123; "term": &#123; "title": "java" &#125; &#125;&#125; 最小评分过滤机制： 相关文档很多的情况下，相关性比较低的文档可以过滤掉 123456789GET books/_search&#123; "min_score": 0.6, "query": &#123; "term": &#123; "title": "java" &#125; &#125;&#125; 高亮查询关键字： 12345678910111213GET books/_search&#123; "query": &#123; "term": &#123; "title": "java" &#125; &#125;, "highlight": &#123; "fields": &#123; "title": &#123;&#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.3、Elasticsearch集群入门 —— 映射详解]]></title>
    <url>%2Felasticsearch%2F5-3-Elasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8-%E6%98%A0%E5%B0%84%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[5.3.1、映射分类 5.3.2、动态映射 5.3.3、日期检测 5.3.4、静态映射 5.3.5、字段类型 5.3.6、元字段 5.3.7、映射参数 5.3.8、映射模板 -————————————— ​ 映射 即 Mapping， 定义 一个文档及其所包含的字段 如何被存储和索引。 ​ 映射中 事先定义 数据类型、分词器 5.3.1、映射分类动态映射：ES 根据 字段类型 自动识别； —— 偷懒方式 静态映射：写入数据之前 对字段的属性 手工设置； 5.3.2、动态映射1234567PUT sinablog/user/1?routing=user123&#123; "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125;GET sinablog/_mappingGET blog/_mapping id 被推测为 long 类型 posttime被推测为 date 类型 title 和 content 被推测为 text 类型 如果 要把Elasticsearch当做 主要的数据存储 使用，动态Mapping 并不适用； 在 Mapping 中通过 dynamic 设置 是否自动新增字段，接收以下参数： true —— 自动新增； false —— 忽略新字段； strict —— 严格模式，发现新字段 抛出异常； 123456789101112PUT sinablog/user/1?routing=user123&#123; "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125;PUT sinablog/user/2?routing=user123&#123; "create_time": "2019-01-09", "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125;GET sinablog/_search?routing=user123 自定义Mapping： 1234567891011121314151617PUT tblog&#123; "mappings": &#123; "dynamic": "strict", "properties": &#123; "posttime": &#123; "type": "date" &#125;, "id": &#123; "type": "long" &#125;, "title": &#123; "type": "text" &#125; &#125; &#125;&#125; 写数据： Elasticsearch 6.0的时候默认一个 index 只能有一个type，7.0的时候已经有移除type的趋势，默认用_doc代替表示默认type； 参考文档：《Elasticsearch 移除 type 之后的新姿势》 123456POST tblog/_doc/1&#123; "id": 2, "title": "Git是一款免费、开源的分布式版本控制系统", "posttime": "2018-01-09"&#125; 查询 Mapping：GET tblog/_mapping 查询：GET tblog/_search 删除：DELETE tblog ​ 5.3.3、日期检测Elasticsearch碰到一个新的字符串类型的字段时，会检查是否是一个日期； 一旦检测为是，以后，如果写入的字段不是type，就会报错； 预先设置不自动检测日期，可避免： 12345678910111213PUT blog&#123; "mappings": &#123; "date_detection": false &#125;&#125;POST blog/_doc/2&#123; "id": 2, "title": "Git简介", "posttime": "2018-01-09", "content": "Git是一款免费、开源的分布式版本控制系统"&#125; GET blog/article/2 GET blog/_mapping GET blog/_search DELETE blog ​ 5.3.4、静态映射定义：创建索引时，手动指定索引映射； ​ 5.3.5、字段类型 文本： string：在 5.X 之后不再支持； text: 可以被全文检索的的，字段会被分拆，分词、生成倒排索引，不会用于 排序，基本不用于聚合； keyword: 只能用于精确搜索，用于排序、聚合 数字： 对于 float、half_float、scaled_float，-0.0 和 +0.0 是不同的值； es 底层会把 scaled_float 作为整形存储，节省空间，可以优先使用； 日期 date： 格式化的字符串，如 ‘2015-01-02’， ‘2015/01/02 12:10:31’； 代表epoch到今天的秒数的一个长整数，epoch 指 1970-01-01 00:00:00 UTC 二进制binary：接收base64编码的字符串，默认不存储（store=false），也不可搜索 数组**array**：es默认任何字段都可以包含一个或多个值，array的值 默认一个数组的值必须是统一类型； object： nested：object的一个特例，可以让数组独立索引和哈希 地理坐标 geo_point：存储 经纬度坐标点， 或 地理坐标的hash值； 可用来查找点周围的区域； 根据到点的距离排序； 地理图形 geo_shape：存储一块区域，矩形，三角形，或者 其他多边形，GeoJson； ip：存储 ipv4 或者 ipv6; range：常用于 时间选择表单 和 年龄范围选择表单： integer_range float_range long_range double_range： date_range：64bit整数，毫秒计时； 123456789101112131415161718192021222324252627PUT range_inde&#123; "mappings": &#123; "properties": &#123; "age_range": &#123; "type": "integer_range" &#125;, "time_range": &#123; "type": "date_range", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis" &#125; &#125; &#125;&#125;PUT range_inde/_doc/1&#123; "age_range": &#123; "gte": 18, "lte": 70 &#125;, "time_range": &#123; "gte": "2015-10-31 01:00:00", "lte": "2019-05-21" &#125;&#125;GET range_inde/_mappingGET range_inde/_search 令牌计数类型 token_count：用于统计字符串分词后的 词项个数。 12345678910111213141516171819202122232425262728PUT my_index&#123; "mappings": &#123; "properties": &#123; "name": &#123; "type": "text", "fields": &#123; "length": &#123; "type": "token_count", "analyzer": "standard" &#125; &#125; &#125; &#125; &#125;&#125;PUT my_index/_doc/1&#123; "name": "My name is xiaozhang"&#125;GET my_index/_search&#123; "query": &#123; "term": &#123; "name.length": 4 &#125; &#125;&#125; ​ 5.3.6、元字段定义：映射中，描述文档本身的字段 分类： 描述文档属性； 源文档的元字段； 索引的元字段； 路由的元字段； ​ 5.3.7、映射参数analyzer：指定文本字段的分词器，对索引和查询都有效； search_analyzer：索引 和 搜索 都应该指定相同的分词器；但有时候就 需要指定不一样的， ​ 默认 查询 会使用 analyzer指定的分词器，但也可以被 search_analyzer 指定 搜索分词器； 1234567891011121314151617181920212223242526272829303132333435PUT analyzer_index&#123; "settings": &#123; "analysis": &#123; "filter": &#123; "autocomplete_filter": &#123; "type": "edge_ngram", "min_gram": 1, "max_gram": 20 &#125; &#125;, "analyzer": &#123; "autocomplete": &#123; "type": "custom", "tokenizer": "standard", "filter": ["lowercase","autocomplete_filter"] &#125; &#125; &#125; &#125;, "mappings": &#123; "properties": &#123; "text": &#123; "type": "text", "analyzer": "autocomplete", "search_analyzer": "standard" &#125; &#125; &#125;&#125;PUT analyzer_index/_doc/1&#123; "text": "Quick Brown Fox"&#125;GET analyzer_index/_search normalizer：用于解析前的标准配置， 比如把所有字符串转化为小写 123456789101112131415161718192021222324252627282930313233343536373839404142434445DELETE my_indexPUT my_index&#123; "settings": &#123; "analysis": &#123; "normalizer": &#123; "my_normalizer": &#123; "type": "custom", "char_filter": [], "filter": ["lowercase", "asciifolding"] &#125; &#125; &#125; &#125;, "mappings": &#123; "properties": &#123; "foo": &#123; "type": "keyword", "normalizer": "my_normalizer" &#125; &#125; &#125;&#125;PUT my_index/_doc/1&#123; "foo": "BAR"&#125;PUT my_index/_doc/2&#123; "foo": "bar"&#125;PUT my_index/_doc/3&#123; "foo": "bazz"&#125;POST my_index/_refreshGET my_index/_search&#123; "query": &#123; "match": &#123; "foo": "BAR" &#125; &#125;&#125; boost：设置字段的权重，比如 设置关键字 在 title的权重是 content的2倍； DELETE my_index mapping指定(不重新索引文档，权重无法更改)： 1234567891011121314PUT my_index&#123; "mappings": &#123; "properties": &#123; "title": &#123; "type": "text", "boost": 2 &#125;, "content": &#123; "type": "text" &#125; &#125; &#125;&#125; 查询时指定（推荐）： 123456789101112131415PUT my_index/_doc/1&#123; "title": "Quick Brown Fox"&#125;POST my_index/_search&#123; "query": &#123; "match": &#123; "title": &#123; "query": "Quick", "boost": 2 &#125; &#125; &#125;&#125; corece：用于清除脏数据，默认值 true； copy_to：用于自定义 _all 字段，可以把 多个字段 复制合并成一个超级字段； doc_values：为了加快排序、聚合操作，建立 倒排索引的时候，额外增加一个列式存储映射，是一种空间换时间的做法 默认是开启的，如果确定不需要聚合或排序，可以关闭 以节省空间； 注：text 类型 不支持 doc_values; 123456789101112131415DELETE my_indexPUT my_index&#123; "mappings": &#123; "properties": &#123; "status_code": &#123; "type": "keyword" &#125;, "session_id": &#123; "type": "keyword", "doc_values": false &#125; &#125; &#125;&#125; enabled：es会默认索引搜有字段，而有些字段只需存储，不需要查询、聚合，设置mapping时，enabled设置为false的字段，es就会跳过字段内容，只能从_source中获取，不能被搜索；也能禁用映射； fielddata： 搜索解决的问题：包含查询关键词的文档哪些； 聚合要解决的：文档包含哪些词项； fielddata 默认是关闭的，开启非常耗内存； format：用于指定 date的格式； ignore_above：用于指定字段分词和索引的字符串最大长度；超多最大值会被忽略，只能用于keyword 类型； ignore_malformed：忽略不规则数据，设为true，异常会被忽略，出异常的字段不会被索引，其他字段正常索引； index_options：控制哪些信息存储到 倒排索引中； fields：让同一字段有多种不同的索引方式； position_increment_gap：支持近似 或者 短语查询； properties： similarty：用于指定 文档评分模型；参数：BM25，classic（TF/IDF评分），boolean 布尔评分模型； store：字段默认是被索引的，也可以搜索，但不存储。store可以指定 存储某个字段； term_vector：词向量，包含文本别解析以后的信息：词项集合、词项位置、词项起始字符映射到原始文档的位置； ​ 5.3.8、映射模板举例：如果字段已 long_ 开头，则将字符串转化为 long 类型； 1234567891011121314151617181920212223242526DELETE my_indexPUT my_index&#123; "mappings": &#123; "dynamic_templates": [ &#123; "longs_as_strings": &#123; "match_mapping_type": "string", "match": "long_*", "unmatch": "*_text", "mapping": &#123; "type": "long" &#125; &#125; &#125; ] &#125;&#125;PUT my_index/_doc/1&#123; "long_num": "5", "long_text": "foo"&#125;GET my_index/_mappingGET my_index/_search]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.2、Elasticsearch集群入门 —— 文档管理]]></title>
    <url>%2Felasticsearch%2F5-2-Elasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8-%E6%96%87%E6%A1%A3%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[5.2.1、新建文档 5.2.2、获取文档 5.2.3、更新文档 5.2.4、查询更新 5.2.5、删除文档 5.2.6、查询删除 5.2.7、批量操作 5.2.8、版本控制 5.2.9、路由机制 -————————————— 5.2.1、新建文档格式： index/type/id，如果不设置id，es会自动生成它 1234567PUT blog/article/1&#123; "id": 1, "title": "Git简介", "posttime": "2018-01-09", "content": "Git是一款免费、开源的分布式版本控制系统"&#125; 1234567POST blog/article&#123; "id": 2, "title": "Git简介", "posttime": "2018-01-09", "content": "Git是一款免费、开源的分布式版本控制系统"&#125; ​ 5.2.2、获取文档GET blog/article/1 如果找不到 可以通过HEAD命令查看一个文档是否存在 HEAD blog/article/2 获取多个，根据index和 type： 12345678910111213141516171819202122232425262728GET blog/_mget&#123; "docs" : [ &#123; "_type": "article", "_id": "1" &#125;, &#123; "_type": "article", "_id": "2" &#125; ]&#125;GET _mget&#123; "docs" : [ &#123; "_index": "blog", "_type": "article", "_id": "1" &#125;, &#123; "_index": "blog", "_type": "article", "_id": "2" &#125; ]&#125; ​ 5.2.3、更新文档文档被索引以后要修改， 12345PUT index1/type1/1&#123; "counter": 1, "tags": ["red"]&#125; 使用脚本更新： 12345678910POST index1/type1/1/_update&#123; "script": &#123; "inline": "ctx._source.counter += params.count", "lang": "painless", "params": &#123; "count" : 4 &#125; &#125;&#125; ​ 5.2.4、条件查询更新如果满足条件则更新 123456789101112131415POST index1/_update_by_query&#123; "script": &#123; "inline": "ctx._source.category = params.category", "lang": "painless", "params": &#123; "category": "git" &#125; &#125;, "query": &#123; "term": &#123; "title": "git" &#125; &#125;&#125; 如果title中包含git关键字，则增加一个 category ​ 5.2.5、删除文档基于指定id从 索引库 中删除一个文档 DELETE blog/article/osGC1WoB4sRau0FmmUYE 再去查询： GET blog/article/osGC1WoB4sRau0FmmUYE HEAD blog/article/osGC1WoB4sRau0FmmUYE 如果 索引文档时指定了路由，那么也可以根据路由参数删除 DELETE blog/article/2?routing=user123 如果参数不正确，会删除失败 ​ 5.2.6、根据条件查询删除删除 title中包含关键字 hibernate的文档 123456POST blog/_delete_by_query&#123; "query": &#123; "title": "hibernate" &#125;&#125; 删除一个type（csdn）下所有的文档 123456POST blog/csdn/_delete_by_query&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; ​ 5.2.7、批量操作执行 Bulk API 可以 执行 批量索引、批量删除、批量更新等操作 一个 Bulk API 允许 单一请求 实现多个文档的 create、index、update、delete ​ 5.2.8、版本控制Elasticsearch API进行文档更新 过程： 读取源文档 -&gt; 对原文档更新 -&gt; 重新索引整个文档 使用同个线程同时修改一个文档， 会发生冲突； 1、 悲观锁控制同一时刻只有一个线程访问数据 2、 乐观锁控制Elasticsearch是分布式系统， 需要确保旧版本不会覆盖新版本 Elasticsearch 使用 _version 自增， 确保所有更新有序进行 内部版本控制： 每次版本号，相等 才能操作成功； 外部版本控制：外部文档 比 内部文档 版本高 时才能更新成功； 5.2.9、路由机制分片的路由机制： shard = hash（routing）% number_of_primary_shards routing 是一个任意字符串，取它的hash值，取模后 放在对应的 分片上； hash值相同的 文档 放在 同一个主分片中； 默认路由模式可以保障 文档id数据平均分布，ES无法根据 id 确定 文档的位置，需要广播到所有分片上查找； 自定义路由模式，可以使查询具有目的性，不用盲目广播 12345PUT sinablog/user/1?routing=user123&#123; "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125; GET sinablog/_search?routing=user123]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.1、Elasticsearch集群入门 —— 索引管理]]></title>
    <url>%2Felasticsearch%2F5-1-Elasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8-%E7%B4%A2%E5%BC%95%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[5.1.1、新建索引 5.1.2、更新副本数 5.1.3、设置索引的读写权限 5.1.4、查看索引 5.1.5、删除索引 5.1.6、索引的打开和关闭 5.1.7、复制索引 5.1.8、收缩索引 5.1.9、索引别名 -—————————————- 5.1.1、新建索引 索引名称 不能有大写字母 使用Kibana的DevTools工具进行 创建索引： PUT test 响应： 12345&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "test"&#125; 并且不能重复创建 -———————————— 创建 5个分片，3个副本的索引： 1234567PUT test&#123; "settings": &#123;​ "number_of_shards": 5,​ "number_of_replicas": 2 &#125;&#125; ​ 5.1.2、更新副本数ES 支持修改已存在索引的副本数，如下： 1234PUT test/_settings&#123; "number_of_replicas": 3&#125; 响应： 123&#123; "acknowledged" : true&#125; ​ 5.1.3、设置索引的读写权限“blocks.read_only”: false ———— 当前索引只读，不允许写和更新 “blocks.read”: true ———— 禁止读 “blocks.write”: true ———— 禁止写 示例 设置禁止写 1234PUT test/_settings&#123; "blocks.write": true&#125; 尝试写入 1234PUT test/article/1&#123; "tile": "Java 虚拟机"&#125; 返回错误 重新设置允许写 1234PUT test/_settings&#123; "blocks.write": false&#125; 再试尝试写入， 写入成功： ​ 5.1.4、查看索引GET test/_settings 查看多个索引：GET test,girl/_settings 查看所有索引：GET _all/_settings ​ 5.1.5、删除索引DELETE test2 响应表示成功： 123&#123; "acknowledged" : true&#125; 删除不存在的索引会报错：404 ​ 5.1.6、索引的打开和关闭一个关闭了的索引，基本不占用系统资源； POST test/close ———— 关闭 test 索引 _POST test,girl/_close ———— 关闭 多个索引 POST _all/_close ———— 关闭所有索引 POST test*/_close ———— 关闭test开头的索引 ​ 5.1.7、复制索引_redinx操作， 把文档 从 源索引 复制到 目标索引，但目标索引的分片数、副本数需要单独设置。 全量复制： 12345POST _reindex&#123; "source": &#123;"index": "test"&#125;, "dest": &#123;"index": "girl"&#125;&#125; 把 test 索引下，type为article下，title中含有关键字 java的文档，复制到 girl 索引中： 123456789101112131415POST _reindex&#123; "source": &#123; "index": "test", "type": "article", "query": &#123; "term": &#123; "FIELD": &#123; "title": "java" &#125; &#125; &#125; &#125;, "dest": &#123;"index": "girl"&#125;&#125; ​ 5.1.8、收缩索引一个索引分片初始化以后无法再做修改，但可以使用shrink index AP提供的分片数机制，把一个索引变成更少的索引。 收缩以后的分片数 是 原始分片数的因子；比如 8个 可以收缩成 4,2,1；7,11只能收缩成1个； ​ 5.1.9、索引别名给一个索引 起另一个名字 1234567891011POST /_aliases&#123; "actions": [ &#123; "add": &#123; "index": "test", "alias": "alias_test" &#125; &#125; ]&#125; 移除别名 1234567891011POST /_aliases&#123; "actions": [ &#123; "remove": &#123; "index": "test", "alias": "alias_test" &#125; &#125; ]&#125; 查看索引的别名 GET /test/_alias 12345GET /test/_analyze&#123; "analyzer":"ik_smart", "text":"洪荒之力"&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.5、Elasticsearch-Head集群操作管理工具]]></title>
    <url>%2Felasticsearch%2F4-5-Elasticsearch-Head%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[4.5.1、Head插件的安装 1、配置Node环境 2、下载Head插件源码 3、修改 Elasticsearch 配置文件 4、修改 Head 插件配置文件 5、启动 Head 插件 4.5.2、Head插件的使用 1、概览界面 2、索引查看界面 3、数据浏览界面 4、基本查询选项卡 5、复合查询 -——————————————– 4.5.1、Head插件的安装1、配置Node环境官网：https://nodejs.org/en/download/ wget https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz xz -d node-v10.15.3-linux-x64.tar.xz tar -xf node-v10.15.3-linux-x64.tar 设置环境变量 vi /etc/profile 添加： 12export NODE_HOME=/data/carloz/tools/node-v10.15.3-linux-x64/export PATH=$NODE_HOME/bin:$PATH source /etc/profile 使用npm安装 Grunt： npm install -g grunt-cli ​ 2、下载Head插件源码https://github.com/mobz/elasticsearch-head/releases wget https://github.com/mobz/elasticsearch-head/archive/v5.0.0.tar.gz mv v5.0.0.tar.gz elasticsearch-head-v5.0.0.tar.gz tar -zxf elasticsearch-head-v5.0.0.tar.gz cd elasticsearch-head-5.0.0/ npm install -g cnpm –registry=https://registry.npm.taobao.org ​ 3、修改 Elasticsearch 配置文件su elsearch vi elasticsearch-7.0.0/config/elasticsearch.yml 添加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 重启 elasticsearch exit —— 切换到 root ​ 4、修改 Head 插件配置文件vi elasticsearch-head-5.0.0/Gruntfile.js 修改connect， 指定 Elasticsearch-head 的访问ip： 不指定的话，所有ip都能访问他 ​ 5、启动 Head 插件cd elasticsearch-head-5.0.0/ grunt server ​ 4.5.2、Head插件的使用1、概览界面 试着去连接 Elasticsearch服务器，但是，我们的 Elasticsearch 服务器只允许本机连，要修改配置 su elsearch vi /data/carloz/tools/elasticsearch-7.0.0/config/elasticsearch.yml 错误参考：https://blog.csdn.net/zhou_p/article/details/80311972 “the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured” -———————————————- vi /data/carloz/tools/elasticsearch-7.0.0/config/elasticsearch.yml 修改： bootstrap.memory_lock: false 添加： bootstrap.system_call_filter: false cluster.initial_master_nodes: [“node-1”] -——————————————— vi /etc/sysctl.conf vm.max_map_count=655360 sysctl -p -——————————————— vi /etc/security/limits.d/90-nproc.conf -———————————————– http://10.10.139.42:9200/ 点击连接 2、索引查看界面 3、数据浏览界面 4、基本查询选项卡 5、复合查询]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.4、Elasticsearch中文分词器配置]]></title>
    <url>%2Felasticsearch%2F4-4-Elasticsearch%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[4.4.1、IK分词器安装 4.4.2、扩展本地词库 4.4.3、配置远程词库 -——————————————– 4.4.1、IK分词器安装安装方式一： 1、打开 https://github.com/medcl/elasticsearch-analysis-ik/releases，因为我安装的 Elasticsearch 版本为 elasticsearch-7.0.0，所以下载 相同版本的 ik分词器 su elsearch wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.0.0/elasticsearch-analysis-ik-7.0.0.zip mkdir -p elasticsearch-7.0.0/plugins/ik unzip -d elasticsearch-7.0.0/plugins/ik elasticsearch-analysis-ik-7.0.0.zip 重启Elasticsearch ps aux | grep elasticsearch* kill 31670 ./bin/elasticsearch -d 安装方式二： 下载IK源码文件：https://github.com/medcl/elasticsearch-analysis-ik.git； 进入目录，执行mvn package，打包完成以后，生成target； target/releases 目录下 即为 IK安装文件； ​ 4.4.2、扩展本地词库未扩展之前 使用Kibana的DevTools 测试分词器 1234567PUT testGET /test/_analyze&#123; "analyzer":"ik_smart", "text":"洪荒之力"&#125; cd /data/carloz/tools/elasticsearch-7.0.0 vi plugins/ik/custom/hotwords.dic 写入 “洪荒之力” vi plugins/ik/config/IKAnalyzer.cfg.xml 重启 Elasticsearch tail -f logs/elasticsearch.log -n 100 扩展之后，再用Kibana测试相同的词汇 ​ 4.4.3、配置远程词库配置为相应的网址即可]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.2、安装Elasticsearch]]></title>
    <url>%2Felasticsearch%2F4-2-%E5%AE%89%E8%A3%85Elasticsearch%2F</url>
    <content type="text"><![CDATA[-——————————————– 参考：《ELK — 单节点系统构建》 -——————————————– 4.2.1、安装Java 4.2.2、下载Elasticsearch 4.2.3、启动Elasticsearch curl http://127.0.0.1:9200 4.2.4、后台运行Elasticsearch 4.2.5、关闭Elasticsearch 4.2.6、基本配置]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.1、Elasticsearch概述]]></title>
    <url>%2Felasticsearch%2F4-1-Elasticsearch%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[特点：分布式、基于JSON和HTTP接口 ​ 4.1.1、Elastic Stack家族基于 Elasticsearch 衍生出一系列开源软件，统称 Elastic Stack 分布式搜索引擎：Elasticsearch 可视化分析平台：Kibana 日志采集、解析工具：Logstash 数据采集工具：Beats 家族 Filebeat：收集文件数据，轻量级日志采集工具； Metricbeat：搜集 系统、进程、文件系统 级别的 CPU和内存使用情况； Packetbeat：收集网络流数据；实时监控 系统应用和服务，将延迟时间、错误、响应、SLA性能等发送到 Logstash或Elasticsearch Winlogbeat：收集 windows 事件日志数据； Heartbeat：监控服务器运行状态； Elastic家族配合使用时，版本必须一致； ​ 4.1.3、架构解读 Gateway 存储索引的文件系统： Local FileSystem —— 存储在本地文件系统； Shared FileSystem —— 共享存储； Hadoop HDFS —— 使用 hdfs 分布式存储； Amazon S3 —— 存储Amazon S3云服务上； 分布式 Lucene 层，Elasticsearch 的底层 API由 Lucene 提供，每一个ES节点 都有 一个Lucene引擎支持： 索引模块 搜索模块 映射解析模块 River模块 —— 用来导入第三方数据源，2.X之后已经废弃 Discovery模块 —— Elasticsearch 节点发现，集群内master选举，节点间通信 Scripting模块 —— 支持 JavaScript、Python 等多种语言 第三方插件模块 —— 支持 多种第三方插件； Transport —— ES传输模块，支持 Thrift，Memcached、HTTP，默认使用 HTTP； JMX——Java管理框架，用来管理 Elasticsearch 应用； RESTFull API —— ES提供给用户的接口 ​ 4.1.4、优点 分布式 —— 灵活的横向扩展，自动识别新节点 并 重新平衡分配数据； 全文检索 —— 多语言支持、强大的查询语言、地理位置支持、上下文感知的建议、自动完成和搜索片段； 近实时搜索和分析 —— 近实时搜索，也可以 聚合分析； 高可用 —— 容错机制，自动发现新节点 和失败节点，重新分配数据，确保集群可用； 模式自由 —— 动态 mapping 机制，自动检测 数据结构和类型，创建索引，使数据可搜索； RESTFul API —— 任何语言都能访问； ​ 4.1.5、应用场景1、站内搜索 —— 全文检索功能； 2、NoSQL数据库 —— 读写性能优于 MongoDB； 3、日志分析 —— 经典的ELK日志分析平台； ​ 4.1.6、核心概念集群 —— 多个节点组成的集群； 节点 —— 一个服务器 索引 —— 拥有 几分相似特征 的 文档的集合； 类型 —— 索引在一个逻辑上的分类 或 分区； 文档 —— JSON格式，被索引的基础信息单元； 分片 —— 一个索引大小 可以超出 单节点硬件限制，一个索引 可以分成多个分片； ​ 分片 水平分割和扩容 你的内存容量； ​ 可以在 主副分片 上 并行查询，提高性能和吞吐量； 副本 —— 分片的 一份或多份拷贝，分为主分片 和 副分片；]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.8、Lucene文件检索实战 —— 结果展示]]></title>
    <url>%2Felasticsearch%2F3-8-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[1、引入 Thymeleaf 模板引擎 2、定义搜索结果 —— result.html 3、调试搜索结果页面 4、点击文件名下载文件 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 工程结构 ​ 1、引入 Thymeleaf 模板引擎1、pom.xml 中引入依赖： 1234&lt;dependency&gt;​ &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;​ &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 2、application.properties 添加 Thymeleaf 配置 12345spring.thymeleaf.mode=HTML5spring.thymeleaf.encoding=UTF-8spring.thymeleaf.content-type=text/htmlspring.thymeleaf.cache=falsespring.thymeleaf.prefix=classpath:/templates/ ​ 2、定义搜索结果 —— result.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475&lt;!DOCTYPE html&gt;&lt;html xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html; charset=UTF-8"/&gt; &lt;title&gt;搜索结果&lt;/title&gt; &lt;link type="text/css" rel="stylesheet" href="css/base.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="css/page_search_result.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="css/moudle_g_search_bar.css"/&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="wrap"&gt; &lt;div class="main clearfix"&gt; &lt;div class="header"&gt;&lt;br/&gt;&lt;br/&gt;&lt;/div&gt; &lt;div id="search_result_panel"&gt; &lt;div class="content_top"&gt; &lt;div class="g_search_bar"&gt; &lt;div style="display: inline-block"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img alt="文件检索" src="images/logo.png" width="40px" height="35px"/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/div&gt; &lt;div style="display: inline-block" &gt; &lt;form class="search_form" action="/search"&gt; &lt;input class="search_text" name="keywords" type="text" th:value="$&#123;keywords&#125;" /&gt; &lt;input class="btn_submit" type="submit" value="搜索" /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="clear"&gt;&lt;/div&gt; &lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="content_left"&gt; &lt;div class="result_item_article" style="display: none;"&gt; &lt;h3&gt;&lt;a href="#"&gt;coding的最新相关信息&lt;/a&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;最新更新：2015-4-3 2:25&lt;/li&gt; &lt;li&gt;122人赞同&lt;/li&gt; &lt;li&gt;44人反对&lt;/li&gt; &lt;/ul&gt; &lt;div class="article_body"&gt; &lt;a href="#"&gt; &lt;img alt="" src="http://p.blog.csdn.net/images/p_blog_csdn_net/sealyao/594039/o_clip_image001_thumb.jpg" /&gt; &lt;/a&gt; &lt;div&gt;哈哈哈哈哈哈嘎嘎嘎嘎嘎嘎哈哈哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈&lt;/div&gt; &lt;/div&gt; &lt;div class="article_label"&gt; &lt;span&gt;标签：&lt;/span&gt; &lt;a href="#"&gt;ide&lt;/a&gt; &lt;a href="#"&gt;coding&lt;/a&gt; &lt;a href="#"&gt;ide&lt;/a&gt; &lt;a href="#"&gt;coding&lt;/a&gt; &lt;a href="#"&gt;ide&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="result_item_article" th:each="resultItem:$&#123;resultList&#125;"&gt; &lt;h3&gt; &lt;a th:href="@&#123;'/download?fileName='+$&#123;resultItem.filename&#125;&#125;" th:utext="$&#123;resultItem.title&#125;"&gt;&lt;/a&gt; &lt;/h3&gt; &lt;div class="article_body" th:utext="$&#123;resultItem.content&#125;"&gt;&lt;/div&gt; &lt;div class="article_label"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div th:if="$&#123;null == resultList || 0 == resultList.size()&#125;"&gt; &lt;h1&gt;没有搜索到相关数据 (;-_-)ᴇᴍᴍᴍ&lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="content_right"&gt;&lt;/div&gt; &lt;div class="clear"&gt;&lt;/div&gt; &lt;div class="refer_link"&gt;&lt;/div&gt; &lt;div class="page_index"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div th:include="footer"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; ​ 3、调试搜索结果页面定义url：search 12345678910111213/** * @param keywords * @return 搜索结果页面 */@RequestMapping("/search")public String searchFile(String keywords, Model model) &#123; String indexPathStr = "indexdir"; ArrayList&lt;FileModel&gt; hitsList = getTopDoc(keywords, indexPathStr, 100); log.info(keywords + "：共搜到：" + hitsList.size() + " 条数据！"); model.addAttribute("keywords", keywords); model.addAttribute("resultList", hitsList); return "result";&#125; 访问它： http://localhost:18080/search?keywords=%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6 http://localhost:18080/search?keywords=%E8%B4%9F%E8%B4%A3 ​ 4、点击文件名下载文件12345678910111213141516171819202122232425262728293031323334353637383940package com.learn.lucenefilesearch.controller;import lombok.extern.slf4j.Slf4j;import org.springframework.core.io.FileSystemResource;import org.springframework.http.*;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import java.io.File;import java.nio.charset.Charset;/** * 文件下载 * 访问：http://localhost:18080/download?fileName=Mycat-config.xml */@Slf4j@Controllerpublic class FileDownloadServletController &#123; @RequestMapping("/download") public ResponseEntity&lt;FileSystemResource&gt; downloadFile(String fileName) throws Exception &#123; String filePath = "files/" + fileName; File file = new File(filePath); log.info("download path:" + file.getPath()); FileSystemResource fileSystemResource = new FileSystemResource(file); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_OCTET_STREAM); headers.setContentLength(file.length()); headers.setCacheControl(CacheControl.noStore()); headers.setContentDisposition( ContentDisposition.builder("attachment") .filename(fileName, Charset.forName("UTF-8")) .size(file.length()) .build() ); return ResponseEntity.ok() .headers(headers) .body(fileSystemResource); &#125;&#125; 访问：http://localhost:18080/download?fileName=Mycat-config.xml http://localhost:18080/download?fileName=session共享版上线（第二版）.pptx 控制台输出：]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.7、Lucene文件检索实战 —— 文件检索]]></title>
    <url>%2Felasticsearch%2F3-7-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[1、根据索引查找文件 2、搜索结果调试 3、整个文件如下 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 1、根据索引查找文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * @param keywords 用户输入的 关键词 * @param indexPathStr 索引路径 * @param N 结果条数 * @return 在索引中搜索 关键词，返回前N条结果 */ public static ArrayList&lt;FileModel&gt; getTopDoc(String keywords, String indexPathStr, int N) &#123; ArrayList&lt;FileModel&gt; hitsList = new ArrayList&lt;FileModel&gt;(); // 检索域 String[] fields = &#123;"title", "content"&#125;; Path indexPath = Paths.get(indexPathStr); try &#123; Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(true); MultiFieldQueryParser parser = new MultiFieldQueryParser(fields, analyzer); // 查询字符串 Query query = parser.parse(keywords); TopDocs topDocs = searcher.search(query, N); // 定制高亮标签 SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter("&lt;span style=\"color:red;\"&gt;", "&lt;/span&gt;"); QueryScorer scoreTitle = new QueryScorer(query, fields[0]); Highlighter hlTitle = new Highlighter(htmlFormatter, scoreTitle); QueryScorer scoreContent = new QueryScorer(query, fields[1]); Highlighter hlContent = new Highlighter(htmlFormatter, scoreContent); TopDocs hits = searcher.search(query, 100); for (ScoreDoc sd : topDocs.scoreDocs) &#123; Document doc = searcher.doc(sd.doc); String title = doc.get("title"); String content = doc.get("content"); TokenStream tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[0], new IKAnalyzer8x(true)); Fragmenter fragmenter = new SimpleSpanFragmenter(scoreTitle); hlTitle.setTextFragmenter(fragmenter); String hlTitleStr = hlTitle.getBestFragment(tokenStream, title); // 获取高亮的片段，可以对其数量进行限制 tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[1], new IKAnalyzer8x(true)); fragmenter = new SimpleSpanFragmenter(scoreContent); String hlContentStr = hlContent.getBestFragment(tokenStream, content); // 获取高亮的片段，可以对其数量进行限制 FileModel fileModel = new FileModel( RegexHtml.delHtmlTag(title), hlTitleStr != null ? hlTitleStr : title, hlContentStr != null ? hlContentStr : content); hitsList.add(fileModel); &#125; reader.close(); directory.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return hitsList; &#125; ​ 2、搜索结果调试定义接口 search-list 用于调试数据 123456789101112/** * @param keywords * @return 搜索数据调试 */@RequestMapping("/search-list")@ResponseBodypublic ArrayList&lt;FileModel&gt; searchFileList(String keywords) &#123; String indexPathStr = "indexdir"; ArrayList&lt;FileModel&gt; hitsList = getTopDoc(keywords, indexPathStr, 100); log.info("共搜到：" + hitsList.size() + " 条数据！"); return hitsList;&#125; http://localhost:18080/search-list?keywords=session ​ 3、整个文件如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.learn.lucenefilesearch.controller;import com.learn.lucenefilesearch.model.FileModel;import com.learn.lucenefilesearch.service.IKAnalyzer8x;import com.learn.lucenefilesearch.service.RegexHtml;import lombok.extern.slf4j.Slf4j;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import java.nio.file.Path;import java.nio.file.Paths;import java.util.ArrayList;@Slf4j@Controllerpublic class SearchFileController &#123; @RequestMapping("/") public String index() &#123; return "index"; &#125; /** * @param keywords * @return 搜索数据调试 */ @RequestMapping("/search-list") @ResponseBody public ArrayList&lt;FileModel&gt; searchFileList(String keywords) &#123; String indexPathStr = "indexdir"; ArrayList&lt;FileModel&gt; hitsList = getTopDoc(keywords, indexPathStr, 100); log.info("共搜到：" + hitsList.size() + " 条数据！"); return hitsList; &#125; /** * @param keywords 用户输入的 关键词 * @param indexPathStr 索引路径 * @param N 结果条数 * @return 在索引中搜索 关键词，返回前N条结果 */ public static ArrayList&lt;FileModel&gt; getTopDoc(String keywords, String indexPathStr, int N) &#123; ArrayList&lt;FileModel&gt; hitsList = new ArrayList&lt;FileModel&gt;(); // 检索域 String[] fields = &#123;"title", "content"&#125;; Path indexPath = Paths.get(indexPathStr); try &#123; Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(true); MultiFieldQueryParser parser = new MultiFieldQueryParser(fields, analyzer); // 查询字符串 Query query = parser.parse(keywords); TopDocs topDocs = searcher.search(query, N); // 定制高亮标签 SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter("&lt;span style=\"color:red;\"&gt;", "&lt;/span&gt;"); QueryScorer scoreTitle = new QueryScorer(query, fields[0]); Highlighter hlTitle = new Highlighter(htmlFormatter, scoreTitle); QueryScorer scoreContent = new QueryScorer(query, fields[1]); Highlighter hlContent = new Highlighter(htmlFormatter, scoreContent); TopDocs hits = searcher.search(query, 100); for (ScoreDoc sd : topDocs.scoreDocs) &#123; Document doc = searcher.doc(sd.doc); String title = doc.get("title"); String content = doc.get("content"); TokenStream tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[0], new IKAnalyzer8x(true)); Fragmenter fragmenter = new SimpleSpanFragmenter(scoreTitle); hlTitle.setTextFragmenter(fragmenter); String hlTitleStr = hlTitle.getBestFragment(tokenStream, title); // 获取高亮的片段，可以对其数量进行限制 tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[1], new IKAnalyzer8x(true)); fragmenter = new SimpleSpanFragmenter(scoreContent); String hlContentStr = hlContent.getBestFragment(tokenStream, content); // 获取高亮的片段，可以对其数量进行限制 FileModel fileModel = new FileModel( RegexHtml.delHtmlTag(title), hlTitleStr != null ? hlTitleStr : title, hlContentStr != null ? hlContentStr : content); hitsList.add(fileModel); &#125; reader.close(); directory.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return hitsList; &#125;&#125; ​]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.6、Lucene文件检索实战 —— 查询界面]]></title>
    <url>%2Felasticsearch%2F3-6-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%9F%A5%E8%AF%A2%E7%95%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[1、工程结构 2、index.html 文件 3、效果图 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 索引构建完成以后，编写index.jsp页面接收用户 关键词。 1、工程结构 ​ 2、index.html 文件对应的css文件，从我的代码仓库中获取 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html; ; charset=UTF-8"/&gt; &lt;title&gt;文件检索&lt;/title&gt; &lt;link type="text/css" rel="stylesheet" href="../css/base.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="../css/index.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="../css/moudle_g_search_bar.css"/&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="wrap"&gt; &lt;div class="main" class="clearfix"&gt; &lt;div class="header"&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/div&gt; &lt;div class="g_search_bar"&gt; &lt;div class="f-large tc"&gt; &lt;h1&gt;文件&amp;nbsp;&amp;nbsp; &lt;img alt="文件检索" src="images/logo.png" width="70px" height="60px"/&gt; &amp;nbsp;&amp;nbsp;搜索 &lt;/h1&gt; &lt;/div&gt; &lt;br/&gt; &lt;div class="tc"&gt; &lt;form class="search_form" style="display: inline-block" action="/search"&gt; &lt;input class="search_text" name="keywords" type="text" /&gt; &lt;input class="btn_submit" type="submit" value="搜索" /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="clear"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="footer"&gt; &lt;div class="copyright"&gt; &lt;div&gt;基于Lucene的文件检索系统&lt;/div&gt; &lt;br/&gt; &lt;div&gt;Copyright ©&lt;strong&gt;CarloZ&lt;/strong&gt;&amp;nbsp; All Rights Reversed.&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617package com.learn.lucenefilesearch.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import java.util.ArrayList;import java.util.List;@Controllerpublic class SearchFileController &#123; @RequestMapping("/") public String index() &#123; return "index"; &#125;&#125; 3、效果图]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.5、Lucene文件检索实战 —— 索引文档]]></title>
    <url>%2Felasticsearch%2F3-4-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E5%B7%A5%E7%A8%8B%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[先建立model对象 创建索引 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 工程搭建完成以后，首先构建索引； 检索的对象：文件； 为了简单：只索引 文档名 和 文档内容； 1、先建立model对象123456789101112131415package com.learn.lucenefilesearch.model;import lombok.*;/** * 文件对象 */@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class FileModel &#123; private String title; // 文件标题 private String content; // 文件内容&#125; 2、创建索引将 IKTokenizer8x 和 IKAnalyzer8x 从第2章的工程里复制过来； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package com.learn.lucenefilesearch.service;import com.learn.lucenefilesearch.model.FileModel;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.tika.exception.TikaException;import org.apache.tika.metadata.Metadata;import org.apache.tika.parser.AutoDetectParser;import org.apache.tika.parser.ParseContext;import org.apache.tika.parser.Parser;import org.apache.tika.sax.BodyContentHandler;import org.xml.sax.SAXException;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.ArrayList;import java.util.Date;import java.util.List;/** * 对 webapp/files 下的文档生成索引，保存在webapp/indexdir中 */public class CreateIndex &#123; public static void main(String[] args) throws IOException &#123; Analyzer analyzer = new IKAnalyzer8x(true); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE); Directory directory = null; IndexWriter indexWriter = null; Path indexPath = Paths.get("indexdir"); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); fieldType.setStored(true); fieldType.setTokenized(true); fieldType.setStoreTermVectors(true); fieldType.setStoreTermVectorPositions(true); fieldType.setStoreTermVectorOffsets(true); Date start = new Date(); if (!Files.isReadable(indexPath)) &#123; System.out.println(indexPath.toAbsolutePath() + "不存在或不可读，请检查"); System.exit(1); &#125; directory = FSDirectory.open(indexPath); indexWriter = new IndexWriter(directory, indexWriterConfig); ArrayList&lt;FileModel&gt; fileModelList = (ArrayList&lt;FileModel&gt;) extractFile(); for (FileModel f : fileModelList) &#123; Document doc = new Document(); doc.add(new Field("title", f.getTitle(), fieldType)); doc.add(new Field("content", f.getContent(), fieldType)); indexWriter.addDocument(doc); &#125; indexWriter.commit(); indexWriter.close(); directory.close(); Date end = new Date(); System.out.println("索引文档完成，共耗时：" + (end.getTime() - start.getTime()) + " 毫秒。"); &#125; public static List&lt;FileModel&gt; extractFile() throws IOException &#123; ArrayList&lt;FileModel&gt; list = new ArrayList&lt;&gt;(); File fileDir = new File("files"); File[] allFiles = fileDir.listFiles(); for (File f : allFiles) &#123; FileModel fm = new FileModel(f.getName(), ParserExtraction(f)); list.add(fm); &#125; return list; &#125; public static String ParserExtraction(File file) &#123; String fileContent = ""; BodyContentHandler handler = new BodyContentHandler(); Parser parser = new AutoDetectParser(); // 自动解析器接口 Metadata metadata = new Metadata(); FileInputStream inputStream; try &#123; inputStream = new FileInputStream(file); ParseContext context = new ParseContext(); parser.parse(inputStream, handler, metadata, context); fileContent = handler.toString(); inputStream.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (SAXException e) &#123; e.printStackTrace(); &#125; catch (TikaException e) &#123; e.printStackTrace(); &#125; return fileContent; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.3、Lucene文件检索实战 —— 文本内容抽取]]></title>
    <url>%2Felasticsearch%2F3-3-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%96%87%E6%9C%AC%E5%86%85%E5%AE%B9%E6%8A%BD%E5%8F%96%2F</url>
    <content type="text"><![CDATA[3.3.1 Tika 简介 3.3.2、Tika下载 3.3.3、搭建工程 3.3.4、内容抽取 3.3.5、自动解析 -—————————————– 代码仓库：https://gitee.com/carloz/lucene-learn/tree/master/tika-demo -—————————————– ​ 3.3.1 Tika 简介Apache Tika 用于 文件类型检测 和 文件内容提取； 使用目标群体： 搜索引擎、 内容索引和分析工具； 编程语言：Java； Tika 可以检测超过1000种不同类型的文档，比如 DOC、DOCX、PPT、PPTX、TXT、PDF； 所有的文本类型 都可以通过一个简单的接口被解析； 广泛应用于 搜索引擎、内容分析、文本翻译、数字资产管理 等领域； 通过一个通用的API提取 多种文件格式 的内容； Tika特点： 统一解析器接口 低内存占用 快速处理 灵活元数据 解析器集成 MIME类型检测 语言检测 ​ 3.3.2、Tika下载官网：https://tika.apache.org/download.html 下载地址：http://mirror.bit.edu.cn/apache/tika/tika-app-1.20.jar 以 GUI 的方式打开：java -jar tika-app-1.20.jar -g 使用 File -&gt; open 代开一个本地文件，查看提取完成以后的数据 ​ 3.3.3、搭建工程——Java中使用Tika1、创建Maven工程 TikaDemo 2、添加pom引用 12345&lt;dependency&gt;​ &lt;groupId&gt;org.apache.tika&lt;/groupId&gt;​ &lt;artifactId&gt;tika-app&lt;/artifactId&gt;​ &lt;version&gt;1.20&lt;/version&gt;&lt;/dependency&gt; 3、工程根目录下 新建 files文件夹， 存放 待提取的测试文件 4、新建名为 TikaParsePdfDemo.java ​ 3.3.4、内容抽取——提取PDF文件的内容123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.learn.tikademo;import org.apache.tika.exception.TikaException;import org.apache.tika.io.TikaInputStream;import org.apache.tika.metadata.Metadata;import org.apache.tika.parser.ParseContext;import org.apache.tika.parser.pdf.PDFParser;import org.apache.tika.sax.BodyContentHandler;import org.xml.sax.SAXException;import java.io.*;/** * 3.3.4 Tika提供PDF文件内容 */public class TikaParsePdfDemo &#123; public static void main(String[] args) throws FileNotFoundException, IOException, TikaException, SAXException &#123; // 文件路径 String filepath = "files/IKAnalyzer中文分词器V2012使用手册.pdf"; // 新建File对象 File pdfFile = new File(filepath); // 创建 内容处理器对象 BodyContentHandler handler = new BodyContentHandler(); // 创建 元数据对象 Metadata metadata = new Metadata(); // 读入文件// FileInputStream inputStream = new FileInputStream(pdfFile); InputStream inputStream = TikaInputStream.get(pdfFile); // 创建解析器对象 ParseContext parseContext = new ParseContext(); // 实例化 PDFParser 对象 PDFParser pdfParser = new PDFParser(); // 调用 parse() 方法解析文件 pdfParser.parse(inputStream, handler, metadata, parseContext); // 遍历元数据内容 System.out.println("文件属性信息"); for (String name : metadata.names()) &#123; System.out.println(name + ":" + metadata.get(name)); &#125; // 打印 pdf 文件中的内容 System.out.println("pdf 文件中的内容："); System.out.println(handler.toString()); &#125;&#125; PDFParser —— 用于解析 PDF 文件； PDFParser pdfParser = new PDFParser(); OOXMLParser —— 用于解析 MS Office 文件；OOXMLParser ooxmlParser = new OOXMLParser(); TXTParser —— 用于解析 文本文件； TXTParser txtParser = new TXTParser(); HtmlParser —— 用于解析 HTML 文件；HtmlParser htmlParser = new HtmlParser(); XMLParser —— 用于解析 XML 文件；XMLParser xmlParser = new XMLParser(); ClassParser —— 用于解析 class 文件；ClassParser classParser = new ClassParser(); Tika还可以解析 图像、音频（如 MP3）、视频（如MP4）等多种类型的文件 ​ 3.3.5、自动解析上述解析pdf的例子如下：确定PDF文件 -&gt; 实例化PDFParser -&gt; 提取内容； 强大之处：Tika可以 先判断文件类型， 再根据文档类型实例化解析接口； 自动解析文档过程： 传任意类型文件到 Tika ==&gt; Tika 自身检测文件类型； Tika解析器库，根据 文件类型，选择合适的解析器； 解析完成后，对文档内容 进行提取，元数据提取； 两种解析方案： 使用Tika对象解析 使用Parse接口解析 使用Tika对象解析：123456789101112131415161718192021222324252627282930313233package com.learn.tikademo;import org.apache.tika.Tika;import org.apache.tika.exception.TikaException;import java.io.File;import java.io.IOException;/** * 3.3.5、自动解析 —— 使用Tika对象解析 * 1、传任意类型文件到 Tika ==&gt; Tika 自身检测文件类型； * 2、Tika解析器库，根据 文件类型，选择合适的解析器； * 3、解析完成后，对文档内容 进行提取，元数据提取； */public class TikaExtractionDemo &#123; public static void main(String[] args) throws IOException, TikaException &#123; Tika tika = new Tika(); // 新建存放各种文件的文件里 files File fileDir = new File("files"); // 如果文件夹路径错误，退出程序 if (!fileDir.exists()) &#123; System.out.println("文件夹不存在，请检查"); System.exit(0); &#125; // 获取文件夹下的所有文件，存放在File数组中 File[] fileArr = fileDir.listFiles(); String fileContent; for (File f : fileArr) &#123; fileContent = tika.parseToString(f); // 自动解析 System.out.println("Extracted Content: " + fileContent); &#125; &#125;&#125; 使用Parse接口解析12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.learn.tikademo;import org.apache.tika.exception.TikaException;import org.apache.tika.metadata.Metadata;import org.apache.tika.parser.AutoDetectParser;import org.apache.tika.parser.ParseContext;import org.apache.tika.parser.Parser;import org.apache.tika.sax.BodyContentHandler;import org.xml.sax.SAXException;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;/** * 3.3.5、自动解析 —— 使用Parse接口解析 * 1、传任意类型文件到 Tika ==&gt; Tika 自身检测文件类型； * 2、Tika解析器库，根据 文件类型，选择合适的解析器； * 3、解析完成后，对文档内容 进行提取，元数据提取； */public class ParserExtractionDemo &#123; public static void main(String[] args) throws FileNotFoundException, IOException, SAXException, TikaException &#123; // 新建存放各种文件的文件里 files File fileDir = new File("files"); // 如果文件夹路径错误，退出程序 if (!fileDir.exists()) &#123; System.out.println("文件夹不存在，请检查"); System.exit(0); &#125; Metadata metadata = new Metadata(); // 创建元数据对象 Parser parser = new AutoDetectParser(); ParseContext parseContext = new ParseContext(); // 自动检测分词器 // 获取文件夹下的所有文件，存放在File数组中 FileInputStream inputStream = null; File[] fileArr = fileDir.listFiles(); for (File f : fileArr) &#123; inputStream = new FileInputStream(f); BodyContentHandler handler = new BodyContentHandler(); // 创建内容处理器对象 parser.parse(inputStream, handler, metadata, parseContext); System.out.println("ParserExtractionDemo-" + f.getName() + ":\n" + handler.toString()); &#125; &#125;&#125; 运行结果查询：]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.2、Lucene文件检索实战 —— 架构设计]]></title>
    <url>%2Felasticsearch%2F3-2-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[文档存储系统 —— 存储了不同类型的文件； 后台 —— 提取出 文件名 和 文档内容； Lucene —— 使用Lucune对 文件名 和 文档内容 进行索引； 前端 —— 对用户提供查询接口； 检索过程 —— 用户提交关键词，检索索引库， 返回匹配文档至前端页面 能够下载检索到的文件； 能够实现关键字的高亮； 工具准备： 使用 Tika 完成信息抽取； 使用 Lucene 构建索引； 使用 JSP页面 给用户提供查询接口； 使用 Servlet 完成搜索； 构建类似百度文库的小型文件检索系统]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.1、Lucene文件检索实战 —— 需求分析]]></title>
    <url>%2Felasticsearch%2F3-1-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[假设有一批文档，文档格式有 DOC、DOCX、PPT、PPTX、TXT、PDF 要实现类似于百度文库的文件检索系统。 需求如下： 能够对文件名进行检索； 能够对文件内容进行检索； 能够下载检索到的文件； 能够实现关键字的高亮；]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.7、Lucene新闻高频词提取]]></title>
    <url>%2Felasticsearch%2F2-7-Lucene%E6%96%B0%E9%97%BB%E9%AB%98%E9%A2%91%E8%AF%8D%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[2.7.1、问题提出 2.7.2、需求分析 2.7.3、编程实现 -———————————————- 2.7.1、问题提出统计 一篇新闻文档，统计出现频率最高的哪些词语 2.7.2、需求分析文本关键词提取算法、开源工具很多 本文：《从Lucene索引中 提取 词项频率Top N》 词条化：从文本中 去除 标点、停用词等； 索引过程的本质：词条化 生成 倒排索引的过程； 代码思路：IndexReader的getTermVector获取文档的某一个字段 Terms，从 terms 中获取 tf（term frequency），拿到词项的 tf 以后，放到map中 降序排序，取出 Top-N. 2.7.3、编程实现网上找到新闻稿《李开复：无人驾驶进入黄金时代 AI有巨大投资机会》，放在 testfile/news.txt 文件中。 对 testfile/news.txt 生成索引： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.learn.lucene.chapter2.highfrequency;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.*;import java.nio.file.Paths;/** * 2.7.1、问题提出 * 统计 一篇新闻文档，统计出现频率最高的哪些词语 * 2.7.2、需求分析 * 文本关键词提取算法、开源工具很多 * 本文：《从Lucene索引中 提取 词项频率Top N》 * 词条化：从文本中 去除 标点、停用词等； * 索引过程的本质：词条化 生成 倒排索引的过程； * 代码思路：IndexReader的getTermVector获取文档的某一个字段 Terms，从 terms 中获取 tf（term frequency），拿到词项的 tf 以后，放到map中 降序排序，取出 Top-N. * 2.7.3、编程实现 * 网上找到新闻稿《李开复：无人驾驶进入黄金时代 AI有巨大投资机会》，放在 testfile/news.txt 文件中。 * * 索引文档 */public class IndexDocs &#123; public static void main(String[] args) throws IOException &#123; File newfile = new File("testfile/news.txt"); String text1 = textToString(newfile); Analyzer smcAnalyzer = new IKAnalyzer8x(true); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(smcAnalyzer); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE); // 索引的存储路径 Directory directory = FSDirectory.open(Paths.get("indexdir")); // 索引的增删改由 IndexWriter 创建 IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 新建 FieldType，用于指定字段索引时的信息 FieldType type = new FieldType(); type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); type.setStored(true); // 原始字符串全部被保存在索引中 type.setStoreTermVectors(true); // 存储词项 量 type.setTokenized(true); // 词条化 Document doc1 = new Document(); Field field1 = new Field("content", text1, type); doc1.add(field1); indexWriter.addDocument(doc1); indexWriter.close(); directory.close(); &#125; public static String textToString(File file) &#123; StringBuilder result = new StringBuilder(); try &#123; // 构造一个 BufferedReader 类来读取文件 BufferedReader bufferedReader = new BufferedReader(new FileReader(file)); String str = null; // 使用 readline 方法，一次读一行 while (null != (str = bufferedReader.readLine())) &#123; result.append(System.lineSeparator() + str); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return result.toString(); &#125;&#125; 运行。 提取高频词： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.learn.lucene.chapter2.highfrequency;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Terms;import org.apache.lucene.index.TermsEnum;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.BytesRef;import java.io.IOException;import java.nio.file.Paths;import java.util.*;/** * 提取高频词 */public class GetTopTerms &#123; public static void main(String[] args) throws IOException &#123; Directory directory = FSDirectory.open(Paths.get("indexdir")); IndexReader reader = DirectoryReader.open(directory); // 因为之索引了一个文档，所以DocID为0 // 通过 getTermVector 获取 content 字段的词项 Terms terms = reader.getTermVector(0, "content"); // 遍历词项 TermsEnum termsEnum = terms.iterator(); Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); BytesRef thisTerm; while (null != (thisTerm = termsEnum.next())) &#123; String termText = thisTerm.utf8ToString(); // 词项 // 通过 totalTermFreq() 方法获取词项频率 map.put(termText, (int) termsEnum.totalTermFreq()); &#125; // 按 value 排序 List&lt;Map.Entry&lt;String, Integer&gt;&gt; sortedMap = new ArrayList&lt;&gt;(map.entrySet()); Collections.sort(sortedMap, new Comparator&lt;Map.Entry&lt;String, Integer&gt;&gt;() &#123; @Override public int compare(Map.Entry&lt;String, Integer&gt; o1, Map.Entry&lt;String, Integer&gt; o2) &#123; return (o2.getValue() - o1.getValue()); &#125; &#125;); getTopN(sortedMap, 10); &#125; public static void getTopN(List&lt;Map.Entry&lt;String, Integer&gt;&gt; sortedMap, int N) &#123; for (int i = 0; i &lt; N; i++) &#123; System.out.println(sortedMap.get(i).getKey() + ":" + sortedMap.get(i).getValue()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.6、Lucene查询高亮]]></title>
    <url>%2Felasticsearch%2F2-6-Lucene%E6%9F%A5%E8%AF%A2%E9%AB%98%E4%BA%AE%2F</url>
    <content type="text"><![CDATA[找到需要高亮的片段 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.learn.lucene.chapter2.hignlight;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 找到需要高亮的片段 */public class HighlighterTest &#123; public static void main(String[] args) throws IOException, ParseException, InvalidTokenOffsetsException &#123; String field = "title"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(); QueryParser parser = new QueryParser(field, analyzer); Query query = parser.parse("北大"); System.out.println("Query: " + query); QueryScorer scorer = new QueryScorer(query, field); SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter("&lt;span style=\"color:red;\"&gt;", "&lt;/span&gt;"); Highlighter highlighter = new Highlighter(htmlFormatter, scorer); // 高亮分词器 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(), scoreDoc.doc, field, analyzer); Fragmenter fragmenter = new SimpleSpanFragmenter(scorer); highlighter.setTextFragmenter(fragmenter); String str = highlighter.getBestFragment(tokenStream, doc.get(field)); System.out.println("高亮的片段：" + str); &#125; directory.close(); reader.close(); &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.5、Lucene查询详解]]></title>
    <url>%2Felasticsearch%2F2-5-Lucene%E6%9F%A5%E8%AF%A2%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录2.5.1、搜索入门 2.5.2、多域搜索（MultiFieldQueryParse） 2.5.3、词项搜索（TermQuery） 2.5.4、布尔搜索（BooleanQuery） 2.5.5、范围搜索（RangeQuery） 2.5.6、前缀搜索（PrefixQuery） 2.5.7、多关键字搜索（PhraseQuery） 2.5.8、模糊搜索（FuzzyQuery） 2.5.9、通配符搜索（WildcardQuery） -————————————————— 文档索引完成以后就能对其进行搜索； 当用户输入一个关键字， ​ –&gt; 首先 对这个关键字 进行 分析和处理， 转化成后台可以理解的形式 ​ –&gt; 进行检索 2.5.1、搜索入门处理关键词 &lt;==&gt; 构建Query对象的过程； 搜索文档 &lt;==&gt; 实例化 IndexSearcher 对象，使用search()方法完成； ​ 参数：Query对象 ​ 结果：保存在 TopDocs 类型的文档集合中； 删除indexdir下的索引文件后，重新使用CreateIndex.java 生成索引 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.learn.lucene.chapter2.queries;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.1、查询 */public class QueryParseTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String field = "title"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(); QueryParser parser = new QueryParser(field, analyzer); parser.setDefaultOperator(QueryParser.Operator.AND); Query query = parser.parse("农村学生"); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： 修改后再运行： ​ 2.5.2、多域搜索（MultiFieldQueryParse）根据多个字段搜索 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.learn.lucene.chapter2.queries;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.2、多域搜索 */public class MultiFieldQueryParseTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String[] field = &#123;"title", "content"&#125;; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(true); MultiFieldQueryParser parser = new MultiFieldQueryParser(field, analyzer); parser.setDefaultOperator(QueryParser.Operator.AND); Query query = parser.parse("美国"); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.3、词项搜索（TermQuery）TermQuery 是 最常用的 Query TermQuery 是 Lucene中搜索的最基本单位 本质上：一个词条就是一个 key/value 对 使用TermQuery： 首先构造一个 Term对象；Term term = new Term(“title”, “美国”); 然后使用Term对象为参数，构造一个TermQuery对象；TermQuery query = new TermQuery(term); 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.3 词项搜索 */public class TermQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Term term = new Term("title", "美国"); TermQuery query = new TermQuery(term); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.4、布尔搜索（BooleanQuery）BooleanQuery 可以 组合 其他 Query，并标明他们的逻辑关系； 例如：查询 content 中包含美国，并且 title 不包含美国的文档； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.4、布尔搜索（BooleanQuery） * 查询 content 中包含美国，并且 title 不包含美国的文档； */public class BooleanQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); TermQuery query1 = new TermQuery(new Term("title", "美国")); TermQuery query2 = new TermQuery(new Term("content", "美国")); BooleanClause booleanClause1 = new BooleanClause(query1, BooleanClause.Occur.MUST_NOT); BooleanClause booleanClause2 = new BooleanClause(query2, BooleanClause.Occur.MUST); BooleanQuery query = new BooleanQuery.Builder().add(booleanClause1).add(booleanClause2).build(); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.5、范围搜索（RangeQuery）举例：查询新闻回复条数在 500~1000 之间的文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.document.IntPoint;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.5、范围搜索（RangeQuery） * 举例：查询新闻回复条数在 500~1000 之间的文档 */public class RangeQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = IntPoint.newRangeQuery("reply", 500, 1000); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; ​ 2.5.6、前缀搜索（PrefixQuery）举例：搜索 包含以“学”开头的词项 的文档 123456789101112131415161718192021222324252627282930313233343536373839404142package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.6、前缀搜索（PrefixQuery） * 举例：搜索 包含以“学”开头的词项 的文档 */public class PrefixQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Term term = new Term("title", "学"); Query query = new PrefixQuery(term); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.7、多关键字搜索（PhraseQuery） PhraseQuery 可以 通过add方法添加多个关键字 还可以通过 setSlop() 设定“坡度”，允许关键字之间 无关词汇存在量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.PhraseQuery;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.7、多关键字搜索（PhraseQuery） * PhraseQuery 可以 通过add方法添加多个关键字 * 还可以通过 setSlop() 设定“坡度”，允许关键字之间 无关词汇存在量 */public class PhraseQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String str = "习近平会见奥巴马，学习国外经验"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); PhraseQuery.Builder builder = new PhraseQuery.Builder(); builder.add(new Term("title", "奥巴马"), str.indexOf("奥巴马")); builder.add(new Term("title", "学习国外经验"), str.indexOf("学习国外经验")); PhraseQuery query = builder.build(); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果(感觉没有成功)： ​ 2.5.8、模糊搜索（FuzzyQuery）它可以简单的识别两个相近的词语。 举例：“Trump”，写成“Trmp”，拼写错误，仍然可以搜索得到正确的结果 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.8、模糊搜索（FuzzyQuery） * 它可以简单的识别两个相近的词语。 * 举例：“Trump”，写成“Trmp”，拼写错误，仍然可以搜索得到正确的结果 */public class FuzzyQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = new FuzzyQuery(new Term("title", "Trmp")); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; ​ 2.5.9、通配符搜索（WildcardQuery）1234567891011121314151617181920212223242526272829303132333435363738394041package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.9、通配符搜索（WildcardQuery） */public class WildcardQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = new WildcardQuery(new Term("title", "习?平")); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.4、Lucene索引详解]]></title>
    <url>%2Felasticsearch%2F2-4-Lucene%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录2.4.1、Lucene 字段类型 2.4.2、索引文档示例 2.4.3、在Luke中查看索引 2.4.4、索引的删除 2.4.5 索引的更新 -———————————————– 上一节 介绍完了 Lucene 分词器，这节介绍 Lucene 是如何索引文档的。 ​ 2.4.1、Lucene 字段类型文档：文档是 Lucene索引的基本单位； 字段：比文档更小的单位，字段是文档的一部分； ​ 每个字段 由 3部分组成：名称（name），类型（type），取值（value）； ​ 字段的取值（value）一般为：文本、二进制、数值 Lucene的主要字段类型： TextField：字段内容 -&gt; 索引并词条化 -&gt; 不保存 词向量 -&gt; 整篇文档的body字段，常用TextField进行索引； StringField：只 索引 -&gt; 不词条化 -&gt; 不保存 词向量； IntPoint：适合 int类型 的索引； LongPoint：适合 long类型的 索引； FloatPoint： DoublePoint： SortedDocValuesField：存储值为 文本内容的 DocValue字段，且需要 按值排序； SortedSetDocValuesField：多值域为 DocValue字段，值为文本内容，且 需要 按值分组、聚合； NumbericDocValuesField： DocValue为（int，long，float，double） SortedNumbericDocValuesField：需要排序的 （int，long，float，double） SortedField：索引 保存字段值，不进行其他操作 Lucene 使用 倒排索引 快速搜索 &lt;===&gt; 建立 词项和文档id的关系映射； 搜索过程： ​ 通过 类似hash算法 -&gt; 定位到 搜索关键词 -&gt; 读取文档id集合 上述过程的缺陷： ​ 当我们需要对数据做 聚合操作，排序、分组时，Lucene会提取所有出现在文档集合中的排序字段，再次构建一个排好序的文件集合；这个过程全部在内存中进行，如果数据量巨大，会造成 内存溢出 和 性能缓慢； Lucene 4.X 之后出现了 DocValues，DocValues是 Lucene 构建索引时，额外建立的一个有序的基于 document =&gt; field/value 的映射列表。 ​ 2.4.2、索引文档示例代表新闻的实例类 News.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.learn.lucene.chapter2.index;/** * 代表新闻的实例类 News.java */public class News &#123; private int id; private String title; private String content; private int reply; public News() &#123; &#125; public News(int id, String title, String content, int reply) &#123; super(); this.id = id; this.title = title; this.content = content; this.reply = reply; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getTitle() &#123; return title; &#125; public void setTitle(String title) &#123; this.title = title; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; public int getReply() &#123; return reply; &#125; public void setReply(int reply) &#123; this.reply = reply; &#125;&#125; 创建索引文件的CreateIndex.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.learn.lucene.chapter2.index;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.Date;/** * 创建索引 */public class CreateIndex &#123; public static void main(String[] args) &#123; // 创建 3 个News对象 News news1 = new News(); news1.setId(1); news1.setTitle("习近平会见美国总统奥巴马，学习国外经验"); news1.setContent("国家主席习近平9月3日在杭州西湖宾馆会见前来出席二十国集团领导人杭州峰会的美国总统奥巴马"); news1.setReply(672); News news2 = new News(); news2.setId(2); news2.setTitle("北大迎4380名新生，农村学生700多人今年最多"); news2.setContent("昨天，北京大学迎来4380名来自全国各地及数十个国家的本科新生。其中，农村学生工700余名，为今年最多..."); news2.setReply(995); News news3 = new News(); news3.setId(3); news3.setTitle("特朗普宣誓（Donald Trump）就任美国第45任总统"); news3.setContent("当地时机1月20日，唐纳德·特朗普在美国国会宣誓就职，正式成为美国第45任总统。"); news3.setReply(1872); // 创建IK分词器 Analyzer analyzer = new IKAnalyzer8x(); IndexWriterConfig iwc = new IndexWriterConfig(analyzer); iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE); Directory directory = null; IndexWriter indexWriter = null; // 索引目录 Path indexPath = Paths.get("indexdir"); // 开始时间 Date start = new Date(); try &#123; if (!Files.isReadable(indexPath)) &#123; System.out.println("Document directory '" + indexPath.toAbsolutePath() + "' is not readable! please check"); System.exit(1); &#125; directory = FSDirectory.open(indexPath); indexWriter = new IndexWriter(directory, iwc); // 设置新闻ID 索引 并存储 FieldType idType = new FieldType(); idType.setIndexOptions(IndexOptions.DOCS); idType.setStored(true); // 设置新闻标题索引文档，词项频率，位移信息，和偏移量，存储并词条化 FieldType titleType = new FieldType(); titleType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); titleType.setStored(true); titleType.setTokenized(true); FieldType contentType = new FieldType(); contentType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); contentType.setStored(true); contentType.setTokenized(true); contentType.setStoreTermVectors(true); contentType.setStoreTermVectorPositions(true); contentType.setStoreTermVectorOffsets(true); contentType.setStoreTermVectorPayloads(true); Document doc1 = new Document(); doc1.add(new Field("id", String.valueOf(news1.getId()), idType)); doc1.add(new Field("title", news1.getTitle(), titleType)); doc1.add(new Field("content", news1.getContent(), contentType)); doc1.add(new IntPoint("reply", news1.getReply())); doc1.add(new StoredField("reply_display", news1.getReply())); Document doc2 = new Document(); doc2.add(new Field("id", String.valueOf(news2.getId()), idType)); doc2.add(new Field("title", news2.getTitle(), titleType)); doc2.add(new Field("content", news2.getContent(), contentType)); doc2.add(new IntPoint("reply", news2.getReply())); doc2.add(new StoredField("reply_display", news2.getReply())); Document doc3 = new Document(); doc3.add(new Field("id", String.valueOf(news3.getId()), idType)); doc3.add(new Field("title", news3.getTitle(), titleType)); doc3.add(new Field("content", news3.getContent(), contentType)); doc3.add(new IntPoint("reply", news3.getReply())); doc3.add(new StoredField("reply_display", news3.getReply())); indexWriter.addDocument(doc1); indexWriter.addDocument(doc2); indexWriter.addDocument(doc3); indexWriter.commit(); indexWriter.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; Date end = new Date(); System.out.println("索引文档用时：" + (end.getTime() - start.getTime()) + " milliseconds."); &#125;&#125; 运行结果： 生成对应的索引文件 ​ 2.4.3、在Luke中查看索引 打开索引文件目录：D:\java\oschina\lucene-learn\lucene-chapter2\indexdir ​ 2.4.4、索引的删除索引同样存在 CRUD 操作 本节演示 根据 Term 来删除点单个或多个Document，删除 title 中 包含关键词“美国”的文档。 1234567891011121314151617181920212223242526272829303132333435363738394041package com.learn.lucene.chapter2.index;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 删除索引 */public class DeleteIndex &#123; public static void main(String[] args) &#123; // 删除 title 中含有关键字“美国”的文档 deleteDoc("title", "美国"); &#125; public static void deleteDoc(String field, String key) &#123; Analyzer analyzer = new IKAnalyzer8x(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); Path indexPath = Paths.get("indexdir"); Directory directory; try &#123; directory = FSDirectory.open(indexPath); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); indexWriter.deleteDocuments(new Term(field, key)); indexWriter.commit(); indexWriter.close(); System.out.println("删除完成"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果： 除此之外，IndexWriter还提供了以下方法： DeleteDocuments(Query query)：根据Query条件来删除单个或多个Document。 DeleteDocuments(Query[] queries)：根据Query条件来删除单个或多个Document。 DeleteDocuments(Term term)：根据Term条件来删除单个或多个Document。 DeleteDocuments(Term[] terms)：根据Term条件来删除单个或多个Document。 DeleteAll()：删除所有的Document。 使用IndexWriter进行Document删除操作时，文档并不会立即被删除，而是把这个删除动作缓存起来，当IndexWriter.Commit() 或 IndexWriter.Close()时，删除操作才会真正执行。 ​ 使用Luke 重新打开 索引之后，只剩下了一个索引文档： ​ 2.4.5 索引的更新本质：先删除索引，再建立新的文档。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.learn.lucene.chapter2.index;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 更新索引 */public class UpdateIndex &#123; public static void main(String[] args) &#123; Analyzer analyzer = new IKAnalyzer8x(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); Path indexPath = Paths.get("indexdir"); Directory directory; try &#123; directory = FSDirectory.open(indexPath); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); Document doc = new Document(); // 设置新闻ID 索引 并存储 FieldType idType = new FieldType(); idType.setIndexOptions(IndexOptions.DOCS); idType.setStored(true); // 设置新闻标题索引文档，词项频率，位移信息，和偏移量，存储并词条化 FieldType titleType = new FieldType(); titleType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); titleType.setStored(true); titleType.setTokenized(true); FieldType contentType = new FieldType(); contentType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); contentType.setStored(true); contentType.setTokenized(true); contentType.setStoreTermVectors(true); contentType.setStoreTermVectorPositions(true); contentType.setStoreTermVectorOffsets(true); contentType.setStoreTermVectorPayloads(true); doc.add(new Field("id", "2", idType)); doc.add(new Field("title", "北大迎4380名新生", titleType)); doc.add(new Field("content", "昨天，北京大学迎来4380名来自全国各地及数十个国家的本科新生。其中，农村学生工700余名，为今年最多...", contentType)); indexWriter.updateDocument(new Term("title", "北大"), doc); indexWriter.commit(); indexWriter.close(); System.out.println("更新完成"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果： 修改前： 修改后：]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.3、Lucene分词详解]]></title>
    <url>%2Felasticsearch%2F2-3-Lucene%E5%88%86%E8%AF%8D%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录2.3.1、Lucene分词系统 2.3.2、分词测试 2.3.3、IK分词器配置 2.3.4、中文分词器对比 2.3.5、扩展停用词词典 2.3.6、扩展自定义词典 -—————————————————————— ​ 2.3.1、Lucene分词系统索引和查询 都是以 词项 为基本单位 Lucene中，分词 主要依靠 Analyzer类 解析实现 Analyzer是抽象类，内部调用 TokenStream 实现 ​ 2.3.2、分词测试StandardAnalyzer 分词器测试： 1234567891011121314151617181920212223242526272829303132333435363738package com.learn.lucene.chapter2.analyzer;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import java.io.IOException;import java.io.StringReader;/** * StandardAnalyzer demo */public class StdAnalyzerDemo &#123; private static String strCh = "中华人名共和国简称中国，是一个有13亿人口的国家"; private static String strEn = "Dogs can not achieve a place, eyes can reach;"; public static void main(String[] args) throws IOException &#123; System.out.println("StandardAnalyzer 对中文分词："); stdAnalyzer(strCh); System.out.println("StandardAnalyzer 对英文分词："); stdAnalyzer(strEn); &#125; public static void stdAnalyzer(String str) throws IOException &#123; Analyzer analyzer = null; analyzer = new StandardAnalyzer(); StringReader reader = new StringReader(str); TokenStream tokenStream = analyzer.tokenStream(str, reader); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class); System.out.println("分词结果："); while (tokenStream.incrementToken()) &#123; System.out.print(charTermAttribute.toString() + "|"); &#125; System.out.println("\n"); analyzer.close(); &#125;&#125; 运行结果： 测试多种Analyzer，注意要特意指定 jdk8： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.learn.lucene.chapter2.analyzer;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.cjk.CJKAnalyzer;import org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer;import org.apache.lucene.analysis.core.KeywordAnalyzer;import org.apache.lucene.analysis.core.SimpleAnalyzer;import org.apache.lucene.analysis.core.StopAnalyzer;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import java.io.IOException;import java.io.StringReader;import java.util.StringJoiner;/** * 测试多种分词器 */public class VariousAnalyzersDemo &#123; private static String strCh = "中华人民共和国简称中国，是一个有13亿人口的国家"; private static String strEn = "Dogs can not achieve a place, eyes can reach;"; public static void main(String[] args) throws IOException &#123; System.out.println("标准分词：" + printAnalyzer(new StandardAnalyzer(), strCh)); System.out.println("空格分词：" + printAnalyzer(new WhitespaceAnalyzer(), strCh)); System.out.println("简单分词：" + printAnalyzer(new SimpleAnalyzer(), strCh)); System.out.println("二分法分词：" + printAnalyzer(new CJKAnalyzer(), strCh)); System.out.println("关键字分词：" + printAnalyzer(new KeywordAnalyzer(), strCh)); System.out.println("停用词分词：" + printAnalyzer(new StopAnalyzer(new StringReader(strCh)), strCh)); System.out.println("中文智能分词：" + printAnalyzer(new SmartChineseAnalyzer(), strCh)); &#125; public static String printAnalyzer(Analyzer analyzer, String str) throws IOException &#123; StringReader reader = new StringReader(str); TokenStream tokenStream = analyzer.tokenStream(str, reader); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class); StringJoiner stringJoiner = new StringJoiner("|"); while (tokenStream.incrementToken()) &#123; stringJoiner.add(charTermAttribute.toString()); &#125; analyzer.close(); return stringJoiner.toString(); &#125;&#125; 运行结果： ​ 2.3.3、IK分词器配置Lucene 8.0 实用 IK分词器需要修改 IKTokenizer 和 IKAnalyzer 在 com.learn.lucene.chapter2.ik 下 新建 IKTokenizer8x.java 和 IKAnalyzer8x.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.learn.lucene.chapter2.ik;import org.apache.lucene.analysis.Tokenizer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;import org.apache.lucene.analysis.tokenattributes.TypeAttribute;import org.wltea.analyzer.core.IKSegmenter;import org.wltea.analyzer.core.Lexeme;import java.io.IOException;/** * 修改 IKTokenizer —— incrementToken() */public class IKTokenizer8x extends Tokenizer &#123; // IK分词器实现 private IKSegmenter _IKIkSegmenter; // 词元文本属性 private final CharTermAttribute termAttribute; // 词元位移属性 private final OffsetAttribute offsetAttribute; // 词元分类属性 // (该属性分类参考 org.wltea.analyzer.core.Lexeme 中的分类常量) private final TypeAttribute typeAttribute; // 记录最后一个词元的结束位置 private int endPosttion; // Lucene 8.x Tokenizer适配器类构造函数，实现最新的 Tokenizer 接口 public IKTokenizer8x(boolean useSmart) &#123; super(); offsetAttribute = addAttribute(OffsetAttribute.class); termAttribute = addAttribute(CharTermAttribute.class); typeAttribute = addAttribute(TypeAttribute.class); _IKIkSegmenter = new IKSegmenter(input, useSmart); &#125; @Override public boolean incrementToken() throws IOException &#123; clearAttributes(); // 清除所有的词元属性 Lexeme nextLexeme = _IKIkSegmenter.next(); if (nextLexeme != null) &#123; // 将 Lexeme 转化成 Attributes termAttribute.append(nextLexeme.getLexemeText()); // 设置词元文本 termAttribute.setLength(nextLexeme.getLength()); // 设置词元长度 offsetAttribute.setOffset(nextLexeme.getBeginPosition(), nextLexeme.getEndPosition()); // 设置词元位移 endPosttion = nextLexeme.getEndPosition(); // 记录 分词的最后位置 typeAttribute.setType(nextLexeme.getLexemeText()); // 记录词元分分类 return true; // 返回true 告知 还有下个词元 &#125; return false; &#125; @Override public void reset() throws IOException &#123; super.reset(); _IKIkSegmenter.reset(input); &#125; @Override public final void end() throws IOException &#123; int finalOffset = correctOffset(this.endPosttion); offsetAttribute.setOffset(finalOffset, finalOffset); &#125;&#125; ​ 1234567891011121314151617181920212223242526272829303132333435package com.learn.lucene.chapter2.ik;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.Tokenizer;/** * 修改IKAnalyzer —— createComponents(String fieldName) */public class IKAnalyzer8x extends Analyzer &#123; private boolean useSmart; public IKAnalyzer8x() &#123; this(false); &#125; public IKAnalyzer8x(boolean useSmart) &#123; super(); this.useSmart = useSmart; &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; Tokenizer _IKTokenizer = new IKTokenizer8x(this.isUseSmart()); return new TokenStreamComponents(_IKTokenizer); &#125; public boolean isUseSmart() &#123; return useSmart; &#125; public void setUseSmart(boolean useSmart) &#123; this.useSmart = useSmart; &#125;&#125; 实例化 IKAnalyzer8x 就能实用IK分词器了 1、 默认使用细粒度切分算法： ​ Analyzer analyzer = new IKAnalyzer8x(); 2、创建智能切分算法的 IKAnalyzer： ​ Analyzer analyzer = new IKAnalyzer8x(true); ​ 2.3.4、中文分词器对比​ 分词效果会直接影响文档搜索的准确性 ​ 我们对比一下 Lucene自带的 SmartChineseAnalyzer 和 IK Analyzer的效率。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.lucene.chapter2.analyzer;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import java.io.IOException;import java.io.StringReader;import java.util.StringJoiner;/** * 中文分词器效果对比 * 1、Lucene 自带的 SmartChineseAnalyzer 分词器 * 2、IKAnalyzer8x分词器 */public class IkVSSmartchDemo &#123; private static String str1 = "公路局正在治理解放大道路面积水问题。"; private static String str2 = "IKAnalyzer 是一个开源的，基于java语言开发的轻量级的中文分词工具包。"; public static void main(String[] args) throws IOException &#123; System.out.println("句子一：" + str1); System.out.println("SmartChineseAnalyzer分词结果：" + printAnalyzer(new SmartChineseAnalyzer(), str1)); System.out.println("IKAnalyzer8x分词结果：" + printAnalyzer(new IKAnalyzer8x(true), str1));// System.out.println("IKAnalyzer分词结果(bug)：" + printAnalyzer(new IKAnalyzer(), str1)); System.out.println("----------------------------------------"); System.out.println("句子二：" + str2); System.out.println("SmartChineseAnalyzer分词结果：" + printAnalyzer(new SmartChineseAnalyzer(), str2)); System.out.println("IKAnalyzer8x分词结果：" + printAnalyzer(new IKAnalyzer8x(true), str2));// System.out.println("IKAnalyzer分词结果(bug)：" + printAnalyzer(new IKAnalyzer(true), str2)); &#125; public static String printAnalyzer(Analyzer analyzer, String str) throws IOException &#123; StringReader reader = new StringReader(str); TokenStream tokenStream = analyzer.tokenStream(str, reader); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class); StringJoiner stringJoiner = new StringJoiner("|"); while (tokenStream.incrementToken()) &#123; stringJoiner.add(charTermAttribute.toString()); &#125; analyzer.close(); return stringJoiner.toString(); &#125;&#125; ​ 2.3.5、扩展停用词词典IK Analyzer 默认的停用词词典为 IKAnalyzer2012_u6/stopword.dic 这个词典只有30多个英文停用词，并不完整 推荐使用扩展额停用词词表：https://github.com/cseryp/stopwords 在工程中新建 ext_stopword.dic，放在IKAnalyzer.cfg.xml同一目录； 编辑IKAnalyzer.cfg.xml， ​]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.2、Lucene开发准备]]></title>
    <url>%2Felasticsearch%2F2-2-Lucene%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[目录 官网 下载Lucune文件库 下载Luke——查看索引的GUI工具 下载 IK 分词工具 —— 轻量级中文分词工具包 工程搭建 -————————————————— ​ 官网http://lucene.apache.org/ 下载Lucune文件库 https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/java/8.0.0/ 或者在工程中添加maven依赖 https://mvnrepository.com/artifact/org.apache.lucene/lucene-core/ 12345&lt;dependency&gt;​ &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;​ &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;​ &lt;version&gt;8.0.0&lt;/version&gt;&lt;/dependency&gt; ​ 下载Luke——查看索引的GUI工具主要功能： 查看文档和分析字段内容； 搜索索引； 执行索引维护； 从HDFS读取索引； 将全部或部分索引转换为XML格式导出； 测试自定义的Lucene分词器； 注意：Luke的版本要跟Lucene保持一致 项目地址：https://github.com/DmitryKey/luke/releases ​ 下载 IK 分词工具 —— 轻量级中文分词工具包独立于Lucene，面向Java的公用分词组件 同时提供了 对Lucene的默认优化实现 IK Analyzer2012的特性： 采用“正向迭代最细力度切分算法”，支持细粒度和智能分词两种切分模式； 具有 160万字/s 的高速处理能力，i73.4G双核、4G内存，win7 64位，jdk1.6； 支持简单的分词排歧义处理、数量词合并输出； 多子处理器分析模式，支持英文字母、数字、中文词汇等分词处理、兼容韩文、日文字符； 优化词典存储，更小的内存占用。 下载地址（需要翻墙）：https://code.google.com/p/ik-analyzer/downloads/list 也可有上我的 oschina上下载： 安装包文件列表： drwxr-xr-x 1 doc/ —— API 文档说明 -rw-r–r– 1 IKAnalyzer.cfg.xml —— 分词器扩展配置文件 -rw-r–r– 1 IKAnalyzer2012_u6.jar —— 主jar包 -rw-r–r– 1 IKAnalyzer中文分词器V2012_U5使用手册.pdf -rw-r–r– 1 IKAnalyzer中文分词器V2012使用手册.pdf -rw-r–r– 1 LICENSE.txt —— Apache版权申明 -rw-r–r– 1 NOTICE.txt —— Apache版权申明 -rw-r–r– 1 stopword.dic —— 停止词典 部署步骤： 把 IKAnalyzer2012_u6.jar 放在部署项目的lib目录下 把 stopword.dic 和 IKAnalyzer.cfg.xml 放在 class根目录，对于web工程时 WEB-INFO/classes 目录下 ​ 工程搭建1、maven 安装 IKAnalyzer2012_u6.jar mvn install:install-file -Dfile=D:\java\oschina\lucene-learn\lucene-chapter2\lib\IKAnalyzer2012_u6.jar -DgroupId=org.wltea.analyzer -DartifactId=IKAnalyzer -Dversion=2012_u6 -Dpackaging=jar -DgeneratePom=true -DcreateChecksum=true 2、在pom.xml中添加 12345&lt;dependency&gt;​ &lt;groupId&gt;org.wltea.analyzer&lt;/groupId&gt;​ &lt;artifactId&gt;IKAnalyzer&lt;/artifactId&gt;​ &lt;version&gt;2012_u6&lt;/version&gt;&lt;/dependency&gt; 3、 stopword.dic 和 IKAnalyzer.cfg.xml 放在 src\main\resources 目录下 这样编译完成后，它就会放在 classes 目录下]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.1、Lucene概述]]></title>
    <url>%2Felasticsearch%2F2-1-Lucene%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Lucene 3个优点1、稳定、索引性能高 2、高效、准确、高性能的搜索算法 3、跨平台解决方案 ​ Lucene架构1、信息采集 —— 文件、数据库、万维网、及手工输入，都能作为采集对象 2、索引文档 —— 即倒排序索引的构建过程 3、搜索文档 —— 用户发起查询 到 拿到结果的过程]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 路由管理]]></title>
    <url>%2Fos%2Fcomputer-network%2Fcentos7%E8%B7%AF%E7%94%B1%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[查看路由：[root@nauru-084 ~]# route -n 添加路由：[root@nauru-084 ~]# route add -host 10.10.87.38 gw 172.18.1.89 [root@nauru-084 ~]# route add -host 10.10.13.39 gw 172.18.1.89]]></content>
      <categories>
        <category>linux</category>
        <category>centos7</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>route</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rocketmq简介]]></title>
    <url>%2Fmq%2Frocketmq-introduce%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka简介]]></title>
    <url>%2Fmq%2Fkafka-introduce%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq简介]]></title>
    <url>%2Fmq%2Frabbitmq-introduce%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Valid Parentheses]]></title>
    <url>%2Fleetcode%2Fvalid-parentheses%2F</url>
    <content type="text"><![CDATA[算法分类：Stack url：https://leetcode.com/problems/valid-parentheses/ 题目 12345678910111213141516171819202120. Valid ParenthesesGiven a string containing just the characters '(', ')', '&#123;', '&#125;', '[' and ']', determine if the input string is valid.An input string is valid if:Open brackets must be closed by the same type of brackets.Open brackets must be closed in the correct order.Note that an empty string is also considered valid.Example 1:Input: "()"Output: trueExample 2:Input: "()[]&#123;&#125;"Output: trueExample 3:Input: "(]"Output: false Java解法： 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public boolean isValid(String s) &#123; Stack&lt;String&gt; stack = new Stack&lt;&gt;(); char[] chArr = s.toCharArray(); for (char ch : chArr) &#123; String top = stack.isEmpty() ? null : stack.peek(); String c = String.valueOf(ch); if (null == top) &#123; stack.push(c); &#125; else if (top.equals("&#123;")) &#123; if (c.equals("&#125;")) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; else if (top.equals("[")) &#123; if (c.equals("]")) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; else if (top.equals("(")) &#123; if (c.equals(")")) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; else &#123; stack.push(c); &#125; &#125; if (stack.isEmpty()) &#123; return true; &#125; else &#123; return false; &#125; &#125;&#125; 12345678910111213141516171819202122class Solution &#123; public boolean isValid(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;(); HashMap&lt;Character, Character&gt; map = new HashMap&lt;Character, Character&gt;() &#123;&#123; put('&#125;', '&#123;'); put(']', '['); put(')', '('); &#125;&#125;; char[] chArr = s.toCharArray(); for (char ch : chArr) &#123; Character top = stack.isEmpty() ? null : stack.peek(); if (null == top) &#123; stack.push(ch); &#125; else if (top == map.get(ch)) &#123; stack.pop(); &#125; else &#123; stack.push(ch); &#125; &#125; return stack.isEmpty(); &#125;&#125; Python解法： 12345678910111213141516class Solution(object): def isValid(self, s): """ :type s: str :rtype: bool """ stack = [] map = &#123;u'&#125;':u'&#123;', u']':u'[', u')':u'('&#125; for ch in s: top = u'#' if (len(stack) == 0) else stack[-1] if map.has_key(ch) and top == map[ch]: stack.pop() else: stack.append(ch) return 0 == len(stack)]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unique Binary Search Trees ii]]></title>
    <url>%2Fleetcode%2Funique-binary-search-trees-ii%2F</url>
    <content type="text"><![CDATA[算法：二叉搜索树 url：https://leetcode-cn.com/problems/unique-binary-search-trees-ii/ 题目 1234567891011121314151617181920给定一个整数 n，生成所有由 1 ... n 为节点所组成的二叉搜索树。示例:输入: 3输出:[ [1,null,3,2], [3,2,null,1], [3,1,null,null,2], [2,1,3], [1,null,2,null,3]]解释:以上的输出对应以下 5 种不同结构的二叉搜索树： 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 分析 首先来计数需要构建的二叉树数量。可能的二叉搜素数数量是一个 卡特兰数。 我们跟随上文的逻辑，只是这次是构建具体的树，而不是计数。 算法 我们从序列 1 ..n 中取出数字 i，作为当前树的树根。于是，剩余 i - 1 个元素可用于左子树，n - i 个元素用于右子树。如 前文所述，这样会产生 G(i - 1) 种左子树 和 G(n - i) 种右子树，其中 G 是卡特兰数。 现在，我们对序列 1 … i - 1 重复上述过程，以构建所有的左子树；然后对 i + 1 … n 重复，以构建所有的右子树。 这样，我们就有了树根 i 和可能的左子树、右子树的列表。 最后一步，对两个列表循环，将左子树和右子树连接在根上。 Java解法 123456789101112131415161718192021222324252627282930313233343536public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if (n &lt;= 0) &#123; return new LinkedList&lt;TreeNode&gt;(); &#125; return listTrees(1, n); &#125; public LinkedList&lt;TreeNode&gt; listTrees(int start, int end) &#123; LinkedList&lt;TreeNode&gt; rootList = new LinkedList&lt;TreeNode&gt;(); if(start &gt; end) &#123; rootList.add(null); return rootList; &#125; for(int i=start; i&lt;=end; ++i) &#123; LinkedList&lt;TreeNode&gt; leftList = listTrees(start, i-1); LinkedList&lt;TreeNode&gt; rightList = listTrees(i+1, end); for(TreeNode left : leftList) &#123; for(TreeNode right : rightList) &#123; TreeNode cur = new TreeNode(i); cur.left = left; cur.right = right; rootList.add(cur); &#125; &#125; &#125; return rootList; &#125;&#125; Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Two Ssum]]></title>
    <url>%2Fleetcode%2Ftwo-sum%2F</url>
    <content type="text"><![CDATA[url：https://leetcode.com/problems/two-sum/submissions/ 题目：Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. Java 解法：12345678910111213class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException("No two sum solution"); &#125;&#125; Python解法：12345678910111213# https://leetcode.com/problems/two-sum/submissions/class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in range(len(nums)): x = target-nums[i]; c = nums[0:i].count(x); if c &gt; 0 and nums.index(x) != i: return [nums.index(x), i]]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单链表反转]]></title>
    <url>%2Fleetcode%2Freverse-linked-list%2F</url>
    <content type="text"><![CDATA[算法：单链表反转 题目 123456这其实是一道变形的链表反转题，大致描述如下给定一个单链表的头节点 head,实现一个调整单链表的函数，使得每K个节点之间为一组进行逆序，并且从链表的尾部开始组起，头部剩余节点数量不够一组的不需要逆序。（不能使用队列或者栈作为辅助）例如：链表:1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;8-&gt;null, K = 3。那么 6-&gt;7-&gt;8，3-&gt;4-&gt;5，1-&gt;2各位一组。调整后：1-&gt;2-&gt;5-&gt;4-&gt;3-&gt;8-&gt;7-&gt;6-&gt;null。其中 1，2不调整，因为不够一组。 解析 这道题的难点在于，是从链表的尾部开始组起的，而不是从链表的头部，如果是头部的话，那我们还是比较容易做的，因为你可以遍历链表，每遍历 k 个就拆分为一组来逆序。但是从尾部的话就不一样了，因为是单链表，不能往后遍历组起； 思路1：整体反转，然后从头开始截取 K个节点 ，被后续截图的K个节点 的尾部连接；最后长度不足K的节点再局部反转 作为 队头 思路2：整体反转， Java解法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Node &#123; int val; Node next; public Node(int d) &#123; this.val = d; &#125;&#125;class Solution &#123; public Node reverse(Node node) &#123; Node pre = node; Node cur = node.next; node.next = null; Node tmp; while(cur!=null) &#123; tmp = cur.next; cur.next = pre; pre = cur; cur = tmp; &#125; return pre; &#125; public Node reverseByK(Node node, int k) &#123; Node tmp = node; for (int i=1; i&lt;k &amp;&amp; tmp != null; i++) &#123; tmp = tmp.next; &#125; if (tmp == null) &#123; return node; &#125; Node childList = tmp.next; tmp.next = null; Node retNode = reverse(node); Node newChildNode = reverseByK(childList, k); node.next = newChildNode; return retNode; &#125; public static void main(String[] args) &#123; Node link = null; Node tmp = link; for(int i=1; i&lt;9; ++i) &#123; if(null == tmp) &#123; tmp = new Node(i); link = tmp; &#125; else &#123; tmp.next = new Node(i); tmp = tmp.next; &#125; &#125; Solution solution = new Solution(); link = solution.reverse(solution.reverseByK(solution.reverse(link), 3)); while(link!=null) &#123; System.out.print(link.val + " "); link = link.next; &#125; &#125;&#125; Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recover Binary Search Tree]]></title>
    <url>%2Fleetcode%2Frecover-binary-search-tree%2F</url>
    <content type="text"><![CDATA[算法：二叉搜索树， 深度优先搜索 url：https://leetcode.com/problems/recover-binary-search-tree/ 题目 123456789101112131415161718192021222324252627282930313233343599. Recover Binary Search TreeTwo elements of a binary search tree (BST) are swapped by mistake.Recover the tree without changing its structure.Example 1:Input: [1,3,null,null,2] 1 / 3 \ 2Output: [3,1,null,null,2] 3 / 1 \ 2Example 2:Input: [3,1,4,null,null,2] 3 / \1 4 / 2Output: [2,1,4,null,null,3] 2 / \1 4 / 3Follow up:A solution using O(n) space is pretty straight forward.Could you devise a constant space solution? 分析 要求，不改变现有树结构的前提，使数恢复为二叉搜索树； 使用DFS便利二叉树，凡是 不符合规则的节点都标记出来； 中序遍历，遍历结果有序，无序的节点就是 不合格的节点 Java解法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; TreeNode pre; TreeNode first, second; public void recoverTree(TreeNode root) &#123; pre = new TreeNode(Integer.MIN_VALUE); inorder(root); swap(first, second); &#125; public void inorder(TreeNode root) &#123; if(null == root) return; inorder(root.left); if(pre.val &gt;= root.val &amp;&amp; pre.val != Integer.MIN_VALUE) &#123; if(null == first) first = pre; second = root; &#125; pre = root; inorder(root.right); &#125; public void swap(TreeNode node1, TreeNode node2) &#123; int tmp = node1.val; node1.val = node2.val; node2.val = tmp; &#125;&#125;public class MainClass &#123; public static TreeNode stringToTreeNode(String input) &#123; input = input.trim(); input = input.substring(1, input.length() - 1); if (input.length() == 0) &#123; return null; &#125; String[] parts = input.split(","); String item = parts[0]; TreeNode root = new TreeNode(Integer.parseInt(item)); Queue&lt;TreeNode&gt; nodeQueue = new LinkedList&lt;&gt;(); nodeQueue.add(root); int index = 1; while(!nodeQueue.isEmpty()) &#123; TreeNode node = nodeQueue.remove(); if (index == parts.length) &#123; break; &#125; item = parts[index++]; item = item.trim(); if (!item.equals("null")) &#123; int leftNumber = Integer.parseInt(item); node.left = new TreeNode(leftNumber); nodeQueue.add(node.left); &#125; if (index == parts.length) &#123; break; &#125; item = parts[index++]; item = item.trim(); if (!item.equals("null")) &#123; int rightNumber = Integer.parseInt(item); node.right = new TreeNode(rightNumber); nodeQueue.add(node.right); &#125; &#125; return root; &#125; public static String treeNodeToString(TreeNode root) &#123; if (root == null) &#123; return "[]"; &#125; String output = ""; Queue&lt;TreeNode&gt; nodeQueue = new LinkedList&lt;&gt;(); nodeQueue.add(root); while(!nodeQueue.isEmpty()) &#123; TreeNode node = nodeQueue.remove(); if (node == null) &#123; output += "null, "; continue; &#125; output += String.valueOf(node.val) + ", "; nodeQueue.add(node.left); nodeQueue.add(node.right); &#125; return "[" + output.substring(0, output.length() - 2) + "]"; &#125; public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); String line; while ((line = in.readLine()) != null) &#123; TreeNode root = stringToTreeNode(line); new Solution().recoverTree(root); String out = treeNodeToString(root); System.out.print(out); &#125; &#125;&#125; Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>深度优先搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Longest Palindromic Substring]]></title>
    <url>%2Fleetcode%2Flongest-palindromic-substring%2F</url>
    <content type="text"><![CDATA[算法：动态规划 URL：https://leetcode-cn.com/problems/longest-palindromic-substring/ 题目 1234567891011给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。示例 1：输入: "babad"输出: "bab"注意: "aba" 也是一个有效答案。示例 2：输入: "cbbd"输出: "bb" 分析 Java解法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.Stack;public class Solution &#123; public String longestPalindrome(String s) &#123; String longestS=null, curS=null; for(int i=0; i&lt;s.length(); ++i) &#123; curS = palindrome(s.toCharArray(), i); if (null == curS) continue; if (null == longestS) longestS = curS; else &#123; if (longestS.length() &lt;= curS.length()) longestS = curS; &#125; &#125; return (null == longestS) ? "" : longestS; &#125; public String palindrome(char[] chArr, int index) &#123; Stack&lt;Character&gt; stack1 = new Stack&lt;Character&gt;(); Stack&lt;Character&gt; stack2 = new Stack&lt;Character&gt;(); int len1=0, len2=0; if(index-1&gt;=0 &amp;&amp; chArr[index] == chArr[index-1]) &#123; for(int i=index-1; i&gt;=0; --i) &#123; int delta = index-1-i; if(index+delta&gt;=chArr.length) break; if(chArr[i] == chArr[index+delta]) &#123; stack1.push(chArr[i]); &#125; else &#123; break; &#125; &#125; len1 = stack1.size()*2; &#125; if(index-2&gt;=0 &amp;&amp; chArr[index] == chArr[index-2]) &#123; stack2.push(chArr[index-1]); for(int i=index-2; i&gt;=0; --i) &#123; int delta = index-2-i; if(index+delta&gt;=chArr.length) break; if(chArr[i] == chArr[index+delta]) &#123; stack2.push(chArr[i]); &#125; else &#123; break; &#125; &#125; len2 = stack2.size()*2 - 1; &#125; // System.out.printf("%d, %d %c \n", len1, len2, chArr[index]); if (0==len1 &amp;&amp; 0==len2) return String.valueOf(chArr[index]); if (len1&gt;len2) &#123; char[] res = new char[len1]; for(int i=0; !stack1.isEmpty(); ++i) &#123; char c = stack1.pop().charValue(); res[i] = res[len1-1-i] = c; &#125; return String.copyValueOf(res); &#125; else &#123; char[] res = new char[len2]; for(int i=0; !stack2.isEmpty(); ++i) &#123; char c = stack2.pop().charValue(); res[i] = res[len2-1-i] = c; &#125; return String.copyValueOf(res); &#125; &#125; public static void main(String[] args) &#123; String s = "ba"; String ret = new Solution().longestPalindrome(s); String out = (ret); System.out.print(out); &#125;&#125; 同样解法的代码优化 1234567891011121314151617181920212223242526class Solution &#123; public String longestPalindrome(String s) &#123; if (s == null || s.length() &lt; 1) return ""; int start = 0, end = 0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expandAroundCenter(s, i, i); int len2 = expandAroundCenter(s, i, i + 1); int len = Math.max(len1, len2); if (len &gt; end - start) &#123; start = i - (len - 1) / 2; end = i + len / 2; &#125; &#125; return s.substring(start, end + 1); &#125; private int expandAroundCenter(String s, int left, int right) &#123; int L = left, R = right; while (L &gt;= 0 &amp;&amp; R &lt; s.length() &amp;&amp; s.charAt(L) == s.charAt(R)) &#123; L--; R++; &#125; return R - L - 1; &#125;&#125; 还有一种动态规划的解法，需要接着研究 Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kth Largest Element in an Array]]></title>
    <url>%2Fleetcode%2Fkth-largest-element-in-an-array%2F</url>
    <content type="text"><![CDATA[算法：Heap 堆题目url:https://leetcode.com/problems/kth-largest-element-in-an-array/ 1234567891011121314215. Kth Largest Element in an ArrayFind the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element.Example 1:Input: [3,2,1,5,6,4] and k = 2Output: 5Example 2:Input: [3,2,3,1,2,4,5,5,6] and k = 4Output: 4Note: You may assume k is always valid, 1 ≤ k ≤ array's length. 分析用数据结构 Head（堆）来实现 堆：完全二叉树，常常用数组表示 用数组表示一棵树时，如果数组中节点的索引位x，则a、它的父节点的下标是：(x-1)/2；b、它的左子节点的下标为：2x + 1；c、它的右子节点的下标是：2x + 2； 堆的数组实现：https://www.cnblogs.com/g177w/p/8469399.html Java解法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133class Solution &#123; class Node &#123; private int data; public Node(int d) &#123; this.data = d; &#125; public int getValue() &#123; return this.data; &#125; public void setValue(int d) &#123; this.data = d; &#125; &#125; class Head &#123; private Node[] headArray; private int maxSize; private int currentSize; public Head(int maxSize) &#123; this.maxSize = maxSize; currentSize = 0; headArray = new Node[maxSize]; &#125; public boolean isEmpty() &#123; return 0 == currentSize; &#125; public boolean insert(int d) &#123; if (this.currentSize == this.maxSize) &#123; return false; &#125; Node node = new Node(d); headArray[currentSize] = node; trickleUp(currentSize++); return true; &#125; public void trickleUp(int index) &#123; int parentIndex = (index - 1) / 2; Node node = headArray[index]; while (index &gt; 0 &amp;&amp; headArray[parentIndex].getValue() &lt; node.getValue()) &#123; headArray[index] = headArray[parentIndex]; index = parentIndex; parentIndex = (index - 1) / 2; &#125; headArray[index] = node; &#125; public Node remove() &#123; Node root = headArray[0]; headArray[0] = headArray[--currentSize]; trickleDown(0); return root; &#125; public void trickleDown(int index) &#123; int largeChild; Node node = headArray[index]; while (index &lt; currentSize/2) &#123; int leftChild = 2 * index + 1; int rightChild = 2 * index +2; if (rightChild &lt; currentSize &amp;&amp; headArray[leftChild].getValue() &lt; headArray[rightChild].getValue()) &#123; largeChild = rightChild; &#125; else &#123; largeChild = leftChild; &#125; if (node.getValue() &gt;= headArray[largeChild].getValue()) &#123; break; &#125; headArray[index] = headArray[largeChild]; index = largeChild; &#125; headArray[index] = node; &#125; public boolean change(int index, int newValue) &#123; if (index &lt; 0 || index &gt; currentSize) &#123; return false; &#125; int oldValue = headArray[index].getValue(); headArray[index].setValue(newValue); if (oldValue &lt; newValue) &#123; trickleUp(index); &#125; else &#123; trickleDown(index); &#125; return true; &#125; public void displayHead() &#123; System.out.print("headArray:"); for (int i = 0; i &lt; currentSize; i++) &#123; if (headArray[i] != null) System.out.print(headArray[i].getValue()+" "); else System.out.print("--"); &#125; System.out.println(""); int nBlanks = 32; int itemsPerrow = 1; int column = 0; int j = 0; String dots = "........................"; System.out.println(dots + dots); while (currentSize &gt; 0)&#123; if (column == 0) for (int i = 0; i &lt; nBlanks; i++) &#123; System.out.print(" "); &#125; System.out.print(headArray[j].getValue()); if (++ j == currentSize) break; if (++ column == itemsPerrow)&#123; nBlanks /= 2; itemsPerrow *= 2; column = 0; System.out.println(); &#125; else for (int i = 0; i &lt; nBlanks * 2 - 2; i++) System.out.print(' '); &#125; System.out.println("\n"+dots + dots); &#125; &#125; public int findKthLargest(int[] nums, int k) &#123; Head head = new Solution().new Head(nums.length); for (int x : nums) &#123; head.insert(x); &#125; for (int i=0; i&lt;k-1; ++i) &#123; head.remove(); &#125; int ret = head.remove().getValue(); return ret; &#125;&#125; Python解法12]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[h index ii]]></title>
    <url>%2Fleetcode%2Fh-index-ii%2F</url>
    <content type="text"><![CDATA[算法：url：https://leetcode.com/problems/h-index-ii/ 题目1234567891011121314151617181920Given an array of citations sorted in ascending order (each citation is a non-negative integer) of a researcher, write a function to compute the researcher's h-index.According to the definition of h-index on Wikipedia: "A scientist has index h if h of his/her N papers have at least h citations each, and the other N − h papers have no more than h citations each."Example:Input: citations = [0,1,3,5,6]Output: 3 Explanation: [0,1,3,5,6] means the researcher has 5 papers in total and each of them had received 0, 1, 3, 5, 6 citations respectively. Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, her h-index is 3.Note:If there are several possible values for h, the maximum one is taken as the h-index.Follow up:This is a follow up problem to H-Index, where citations is now guaranteed to be sorted in ascending order.Could you solve it in logarithmic time complexity? 思路分析[0,1,3,5,6] 数组长度 n，存在一个元素 h， 使得 n 中有 h个元素 大于等于h， 其他 （n-h）个元素 &lt; h； 求h？ 数组是有序的 递增数组 遍历 arr，存在 arr[i], 使得 n-i == arr[i] Java解法12345678910class Solution &#123; public int hIndex(int[] citations) &#123; for(int i=citations.length-1; i&gt;=0; --i) &#123; if(citations[i] == citations.length-i) &#123; return citations[i]; &#125; &#125; return 0; &#125;&#125; 1234567891011121314151617181920public int hIndex(int[] citations) &#123; int l = 1, r = citations.length; int ans = 0; while(l &lt;= r)&#123; int m = l + ((r - l)&gt;&gt;1); int p = citations[citations.length - m]; if(m == p)&#123; return m; &#125; if(m &gt; p)&#123; r = m - 1; &#125; else&#123; ans = Math.max(ans, m); l = m + 1; &#125; &#125; return ans; &#125;&#125; Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Tree Right Side View]]></title>
    <url>%2Fleetcode%2Fbinary-tree-right-side-view%2F</url>
    <content type="text"><![CDATA[算法：二叉树广度优先遍历 BFS URL：https://leetcode.com/problems/binary-tree-right-side-view/ 题目 1234567891011121314151617181920212223242526Binary Tree Right Side ViewGiven a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.Example:Input: [1,2,3,null,5,null,4]Output: [1, 3, 4]Explanation: 1 &lt;--- / \2 3 &lt;--- \ \ 5 4 &lt;---Input: [1,2,3,null,5,null,4,null,null,8]Output: [1, 3, 4, 8]Explanation: 1 &lt;--- / \ 2 3 &lt;--- \ \ 5 4 &lt;--- / 8 &lt;--- 分析 使用层序遍历（level traversal），即广度优先搜索 —— 借助Queue 难点是 标注每一层，来标注 每层的最后一个节点 当前层节点总数：count1，当前访问到节点数 i 下一层几点总数：count2 Java解法 12345678910111213141516171819202122232425262728293031323334353637383940public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; return BFS(root); &#125; public List&lt;Integer&gt; BFS(TreeNode root) &#123; List&lt;Integer&gt; retList = new ArrayList&lt;Integer&gt;(); if(null == root) return retList; int count1=1, count2=0; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); while(!queue.isEmpty()) &#123; for(int i=0; i&lt;count1; ++i) &#123; TreeNode tn = queue.poll(); if(null != tn) &#123; if (i == count1-1) &#123; retList.add(tn.val); &#125; if(null != tn.left) &#123; queue.add(tn.left); count2++; &#125; if (null != tn.right) &#123; queue.add(tn.right); count2++; &#125; &#125; &#125; count1 = count2; count2 = 0; &#125; return retList; &#125;&#125; 结果 Runtime: 1 ms, faster than 98.22% of Java online submissions for Binary Tree Right Side View. Memory Usage: 36.3 MB, less than 100.00% of Java online submissions for Binary Tree Right Side View. 1次AC，Niubility Python解法 12]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>广度优先遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Tree Maximum Path Sum]]></title>
    <url>%2Fleetcode%2Fbinary-tree-maximum-path-sum%2F</url>
    <content type="text"><![CDATA[算法：二叉树 + 动态规划 url：https://leetcode.com/problems/binary-tree-maximum-path-sum/ 题目 12345678910111213141516171819202122232425124. Binary Tree Maximum Path SumGiven a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root.Example 1:Input: [1,2,3] 1 / \ 2 3Output: 6Example 2:Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 分析： 不要求 最大值的路径，只要求最大值 Java解法 1234567891011121314151617181920public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; int ret = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; return maxSum(root); &#125; public int maxSum(TreeNode root) &#123; if (null == root) return 0; int l = Math.max(0, maxSum(root.left)); int r = Math.max(0, maxSum(root.right)); int sum = l + r + root.val; ret = Math.max(ret, sum); return Math.max(l, r) + root.val; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Add Two Numbers]]></title>
    <url>%2Fleetcode%2Fadd-two-numbers%2F</url>
    <content type="text"><![CDATA[算法：链表 Linked list url：https://leetcode.com/problems/add-two-numbers/ 题目：Add Two Numbers 1234567You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself.Example:Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 分析 百位大数相加 Java解法 方法 定义 链表增加节点 单链表反转 不需要，只是为了练习算法 遍历链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110package com.test.demo;import java.io.IOException;class ListNode &#123; int val; ListNode next; public ListNode(int x) &#123;this.val = x;&#125;&#125;class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode retNode = null; int carryBit = 0; while(l1 != null || l2!= null) &#123; int d1 = null==l1 ? 0 : l1.val; int d2 = null==l2 ? 0 : l2.val; int d = d1 + d2 + carryBit; retNode = add(retNode, new ListNode(d%10)); carryBit = d/10; l1 = null==l1 ? l1 : l1.next; l2 = null==l2 ? l2 : l2.next; &#125; for (int i = carryBit; i&gt;0; i = i/10) &#123; retNode = add(retNode, new ListNode(i%10)); &#125; return retNode;// return reverse(retNode); &#125; public ListNode add(ListNode headNode, ListNode node) &#123; if (null == headNode) &#123; headNode = node; &#125; else &#123; ListNode index = headNode; while(index.next != null) &#123;index = index.next;&#125; index.next = node; &#125; return headNode; &#125; public ListNode reverse(ListNode headNode) &#123; if(headNode == null) return headNode; ListNode pre = headNode; ListNode cur; ListNode temp; for(cur = headNode.next; cur!=null; ) &#123; temp = cur.next; cur.next = pre; pre = cur; cur = temp; &#125; headNode.next = null; return pre; &#125;&#125;public class MainClass &#123; public static int[] stringToIntegerArray(String input) &#123; input = input.trim(); input = input.substring(1, input.length() - 1); if (input.length() == 0) &#123; return new int[0]; &#125; String[] parts = input.split(","); int[] output = new int[parts.length]; for(int index = 0; index &lt; parts.length; index++) &#123; String part = parts[index].trim(); output[index] = Integer.parseInt(part); &#125; return output; &#125; public static ListNode stringToListNode(String input) &#123; // Generate array from the input int[] nodeValues = stringToIntegerArray(input); // Now convert that list into linked list ListNode dummyRoot = new ListNode(0); ListNode ptr = dummyRoot; for(int item : nodeValues) &#123; ptr.next = new ListNode(item); ptr = ptr.next; &#125; return dummyRoot.next; &#125; public static String listNodeToString(ListNode node) &#123; if (node == null) &#123; return "[]"; &#125; String result = ""; while (node != null) &#123; result += Integer.toString(node.val) + ", "; node = node.next; &#125; return "[" + result.substring(0, result.length() - 2) + "]"; &#125; public static void main(String[] args) throws IOException &#123; ListNode l1 = stringToListNode("[9]"); ListNode l2 = stringToListNode("[1,9,9,9,9,9,9,9,9,9]"); ListNode ret = new Solution().addTwoNumbers(l1, l2); String out = listNodeToString(ret); System.out.print(out); &#125;&#125; Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Linked list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java异步回调demo]]></title>
    <url>%2Fjava%2Fasync-callback-demo%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.*;interface ICallback &#123; void getResult(int s);&#125;class Calc &#123; public void calc(int i, ICallback callback) throws Exception &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int s = i * i; Thread.sleep(5000); System.out.println("do in future task. " + Thread.currentThread().getName()); callback.getResult(s); return s; &#125; &#125;); new Thread(futureTask).start(); &#125;&#125;public class App &#123; public static void main(String[] args) throws Exception &#123; ICallback callback = new ICallback() &#123; @Override public void getResult(int s) &#123; System.out.println(s); &#125; &#125;; new Calc().calc(1000, callback); System.out.println("do in main"); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>juc</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Callable</tag>
        <tag>Callback</tag>
        <tag>FutureTask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to 0ms AC]]></content>
  </entry>
</search>
