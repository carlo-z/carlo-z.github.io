<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[从单体迈向Serverless的避坑指南]]></title>
    <url>%2Fdevops%2Fserverless%2Ffrom-single-to-serverless%2F</url>
    <content type="text"><![CDATA[简介： 用户需求和云的发展两条线推动了云原生技术的兴起、发展和大规模应用。本文将主要讨论什么是云原生应用，构成云原生应用的要素是什么，什么是Serverless 计算，以及Serverless如何简化技术复杂度，帮助用户应对快速变化的需求，实现弹性、高可用的服务，并通过具体的案例和场景进行说明。 作者 | 不瞋来源 | 凌云时刻（微信号：linuxpk） 前言如今，各行各业都在谈数字化转型，尤其是新零售、传媒、交通等行业。数字化的商业形态已经成为主流，逐渐替代了传统的商业形态。在另外一些行业里（如工业制造），虽然企业的商业形态并非以数字化的形式表现，但是在数字孪生理念下，充分利用数据科技进行生产运营优化也正在成为研究热点和行业共识。 企业进行数字化转型，从生产资料、生产关系、战略规划、增长曲线四个层面来看： 生产资料：数据成为最重要的生产资料，需求/风险随时变化，企业面临巨大的不确定性。 生产关系：数据为中心，非基于流程和规则的固定生产关系。网络效应令生产关系跨越时空限制，多连接方式催生新的业务和物种。 战略规划：基于数据决策，快速应对不确定的商业环境。 增长曲线：数字化技术带来触达海量用户的能力，可带来突破性的增长。 从云服务商的角度来看云的演进趋势，在Cloud 1.0时代，基础设施的云化是其主题，采用云托管模式，云上云下的应用保持兼容，传统的应用可以直接迁移到云上，这种方式的核心价值在于资源的弹性和成本的低廉；在基础设施提供了海量算力之后，怎么帮助用户更好地利用算力，加速企业创新的速度，就成为云的核心能力。 如果仍在服务器上构建基础应用，那么研发成本就会很高，管理难度也很大，因此有了Cloud 2.0，也就是云原生时代。在云原生时代，云服务商提供了丰富的托管服务，助力企业数字化转型和创新，用户可以像搭积木一样基于各种云服务来构建应用，大大降低了研发成本。 云原生应用要素云原生应用有三个非常关键的要素：微服务架构，应用容器化和Serverless化，敏捷的软件交付流程。 微服务架构 单体架构和微服务架构各有各的特点，其主要特点对比如下图所示。总的来说，单体架构上手快，但是维护难，微服务架构部署较难，但是独立性和敏捷性更好，更适合云原生应用。 应用容器化和Serverless化 容器是当前最流行的代码封装方式，借助K8s及其生态的能力，大大降低了整个基础设施的管理难度，而且容器在程序的支撑性方面提供非常出色的灵活性和可移植性，越来越多的用户开始使用容器来封装整个应用。 Serverless计算是另外一种形态，做了大量的端到端整合和云服务的集成，大大提高了研发效率，但是对传统应用的兼容性没有容器那么灵活，但是也带来了很大的整洁性，用户只需要专注于业务逻辑的编码，聚焦于业务逻辑的创新即可。 敏捷的应用交付流程 敏捷的应用交付流程是非常重要的一个要素，主要包括流程自动化，专注于功能开发，快速发现问题，快速发布上线。 Serverless 计算 阿里云函数计算 Serverless是一个新的概念，但是其内涵早就已经存在。阿里云或者AWS的第一个云服务都是对象存储，对象储存实际上就是一个存储领域的Serverless服务；另外，Serverless指的是一个产品体系，而不是单个产品。当前业界云服务商推出的新功能或者新产品绝大多数都是Serverless形态的。阿里云Serverless产品体系包括计算、存储、API、分析和中间件等，目前云的产品体系正在Serverless化。 阿里云Serverless计算平台函数计算，有4个特点： 和云端无缝集成：通过事件驱动的方式将云端的各种服务与函数计算无缝集成，用户只需要关注函数的开发，事件的触发等均由服务商来完成。 实时弹性伸缩：由系统自动完成函数计算的弹性伸缩，且速度非常快，用户可以将这种能力用在在线应用上。 次秒级计量：次秒级的计量方式提供了一种完全的按需计量方式，资源利用率能达到百分之百。 高可用：函数计算平台做了大量工作帮助用户构建高可用的应用。 那么，阿里云函数计算是如何做到以上4点呢？阿里云函数计算的产品能力大图如下图所示，首先函数计算产品是建立在阿里巴巴的基础设施服务之上的产品，对在其之上的计算层进行了大量优化。接着在应用层开发了大量能力和工具，基于以上产品能力，为用户提供多种场景下完整的解决方案，才有了整个优秀的函数计算产品。函数计算是阿里云的一个非常基础的云产品，阿里云的许多产品和功能均是建立在函数计算的基础上。目前阿里云函数计算已经在全球19个区域提供服务。 Serverless帮助用户简化云原生应用高可用设计、实施的复杂度 云原生应用的高可用是一个系统的工程，包括众多方面，完整的高可用体系构建需要很多时间和精力。那么Serverless计算是如何帮助用户简化云原生应用高可用设计、实施的复杂度呢？ 如下图所示，高可用体系建设要考虑的点包括基础设施层、运行时层、数据层以及应用层，且每一层都有大量的工作要做才可以实现高可用。函数计算主要是从容错、弹性、流控、监控四方面做了大量工作来实现高可用，下图中蓝色虚线框所对应的功能均由平台来实现，用户是不需要考虑的。蓝色实线框虽然平台做了一些工作来简化用户的工作难度，但是仍需要用户来进行关注，而橘红色的实线框代表需要用户去负责的部分功能。结合平台提供的功能和用户的部分精力投入，可以极大地减轻用户进行高可用体系建设的难度。 函数计算在很多方面做了优化来帮助用户建设高可用体系。下图展示了函数计算在可用区容灾方面的能力。从图中可知，函数计算做了相应的负载均衡，使得容灾能力大大提升。 下图展示的是函数计算对事件的异步处理，其处理流水线主要包括事件队列、事件分发、事件消费三个环节，在每一个环节上都可以进行水平伸缩，其中一个比较关键的点是事件的分发需要匹配下游的消费能力。另外，通过为不同函数指定不同数量的计算资源，用户能方便地动态调整不同类型事件的消费速度。此外，还可以自定义错误重试逻辑，并且有背压反馈和流控，不会在短时间内产生大量请求时压垮下一个服务。 在函数计算的可观测性上面，提供了日志收集和查询功能，除了默认的简单日志查询功能外，还提供了高级日志查询，用户可以更方便地进行日志分析。在指标收集和可视化方面，函数计算提供了丰富的指标收集能力，并且提供了标准指标、概览信息等视图，可以更方便用户进行运维工作。 下图是应用交付的一个示意图，在整个应用的交付过程中，只有每个环节都做好，才能够建设一个敏捷的应用交付流程，其核心是自动化，只有做到了自动化，才能提升整个流水线的效率和敏捷度。 下图展示了自动化应用交付流水线在每个环节的具体任务。其中需要注意的是做到基础设施即代码，才能进行模板定义和自动化设置应用运行环境，进而实现自动化的持续集成等。 做到了应用的自动化交付之后，对整个研发效率的帮助是非常大的。在Serverless应用上，阿里云提供了多种工具来帮助用户实现基础设施即代码。Serverless的模型有一个很好的能力，就是同一份模板可以传入不同的参数，进而生成不同环境的定义，然后通过自动化地管理这些环境。 对于应用本身不同服务版本的交付和灰度发布，函数计算提供了服务版本和服务别名来提供相应的服务，整个应用的灰度发布流程可以简化成一些API的操作，大大提升业务的效率。通过Serverless计算平台提供的这些能力，整个软件应用的交付流水线自动化程度得到了大幅度的提高。 函数计算还有一个很有用的功能——对存量应用的兼容性。通过Custom runtime，能够适配很多的流行框架，兼容传统应用，使其能够很容易地适配到Serverless平台上面，由控制台提供应用的创建、部署、关联资源管理、监控等一系列服务。 除了函数计算，还可以用Serverless工作流对不同的应用环节、不同的函数进行编排，通过描述性的语言去定义工作流，由其可靠地执行每一个步骤，这就大幅度降低用户对于复杂任务的编排难度。 应用场景案例函数计算有几个典型的应用场景，一个就是Web/API后端服务，阿里云已经有包括石墨文档、微博、世纪华联在内的多个成功应用案例。 函数计算的另外一个应用场景就是大规模的数据并行处理，比如往OSS上面上传大量的图片、音频、文本等数据，可以触发函数做自定义的处理，比如转码、截帧等。这方面的成功案例包括虎扑、分众传媒、百家互联等。 函数计算还有一个应用场景就是数据实时流式处理，比如不同的设备产生的消息、日志发送到消息队列等管道类似的服务中，就可以触发函数来进行流式处理。 最后一个应用场景就是运维的自动化，通过定时触发、云监控事件触发、流程编排等方式调用函数完成运维任务，大大降低运维成本和难度，典型的成功案例有图森未来等。 图森未来是一家专注于L4级别无人驾驶卡车技术研发与应用的人工智能企业，面向全球提供可大规模商业化运营的无人驾驶卡车技术，为全球物流运输行业赋能。在路测过程中会有大量数据产生，而对这些数据的处理流程复杂多变，即使对于同一批数据，不同的业务小组也会有不同的使用及处理方式。如何有效管理不同的数据处理流程、降低人为介入频率能够大幅的提高生产效率。 路测不定时运行的特点使得流程编排任务运行时间点、运行时长具有极大的不确定性，本地机房独自建立流程管理系统难以最大优化机器利用率，造成资源浪费。而图森未来本地已有许多单元化业务处理脚本及应用程序，但因为各种限制而无法全量的迁移上云，这也对如何合理化使用云上服务带来了挑战。 针对上述情况，图森未来开始探索数据处理平台的自动化。阿里云 Serverless 工作流按执行调度的次数计费，具有易用易集成、运维简单等诸多优点，能够很好的解决上述场景中所遇到的问题，非常适合这类不定时运行的离线任务场景。 Serverless 工作流还支持编排本地或自建机房的任务，图森未来通过使用Serverless 工作流原生支持的消息服务MNS解决了云上云下的数据打通问题，使得本地的原有任务得到很好的编排及管理。 除了调度外，Serverless 工作流也支持对任务的状态及执行过程中所产生的数据进行维护。图森未来通过使用任务的输入输出映射及状态汇报机制，高效的管理了流程中各任务的生命周期及相互间的数据传递。 在未来，随着业务规模的扩大，图森未来将持续优化离线大数据处理流程的运行效率及自动化水平。通过各种探索，图森未来将进一步提升工程团队的效率，将更多的精力和资金投入到业务创新中去。 总结Serverless 工作流是阿里云 Serverless 产品体系中的关键一环。通过 Serverless 工作流，用户能够将函数计算、视觉智能平台等多个阿里云服务，或者自建的服务，以简单直观的方式编排为工作流，迅速构建弹性高可用的云原生应用。 自2017年推出函数计算起，该服务根据应用负载变化实时智能地弹性扩缩容，1分钟完成上万实例的伸缩并保证稳定的延时。目前已经支撑微博、芒果TV、华大基因、图森未来、石墨科技等用户的关键应用，轻松应对业务洪峰。]]></content>
      <categories>
        <category>serverless</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink 年度学习资料大礼包]]></title>
    <url>%2Fbig-data%2Fflink%2Fday4-flink-learn-gift-pack%2F</url>
    <content type="text"><![CDATA[Flink 年度学习资料大礼包！大数据实时计算及 Apache Flink 年度Flink 年度学习资料大礼包，300+页实战应用精华总结！ 零基础入门，30 天成长为 Flink 大神的经典教程。 Apache Flink 核心贡献者及阿里巴巴技术专家的一线实战经验总结。 收录来自 bilibili、美团点评、小米、OPPO、快手、Lyft、Netflix 等国内外一线大厂实时计算平台及实时数仓最佳实践案例。 点击免费下载：《零基础入门：从 0 到 1 学会 Apache Flink》 《Apache Flink 年度最佳实践》 《Apache Flink 十大技术难点实战》 为什么要学习 Flink?面对全量数据和增量数据，能否用一套统一的大数据引擎技术来处理？ Apache Flink 被业界公认为最好的流计算引擎，其计算能力不仅仅局限于做流处理，而是一套兼具流、批、机器学习等多种计算功能的大数据引擎，用户只需根据业务逻辑开发一套代码，无论是全量数据还是增量数据，亦或者实时处理，一套方案即可全部支持。为了让大家更全面地了解 Apache Flink 背后的技术以及应用实践，Flink 社区推出年度学习资料大礼包！马上下载，越早学习，越能抓住时代先机！ 《零基础入门：从 0 到 1 学会 Apache Flink》Apache Flink 零基础入门系列教程重磅发布！由多位 Flink PMC 及核心贡献者出品，帮你建立系统框架体系，最详细的免费教程，Flink 入门必读经典！ 点击免费下载《零基础入门：从 0 到 1 学会 Apache Flink》&gt;&gt;&gt; 课程亮点 实现从0到1了解 Flink 建立 Flink 的系统框架体系，为大数据引擎学习打下基础。 通过实际案例，带你快速上手 Flink 这个分布式、高性能、高可用、高精确的为数据流应用而生的开源流式处理框架，带你领略计算之美。 课程内容侧重于原理解析与基础应用，通过对Flink流计算的概念、技术原理、实践操作等详细解析，从最实际的应用场景出发引导你深入了解Flink，帮助你从 Flink 小白成长为 Flink 技术专家。 课程目录 Apache Flink 进阶（一）：Runtime 核心机制剖析 Apache Flink 进阶（二）：时间属性深度解析 Apache Flink 进阶（三）：Checkpoint 原理剖析与应用实践 Apache Flink 进阶（四）：Flink on Yarn/K8s 原理剖析及实践 Apache Flink 进阶（五）：数据类型和序列化 Apache Flink 进阶（六）：Flink 作业执行深度解析 Apache Flink 进阶（七）：网络流控及反压剖析 Apache Flink 进阶（八）：详解 Metrics 原理与实战 Apache Flink 进阶（九）：Flink Connector 开发 Apache Flink 进阶（十）：Flink State 最佳实践 Apache Flink 进阶（十一）：TensorFlow On Flink Apache Flink 进阶（十二）：深度探索 Flink SQL Apache Flink 进阶（十三）：Python API 应用实践 《Apache Flink 十大技术难点实战》Apache Flink 核心贡献者及阿里巴巴技术专家总结 Flink 生产环境应用的十大常见难点，10 篇技术实战文章帮你完成故障识别、问题定位、性能优化等全链路过程，实现从基础概念的准确理解到上手实操的精准熟练，从容应对生产环境中的技术难题！ 点击免费下载《Apache Flink 十大技术难点实战》&gt;&gt;&gt; 目录 深度解读 ｜102万行代码，1270个问题，Flink 1.10 发布了什么？ 从开发到生产上线，如何确定集群规划大小? Demo：基于 Flink SQL 构建流式应用 Flink Checkpoint 问题排查实用指南 如何分析及处理 Flink 反压？ Flink on YARN（上）：一张图轻松掌握基础架构与启动流程 Flink on YARN（下）：常见问题与排查思路 Apache Flink与Apache Hive的集成 Flink Batch SQL 1.10 实践 如何在 PyFlink 1.10 中自定义 Python UDF？ Flink 1.10 Native Kubernetes 原理与实践 在大数据的日常场景中，从数据生产者，到数据收集、数据处理、数据应用（BI+AI），整个大数据 + AI 全栈的每个环节，Flink 均可应用于其中。作为新一代开源大数据计算引擎，Flink 不仅满足了工业界对实时性的需求，还能够打通端到端的数据价值挖掘全链路。 《Apache Flink 年度最佳实践》首次一次性公布来自 bilibili、美团点评、小米、快手、菜鸟、Lyft、Netflix 等精彩内容，9 篇深度文章揭秘一线大厂实时计算平台从无到有到有、持续优化的详细细节！不容错过的精品电子书，大数据工程师必读实战“真经”！ 点击免费下载《Apache Flink 年度最佳实践》&gt;&gt;&gt; 目录 仅1年GitHub Star数翻倍，Apache Flink 做了什么？ Lyft基于Apache Flink的大规模准实时数据分析平台 Apache Flink在快手实时多维分析场景的应用 Bilibili基于Apache Flink的平台化探索与实践 美团点评基于 Apache Flink 的实时数仓平台实践 小米流式平台架构演进与实践 Netflix：Evolving Keystone to an Open Collaborative Real-time ETL Platform OPPO 基于 Apache Flink 的实时数仓实践 菜鸟供应链实时数仓的架构演进及应用场景 从媒体的最新资讯推送，到购物狂欢的实时数据大屏，甚至城市级计算的工业大脑，实时计算已经应用到了多个生活、工作场景，随着业务的快速增长，企业对大数据处理的需求越来越高，Flink的应用也越来越广泛，相信在不久的将来，Flink将会成为各行业不同规模企业主流的大数据处理框架，并最终成为下一代大数据处理框架的标准。越早学习，越能抓住时代先机。 阿里云开发者社区——藏经阁系列电子书，汇聚了一线大厂的技术沉淀精华，爆款不断。点击链接获取海量免费电子书：https://developer.aliyun.com/topic/ebook]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink 1.11 究竟有哪些易用性上的改善？]]></title>
    <url>%2Fbig-data%2Fflink%2Fflink-1.11-what-the-improve-of-easy-to-use%2F</url>
    <content type="text"><![CDATA[7月7日，Flink 1.11.0 正式发布了，作为这个版本的 release manager 之一，我想跟大家分享一下其中的经历感受以及一些代表性 feature 的解读。在进入深度解读前，我们先简单了解下社区发布的一般流程，帮助大家更好的理解和参与 Flink 社区的工作。 首先在每个版本的规划初期，会从志愿者中选出 1-2 名作为 Release Manager。1.11.0 版本我作为中国这边的 Release Manager，同时还有一名来自 Ververica 的 Piotr Nowojski 作为德国方的 Release Manager，这在某种程度上也说明中国的开发者和贡献度在整个社区的占比很重要。 接下来会进行这个版本的 Feature Kickoff。在一些大的方向上，社区的规划周期可能比较久，会分阶段、分步骤跨越多个版本完成，确保质量。每个版本的侧重点也会有所不同，比如前两个版本侧重于批处理的加强，而这个版本更侧重于流处理易用性的提升。社区规划的 Feature 列表会在邮件列表中发起讨论，以收集更多的用户/开发者意见和反馈。 一般的开发周期为 2-3 个月时间，提前会明确规划出大概的 Feature Freeze 时间，之后进行 Release Candidate 的发布和测试、以及 Bug Fix。一般经过几轮的迭代周期后会正式投票通过一个相对稳定的 Candidate 版本，然后基于这个版本正式发布。 Flink 1.11.0 从 3 月初的功能规划到 7 月初的正式发布，历经了差不多 4 个月的时间，对 Flink 的生态、易用性、生产可用性、稳定性等方面都进行了增强和改善，下面将一一跟大家分享。 一、综述Flink 1.11.0 从 Feature 冻结后发布了 4 次 Candidate 才最终通过。经统计，一共有 236 个贡献者参与了这次版本开发，解决了 1474 个 Jira 问题，涉及 30 多个 FLIP，提交了 2325 个 Commit。 纵观近五次版本发布，可以看出从 1.9.0 开始 Flink 进入了一个快速发展阶段，各个维度指标相比之前都有了几乎翻倍的提高。也是从 1.9.0 开始阿里巴巴内部的 Blink 项目开始被开源 Flink 整合，到 1.10.0 经过两个大版本已经全部整合完毕，对 Flink 从生态建设、功能性、性能和生产稳定性上都有了大幅的增强。 Flink 1.11.0 版本的最初定位是重点解决易用性问题，提升用户业务的生产使用体验，整体上不做大的架构调整和功能开发，倾向于快速迭代的小版本开发。但是从上面统计的各个指标来看，所谓的“小版本”在各个维度的数据也丝毫不逊色于前两个大版本，解决问题的数量和参与的贡献者人数也在持续增加，其中来自中国的贡献者比例达到 62%。 下面我们会深度剖析 Flink 1.11.0 带来了哪些让大家期待已久的特性，从用户直接使用的 API 层一直到执行引擎层，我们都会选择一些有代表性的 Feature 从不同维度解读，更完整的 Feature 列表请大家关注发布的 Release Blog。 二、生态完善和易用性提升这两个维度在某种程度上是相辅相成的，很难严格区分开，生态兼容上的缺失常常造成使用上的不便，提升易用性的过程往往也是不断完善相关生态的过程。在这方面用户感知最明显的应该就是 Table &amp; SQL API 层面的使用。 1、Table &amp; SQL 支持 Change Data Capture（CDC）CDC 被广泛使用在复制数据、更新缓存、微服务间同步数据、审计日志等场景，很多公司都在使用开源的 CDC 工具，如 MySQL CDC。通过 Flink 支持在 Table &amp; SQL 中接入和解析 CDC 是一个强需求，在过往的很多讨论中都被提及过，可以帮助用户以实时的方式处理 Changelog 流，进一步扩展 Flink 的应用场景，例如把 MySQL 中的数据同步到 PG 或 ElasticSearch 中，低延时的 Temporal Join 一个 Changelog 等。 除了考虑到上面的真实需求，Flink 中定义的“Dynamic Table”概念在流上有两种模型：Append 模式和 Update 模式。通过 Append 模式把流转化为“Dynamic Table”在之前的版本中已经支持，因此在 1.11.0 中进一步支持 Update 模式也从概念层面完整的实现了“Dynamic Table”。 为了支持解析和输出 Changelog，如何在外部系统和 Flink 系统之间编解码这些更新操作是首要解决的问题。考虑到 Source 和 Sink 是衔接外部系统的一个桥梁，因此 FLIP-95 在定义全新的 Table Source 和 Table Sink 接口时解决了这个问题。 在公开的 CDC 调研报告中，Debezium 和 Canal 是用户中最流行使用的 CDC 工具，这两种工具用来同步 Changelog 到其它的系统中，如消息队列。据此，FLIP-105 首先支持了 Debezium 和 Canal 这两种格式，而且 Kafka Source 也已经可以支持解析上述格式并输出更新事件，在后续的版本中会进一步支持 Avro（Debezium） 和 Protobuf（Canal）。 1234567CREATE TABLE my_table ( ...) WITH ( 'connector'='...', -- e.g. 'kafka' 'format'='debezium-json', 'debezium-json.schema-include'='true' -- default: false (Debezium can be configured to include or exclude the message schema) 'debezium-json.ignore-parse-errors'='true' -- default: false); 2 、Table &amp; SQL 支持 JDBC Catalog1.11.0 之前，用户如果依赖 Flink 的 Source/Sink 读写关系型数据库或读取 Changelog 时，必须要手动创建对应的 Schema。而且当数据库中的 Schema 发生变化时，也需要手动更新对应的 Flink 作业以保持一致和类型匹配，任何不匹配都会造成运行时报错使作业失败。用户经常抱怨这个看似冗余且繁琐的流程，体验极差。 实际上对于任何和 Flink 连接的外部系统都可能有类似的上述问题，在 1.11.0 中重点解决了和关系型数据库对接的这个问题。FLIP-93 提供了 JDBC catalog 的基础接口以及 Postgres catalog 的实现，这样方便后续实现与其它类型的关系型数据库的对接。 1.11.0 版本后，用户使用 Flink SQL 时可以自动获取表的 Schema 而不再需要输入 DDL。除此之外，任何 Schema 不匹配的错误都会在编译阶段提前进行检查报错，避免了之前运行时报错造成的作业失败。这是提升易用性和用户体验的一个典型例子。 3、Hive 实时数仓从 1.9.0 版本开始 Flink 从生态角度致力于集成 Hive，目标打造批流一体的 Hive 数仓。经过前两个版本的迭代，已经达到了 Batch 兼容且生产可用，在 TPC-DS 10T Benchmark 下性能达到 Hive 3.0 的 7 倍以上。 1.11.0 在 Hive 生态中重点实现了实时数仓方案，改善了端到端流式 ETL 的用户体验，达到了批流一体 Hive 数仓的目标。同时在兼容性、性能、易用性方面也进一步进行了加强。 在实时数仓的解决方案中，凭借 Flink 的流式处理优势做到实时读写 Hive： Hive 写入：FLIP-115 完善扩展了 FileSystem Connector 的基础能力和实现，Table/SQL 层的 sink 可以支持各种格式（CSV、Json、Avro、Parquet、ORC），而且支持 Hive Table 的所有格式。 Partition 支持：数据导入 Hive 引入 Partition 提交机制来控制可见性，通过sink.partition-commit.trigger 控制 Partition 提交的时机，通过 sink.partition-commit.policy.kind 选择提交策略，支持 SUCCESS 文件和 Metastore 提交。 Hive 读取：实时化的流式读取 Hive，通过监控 Partition 生成增量读取新 Partition，或者监控文件夹内新文件生成来增量读取新文件。 在 Hive 可用性方面的提升： FLIP-123 通过 Hive Dialect 为用户提供语法兼容，这样用户无需在 Flink 和 Hive 的 CLI 之间切换，可以直接迁移 Hive 脚本到 Flink 中执行。 提供 Hive 相关依赖的内置支持，避免用户自己下载所需的相关依赖。现在只需要单独下载一个包，配置 HADOOP_CLASSPATH 就可以运行。 在 Hive 性能方面，1.10.0 中已经支持了 ORC（Hive 2+）的向量化读取，1.11.0 中我们补全了所有版本的 Parquet 和 ORC 向量化支持来提升性能。 4、全新 Source API前面也提到过，Source 和 Sink 是 Flink 对接外部系统的一个桥梁，对于完善生态、可用性及端到端的用户体验是很重要的环节。社区早在一年前就已经规划了 Source 端的彻底重构，从 FLIP-27 的 ID 就可以看出是很早的一个 Feature。但是由于涉及到很多复杂的内部机制和考虑到各种 Source Connector 的实现，设计上需要考虑的很全面。从 1.10.0 就开始做 POC 的实现，最终赶上了 1.11.0 版本的发布。 先简要回顾下 Source 之前的主要问题： 对用户而言，在 Flink 中改造已有的 Source 或者重新实现一个生产级的 Source Connector 不是一件容易的事情，具体体现在没有公共的代码可以复用，而且需要理解很多 Flink 内部细节以及实现具体的 Event Time 分配、Watermark 产出、Idleness 监测、线程模型等。 批和流的场景需要实现不同的 Source。 Partitions/Splits/Shards 概念在接口中没有显式表达，比如 Split 的发现逻辑和数据消费都耦合在 Source Sunction 的实现中，这样在实现 Kafka 或 Kinesis 类型的 Source 时增加了复杂性。 在 Runtime 执行层，Checkpoint 锁被 Source Function 抢占会带来一系列问题，框架很难进行优化。 FLIP-27 在设计时充分考虑了上述的痛点： 首先在 Job Manager 和 Task Manager 中分别引入两种不同的组件 Split Enumerator 和 Source Reader，解耦 Split 发现和对应的消费处理，同时方便随意组合不同的策略。比如现有的 Kafka Connector 中有多种不同的 Partition 发现策略和实现耦合在一起，在新的架构下，我们只需要实现一种 Source Reader，就可以适配多种 Split Enumerator 的实现来对应不同的 Partition 发现策略。 在新架构下实现的 Source Connector 可以做到批流统一，唯一的小区别是对批场景的有限输入，Split Enumerator 会产出固定数量的 Split 集合并且每个 Split 都是有限数据集；对于流场景的无限输入，Split Enumerator 要么产出无限多的 Split 或者 Split 自身是无限数据集。 复杂的 Timestamp Assigner 以及 Watermark Generator 透明的内置在 Source Reader 模块内运行，对用户来说是无感知的。这样用户如果想实现新的 Source Connector，一般不再需要重复实现这部分功能。 目前 Flink 已有的 Source Connector 会在后续的版本中基于新架构来重新实现，Legacy Source 也会继续维护几个版本保持兼容性，用户也可以按照 Release 文档中的说明来尝试体验新 Source 的开发。 5、PyFlink 生态众所周知，Python 语言在机器学习和数据分析领域有着广泛的使用。Flink 从 1.9.0 版本开始发力兼容 Python 生态，Python 和 Flink 合力为 PyFlink，把 Flink 的实时分布式处理能力输出给 Python 用户。前两个版本 PyFlink 已经支持了 Python Table API 和 UDF，在 1.11.0 中扩大对 Python 生态库 Pandas 的支持以及和 SQL DDL/Client 的集成，同时 Python UDF 性能有了极大的提升。 具体来说，之前普通的 Python UDF 每次调用只能处理一条数据，而且在 Java 端和 Python 端都需要序列化/反序列化，开销很大。1.11.0 中 Flink 支持在 Table &amp; SQL 作业中自定义和使用向量化 Python UDF，用户只需要在 UDF 修饰中额外增加一个参数 udf_type=“pandas” 即可。这样带来的好处是： 每次调用可以处理 N 条数据。 数据格式基于 Apache Arrow，大大降低了 Java、Python 进程之间的序列化/反序列化开销。 方便 Python 用户基于 Numpy 和 Pandas 等数据分析领域常用的 Python 库，开发高性能的 Python UDF。 除此之外，1.11.0 中 PyFlink 还支持： PyFlink table 和 Pandas DataFrame 之间无缝切换（FLIP-120），增强 Pandas 生态的易用性和兼容性。 Table &amp; SQL 中可以定义和使用 Python UDTF（FLINK-14500），不再必需 Java/Scala UDTF。 Cython 优化 Python UDF 的性能（FLIP-121），对比 1.10.0 可以提升 30 倍。 Python UDF 中用户自定义 Metric（FLIP-112），方便监控和调试 UDF 的执行。 上述解读的都是侧重 API 层面，用户开发作业可以直接感知到的易用性的提升。下面我们看看执行引擎层在 1.11.0 中都有哪些值得关注的变化。 三、生产可用性和稳定性提升1、支持 Application 模式和 Kubernetes 增强1.11.0 版本前，Flink 主要支持如下两种模式运行： Session 模式：提前启动一个集群，所有作业都共享这个集群的资源运行。优势是避免每个作业单独启动集群带来的额外开销，缺点是隔离性稍差。如果一个作业把某个 Task Manager（TM）容器搞挂，会导致这个容器内的所有作业都跟着重启。虽然每个作业有自己独立的 Job Manager（JM）来管理，但是这些 JM 都运行在一个进程中，容易带来负载上的瓶颈。 Per-job 模式：为了解决 Session 模式隔离性差的问题，每个作业根据资源需求启动独立的集群，每个作业的 JM 也是运行在独立的进程中，负载相对小很多。 以上两种模式的共同问题是需要在客户端执行用户代码，编译生成对应的 Job Graph 提交到集群运行。在这个过程需要下载相关 Jar 包并上传到集群，客户端和网络负载压力容易成为瓶颈，尤其当一个客户端被多个用户共享使用。 1.11.0 中引入了 Application 模式（FLIP-85）来解决上述问题，按照 Application 粒度来启动一个集群，属于这个 Application 的所有 Job 在这个集群中运行。核心是 Job Graph 的生成以及作业的提交不在客户端执行，而是转移到 JM 端执行，这样网络下载上传的负载也会分散到集群中，不再有上述 Client 单点上的瓶颈。 用户可以通过 bin/flink run-application 来使用 Application 模式，目前 Yarn 和 Kubernetes（K8s）都已经支持这种模式。Yarn application 会在客户端将运行作业需要的依赖都通过 Yarn Local Resource 传递到 JM。K8s Application 允许用户构建包含用户 Jar 与依赖的镜像，同时会根据作业自动创建 TM，并在结束后销毁整个集群，相比 Session 模式具有更好的隔离性。K8s 不再有严格意义上的 Per-Job 模式，Application 模式相当于 Per-Job 在集群进行提交作业的实现。 除了支持 Application 模式，Flink 原生 K8s 在 1.11.0 中还完善了很多基础的功能特性（FLINK-14460），以达到生产可用性的标准。例如 Node Selector、Label、Annotation、Toleration 等。为了更方便的与 Hadoop 集成，也支持根据环境变量自动挂载 Hadoop 配置的功能。 2、Checkpoint &amp; Savepoint 优化Checkpoint 和 Savepoint 机制一直是 Flink 保持先进性的核心竞争力之一，社区在这个领域的改动很谨慎，最近的几个大版本中几乎没有大的功能和架构上的调整。在用户邮件列表中，我们经常能看到用户反馈和抱怨的相关问题：比如 Checkpoint 长时间做不出来失败，Savepoint 在作业重启后不可用等等。1.11.0 有选择的解决了一些这方面的常见问题，提高生产可用性和稳定性。 1.11.0 之前， Savepoint 中 Meta 数据和 State 数据分别保存在两个不同的目录中，这样如果想迁移 State 目录很难识别这种映射关系，也可能导致目录被误删除，对于目录清理也同样有麻烦。1.11.0 把两部分数据整合到一个目录下，这样方便整体转移和复用。另外，之前 Meta 引用 State 采用的是绝对路径，这样 State 目录迁移后路径发生变化也不可用，1.11.0 把 State 引用改成了相对路径解决了这个问题（FLINK-5763），这样 Savepoint 的管理维护、复用更加灵活方便。 实际生产环境中，用户经常遭遇 Checkpoint 超时失败、长时间不能完成带来的困扰。一旦作业 failover 会造成回放大量的历史数据，作业长时间没有进度，端到端的延迟增加。1.11.0 从不同维度对 Checkpoint 的优化和提速做了改进，目标实现分钟甚至秒级的轻量型 Checkpoint。 首先，增加了 Checkpoint Coordinator 通知 Task 取消 Checkpoint 的机制（FLINK-8871），这样避免 Task 端还在执行已经取消的 Checkpoint 而对系统带来不必要的压力。同时 Task 端放弃已经取消的 Checkpoint，可以更快的参与执行 Coordinator 新触发的 Checkpoint，某种程度上也可以避免新 Checkpoint 再次执行超时而失败。这个优化也对后面默认开启 Local Recovery 提供了便利，Task 端可以及时清理失效 Checkpoint 的资源。 其次，在反压场景下，整个数据链路堆积了大量 Buffer，导致 Checkpoint Barrier 排在数据 Buffer 后面，不能被 Task 及时处理对齐，也就导致了 Checkpoint 长时间不能执行。1.11.0 中从两个维度对这个问题进行解决： 1）尝试减少数据链路中的 Buffer 总量（FLINK-16428），这样 Checkpoint Barrier 可以尽快被处理对齐。 上游输出端控制单个 Sub Partition 堆积 Buffer 的最大阈值（Backlog），避免负载不均场景下单个链路上堆积大量 Buffer。 在不影响网络吞吐性能的情况下合理修改上下游默认的 Buffer 配置。 上下游数据传输的基础协议进行了调整，允许单个数据链路可以配置 0 个独占 Buffer 而不死锁，这样总的 Buffer 数量和作业并发规模解耦。根据实际需求在吞吐性能和 Checkpoint 速度两者之间权衡，自定义 Buffer 配比。 这个优化有一部分工作已经在 1.11.0 中完成，剩余部分会在下个版本继续推进完成。 2）实现了全新的 Unaligned Checkpoint 机制（FLIP-76）从根本上解决了反压场景下 Checkpoint Barrier 对齐的问题。实际上这个想法早在 1.10.0 版本之前就开始酝酿设计，由于涉及到很多模块的大改动，实现机制和线程模型也很复杂。我们实现了两种不同方案的原型 POC 进行了测试、性能对比，确定了最终的方案，因此直到 1.11.0 才完成了 MVP 版本，这也是 1.11.0 中执行引擎层唯一的一个重量级 Feature。其基本思想可以概括为： Checkpoint Barrier 跨数据 Buffer 传输，不在输入输出队列排队等待处理，这样就和算子的计算能力解耦，Barrier 在节点之间的传输只有网络延时，可以忽略不计。 每个算子多个输入链路之间不需要等待 Barrier 对齐来执行 Checkpoint，第一个到的 Barrier 就可以提前触发 Checkpoint，这样可以进一步提速 Checkpoint，不会因为个别链路的延迟而影响整体。 为了和之前 Aligned Checkpoint 的语义保持一致，所有未被处理的输入输出数据 Buffer 都将作为 Channel State 在 Checkpoint 执行时进行快照持久化，在 Failover 时连同 Operator State 一同进行恢复。换句话说，Aligned 机制保证的是 Barrier 前面所有数据必须被处理完，状态实时体现到 Operator State 中；而 Unaligned 机制把 Barrier 前面的未处理数据所反映的 Operator State 延后到 Failover Restart 时通过 Channel State 回放进行体现，从状态恢复的角度来说最终都是一致的。注意这里虽然引入了额外的 In-Flight Buffer 的持久化，但是这个过程实际是在 Checkpoint 的异步阶段完成的，同步阶段只是进行了轻量级的 Buffer 引用，所以不会过多占用算子的计算时间而影响吞吐性能。 Unaligned Checkpoint 在反压严重的场景下可以明显加速 Checkpoint 的完成时间，因为它不再依赖于整体的计算吞吐能力，而和系统的存储性能更加相关，相当于计算和存储的解耦。但是它的使用也有一定的局限性，它会增加整体 State 的大小，对存储 IO 带来额外的开销，因此在 IO 已经是瓶颈的场景下就不太适合使用 Unaligned Checkpoint 机制。 1.11.0 中 Unaligned Checkpoint 还没有作为默认模式，需要用户手动配置来开启，并且只在 Exactly-Once 模式下生效。但目前还不支持 Savepoint 模式，因为 Savepoint 涉及到作业的 Rescale 场景，Channel State 目前还不支持 State 拆分，在后面的版本会进一步支持，所以 Savepoint 目前还是会使用之前的 Aligned 模式，在反压场景下有可能需要很长时间才能完成。 四、总结Flink 1.11.0 版本的开发过程中，我们看到越来越多来自中国的贡献者参与到核心功能的开发中，见证了 Flink 在中国的生态发展越来越繁荣，比如来自腾讯公司的贡献者参与了 K8s、Checkpoint 等功能开发，来自字节跳动公司的贡献者参与了 Table &amp; SQL 层以及引擎网络层的一些开发。希望更多的公司能够参与到 Flink 开源社区中，分享在不同领域的经验，使 Flink 开源技术一直保持先进性，能够普惠到更多的受众。 经过 1.11.0 “小版本”的短暂调整，Flink 正在酝酿下一个大版本的 Feature，相信一定会有很多重量级的特性登场，让我们拭目以待！]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何成为一名Flink Committer]]></title>
    <url>%2Fbig-data%2Fflink%2Fday3-how-to-be-flink-committer%2F</url>
    <content type="text"><![CDATA[如何成为一名Flink Committer 3、如何参与Flink社区 订阅邮件列表，发一封邮件到对应的邮件列表，对方会自动回复确认链接，确认即可 4、如何提交第一个PR]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink-1.11 知识图谱]]></title>
    <url>%2Fbig-data%2Fflink%2Fday2-flink-knowledgge-graph%2F</url>
    <content type="text"><![CDATA[速度收藏！看完这份知识图谱，才算搞懂 Flink！简介： 社区整理了这样一份知识图谱，由 Apache Flink Committer 执笔，四位 PMC 成员审核，将 Flink 9 大技术版块详细拆分，突出重点内容并搭配全面的学习素材。看完这份图谱，才算真的搞懂 Flink！ 先跟大家分享一个好消息！即日起，Apache Flink 社区微信公众号 Ververica 正式更名为「Flink 中文社区」并由 Apache Flink PMC 成员进行维护，是国内唯一的 Flink 社区官方微信公众号。 在去年的一年中，Flink 中文社区共发布文章 144 篇，通过提供 Flink 技术原理剖析、上手实操、多场景下的最佳实践以及社区的最新资讯等帮助大家更好的理解、使用 Flink。 同时，我们也发现当前社区除文章、直播教程、Meetup 外还缺少一个清晰的图谱让大家了解 Flink 完整的技术体系与学习路径。因此，社区整理了这样一份知识图谱，由 Apache Flink Committer 执笔，四位 PMC 成员审核，将 Flink 9 大技术版块详细拆分，突出重点内容并搭配全面的学习素材。看完这份图谱，才算真的搞懂 Flink！ ▽ Flink 知识图谱概览 ▽ 如何获取？ 点击下方链接即可马上下载，知识图谱 PDF 版本内含大量补充链接，一键点击即可查看相关扩展素材！ 《Apache Flink 知识图谱》https://ververica.cn/developers/special-issue/ 最实用的知识图谱1.内容全面，将 Flink 所涉及的技术内容划分为 9 大版块，每部分内容进行详细分解，并提供学习路径及深入了解的学习素材。 Streaming Processing Concepts(common concepts for stream processing) Architecture State Management DataStream Libraries Table API &amp; SQL Deployment and Operations Debugging and Monitoring Ecosystem Use Cases 2.层次分明，将各部分技术内容中的基础入门知识进行标示，重点突出，帮你找到清晰的学习路径。 3.方便实用，每个知识点附带补充说明的链接与最佳学习素材，可及时进行深度探索，有助于理解消化。 4.强大的拓展阅读资料配置，整合了社区全年输出的技术文章、系列直播教程、线下 Meetup 及 Flink Forward Asia 的精华内容，一图在手，学好 Flink 不用愁！ 各版块知识点详解 Streaming Processing Concepts(common concepts for stream processing) Architecture State Management DataStream Libraries Table API &amp; SQL Deployment and Operations Debugging and Monitoring Ecosystem Use Cases 部分知识图谱扩展素材直播教程| Flink 基础概念解析| Flink 开发环境搭建和应用的配置、部署及运行| Flink Datastream API 编程| Flink 客户端操作| Flink Time &amp; Window| Flink 状态管理及容错机制| Flink Table API 编程| Flink SQL 编程实践| 5分钟从零构建第一个 Flink 应用| 零基础实战教程：如何计算实时热门商品| Runtime 核心机制剖析| 时间属性深度解析| Checkpoint 原理剖析与应用实践| Flink on Yarn / K8s 原理剖析及实践| 数据类型和序列化| Flink 作业执行深度解析| 网络流控和反压剖析| 详解 Metrics 原理与实战 User Case 及补充| 小米流式平台架构演进与实践| 美团点评基于 Flink 的实时数仓平台实践| 监控指标10K+！携程实时智能检测平台实践| Lyft 基于 Flink 的大规模准实时数据分析平台| 基于 Flink 构建 CEP 引擎的挑战和实践| 趣头条基于 Flink 的实时平台建设实践| G7 在实时计算的探索与实践| Flink 靠什么征服饿了么工程师？| Apache Flink 的迁移之路，2 年处理效果提升 5 倍| 日均百亿级日志处理：微博基于 Flink 的实时计算平台建设| Flink 在同程艺龙实时计算平台的研发与应用实践| 从 Storm 到 Flink，汽车之家基于 Flink 的实时 SQL 平台设计思路与实践| 日均处理万亿数据！Apache Flink在快手的应用实践与技术演进之路| 从 Spark Streaming 到 Apache Flink : 实时数据流在爱奇艺的演进| Apache Flink 在 eBay 监控系统上的实践和应用| 每天30亿条笔记展示，小红书如何实现实时高效推荐？| 360深度实践：Flink 与 Storm 协议级对比| Blink 有何特别之处？菜鸟供应链场景最佳实践| 58 集团大规模Storm 任务平滑迁移至 Flink 的秘密| 从Storm到Flink，有赞五年实时计算效率提升实践 拓展链接| https://ververica.cn/developers/table-api-programming/| https://sf-2017.flink-forward.org/kb_sessions/streaming-models-how-ing-adds-models-at-runtime-to-catch-fraudsters/| https://sf-2017.flink-forward.org/kb_sessions/building-a-real-time-anomaly-detection-system-with-flink-mux/| https://sf-2017.flink-forward.org/kb_sessions/dynamically-configured-stream-processing-using-flink-kafka/| https://jobs.zalando.com/en/tech/blog/complex-event-generation-for-business-process-monitoring-using-apache-flink/| https://berlin-2017.flink-forward.org/kb_sessions/drivetribes-kappa-architecture-with-apache-flink/| https://2016.flink-forward.org/kb_sessions/a-brief-history-of-time-with-apache-flink-real-time-monitoring-and-analysis-with-flink-kafka-hb/| https://ci.apache.org/projects/flink/flink-docs-master/monitoring/checkpoint_monitoring.html| https://ci.apache.org/projects/flink/flink-docs-master/dev/table/functions/udfs.html| https://ci.apache.org/projects/flink/flink-docs-master/dev/table/functions/systemFunctions.html| https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/dynamic_tables.html| https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html 重磅福利Flink 社区知识图谱免费下载链接来啦～关注点击下方链接即可下载，并有直播课程详解知识图谱的正确打开方式，让你一图在手，学好 Flink 不用愁！ 《Apache Flink 知识图谱》链接地址：https://ververica.cn/developers/special-issue/ 知识图谱作者介绍： 程鹤群（军长），Apache Flink Committer，阿里巴巴技术专家，2015 年 4 月加入阿里巴巴，从事主搜离线相关开发。2016 年开始参与 Flink SQL 相关的研发。2018 年开始核心参与 Flink Table API 相关的研发。]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[什么是Flink？]]></title>
    <url>%2Fbig-data%2Fflink%2Fday1-what-is-flink%2F</url>
    <content type="text"><![CDATA[Apache Flink 是什么？简介： Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。 Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。 接下来，我们来介绍一下 Flink 架构中的重要方面。 处理无界和有界数据任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。 数据可以被作为 无界 或者 有界 流来处理。 1.无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 2.有界流 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。 Apache Flink 擅长处理无界和有界数据集 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。 通过探索 Flink 之上构建的 用例 来加深理解。 部署应用到任意地方Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。Flink 集成了所有常见的集群资源管理器，例如 Hadoop YARN、 Apache Mesos 和 Kubernetes，但同时也可以作为独立集群运行。 Flink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。 部署 Flink 应用程序时，Flink 会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。 运行任意规模应用Flink 旨在任意规模上运行有状态流式应用。因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO。而且 Flink 很容易维护非常大的应用程序状态。其异步和增量的检查点算法对处理延迟产生最小的影响，同时保证精确一次状态的一致性。 Flink 用户报告了其生产环境中一些令人印象深刻的扩展性数字 处理每天处理数万亿的事件, 应用维护几TB大小的状态, 和 应用在数千个内核上运行。 利用内存性能有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。 原文链接：https://flink.apache.org/zh/flink-architecture.html]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink-1.11 7天训练营]]></title>
    <url>%2Fbig-data%2Fflink%2Fflink-7-days%2F</url>
    <content type="text"><![CDATA[https://flink-learning.org.cn/ CDC：在数据迁移场景非常实用 PyFink：提升对Pandas UDF的支持，扩展python在支持分布式上的能力 1.1、流批的本质有界的数据是批，无界的数据是流 海量数据处理三辆马车 GFS BigTable MapReduce 对应的开源框架 Hadoop 流与批的区别：业务延时的不同 流是批的特例，Flink（Native-Streaming） 批是流的特例，Spark（Micro-Batching） 1.2、流计算的核心问题问题： 延问题 更新撤回： 容错续跑：机器故障以后，可以继续跑 透明升级：某个Job升级以后，可以继续前面的任务跑 乱序问题：数据到达顺序问题 正确性问题： 部署问题 弹性扩容问题 延时问题Native Stream + Early-Fire 更新撤回告诉下游节点上一次计算结果无效 使用”-“标记上一次计算结果失效 容错续跑 透明升级 乱序问题数据产生时间 EventTime 数据进入流计算引擎的时间 Watermark 正确性问题 数据是否有丢失 数据是否只参与了一次计算 1.3、Flink应用场景事件驱动型应用社交：关注事件 引发的 后续操作 网购：恶意差评事件 -&gt; 封号 金融：欺诈 -&gt; 反欺诈 数据分析型应用 数据管道型应用数据清洗 1.4、怎样理解流批一体融合？用户、运行、运维 三个维度看 第二章 Stream Process 在 Flink 的实现 2.1、并行处理和编程范式任务划分 类似 有向无环图（DAG）的 2.2、DataStream API 概览 批处理 的数据分区类似于洗牌，将相关的牌放在一起 流处理 分区是随着数据的到来 动态完成的 2.3、状态和时间 时间的使用案例： 运行结果： 作业框架，基于框架进行开发 【Stream Processing with Apache Flink】讲师：崔星灿，Apache Flink Committer时间：7月14日 20:00-21:00（UTC+8）【作业内容】- 准备环境：能clone和运行源码中的streaming-examples- 下载地址：https://github.com/flink-china/Flink-Geek-Training/blob/master/Training2.java 第三章 运行时架构 3.1 、Runtime 总览 三大核心组件 3.2、 JobMaster — 作业的控制中心 Lost Leadership：JobMaster出问题了，异常终止 Eager：服务于流式处理作业，节省调度花费的时间； Lazy from sources：服务于批处理作业，延迟下游资源的调度，避免空转浪费资源； （WIP）Piplined region based：类似Lazy from sources，既能减少调度时间，又能避免空转浪费资源； 出错恢复策略 与 Pipiline 有关，出错后，会重新pipline里相关的任务 单点重启：只重启出错的节点，接下来的版本会牺牲数据一致性来实现 3.3、TaskExecutor — 任务的运行容器 3.4、ResourceManager — 资源管理中心 第四章 流计算的状态 和 容错机制 4.1、流计算中的状态 状态的种类 4.2、全局一致性快照 第三张图，后发生的事件包含在快照中，而 先发生的事件没有包含在快照中，所以不是 全局一致性快照 全局一致性快照的实现方法 异步全局一致性快照算法 Chandy-Lamport 算法流程 实线代表： 开始快照 虚线代表：结束快照 A 是自己内部发生的事件，与其他进程没有交互，我们认为是P1自己发给自己的消息 4.3、Flink的容错机制 如何保证 Exactly once 需要一个可回退的 source 复杂场景 —— 多流输入 降低一致性要求 通过异步快照减少停顿 4.4、Flink的状态管理定义一个状态 本地状态后端 JVM 形成快照的过程需要序列化 优点： 缺点： RocksDB 作为状态后端 第5章 Flink SQL/Table API 介绍与实战 5.1、Flink SQL 声明式：用户只需要表达他需要什么，不需要关心怎么计算的 批流统一：SQL很容易做批处理，不管输入是静态的批数据，还是动态的流数据，结果是相同的 sql/python/scala/java 的api 被翻译成 Logical Plan，Logical Plan 被优化器 优化成 Physical Plan，然后翻译成 Transformations 的DAG，再交给 JobGraph 执行 完整的类型系统：数据的精度 TopN： 高效流式去重：在明细层去重，交给汇总层的时候才会精确 维表关联：Mysql、hive、hbase cdc：对接canal等 binlog的数据 内聚函数：超过230个内置函数 MiniBash： 多种解热点手段： 完整的批处理支持 Python Table API：多语言支持 Hive集成：读写hive，支持hive sql 语法 5.3 实战Demo 前提条件：需要基础的 SQL 知识 准备环境：你将需要一个至少 8 GB 内存和安装了 Docker 的笔记本电脑。 下载地址：https://www.docker.com/get-started 代码：https://github.com/wuchong/flink-sql-demo 6.1、PyFlink简介 6.2、PyFlink 功能介绍 Python Table API Python UDF Python UDF 架构 向量化 Python UDF Python UDF Metrics Python UDF 执行优化 6.4、PyFlink 下一步规划 第七章 Flink Ecosystems 连接外部生态系统 7.1、连接外部系统 通过DDL创建的表是如何被使用的： 7.2、常用ConnectorKafka Connector Elasticsearch Connector FileSystem Connector Hive Connector DataGen Connector Print Connector BlackHole Connector可以做性能测试，数据来了以后不做任何处理，直接丢弃 7.3、示例 &amp; Demokafka交互 写入es hive交互]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2Fdevops%2Fk8s%2Fk8s-yunxi%2F</url>
    <content type="text"><![CDATA[k8s简介和安装 k8s核心概念：Pod、控制器、Services、网络通信 k8s 资源清单及编写、Pod生命周期 k8s 存储类型 k8s 调度器及HELM k8s 集群安全机制 k8s简介和安装1、k8s简介IaaS：Infrastructure as a Service PaaS：Platform as a Service（类似：阿里云、腾讯云） SaaS：Software as a Service（类似：腾讯文档、有道云笔记等产品） 软件部署演进路线 3、基础知识 4、安装配置系统要求： 64位操作系统 lInux 内核3.10以上，建立4.4以上内核 （Ubuntu/Centos7+），3台 cpu至少2核，最好4核 内存最少2G，推荐8G etcd 3.0版本 docker 18.03 版本及以上 Flannel Kubernetes 1.18.5 内核升级12cat /etc/redhat-releaseuname -r k8s核心概念：Pod、控制器、Services、网络通信kubernetes 对象基本对象Pod网络存储高级对象扩容缩容 1kubectl scale deployment nginx-deploy --replicas=5 查看节点分布 1kubectl get po -o wide 自动扩容缩容 1kubectl autoscale deployment nginx-deploy --min=3 --max=7 --cpu-percent=60 滚动升级 12docker pull nginx:1.18.0docker pull nginx:1.19.1 12345kubectl delete deployment nginx-deploykubectl get deploymentkubectl get podkubectl create -f nginx-deploy.yamlkubectl describe deployment nginx-deploy 12kubectl set image deployment/nginx-deploy nginx=docker.io/nginx:1.19.1kubectl get rs 回滚 123kubectl rollout undo deployment/nginx-depoly# 回滚到某个版本kubectl rollout undo deployment/nginx-depoly --to-reversion=2 AB测试 DaemonSet确保集群中的每个pod有且只有一个副本 Filebeat、Logstash、Flume （agent） Promethues node Exporter 监控每台机器的CPU、内存指标 k8s 资源清单及编写、Pod生命周期 12kubectl api-versionskubectl api-resouces 查看label 修改label（修改后，又重新创建了一个新pod，补齐3个nginx） 查看每个对象有哪些属性 123456kubectl explain deploymentkubectl explain deployment.statuskubectl explain podkubectl explain kindkubectl explain apiVersionkubectl explain metadata 编写yaml文件 nginx-daemon.yaml 1234567891011121314151617kind: DaemonSetapiVersion: apps/v1metadata: name: nginx-daemonspec: selector: matchLabels: app: nginx template: metadata: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 Job1234567891011121314kind: JobapiVersion: batch/v1metadata: name: job-demospec: template: metadata: name: job-demo spec: containers: - name: job-demo image: docker.io/busybox:latest command: ["echo", "'scale=10; 4*a(1)'", "|", "bc", "-l"] restartPolicy: Never Pause 是根容器 CronJobPod的生命周期 ImagePullBackOff ， 镜像拉取失败，采用退避算法重新拉取镜像 pending running succeeded failed unknown Init Container 按顺序执行 Main Container 探针：存活检测 启动延迟5秒，检测周期3秒 nginx 一般都是80端口，去检测8008端口，是不存在的，会认为容器启动失败，会再启动一个容器 第二个例子： 启动后先休眠60秒，移除a文件，再休眠 检测a文件是否存在，不存在则删除pod，再创建新的Pod postStartpreStop探针：读取检测 读取检测，如果能读取到，则代表容器达到了ready状态，可以供外界访问，否则就被认为不可访问 Service代理模式暴露固定的ip，端口，供外界访问 123kubectl create -f nginx-deploy.yamlkubectl get deploykubectl expose deploy nginx-deploy --port=8000 --target-port=80 删除暴露 1kubectl delete 域名格式 k8s 转发模式 linux 内核态，用户态 userspace k8s 1.4 之前 使用 iptables ipvs（ipv server） 虚拟ip地址服务 VIP 在1.8-beta版本里加入使用，在1.14版本里成为正式的转发协议 服务类型DNSingressk8s 存储类型k8s 调度器及HELMk8s 集群安全机制]]></content>
  </entry>
  <entry>
    <title><![CDATA[走出微服务误区：避免从单体到分布式单体]]></title>
    <url>%2Fmicro-service%2Fmicroserv-command-and-event-model%2F</url>
    <content type="text"><![CDATA[作者 | 敖小剑 最近，社区频繁出现各种对微服务的质疑和反思声音，甚至放弃微服务回归单体。鉴于此，本文从“分布式单体”问题出发，介绍通过引入非侵入式方案和引入 Event/EDA 来走出微服务实践误区——从单体到微服务，最后却沦为分布式单体。 1、回顾：从单体到微服务到 Function在过去几年间，微服务架构成为业界主流，很多公司开始采用微服务，并将原有的单体应用迁移到微服务架构。从架构上说，微服务和单体之间最大的变化在于微服务架构下应用的粒度被“拆小”：将所有业务逻辑都集中在一起的单体应用，按照领域模型拆分为多个内聚而自治的“更小”应用。而 Function 则在拆分上更进一步，拆分粒度变成“单个操作”，基于 Function 逐渐演进出现了 FaaS 形态和 Serverless 架构。 在微服务和 Serverless 喧嚣中，业界逐渐出现很多质疑和反对的声音：越来越多的人发现，当他们兴冲冲的迁移单体应用到微服务和 Serverless 架构后，收益却并没有期望中的那么理想。而最近，也出现了一些对微服务的各种质疑、反思的声音，甚至放弃微服务回归单体。举例，我在Infoq 中国网站简单搜索关键字“微服务”，前三页中就出现了如下内容： 我们为什么停用微服务？ 这些公司为什么放弃微服务？ https://www.infoq.cn/article/KSzctluch2ijbRbKYBgO 致传统企业朋友：不够痛就别微服务，有坑 https://www.infoq.cn/article/Nd0RofAUp0WtlvlQArbu 微服务带来的心理阴影 Uber 团队放弃微服务改用宏服务，网友评论炸锅了 为什么 Segment 从微服务回归单体 无论是支持还是反对微服务的声音，大多都是着眼于组织架构（康威定律，对应用和代码的 ownership）、微服务拆分（粒度大小，如何识别领域模型和业务边界）、分布式事务（跨多个微服务调用时维持一致性），工具（自动化构建、部署、可测试性、监控、分布式链路跟踪、CI/CD），数据库分离（避免多个微服务，尤其是领域模型外的微服务共享数据库）等方面进行合理性分析和观点阐述，相信大家都对这些问题都有了解。 而我今天的文章，将从另外一个角度来看待微服务（也包括 Serverless）实践中存在的误区——辛辛苦苦从单体走到微服务，却最后沦为分布式单体。 2、分布式单体“Distributed Monolith”，分布式单体，这真是一个悲伤的技术术语。而这偏偏是企业采用微服务后通常最容易掉进去的一个“陷阱”。事实上，我看到的很多微服务落地最终都是以”分布式单体”收场，无法获得微服务的完整收益。 问题源于微服务实施的方式 —— 按照业务逻辑拆解单体，划分为多个微服务，定义 API 接口，然后通过 REST 或者 RPC 进行远程调用，最终把这些微服务组合起来提供各种业务功能。简单说，就是在业务拆分的基础上，用进程间的远程调用简单替代原来进程内的方法调用。期间，对于原来使用的各种分布式能力，继续采用之前的方式。简单说：方式不变，只是粒度变小。 从方法论说，这样做无可厚非，这也是微服务采用过程中非常标准的做法。但问题在于，止步于此是不够的 —— 至少存在两个有待继续努力改进的地方。 分布式单体起因之一：通过共享库和网络客户端访问分布式能力分布式能力的共享库和网络客户端是造成分布式单体问题的原因之一，关于这一点，来自 verizon 的 Mohamad Byan 在他的名为 Avoid the Distributed Monolith!! 的演讲中有详细阐述，我这里援引他的图片和观点： https://www.slideshare.net/DevOpsDaysDFW/avoid-the-distributed-monolith 上图是微服务体系的逻辑架构，由两部分组成： 内层架构（图上浅蓝色部分），是每个微服务的实现架构； 外层架构 (图上黄色部分)，是构建强大微服务架构所需要的各种能力。这里通常有大家熟悉的各种分布式能力。 特别提示：这里说的“网络客户端”是各种分布式能力的客户端，如服务注册发现 /MQ 中间件 /Redis 等 key-value 存储 / 数据库 / 监控日志追踪系统 / 安全体系等，不是服务间通讯如 RPC 的客户端。 而内层的微服务是通过共享类库和网络客户端来访问外层架构提供的分布式能力： 分布式能力的共享类库和网络客户端会迫使内层微服务和外层架构的各种分布式能力之间产生强耦合，增加运维的复杂性（如升级困难造成版本碎片化），多语言受限于类库和网络客户端支持的语言，各种组件（如消息中间件）往往使用自定义数据格式和通讯协议 —— 所有这些迫使内层微服务不得不实质性受限于外层架构的技术选型。 对于 Function，这个问题就更加明显：Function 的粒度更小，更专注业务逻辑。某些简短的 Function 可能只有几百行代码，但是，为了让这几百行代码运转起来而需要引入的共享类库和网络客户端可能相比之下就规模惊人了。援引一张网上图片作为示意： 分布式单体起因之二：简单用远程调用替代进程内方法调用在微服务架构改造过程中，熟悉单体系统和架构的开发人员，习惯性的会将这些单体时代的知识和经验重用到新的微服务架构之中。其中最典型的做法就是：在遵循领域模型将现有单体应用按照业务边界拆分为多个微服务时，往往选择用 REST 或者 RPC 等远程调用方式简单替代原有的进程内方法调用。 当两个逻辑上的业务模块存在协作需求时： 从单体到微服务，直接方法调用被替换为远程调用（REST 或者 RPC），即使采用 Service Mesh 也只是在链路中多增加了 sidecar 节点，并未改变远程调用的性质： 这导致了前面所说的 “分布式单体”： 在微服务之前：应用程序由多个耦合在一起的模块组成，这些模块通过内存空间进行方法调用….. 在微服务之后：应用程序由多个耦合在一起的微服务组成，这些微服务通过网络进行远程调用….. 抛开调用方式的差异来看采用微服务前后的系统架构，会发现：两者几乎是完全一样的！！ 而微服务版本在某些情况下可能表现的更糟糕：因为调用方式更脆弱，因为网络远比内存不可靠。而我们将网络当成 “胶水” 来使用，试图把分散的业务逻辑模块（已经拆分为微服务）按照单体时代的同样方式简单粘在一起，这当然比单体在同一个进程内直接方法调用更加的不可靠。 关于这一点，在”The Eight Fallacies of Distributed Computing/ 分布式计算的 8 个谬论”一文中有详细阐述。 https://www.red-gate.com/simple-talk/blogs/the-eight-fallacies-of-distributed-computing/ 类似的，在采用 Function 时，如果依然沿用上面的方式，以单体或微服务架构的思维方式和设计模式来创建 FaaS/Serverless 架构： 其本质不会发生变化 —— 不过是将微服务变成粒度更小的函数，导致系统中的远程调用数量大为增加： 系统内的耦合并没有发生变化，Serverless 并不能改变微服务中存在的这个内部耦合问题：调用在哪里，则耦合就在哪里！只是把将组件的粒度从 “微服务“换成了 “Function/ 函数”。 耦合的存在是源于系统不同组件之间的通讯模式，而不是实现通讯的技术。 如果让两个组件通过“调用”（后面再展开讲何为调用）进行远程通信，那么不管调用是如何实现的，这两个组件都是紧密耦合。因此，当系统从单体到微服务到 Serverless，如果止步于简单的用远程调用替代进程内方法调用，那么系统依然是高度耦合的，从这个角度来说： 单体应用 ≈ 分布式单体 ≈ Serverless 单体 分布式单体起因小结上面，我们列出了微服务和 Serverless 实践中容易形成“分布式单体”的两个主要原因： 通过共享库和网络客户端访问分布式能力； 简单用远程调用替代进程内方法调用。 下面我们针对这两个问题探讨解决的思路和对策。 3、引入非侵入式方案：物理隔离 + 逻辑抽象前面谈到分布式单体产生的一个原因是“通过共享库和网络客户端访问分布式能力”，造成微服务和 Lambda 函数和分布式能力强耦合。以 Service Mesh 为典型代表的非侵入式方案是解决这一问题的有效手段，其他类似方案有 RSocket / Multiple Runtime Architecture，以及数据库和消息的 Mesh 化产品，其基本思路有两点： 委托：通过 Sidecar 或者 Runtime 来进行对分布式能力的访问，避免应用和提供分布式能力的组件直接通讯造成强绑定 —— 通过物理隔离进行解耦。 抽象：对内层微服务隐藏实现细节，只暴露网络协议和数据契约，将外围架构的各种分布式能力以 API 的方式暴露出来，而屏蔽提供这些能力的具体实现 —— 通过逻辑抽象进行解耦。 以 Service Mesh 的 Sidecar 为例，在植入 Sidecar 之后，业务应用需要直接对接的分布式能力就大为减少（物理隔离）： 最近出现的 Multiple Runtime / Mecha 架构，以及遵循这一架构思想的微软开源产品 Dapr ，则将这个做法推进到服务间通讯之外更多的分布式能力。 此外在委托之外，还提供对分布式能力的抽象。比如在 Dapr 中，业务应用只需要使用 Dapr 提供的标准 API，就可以使用这些分布式能力而无法关注提供这些能力的具体产品（逻辑抽象）： 以 pub-sub 模型中的发消息为例，这是 Dapr 提供的 Java 客户端 SDK API： 1234public interface DaprClient &#123; Mono&lt;Void&gt; publishEvent(String topic, Object event); Mono&lt;Void&gt; publishEvent(String topic, Object event, Map&lt;String, String&gt; metadata);&#125; 可见在发送事件时，Dapr 完全屏蔽了底层消息机制的具体实现，通过客户端 SDK 为应用提供发送消息的高层抽象，在 Dapr Runtime 中对接底层 MQ 实现——完全解耦应用和 MQ： 关于 Multiple Runtime / Mecha 架构的介绍不在这里深入展开，有兴趣的同学可以浏览我之前的博客文章——“Mecha：将 Mesh 进行到底”。 https://skyao.io/talk/202004-mecha-mesh-through-to-the-end/ 稍后我会有一篇深度文章针对上面这个话题，详细介绍在消息通讯领域和 EDA 架构下如何实现消息通讯和事件驱动的抽象和标准化，以避免业务应用和底层消息产品绑定和强耦合，敬请关注。 4 引入 Event：解除不必要的强耦合在解决了微服务 /Serverless 系统和外部分布式能力之间紧耦合的问题后，我们继续看微服务 /Serverless 系统内部紧耦合的问题。前面讨论到，从单体到微服务到 Function/Serverless，如果只是简单的将直接方法调用替换为远程调用（REST 或者 RPC），那么两个通讯的模块之间会因为这个紧密耦合的调用而形成依赖，而且依赖关系会伴随调用链继续传递，导致形成一个树形的依赖关系网络，表现为系统间的高度耦合： 要解决这个问题，基本思路在于审视两个组件之间通讯行为的业务语义，然后据此决定两者之间究竟是应该采用 Command/ 命令模式还是 Event/ 事件模式。 温故而知新：Event 和 Command首先，我们来温习一下 Event 和 Command 的概念和差别，借用一张图片，总结的非常到位： 什么是 Event？ Event: “A significant change in state” — K. Mani Chandy Event 代表领域中已经发生的事情：通常意味着有行为（Action）已经发生，有状态（Status）已经改变。 因为是已经发生的事情，因此： Event 可以被理解为是对已经发生的事实的客观陈述； 这意味着 Event 通常是不可变的：Event 的信息（代表着客观事实）不能被篡改，Event 的产生不能逆转； 命名：Event 通常以动词的完成时态命名，如 UserRegistredEvent。 产生 Event 的目标是为了接下来的 Event 传播： 将已经发生的 Event 通知给对此感兴趣的观察者； 收到 Event 的观察者将根据 Event 的内容进行判断和决策：可能会有接下来的动作（Action），有些动作可能需要和其他模块通讯而触发命令（Command），这些动作执行完毕可能会造成领域状态的改变从而继续触发新的事件（Event）。 Event 传播的方式： Event 有明确的“源 /source”，即 Event 产生（或者说状态改变）的发生地； 但由于生产者并不知道（不愿意 / 不关心）会有哪些观察者对 Event 感兴趣，因此 Event 中并不包含“目的地 /Destination”信息； Event 通常是通过 MessageQueue 机制，以 pub-sub 的方式传播； Event 通常不需要回复（ reply）或者应答（response）； Event 通常用 发布（publish）。 什么是 Command？ Command 用于传递一个要求执行某个动作（Action）的请求。 Command 代表将要发生的事情： 通常意味着行为（Action）还未发生但即将发生（如果请求被接受和执行）； 存在被拒绝的可能：不愿意执行（参数校验失败，权限不足），不能执行（接收者故障或者资源无法访问）； 命名：Command 通常以动词的普通形态命名，如 UserRegisterCommand。 产生 Command 的目标是为了接下来的 Command 执行： 将 Command 发送给期望的执行者； 收到 Command 的执行者将根据 Command 的要求进行执行：在执行的过程中内部可能有多个动作（Action），有些动作可能需要和其他模块通讯而触发命令（Command），这些动作执行完毕可能会造成领域状态的改变从而继续触发新的事件（Event）。 Command 的传播方式： Command 有明确的源（Source），即 Command 的发起者； Command 也有非常明确的执行者（而且通常是一个），因此命名通常包含“目的地 /Destination”信息； Command 通常是通过 HTTP / RPC 这样的点对点远程通讯机制，通常是同步； Command 通常需要应答（Response）：回应 Command 是否被执行（因为可能被拒绝），执行结果（因为可能执行失败）； Command 通常用 发送（Send）。 Command 和 Event 总结 总结 —— Command 和 Event 的本质区别在于他们的意图： Command 的意图是告知希望发生的事情； Event 的意图是告知已经发生的事情。 意图上的差异最终会在服务间依赖关系上有特别的体现： Command 的发起者必须明确知晓 Command 的接收者并明确指示需要做什么（所谓的命令、指示、操纵、编排），尤其当发起者连续发出多个 Command 时，通常这些 Command 会有非常明确的顺序和逻辑关系，以组合为特定的业务逻辑。 Command 的依赖关系简单明确: 发起者 “显式依赖” 接收者 Event 的发起者只需负责发布 Event，而无需关注 Event 的接收者，包括接收者是谁（一个还是多个）以及会做什么（所谓的通知、驱动、协调）。即使 Event 实际有多个接收者，这些接受者之间往往没有明确的顺序关系，其处理过程中的业务逻辑也往往是彼此独立的。 Event 的依赖关系稍微复杂一些：发起者明确不依赖接收者，接收者则存在对发起者 “隐式的反向依赖” ——反向是指和 Command 的依赖关系相比方向调转，是接受者反过来依赖发起者；隐式则是指这种依赖只体现于 “接受者依赖 Event，而 Event 是由发起者发布” 的间接关系中，接受者和发起者之间并不存在直接依赖关系。 从业务视角出发：关系模型决定通讯行为在温习完 Command 和 Event 之后，我们再来看我们前面的问题：为什么简单的将直接方法调用替换为远程调用（REST 或者 RPC）会出问题？主要原因是在这个替换过程中，所谓简单是指不假思索直接选择远程调用，也就是选择全程 Command 方式： 真实业务场景下各个组件（微服务或者 Function）的业务逻辑关系，通常不会像上图这么夸张，不应该全是 Command （后面会谈到也不应该全是 Event） ，而应该是类似下图描述的两者结合，以微服务为例（Function 类推）： 业务输入：图上微服务 A 接收到业务请求的输入（可能是 Command 方式，也可能是 Event 方式） 业务逻辑 “实现” 的执行过程： 微服务 A 在执行 Command（或者被 Event 触发）的过程中，会有很多动作（Action）； 有些是微服务 A 内部的动作，比如操作数据库，操作 key-value 存储，内存中的业务逻辑处理等； 有些是和外部微服务进行通讯，如执行查询或要求对方进行某些操作，这些通讯方式是以 Command 的形式，如图上和微服务 B 的通讯； 在这些内部和外部动作完成之后，执行过程完成； 如果是 Command，则需要以应答的形式给回 Command 操作的结果。 业务状态变更触发的后续行为： 在上面的执行过程完成后，如果涉及到业务状态的变更，则需要为此发布事件； 事件通过 event bus 分发给对该事件感兴趣的其他微服务：注意这个过程是解耦的，微服务 A 不清楚也不关心哪些微服务对此事件感兴趣，事件也不需要应答。 上面微服务 A 的业务逻辑执行处理过程中，需要以 Command 或者 Event 方式和其他微服务通讯，如图中的微服务 B/C/D/E。而对于这些微服务 B/C/D/E（视为微服务 A 的下游服务），他们在接受到业务请求后的处理流程和微服务 A 的处理流程是类似的。 因此我们可以简单推导一下，当业务处理逻辑从微服务 A 延展到微服务 A 的下游服务（图中的微服务 B/C/D/E）时的场景： 将图中涉及的微服务 A/B/C/D/E 在处理业务逻辑的行为总结下来，通讯行为大体是一样的： 抽象起来，一个典型的微服务在业务处理流程中的通讯行为可以概括为以下四点： 输入：以一个 Command 请求或者一个 Event 通知为输入，这是业务处理流程的起点。 内部 Action：微服务的内部逻辑，典型如数据库操作，访问 Redis 等 key-value 存储（对应于 Multiple Runtime/Mecha 架构中的各种分布式能力）。可选，通常为 0-N 个。 外部访问：以 Command 形式访问外部的其他微服务。可选，通常为 0-N 个。 通告变更：以 Event 形式对外发布事件，通告上述操作产生的业务状态的变更。可选，通常为 0-1 个。 在这个行为模式中，2 和 3 是没有顺序的，而且可能交错执行，而 4 通常都是在流程的最后：只有当各种内部 Action 和外部 Command 都完成，业务逻辑实现结束，状态变更完成，“木已成舟”，才能以 Event 的方式对外发布：“操作已完成，状态已变更，望周知”。 这里我们回顾一下前面的总结 —— Event 和 Command 的本质区别在于他们的意图： Event 的意图是告知已经发生的事情； Command 的意图是告知希望发生的事情。 从业务逻辑处理的角度来看，外部访问的 Command 和内部操作的 Action 是业务逻辑的 “实现” 部分：这些操作组成了完整的业务逻辑——如果这些操作失败，则业务处理将会直接影响（失败或者部分失败）。而发布事件则是业务逻辑完成之后的后续 “通知” 部分：当业务逻辑处理完毕，状态变更完成后，以事件的方式驱动后续的进一步处理。注意是驱动，而不是直接操纵。 从时间线的角度来看整个业务处理流程如下图所示： 全程 Command 带来的问题：不必要的强耦合全程 Command 的微服务系统，存在的问题就是在上述最后阶段的“状态变更通知”环节，没有采用 Event 和 pub-sub 模型，而是继续使用 Command 逐个调用下游相关的其他微服务： Event 可以解耦生产者和消费者，因此图中的微服务 A 和微服务 C/D/E 之间没有强烈的依赖关系，彼此无需锁定对方的存在。但是 Command 不同，在采用 Command 方式后微服务 A 和下游相关微服务 C/D/E 会形成强依赖，而且这种依赖关系会蔓延，最终导致形成一颗巨大而深层次的依赖树，而 Function 由于粒度更细，问题往往更严重： 而如果在“状态变更通知”环节引入 Event，则可以解耦微服务和下游被通知的微服务，从而将依赖关系解除，避免无限制的蔓延。如下图所示，左边图形是使用 Event 代替 Command 来进行状态变更通知之后的依赖关系，考虑到 Event 对生产者和消费者的解耦作用，我们“斩断”绿色的 Event 箭头，这样就得到了右边这样一个被分解为多个小范围依赖树的系统依赖关系图： 对 Event 和 Command 使用的建议： 在单体应用拆分为微服务时，不应该简单的将原有的方法调用替换为 Command； 应该审视每个调用在业务逻辑上的语义：是业务逻辑执行的组成部分？还是执行完成之后的状态通知？ 然后据此决定采用 Command 还是 Event。 编排和协调在 Command 和 Event 的使用上，还有两个概念：编排和协调。 这里强烈推荐一篇博客文章， Microservices Choreography vs Orchestration: The Benefits of Choreography，作者 Jonathan Schabowsky ，Solace 的 CTO。他在这边博客中总结了让微服务协同工作的两种模式，并做了一个生动的比喻： https://solace.com/blog/microservices-choreography-vs-orchestration/ 编排（Orchestration）：需要主动控制所有的元素和交互，就像指挥家指挥乐团的乐手一样——对应 Command。 协调（Choreography）：需要建立一个模式，微服务会跟随音乐起舞，不需要监督和指令——对应 Event。 也曾看到很多持类似观点的文章，其中有一张图片印象深刻，我摘录过来： 左边是期望通过编排（Orchestration）方式得到的整齐划一的理想目标，右边是实际得到的大型翻车现场。 全程 Event 带来的问题：开发困难和业务边界不清晰在 Command 和 Event 的使用上，除了全程使用 Command 之外，还有一个极端是全程使用 Event，这一点在 Lambda（FaaS）中更常见一些： 这个方式首当其冲的问题就是在适用 Command 语义的地方采用了 Event 来替代，而由于 Command 和 Event 在使用语义上的差异，这个替代会显得别扭： Command 是一对一的，替代它的 Event 也不得不从 “1:N” 退化为 “1:1”，pub-sub 模型不再存在。 Command 是需要返回结果的，尤其是 Query 类的 Command 必须要有查询结果，使用 Event 替代之后，就不得不实现 “支持 Response 的 Event”，典型如在消息机制中实现 Request-Reply 模型的。 或者引入另外一个 Event 来反向通知结果，即用两个异步 Event 来替代一个同步的 Command —— 这需要让发起者进行额外的订阅和处理，开发复杂性远远超过使用简单的 Command。 而且还引入了一个非常麻烦的状态问题：即服务间通讯的上下文中通常是有状态的，Reply Event 必须准确的发送给 Request Event 的发起者的实例，而不能任意选择一个。这使得 Reply Event 不仅仅要 1:1 的绑定订阅者服务，还必须绑定这个服务的特定实例 —— 这样的 Reply Event 已经没法称为 Event 了。 绕开这个状态问题的常见方案是选择无状态的场景，如果处理 Reply Event 时无需考虑状态，那么 Event Reply 才能简单的发送给任意的实例。 对于粒度较大的微服务系统，通常很难实现无状态，所以在微服务中全程采用 Event 通常会比较别扭的，事实上也很少有人这样做。而在粒度非常小的 Function/FaaS 系统中，全程采用 Event 方式比较常见。 关于全程使用 Event，我个人持保留态度，我倾向于即使是在 FaaS 中，也适当保留 Command 的用法：如果某个操作是“业务逻辑”执行中不可或缺的一部分，那么 Command 方式的紧耦合反而更能体现出这个“业务逻辑”的存在： 如果完全采用 Event 方式，“彻底”解耦，则产生新的问题（且不论在编码方面额外带来的复杂度） —— 在海量细粒度的 Event 调用下，业务逻辑已经很难体现，领域模型（Domain Modeling）和 有界上下文（Bounded Context）则淹没在这些 Event 调用下，难于识别： 备注：这个问题被称为“Lambda Pinball”，这里不深入展开，后续计划会有一篇文章单独详细探讨“Lambda Pinball”的由来和解决的思路。 Command 和 Event 的选择：实事求是不偏不倚总结一下 Command 和 Event 的选择，我个人的建议是不要一刀切：全程 Command 方式的缺点容易理解，但简单替换为全程 Event 也未必合适。 我的个人观点是倾向于从实际“业务逻辑”处理的语义出发，判断： 如果是业务逻辑的 “实现” 部分：倾向于选择使用 Command； 如果是业务逻辑完成之后的后续 “通知” 部分：强烈建议选择使用 Event。 5、总结与反思警惕：不要沦为分布式单体上面我们列出了微服务和 Serverless 实践中容易形成 “分布式单体” 的两个主要原因和对策： 通过共享库和网络客户端访问分布式能力：引入非侵入方案解耦应用和各种分布式能力； 简单用远程调用替代进程内方法调用：区分 Command 和 Event，引入 Event 来解除微服务间不必要的强耦合。 前者在技术上目前还不太成熟，典型如 Istio/Dapr 项目都还有待加强，暂时在落地上阻力比较大。但后者已经是业界多年的成熟实践，甚至在微服务和 Serverless 兴起之前就广泛使用，因此建议可以立即着手改进。 关于如何更方便的将 Event 和 Event Driven Architecture 引入到微服务和 Serverless 中，同时又不与提供 Message Queue 分布式能力的具体实现耦合，我将在稍后文章中详细展开，敬请期待。 反思：喧闹和谩骂之外的冷静思考如果我们在微服务和 Serverless 实践中，始终停留在“用远程调用简单替代进程内方法调用”的程度，并固守单体时代的习惯引入各种 SDK，那么分布式单体问题就必然不可避免。我们的微服务转型、Serverless 实践最后得到的往往是： 把单体变成…更糟糕的分布式单体当然，微服务可能成为分布式单体，但这并不意味着微服务架构是个谎言，也不意味着比单体架构更差。Serverless 可能同样遭遇分布式单体（还有后续要深入探讨的 Lambda Pinball），但这也不意味着 Serverless 不可取 —— 微服务和 Serverless 都是解决特定问题的工具，和所有的工具一样，在使用工具之前，我们需要先研究和了解它们，学习如何正确的使用它们： 需要为微服务创建正确的架构，和单体架构必然会有很大的不同：一定不是“原封不动”的将方法调替换为远程调用，最好不要用共享类库和网络客户端的方式直接使用各种分布式能力； Serverless 更是需要我们对架构进行彻底的反思，需要改变思维方式，才能保证收益大于弊端。 参考资料和推荐阅读： 《Avoid the Distributed Monolith!!》：Verizon 的 Mohamad Byan 在 2018 年 9 月的一个演讲，描述微服务实践中的分布式单体陷阱和解决的方式。——https://www.slideshare.net/DevOpsDaysDFW/avoid-the-distributed-monolith 《Mecha：将 Mesh 进行到底》：详细介绍 Multiple Runtime / Macha 架构，将更多的分布式能力进行 Mesh 化——https://skyao.io/talk/202004-mecha-mesh-through-to-the-end/ 《The Eight Fallacies of Distributed Computing》：分布式计算领域的经典文章，中文翻译如下——http://www.xumenger.com/the-eight-fallacies-of-distributed-computing-20180817/ 《Opportunities and Pitfalls of Event-driven Utopia》：Bernd Rücker 在 QCon 上的一个演讲，讲述“事件驱动乌托邦的机遇与陷阱”，本文部分图片来自这份 PPT——https://www.youtube.com/watch?v=jjYAZ0DPLNM 《Practical DDD: Bounded Contexts + Events =&gt; Microservices》：Indu Alagarsamy 的一个演讲，介绍领域驱动开发（DDD）和 Messaging 的交集。推荐使用消息技术在干净、定义良好的有界上下文之间进行通信，以去除时空耦合。——https://www.infoq.com/presentations/microservices-ddd-bounded-contexts/ 《Building Event-Driven Cloud Applications and Services》：讨论构建事件驱动的应用和服务的通用实践和技术，是一个序列教程——https://medium.com/@ratrosy/building-event-driven-cloud-applications-and-services-ad0b5b970036。 《The Architect’s Guide to Event-Driven Microservices》：Solace 公司网站上的一份 PDF 格式的小册子——https://go.solace.com/wp-download-eventdrivenmicroservices.html 《致传统企业朋友：不够痛就别微服务，有坑》：网易云刘超刘老师的超级好文章，极其实在而全面的讲述微服务落地需要考虑的方方面面以及各种问题，强烈推荐阅读。——https://www.infoq.cn/article/Nd0RofAUp0WtlvlQArbu]]></content>
      <categories>
        <category>micro-service</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入分析Flink SQL的工作机制]]></title>
    <url>%2Fflink%2Fflink-sql%2F</url>
    <content type="text"><![CDATA[摘要：本文整理自 Flink Forward 2020 全球在线会议中文精华版，由 Apache Flink PMC 伍翀（云邪）分享，社区志愿者陈婧敏（清樾）整理。旨在帮助大家更好地理解 Flink SQL 引擎的工作原理。文章主要分为以下四部分： Flink SQL Architecture How Flink SQL Works? Flink SQL Optimizations Summary and Futures Tips：点击下方链接可查看作者分享的原版视频～https://ververica.cn/developers/flink-forward-virtual-conference/ Apache Flink 社区在最近的两个版本（1.9 &amp; 1.10 ）中为面向未来的统一流批处理在架构层面做了很多优化，其中一个重大改造是引入了 Blink Planner，开始支持 SQL &amp; Table API 使用不同的 SQL Planner 进行编译（Planner 的插件化）。 本文首先会介绍推动这些优化背后的思考，展示统一的架构如何更好地处理流式和批式查询，其次将深入剖析 Flink SQL 的编译及优化过程，包括： Flink SQL 利用 Apache Calcite 将 SQL 翻译为关系代数表达式，使用表达式折叠（Expression Reduce），下推优化（Predicate / Projection Pushdown ）等优化技术生成物理执行计划（Physical Plan），利用 Codegen 技术生成高效执行代码。 Flink SQL 使用高效的二进制数据存储结构 BinaryRow 加速计算性能；使用 Mini-batch 攒批提高吞吐，降低两层聚合时由 Retraction 引起的数据抖动；聚合场景下数据倾斜处理和 Top-N 排序的优化原理。 Flink SQL 架构 &amp; Blink Planner（1.9+ ）1.1 Old Planner 的限制要想了解 Flink SQL 在1.9 版本引入新架构的动机，我们首先看下 1.9 版本之前的架构设计 从图中可以看出，虽然面向用户的 Table API &amp; SQL 是统一的，但是流式和批式任务在翻译层分别对应了 DataStreamAPI 和 DataSetAPI，在 Runtime 层面也要根据不同的 API 获取执行计划，两层的设计使得整个架构能够复用的模块有限，不易扩展。 1.2 统一的 Blink PlannerFlink 在设计之初就遵循“批是流的特例”的理念，在架构上做到流批统一是大势所趋。在社区和阿里巴巴的共同努力下，1.9 版本引入了新的 Blink Planner，将批 SQL 处理作为流 SQL 处理的特例，尽量对通用的处理和优化逻辑进行抽象和复用，通过 Flink 内部的 Stream Transformation API 实现流 &amp; 批的统一处理，替代原 Flink Planner 将流 &amp; 批区分处理的方式。 此外，新架构通过灵活的插件化方式兼容老版本 Planner，用户可自行选择。不过在 1.11 版本 Blink Planner 会代替 Old Planner 成为默认的 Planner 来支持流 &amp; 批进一步融合统一（ Old Planner 将在之后逐步退出历史舞台）。 Flink SQL 工作流 Flink SQL 引擎的工作流总结如图所示。 从图中可以看出，一段查询 SQL / 使用TableAPI 编写的程序（以下简称 TableAPI 代码）从输入到编译为可执行的 JobGraph 主要经历如下几个阶段 将 SQL文本 / TableAPI 代码转化为逻辑执行计划（Logical Plan） Logical Plan 通过优化器优化为物理执行计划（Physical Plan） 通过代码生成技术生成 Transformations 后进一步编译为可执行的 JobGraph 提交运行 本节将重点对 Flink SQL 优化器的常用优化方法和 CodeGen 生成 Transformations 进行介绍。 2.1 Logical PlanningFlink SQL 引擎使用 Apache Calcite SQL Parser 将 SQL 文本解析为词法树，SQL Validator 获取 Catalog 中元数据的信息进行语法分析和验证，转化为关系代数表达式（RelNode），再由 Optimizer 将关系代数表达式转换为初始状态的逻辑执行计划。 备注：TableAPI 代码使用 TableAPI Validator 对接 Catalog 后生成逻辑执行计划。 E.g.1 考虑如下表达 JOIN 操作的一段 SQL。 123456SELECT t1.id, 1 + 2 + t1.value AS v FROM t1, t2 WHERE t1.id = t2.id AND t2.id &lt; 1000 经过上述操作后得到了一个树状结构的逻辑执行计划，根节点对应最上层的 Select 语句，叶子节点对应输入表 t1 和 t2 的 TableScan 操作，Join 和 Where 条件过滤 分别对应了 Join 和 Filter 节点。 12345LogicalProject(id=[$0], v=[+(+(1, 2), $1)])+- LogicalFilter(condition=[AND(=($0, $3), &lt;($3, 1000))]) +- LogicalJoin(condition=[true], joinType=[inner]) :- LogicalTableScan(table=[[default_catalog, default, t1]]) +- LogicalTableScan(table=[[default_catalog, default, t2]]) 可视化后如图所示，这是优化器开始工作的初始状态。 下面开始介绍 Flink SQL 优化器常见的几种优化方式。 ■ 2.1.1 Expression Reduce 表达式（Expression） 是 SQL 中最常见的语法。比如 t1.id 是一个表达式， 1 + 2 + t1.value 也是一个表达式。优化器在优化过程中会递归遍历树上节点，尽可能预计算出每个表达式的值，这个过程就称为表达式折叠。这种转换在逻辑上等价，通过优化后，真正执行时不再需要为每一条记录都计算一遍 1 + 2。 ■ 2.1.2 PushDown Optimization 下推优化是指在保持关系代数语义不变的前提下将 SQL 语句中的变换操作尽可能下推到靠近数据源的位置以获得更优的性能，常见的下推优化有谓词下推（Predicate Pushdown），投影下推（Projection Pushdown，有时也译作列裁剪）等。 Predicate Pushdown 回顾 E.g.1，我们发现 WHERE 条件表达式中 t2.id &lt; 1000 这个过滤条件描述的是对表 t2 的约束，跟表 t1 无关，完全可以下推到 JOIN 操作之前完成。假设表 t2 中有一百万行数据，但是满足 id &lt; 1000 的数据只有 1,000 条，则通过谓词下推优化后到达 JOIN 节点的数据量降低了1,000 倍，极大地节省了 I / O 开销，提升了 JOIN 性能。 谓词下推（Predicate Pushdown）是优化 SQL 查询的一项基本技术，谓词一词来源于数学，指能推导出一个布尔返回值（TRUE / FALSE）的函数或表达式，通过判断布尔值可以进行数据过滤。谓词下推是指保持关系代数语义不变的前提下将 Filter 尽可能移至靠近数据源的位置（比如读取数据的 SCAN 阶段）来降低查询和传递的数据量（记录数）。 Projection Pushdown 列裁剪是 Projection Pushdown 更直观的描述方式，指在优化过程中去掉没有使用的列来降低 I / O 开销，提升性能。但与谓词下推只移动节点位置不同，投影下推可能会增加节点个数。比如最后计算出的投影组合应该放在 TableScan 操作之上，而 TableScan 节点之上没有 Projection 节点，优化器就会显式地新增 Projection 节点来完成优化。另外如果输入表是基于列式存储的（如 Parquet 或 ORC 等），优化还会继续下推到 Scan 操作中进行。 回顾 E.g.1，我们发现整个查询中只用到了表 t1 的 id 和 value 字段，表 t2 的 id 字段，在 TableScan 节点之上分别增加 Projection 节点去掉多余字段，极大地节省了 I / O 开销 简要总结一下，谓词下推和投影下推分别通过避免处理不必要的记录数和字段数来降低 I / O 开销提升性能。 2.2 Physical Planning on Batch通过上述一系列操作后，我们得到了优化后的逻辑执行计划。逻辑执行计划描述了执行步骤和每一步需要完成的操作，但没有描述操作的具体实现方式。而物理执行计划会考虑物理实现的特性，生成每一个操作的具体实现方式。比如 Join 是使用 SortMergeJoin、HashJoin 或 BroadcastHashJoin 等。优化器在生成逻辑执行计划时会计算整棵树上每一个节点的 Cost，对于有多种实现方式的节点（比如 Join 节点），优化器会展开所有可能的 Join 方式分别计算。最终整条路径上 Cost 最小的实现方式就被选中成为 Final Physical Plan。 回顾 E.g.1，当它以批模式执行，同时我们可以拿到输入表的 Statistics 信息。在经过前述优化后，表 t2 到达 Join 节点时只有 1,000 条数据，使用 BroadcastJoin 的开销相对最低，则最终的 Physical Plan 如下图所示。 2.3 Translation &amp; Code Generation代码生成（Code Generation） 在计算机领域是一种广泛使用的技术。在 Physical Plan 到生成 Transformation Tree 过程中就使用了 Code Generation。 回顾 E.g.1，以 表 t2 之上的 Calc 节点 t2.id &lt; 1000 表达式为例，通过 Code Generation 后生成了描述 Transformation Operator 的一段 Java 代码，将接收到的 Row 中 id &lt; 1000 的 Row 发送到下一个 Operator。 Flink SQL 引擎会将 Physical Plan 通过 Code Generation 翻译为 Transformations，再进一步编译为可执行的 JobGraph。 2.4 Physical Planning on Stream以上介绍了 Flink SQL 引擎的整体工作流，上述例子是假定以批模式编译的，下面我们来介绍一下以流模式编译时，在生成 Physical Plan 过程中的一个重要机制：Retraction Mechanism （aka. Changelog Mechanism）。 ■ 2.4.1 Retraction Mechanism Retraction 是流式数据处理中撤回过早下发（Early Firing）数据的一种机制，类似于传统数据库的 Update 操作。级联的聚合等复杂 SQL 中如果没有 Retraction 机制，就会导致最终的计算结果与批处理不同，这也是目前业界很多流计算引擎的缺陷。 E.g.2 考虑如下统计词频分布的 SQL。 123456SELECT cnt, COUNT(cnt) as freqFROM ( SELECT word, COUNT(*) as cnt FROM words GROUP BY word)GROUP BY cnt 假设输入数据是： word Hello World Hello 则经过上面的计算后，预期的输出结果应该是： cnt freq 1 1 2 1 但与批处理不同，流处理的数据是一条条到达的，理论上每一条数据都会触发一次计算，所以在处理了第一个 Hello 和第一个 World 之后，词频为 1 的单词数已经变成了 2，此时再处理第二个 Hello 时，如果不能修正之前的结果，Hello 就会在词频等于 1 和词频等于 2 这两个窗口下被同时统计，显然这个结果是错误的，这就是没有 Retraction 机制带来的问题。 Flink SQL 在流计算领域中的一个重大贡献就是首次提出了这个机制的具体实现方案。Retraction 机制又名 Changelog 机制，因为某种程度上 Flink 将输入的流数据看作是数据库的 Changelog，每条输入数据都可以看作是对数据库的一次变更操作，比如 Insert，Delete 或者 Update。以 MySQL 数据库为例，其Binlog 信息以二进制形式存储，其中 Update_rows_log_event 会对应 2 条标记 Before Image （BI） 和 After Image （AI），分别表示某一行在更新前后的信息。 在 Flink SQL 优化器生成流作业的 Physical Plan 时会判断当前节点是否是更新操作，如果是则会同时发出 2 条消息 update_before 和 update_after 到下游节点，update_before 表示之前“错误”下发的数据，需要被撤回，update_after 表示当前下发的“正确”数据。下游收到后，会在结果上先减去 update_before，再加上 update_after。 回顾 E.g.2，下面的动图演示了加入 Retraction 机制后正确结果的计算过程。 update_before 是一条非常关键的信息，相当于标记出了导致当前结果不正确的那个“元凶”。不过额外操作会带来额外的开销，有些情况下不需要发送 update_before 也可以获得正确的结果，比如下游节点接的是 UpsertSink（MySQL 或者 HBase的情况下，数据库可以按主键用 update_after 消息覆盖结果）。是否发送 update_before 由优化器决定，用户不需要关心。 ■ 2.4.2 Update_before Decision 前面介绍了 Retraction 机制和 update_before，那优化器是怎样决定是否需要发送update_before 呢？本节将介绍这一部分的工作。 Step1：确定每个节点对应的 Changelog 变更类型 数据库中最常见的三种操作类型分别是 Insert （记为 [I]），Delete（记为 [D]），Update（记为 [U]）。优化器首先会自底向上检查每个节点，判断它属于哪（几）种类型，分别打上对应标记。 回顾 E.g.2，第一个 Source 节点由于只产生新数据，所以属于 Insert，记为 [I]；第二个节点计算内层的聚合，所以会发出更新的消息，记为 [I，U]；第三个节点裁掉 word 字段，属于简单计算，传递了上游的变更类型，记为 [I，U]；第四个节点是外层的聚合计算，由于它收到了来自上游的 Update 消息，所以额外需要 Delete 操作来保证更新成功，记为 [I，U，D]。 Step2：确定每个节点发送的消息类型 在介绍 Step2 之前，我们先介绍下 Flink 中 Update 消息类型的表示形式。在 Flink 中 Update 由两条 update_before（简称 UB）和 update_after （简称 UA）来表示，其中 UB 消息在某些情况下可以不发送，从而提高性能。 在 Step1 中优化器自底向上推导出了每个节点对应的 Changelog 变更操作，这一步里会先自顶向下推断当前节点需要父节点提供的消息类型，直到遇到第一个不需要父节点提供任何消息类型的节点，再往上回推每个节点最终的实现方式和需要的消息类型。 回顾 E.g.2，由于最上层节点是 UpsertSink 节点，只需要它的父节点提供 [UA] 即可。到了外层聚合的 Aggregate 节点，由于 Aggregate 节点的输入有 Update 操作，所以需要父节点需要提供 [UB，UA]，这样才能正确更新自己的计算状态。 再往下到 Calc 节点，它需要传递 [UB，UA] 的需求给它的父节点，也就是内层的 Aggregate 节点。而到了内层 Aggregation 节点，它的父节点是 Source 节点，不会产生 Update 操作，所以它不需要 Source 节点额外发送任何 [UB / UA ]。当优化器遍历到 Source 节点，便开始进行回溯，如果当前节点能满足子节点的 requirement，则将对应的标签更新到节点上，否则便无法生成 plan。首先内层的 Aggregate 能产生 UB，所以能满足子节点的 requirement，所以优化器会给内层的 Aggregate 节点打上 [UB，UA] 的标签，然后向上传递到 Calc 节点，同样打上 [UB，UA] ，再到外层的 Aggregate 节点，由于它的下游只需要接受更新后的消息，所以打上 [UA] 标签，表示它只需要向下游发送 update_after 即可。 这些标签最终会影响算子的物理实现，比如外层的 Aggregate 节点，由于它会接收到来自上游的 [UB]，所以物理实现会使用带 Retract 的 Count，同时它只会向 Sink 发送 update_after。而内层的 Aggregate 节点，由于上游发送过来的数据没有 [UB]，所以可以采用不带 Retract 的 Count 实现，同时由于带有 [UB] 标签，所以需要往下游发送 update_before。 Flink SQL Internal Optimization前面介绍了 Flink SQL 引擎的工作原理，接下来会简要概括一下 Flink SQL 内部的一些优化，更多资料可以在 Flink Forward Asia 2019 查看。 3.1 BinaryRow在 Flink 1.9+ 前， Flink Runtime 层各算子间传递的数据结构是 Row，其内部实现是 Object[]。这种数据结构的问题在于不但需要额外开销存 Object Metadata，计算过程中还涉及到大量序列化 / 反序列 （特别是只需要处理某几个字段时需要反序列化整个 Row），primitive 类型的拆 / 装箱等，都会带来大量额外的性能开销。 Flink 1.9 开始引入了 Blink Planner，使用二进制数据结构的 BinaryRow 来表示 Record。BinaryRow 作用于默认大小为 32K 的 Memory Segment，直接映射到内存。BinaryRow 内部分为 Header，定长区和变长区。Header 用于存储 Retraction 消息的标识，定长区使用 8 个 bytes 来记录字段的 Nullable 信息及所有 primitive 和可以在 8 个 bytes 内表示的类型。其它类型会按照基于起始位置的 offset 存放在变长区。 BinaryRow 作为 Blink Planner 的基础数据结构，带来的好处是显而易见的：首先存储上更为紧凑，去掉了额外开销；其次在序列化和反序列化上带来的显著性能提升，可根据 offset 只反序列化需要的字段，在开启 Object Reuse 后，序列化可以直接通过内存拷贝完成。 3.2 Mini-batch ProcessingFlink 是纯流式处理框架，在理论上每一条新到的数据都会触发一次计算。然而在实现层面，这样做会导致聚合场景下每处理一条数据都需要读写 State 及序列化 / 反序列化。如果能够在内存中 buffer 一定量的数据，预先做一次聚合后再更新 State，则不但会降低操作 State 的开销，还会有效减少发送到下游的数据量，提升 throughput，降低两层聚合时由 Retraction 引起的数据抖动, 这就是 Mini-batch 攒批优化的核心思想。 3.3 Skew Processing对于数据倾斜的优化，主要分为是否带 DISTINCT 去重语义的两种方式。对于普通聚合的数据倾斜，Flink 引入了 Local-Global 两阶段优化，类似于 MapReduce 增加 Local Combiner 的处理模式。而对于带有去重的聚合，Flink 则会将用户的 SQL 按原有聚合的 key 组合再加上 DISTINCT key 做 Hash 取模后改写为两层聚合来进行打散。 3.4 Top-N Rewrite全局排序在流式的场景是很难实现的，但如果只需要计算到目前的 Top-N 极值，问题就变得可解。不过传统数据库求排序的 SQL 语法是通过 ORDER BY 加 LIMIT 限制条数，背后实现的机制也是通过扫描全表排序后再返回 LIMIT 条数的记录。另外如果按照某些字段开窗排序，ORDER BY 也无法满足要求。Flink SQL 借鉴了批场景下开窗求 Top-N 的语法，使用 ROW_NUMBER 语法来做流场景下的 Top-N 排序。 E.g.3 下面这段 SQL 计算了每个类目下销量 Top3 的店铺 123456SELECT*FROM( SELECT *, -- you can get like shopId or other information from this ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) AS rowNum FROM shop_sales ) WHERE rowNum &lt;= 3 在生成 Plan 方面，ROW_NUMBER 语义对应 OverAggregate 窗口节点和一个过滤行数的 Calc 节点，而这个窗口节点在实现层面需要为每一个到达的数据重新将 State 中的历史数据拿出来排序，这显然不是最优解。 我们知道流式场景求解极大 / 小值的最优操作是通过维护一个 size 为 N 的 minHeap / maxHeap。由实现反推出我们需要在优化器上新增一条规则，在遇到 ROW_NUMBER 生成的逻辑节点后，将其优化为一个特殊的 Rank 节点，对应上述的最优实现方式（当然这只是特殊 Rank 对应的其中一种实现）。这便是 Top-N Rewrite 的核心思想。 Summary &amp; Futures本文内容回顾 简要介绍 Flink 1.9 + 在 SQL &amp; TableAPI 上引入新架构，统一技术栈，朝着流 &amp; 批一体的方向迈进了一大步。 深入介绍 Flink SQL 引擎的内部运行机制，以及在对用户透明的同时，Flink SQL 在优化方面做的许多工作。 未来工作计划 在 Flink 1.11+ 后的版本，Blink Planner 将作为默认的 Planner 提供生产级别的支持。 FLIP-95：重构 TableSource &amp; TableSink 的接口设计，面向流批一体化，在 Source 端支持 changelog 消息流，从而支持 FLIP-105 的 CDC 数据源。 FLIP-105：Flink TableAPI &amp; SQL 对 CDC 的支持。 FLIP-115：扩展目前只支持 CSV 的 FileSystem Connector，使其成为流批统一的 Generalized FileSystem Connector。 FLIP-123：对 Hive DDL 和 DML 的兼容，支持用户在 Flink 中运行 Hive DDL。]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Istio 介绍]]></title>
    <url>%2Fmicro-service%2Fistio%2Fistio-doc%2F</url>
    <content type="text"><![CDATA[官方文档：https://istio.io/zh/docs/ 概念Istio 是什么？云平台令使用它们的公司受益匪浅。但不可否认的是，上云会给 DevOps 团队带来压力。为了可移植性，开发人员必须使用微服务来构建应用，同时运维人员也正在管理着极端庞大的混合云和多云的部署环境。 Istio 允许您连接、保护、控制和观察服务。 从较高的层面来说，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，作为透明的一层接入到现有的分布式应用程序里。它也是一个平台，拥有可以集成任何日志、遥测和策略系统的 API 接口。Istio 多样化的特性使您能够成功且高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。 服务网格是什么？Istio 解决了开发人员和运维人员所面临的从单体应用向分布式微服务架构转变的挑战。了解它是如何做到这一点的可以让我们更详细地理解 Istio 的服务网格。 术语服务网格用来描述组成这些应用程序的微服务网络以及它们之间的交互。随着服务网格的规模和复杂性不断的增长，它将会变得越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、度量和监控等。服务网格通常还有更复杂的运维需求，比如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端认证。 Istio 提供了对整个服务网格的行为洞察和操作控制的能力，以及一个完整的满足微服务应用各种需求的解决方案。 为什么使用 Istio？通过负载均衡、服务间的身份验证、监控等方法，Istio 可以轻松地创建一个已经部署了服务的网络，而服务的代码只需很少更改甚至无需更改。通过在整个环境中部署一个特殊的 sidecar 代理为服务添加 Istio 的支持，而代理会拦截微服务之间的所有网络通信，然后使用其控制平面的功能来配置和管理 Istio，这包括： 为 HTTP、gRPC、WebSocket 和 TCP 流量自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行细粒度控制。 可插拔的策略层和配置 API，支持访问控制、速率限制和配额。 集群内（包括集群的入口和出口）所有流量的自动化度量、日志记录和追踪。 在具有强大的基于身份验证和授权的集群中实现安全的服务间通信。 Istio 为可扩展性而设计，可以满足不同的部署需求。 核心特性Istio 以统一的方式提供了许多跨服务网络的关键功能： 流量管理Istio 简单的规则配置和流量路由允许您控制服务之间的流量和 API 调用过程。Istio 简化了服务级属性（如熔断器、超时和重试）的配置，并且让它轻而易举的执行重要的任务（如 A/B 测试、金丝雀发布和按流量百分比划分的分阶段发布）。 有了更好的对流量的可视性和开箱即用的故障恢复特性，您就可以在问题产生之前捕获它们，无论面对什么情况都可以使调用更可靠，网络更健壮。 请参考流量管理文档获取更多细节。 安全Istio 的安全特性解放了开发人员，使其只需要专注于应用程序级别的安全。Istio 提供了底层的安全通信通道，并为大规模的服务通信管理认证、授权和加密。有了 Istio，服务通信在默认情况下就是受保护的，可以让您在跨不同协议和运行时的情况下实施一致的策略——而所有这些都只需要很少甚至不需要修改应用程序。 Istio 是独立于平台的，可以与 Kubernetes（或基础设施）的网络策略一起使用。但它更强大，能够在网络和应用层面保护pod到 pod 或者服务到服务之间的通信。 请参考安全文档获取更多细节。 可观察性Istio 健壮的追踪、监控和日志特性让您能够深入的了解服务网格部署。通过 Istio 的监控能力，可以真正的了解到服务的性能是如何影响上游和下游的；而它的定制 Dashboard 提供了对所有服务性能的可视化能力，并让您看到它如何影响其他进程。 Istio 的 Mixer 组件负责策略控制和遥测数据收集。它提供了后端抽象和中介，将一部分 Istio 与后端的基础设施实现细节隔离开来，并为运维人员提供了对网格与后端基础实施之间交互的细粒度控制。 所有这些特性都使您能够更有效地设置、监控和加强服务的 SLO。当然，底线是您可以快速有效地检测到并修复出现的问题。 请参考可观察性文档获取更多细节。 平台支持Istio 独立于平台，被设计为可以在各种环境中运行，包括跨云、内部环境、Kubernetes、Mesos 等等。您可以在 Kubernetes 或是装有 Consul 的 Nomad 环境上部署 Istio。Istio 目前支持： Kubernetes 上的服务部署 基于 Consul 的服务注册 服务运行在独立的虚拟机上 整合和定制Istio 的策略实施组件可以扩展和定制，与现有的 ACL、日志、监控、配额、审查等解决方案集成。 安装要开始使用 Istio，只需遵循以下三个步骤： 搭建平台 下载 Istio 安装 Istio 搭建平台在安装 Istio 之前，需要一个运行着 Kubernetes 的兼容版本的 cluster。 Istio 1.6 已经在 Kubernetes 版本 1.15, 1.16, 1.17, 1.18 中测试过。 通过选择合适的 platform-specific setup instructions 来创建一个集群。 有些平台提供了 managed control plane，您可以使用它来代替手动安装 Istio。如果您选择的平台支持这种方式，并且您选择使用它，那么，在创建完集群后，您将完成 Istio 的安装。因此，可以跳过以下说明。 下载 Istio下载 Istio，下载内容将包含：安装文件、示例和 istioctl 命令行工具。 访问 Istio release 页面下载与您操作系统对应的安装文件。在 macOS 或 Linux 系统中，也可以通过以下命令下载最新版本的 Istio： 1$ curl -L https://istio.io/downloadIstio | sh - 切换到 Istio 包所在目录下。例如：Istio 包名为 istio-1.6.0，则： 1$ cd istio-1.6.0 安装目录包含如下内容： install/kubernetes 目录下，有 Kubernetes 相关的 YAML 安装文件 samples/ 目录下，有示例应用程序 bin/ 目录下，包含 istioctl 的客户端文件。istioctl 工具用于手动注入 Envoy sidecar 代理。 将 istioctl 客户端路径增加到 path 环境变量中，macOS 或 Linux 系统的增加方式如下： 1$ export PATH=$PWD/bin:$PATH 在使用 bash 或 ZSH 控制台时，可以选择启动 auto-completion option。 安装 Istio请按照以下步骤在您所选的平台上使用 demo 配置文件安装 Istio。 安装 demo 配置 1$ istioctl manifest apply --set profile=demo 为了验证是否安装成功，需要先确保以下 Kubernetes 服务正确部署，然后验证除 jaeger-agent 服务外的其他服务，是否均有正确的 CLUSTER-IP： 123456789101112131415161718$ kubectl get svc -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgrafana ClusterIP 172.21.211.123 &lt;none&gt; 3000/TCP 2mistio-citadel ClusterIP 172.21.177.222 &lt;none&gt; 8060/TCP,15014/TCP 2mistio-egressgateway ClusterIP 172.21.113.24 &lt;none&gt; 80/TCP,443/TCP,15443/TCP 2mistio-galley ClusterIP 172.21.132.247 &lt;none&gt; 443/TCP,15014/TCP,9901/TCP 2mistio-ingressgateway LoadBalancer 172.21.144.254 52.116.22.242 15020:31831/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:30318/TCP,15030:32645/TCP,15031:31933/TCP,15032:31188/TCP,15443:30838/TCP 2mistio-pilot ClusterIP 172.21.105.205 &lt;none&gt; 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2mistio-policy ClusterIP 172.21.14.236 &lt;none&gt; 9091/TCP,15004/TCP,15014/TCP 2mistio-sidecar-injector ClusterIP 172.21.155.47 &lt;none&gt; 443/TCP,15014/TCP 2mistio-telemetry ClusterIP 172.21.196.79 &lt;none&gt; 9091/TCP,15004/TCP,15014/TCP,42422/TCP 2mjaeger-agent ClusterIP None &lt;none&gt; 5775/UDP,6831/UDP,6832/UDP 2mjaeger-collector ClusterIP 172.21.135.51 &lt;none&gt; 14267/TCP,14268/TCP 2mjaeger-query ClusterIP 172.21.26.187 &lt;none&gt; 16686/TCP 2mkiali ClusterIP 172.21.155.201 &lt;none&gt; 20001/TCP 2mprometheus ClusterIP 172.21.63.159 &lt;none&gt; 9090/TCP 2mtracing ClusterIP 172.21.2.245 &lt;none&gt; 80/TCP 2mzipkin ClusterIP 172.21.182.245 &lt;none&gt; 9411/TCP 2m 如果集群运行在一个不支持外部负载均衡器的环境中（例如：minikube），istio-ingressgateway 的 EXTERNAL-IP 将显示为 `` 状态。请使用服务的 NodePort 或 端口转发来访问网关。 请确保关联的 Kubernetes pod 已经部署，并且 STATUS 为 Running： 1234567891011121314$ kubectl get pods -n istio-systemNAME READY STATUS RESTARTS AGEgrafana-f8467cc6-rbjlg 1/1 Running 0 1mistio-citadel-78df5b548f-g5cpw 1/1 Running 0 1mistio-egressgateway-78569df5c4-zwtb5 1/1 Running 0 1mistio-galley-74d5f764fc-q7nrk 1/1 Running 0 1mistio-ingressgateway-7ddcfd665c-dmtqz 1/1 Running 0 1mistio-pilot-f479bbf5c-qwr28 1/1 Running 0 1mistio-policy-6fccc5c868-xhblv 1/1 Running 2 1mistio-sidecar-injector-78499d85b8-x44m6 1/1 Running 0 1mistio-telemetry-78b96c6cb6-ldm9q 1/1 Running 2 1mistio-tracing-69b5f778b7-s2zvw 1/1 Running 0 1mkiali-99f7467dc-6rvwp 1/1 Running 0 1mprometheus-67cdb66cbb-9w2hm 1/1 Running 0 1m 后续步骤安装 Istio 后，就可以部署您自己的服务，或部署安装程序中系统的任意一个示例应用。 应用程序必须使用 HTTP/1.1 或 HTTP/2.0 协议用于 HTTP 通信；HTTP/1.0 不支持。 当使用 kubectl apply 来部署应用时，如果 pod 启动在标有 istio-injection=enabled 的命名空间中，那么，Istio sidecar 注入器将自动注入 Envoy 容器到应用的 pod 中： 12$ kubectl label namespace &lt;namespace&gt; istio-injection=enabled$ kubectl create -n &lt;namespace&gt; -f &lt;your-app-spec&gt;.yaml 在没有 istio-injection 标记的命名空间中，在部署前可以使用 istioctl kube-inject 命令将 Envoy 容器手动注入到应用的 pod 中： 1$ istioctl kube-inject -f &lt;your-app-spec&gt;.yaml | kubectl apply -f - 如果您不确定要从哪开始，可以先部署 Bookinfo 示例，它会让您体验到 Istio 的流量路由、故障注入、速率限制等功能。 然后您可以根据您的兴趣浏览各种各样的 Istio 任务。 下列任务都是初学者开始学习的好入口： 请求路由 故障注入 流量转移 查询指标 可视化指标 日志收集 速率限制 Ingress 网关 访问外部服务 可视化您的网格 下一步，可以定制 Istio 并部署您自己的应用。在您开始自定义 Istio 来适配您的平台或者其他用途之前，请查看以下资源： 部署模型 部署最佳实践 Pod 需求 常规安装说明 任务示例运维]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fjava%2Fsharding-sphere%2Fvitness%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[地址导航]]></title>
    <url>%2Ftools%2Fjznav%2F</url>
    <content type="text"><![CDATA[工具 webtools 禅道 wiki OA jira Jenkins 腾讯文档 开发 bweb-dev rabbitmq-dev 测试 生产 rabbitmq-qingyun rabbitmq-cluster]]></content>
  </entry>
  <entry>
    <title><![CDATA[永远不要使用双花括号初始化实例，否则可能会OOM！]]></title>
    <url>%2Fjava%2Fcollection%2Fjava-collection-map-init-oom-bug%2F</url>
    <content type="text"><![CDATA[生活中的尴尬无处不在，有时候你只是想简单的装一把，但某些“老同志”总是在不经意之间，给你无情的一脚，踹得你简直无法呼吸。 但谁让咱年轻呢？吃亏要趁早，前路会更好。 喝了这口温热的鸡汤之后，咱们来聊聊是怎么回事。 事情是这样的，在一个不大不小的项目中，小王写下了这段代码： 12345678Map&lt;String, String&gt; map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3");&#125;&#125;;map.forEach((k, v) -&gt; &#123; System.out.println("key:" + k + " value:" + v);&#125;); 本来是用它来替代下面这段代码的： 1234567Map&lt;String, String&gt; map = new HashMap();map.put("map1", "value1");map.put("map2", "value2");map.put("map3", "value3");map.forEach((k, v) -&gt; &#123; System.out.println("key:" + k + " value:" + v);&#125;); 两块代码的执行结果也是完全一样的： key:map3 value:value3 key:map2 value:value2 key:map1 value:value1 所以小王正在得意的把这段代码介绍给部门新来的妹子小甜甜看，却不巧被正在经过的老张也看到了。 老张本来只是想给昨天的枸杞再续上一杯 85° 的热水，但说来也巧，刚好撞到了一次能在小甜甜面前秀技术的一波机会，于是习惯性的整理了一下自己稀疏的秀发，便开启了 diss 模式。 “小王啊，你这个代码问题很大啊！” “怎么能用双花括号初始化实例呢？” 此时的小王被问的一脸懵逼，内心有无数个草泥马奔腾而过，心想你这头老牛竟然也和我争这颗嫩草，但内心却有一种不祥的预感，感觉自己要输，瞬间羞涩的不知该说啥，只能红着小脸，轻轻的“嗯？”了一声。 老张：“使用双花括号初始化实例是会导致内存溢出的啦！侬不晓得嘛？” 小王沉默了片刻，只是凭借着以往的经验来看，这“老家伙”还是有点东西的，于是敷衍的“哦~”了一声，仿佛自己明白了怎么回事一样，，其实内心仍然迷茫的一匹，为了不让其他同事发现，只得这般作态。 于是片刻的敷衍，待老张离去之后，才悄悄的打开了 Google，默默的搜索了一下。 小王：哦，原来如此…… 双花括号初始化分析首先，我们来看使用双花括号初始化的本质是什么？ 以我们这段代码为例： 12345Map&lt;String, String&gt; map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3");&#125;&#125;; 这段代码其实是创建了匿名内部类，然后再进行初始化代码块。 这一点我们可以使用命令javac将代码编译成字节码之后发现，我们发现之前的一个类被编译成两个字节码（.class）文件，如下图所示： 我们使用 Idea 打开DoubleBracket$1.class文件发现： 123456789import java.util.HashMap;class DoubleBracket$1 extends HashMap &#123; DoubleBracket$1(DoubleBracket var1) &#123; this.this$0 = var1; this.put("map1", "value1"); this.put("map2", "value2"); &#125;&#125; 此时我们可以确认，它就是一个匿名内部类。那么问题来了，匿名内部类为什么会导致内存溢出呢？ 匿名内部类的“锅”在 Java 语言中非静态内部类会持有外部类的引用，从而导致 GC 无法回收这部分代码的引用，以至于造成内存溢出。 思考 1：为什么要持有外部类？这个就要从匿名内部类的设计说起了，在 Java 语言中，非静态匿名内部类的主要作用有两个。 1、当匿名内部类只在外部类（主类）中使用时，匿名内部类可以让外部不知道它的存在，从而减少了代码的维护工作。 2、当匿名内部类持有外部类时，它就可以直接使用外部类中的变量了，这样可以很方便的完成调用，如下代码所示： 1234567891011public class DoubleBracket &#123; private static String userName = "磊哥"; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Map&lt;String, String&gt; map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3"); put(userName, userName); &#125;&#125;; &#125;&#125; 从上述代码可以看出在HashMap的方法内部，可以直接使用外部类的变量userName。 思考 2：它是怎么持有外部类的？关于匿名内部类是如何持久外部对象的，我们可以通过查看匿名内部类的字节码得知，我们使用javap -c DoubleBracket\$1.class命令进行查看，其中$1为以匿名类的字节码，字节码的内容如下； 123456789101112131415161718192021222324javap -c DoubleBracket\$1.classCompiled from "DoubleBracket.java"class com.example.DoubleBracket$1 extends java.util.HashMap &#123; final com.example.DoubleBracket this$0; com.example.DoubleBracket$1(com.example.DoubleBracket); Code: 0: aload_0 1: aload_1 2: putfield #1 // Field this$0:Lcom/example/DoubleBracket; 5: aload_0 6: invokespecial #7 // Method java/util/HashMap."&lt;init&gt;":()V 9: aload_0 10: ldc #13 // String map1 12: ldc #15 // String value1 14: invokevirtual #17 // Method put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 17: pop 18: aload_0 19: ldc #21 // String map2 21: ldc #23 // String value2 23: invokevirtual #17 // Method put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 26: pop 27: return&#125; 其中，关键代码的在putfield这一行，此行表示有一个对DoubleBracket的引用被存入到this$0中，也就是说这个匿名内部类持有了外部类的引用。 如果您觉得以上字节码不够直观，没关系，我们用下面的实际的代码来证明一下： 1234567891011121314151617181920212223import java.lang.reflect.Field;import java.util.HashMap;import java.util.Map;public class DoubleBracket &#123; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Map map = new DoubleBracket().createMap(); // 获取一个类的所有字段 Field field = map.getClass().getDeclaredField("this$0"); // 设置允许方法私有的 private 修饰的变量 field.setAccessible(true); System.out.println(field.get(map).getClass()); &#125; public Map createMap() &#123; // 双花括号初始化 Map map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3"); &#125;&#125;; return map; &#125;&#125; 当我们开启调试模式时，可以看出map中持有了外部对象DoubleBracket，如下图所示： 以上代码的执行结果为： class com.example.DoubleBracket 从以上程序输出结果可以看出：匿名内部类持有了外部类的引用，因此我们才可以使用$0正常获取到外部类，并输出相关的类信息。 什么情况会导致内存泄漏？当我们把以下正常的代码： 12345678public void createMap() &#123; Map map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3"); &#125;&#125;; // 业务处理....&#125; 改为下面这个样子时，可能会造成内存泄漏： 12345678public Map createMap() &#123; Map map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3"); &#125;&#125;; return map;&#125; 为什么用了「可能」而不是「一定」会造成内存泄漏？ 这是因为当此map被赋值为其他类属性时，可能会导致 GC 收集时不清理此对象，这时候才会导致内存泄漏。可以关注我「Java中文社群」后面会专门写一篇关于此问题的文章。 如何保证内存不泄露？要想保证双花扣号不泄漏，办法也很简单，只需要将map对象声明为static静态类型的就可以了，代码如下： 12345678public static Map createMap() &#123; Map map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2"); put("map3", "value3"); &#125;&#125;; return map;&#125; 什么？你不相信！ 没关系，我们用事实说话，使用以上代码，我们重新编译一份字节码，查看匿名类的内容如下： 123456789101112131415161718192021222324javap -c DoubleBracket\$1.classCompiled from "DoubleBracket.java"class com.example.DoubleBracket$1 extends java.util.HashMap &#123; com.example.DoubleBracket$1(); Code: 0: aload_0 1: invokespecial #1 // Method java/util/HashMap."&lt;init&gt;":()V 4: aload_0 5: ldc #7 // String map1 7: ldc #9 // String value1 9: invokevirtual #11 // Method put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 12: pop 13: aload_0 14: ldc #17 // String map2 16: ldc #19 // String value2 18: invokevirtual #11 // Method put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 21: pop 22: aload_0 23: ldc #21 // String map3 25: ldc #23 // String value3 27: invokevirtual #11 // Method put:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; 30: pop 31: return&#125; 从这次的代码我们可以看出，已经没有putfield关键字这一行了，也就是说静态匿名类不会持有外部对象的引用了。 为什么静态内部类不会持有外部类的引用？原因其实很简单，因为匿名内部类是静态的之后，它所引用的对象或属性也必须是静态的了，因此就可以直接从 JVM 的 Method Area（方法区）获取到引用而无需持久外部对象了。 双花括号的替代方案即使声明为静态的变量可以避免内存泄漏，但依旧不建议这样使用，为什么呢？ 原因很简单，项目一般都是需要团队协作的，假如那位老兄在不知情的情况下把你的static给删掉呢？这就相当于设置了一个隐形的“坑”，其他不知道的人，一不小心就跳进去了，所以我们可以尝试一些其他的方案，比如 Java8 中的 Stream API 和 Java9 中的集合工厂等。 替代方案 1：Stream使用 Java8 中的 Stream API 替代，示例如下。原代码： 1234List&lt;String&gt; list = new ArrayList() &#123;&#123; add("Java"); add("Redis");&#125;&#125;; 替代代码： 1List&lt;String&gt; list = Stream.of("Java", "Redis").collect(Collectors.toList()); 替代方案 2：集合工厂使用集合工厂的of方法替代，示例如下。原代码： 1234Map map = new HashMap() &#123;&#123; put("map1", "value1"); put("map2", "value2");&#125;&#125;; 替代代码： 1Map map = Map.of("map1", "Java", "map2", "Redis"); 显然使用 Java9 中的方案非常适合我们，简单又酷炫，只可惜我们还在用 Java 6...6...6...心碎了一地。 总结本文我们讲了双花括号初始化因为会持有外部类的引用，从而可以会导致内存泄漏的问题，还从字节码以及反射的层面演示了这个问题。 要想保证双花括号初始化不会出现内存泄漏的办法也很简单，只需要被static修饰即可，但这样做还是存在潜在的风险，可能会被某人不小心删除掉，于是我们另寻它道，发现了可以使用 Java8 中的 Stream 或 Java9 中的集合工厂of方法替代 双大括号。]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[被我误解的max_connect_errors]]></title>
    <url>%2Fmysql%2Fmysql-issue-max-connect-errors%2F</url>
    <content type="text"><![CDATA[原文：https://developer.aliyun.com/article/755441 第一节 什么是max_connect_errors一开始接触这个参数的时候，感觉他和max_connections的含义差不多，字面意思简单明了，这个参数的含义是最大连接错误数，翻翻mysql的文档中的解释是If more than this many successive connection requests from a host are interrupted without a successful connection, the server blocks that host from further connections,大意是：如果mysql服务器连续接收到了来自于同一个主机的请求，且这些连续的请求全部都没有成功的建立连接就被断开了，当这些连续的请求的累计值大于 max_connect_errors的设定值时，mysql服务器就会阻止这台主机后续的所有请求。”without a successful connection”那太好办了，故意输错密码不就行了，并且网上搜索了下该参数的说明，大量的文章充斥着” 防止暴力破解密码”的内容，于是兴高采烈的去做了测试。以下测试基于自建的mysql（非rds for mysql），由于rds for mysql无法直接设置set global，设置时需要在”rds控制台-参数这里”里进行设置：https://help.aliyun.com/document_detail/26179.html?spm=5176.11065259.1996646101.searchclickresult.44156de7pLffcV 第二节 测试max_connect_errors1,创建账号： 2,设置max_connect_errors为3： 3，故意输错密码3次，第四次使用正确密码登录进行验证： 4,结论是第四次依然可以登录，即密码不对（认证失败）不属于” ”without a successful connection””的范畴，网上的” 防止暴力破解密码”也不成立了。 第三节 继续分析max_connect_errors再继续看文档，发现还有以下说明： You can unblock blocked hosts by flushing the host cache. To do so, issue a FLUSH HOSTS statement or execute a mysqladmin flush-hosts command. 大意是： 当你遇到主机被阻止的时候，你可以清空host cache来解决，具体的清空方法是执行flush hosts或者在mysql服务器的shell里执行 mysqladmin flush-hosts操作 既然清空host cache可以解决主机被阻止访问的问题，那应该与host cache有些关系，看看host cache的介绍可能会有些眉目，关于host cache，文档解释如下： The MySQL server maintains a host cache in memory that contains information about clients: IP address, host name, and error information. The server uses this cache for nonlocal TCP connections. It does not use the cache for TCP connections established using a loopback interface address (127.0.0.1 or ::1), or for connections established using a Unix socket file, named pipe, or shared memory. 大意是： Mysql服务器会在内存里管理一个host cache，host cache里保存了一些客户端的ip地址，主机名，以及这个客户端在与server建立连接时遇到的一些错误信息，host cache对不是本地的TCP连接才有效，所以host cache对127.0.0.1 或者::1是无效的，并且对于Unix socket file、named pipe以及 shared memory方式建立的连接也是无效的。并且通过了解，host cache的内容可以通过performance_schema.host_cache来查看，通过performance_schema.host_cache表里的几个列的描述信息，对之前的测试不成立的原因有些了解了，部分相关列如下： IPThe IP address of the client that connected to the server, expressed as a string. 连接到mysql server的主机的连接地址 HOSTThe resolved DNS host name for that client IP, or NULL if the name is unknown. 通过dns解析IP地址获取到的该IP地址对应的mysql client的主机名 SUM_CONNECT_ERRORSThe number of connection errors that are deemed “blocking” (assessed against the max_connect_errors system variable). Only protocol handshake errors are counted, and only for hosts that passed validation (HOST_VALIDATED = YES). COUNT_HANDSHAKE_ERRORSThe number of errors detected at the wire protocol level. 通过SUM_CONNECT_ERRORS(连接错误计数)描述，重点是红色部分：只计算协议握手过程的错误(Only protocol handshake errors are counted),也就是说max_connect_errors 可能记录的是协议(不确定是tcp协议还是应用协议，通过抓包以及COUNT_HANDSHAKE_ERRORS的” the wire protocol level”说明可能是指应用协议)的握手过程中出现的错误 ，也就是可以说网络不好(无法顺利握手)会导致该问题。 第四节 继续测试max_connect_errors通过之前的说明，需要模拟应用协议握手失败的情况，最后考虑使用telnet一些来做测试 1,创建账号 2,设置max_connect_errors为3： 3,先使用telnet 10.26.254.217 3306连接3次，第四次使用正确的账号密码尝试登陆： telnet前查看performance_schema.host_cache的记录为空 第一次telnet 10.26.254.217 3306 第二次 telnet 10.26.254.217 3306 第三次telnet 10.26.254.217 3306 第四次执行mysql -h10.26.254.217 -utestcon -p123 -P3306 问题复现了，出现了错误提示ERROR 1129 (HY000): Host ‘10.24.236.231’ is blocked because of many connection errors; unblock with ‘mysqladmin flush-hosts’ 第五节 ERROR 1129 (HY000)问题延伸解决ERROR 1129 (HY000)的方法是执行flush host或者 mysqladmin flush-hosts，其目的是为了清空host cache里的信息，那是不是说不使用host cache就可以了？使host cache不生效的方式有如下两种： 1,设置 host_cache_size 为0/ 打开skip-host-cache 2,打开skip-name-resolve 需要通过测试看下推测是否生效 5.1 设置 host_cache_size 为0/ 打开skip-host-cache1,设置host_cache_size为0 2,再次查询performance_schema.host_cache 3,继续之前的测试：先使用telnet 10.26.254.217 3306连接3次，第四次使用正确的账号密码尝试登陆 更改已经生效，max_connect_errors的作用无效了 5.2 打开skip-name-resolve1,在cnf配置文件里设置skip-name-resolve 以此打开skip-name-resolve 2,继续之前的测试：先使用telnet 10.26.254.217 3306连接3次，第四次使用正确的账号密码尝试登陆 3,查询performance_schema.host_cache 更改已经生效，max_connect_errors的作用无效了，RDS for mysql 的skip_name_resolve是on的状态， 所以很少会出现ERROR 1129 (HY000)的错误]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[理解epoll（基于linux2.6.12.1）]]></title>
    <url>%2Fcomputer-network%2Flinux-srcode-epoll%2F</url>
    <content type="text"><![CDATA[epoll在现在的软件中占据了很大的分量，nginx，libuv等单线程事件循环的软件都使用了epoll。之前分析过select，今天分析一下epoll。 简介 epoll与select epoll_create epoll_ctl epoll_wait ET、LT模式 1#include &lt;sys/epoll.h&gt; epoll与select Epoll 没有最大并发连接的限制，上限是最大可以打开文件的数目 效率提升，epoll对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高，内核将句柄用红黑树保存的，IO效率不随FD数目增加而线性下降。 内存拷贝， select让内核把 FD 消息通知给用户空间的时候使用了内存拷贝的方式，开销较大，但是Epoll 在这点上使用了共享内存的方式，这个内存拷贝也省略了。 相比于select，epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。 并且，在linux/posix_types.h头文件有这样的声明： #define __FD_SETSIZE 1024 表示select最多同时监听1024个fd，当然，可以通过修改头文件再重编译内核来扩大这个数目，但这似乎并不治本。 epoll_create 1int epoll_create(int size); 创建一个epoll的句柄， size用来告诉内核这个监听的数目一共有多大。 这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 epoll_ctl 1int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型 EPOLL_CTL_ADD 注册新的fd到epfd中； EPOLL_CTL_MOD 修改已经注册的fd的监听事件； EPOLL_CTL_DEL 从epfd中删除一个fd； fd 是要监听的fd event 是要监听什么样的事件 12345678910typedef union epoll_data &#123; void *ptr; int fd; uint32_t u32; uint64_t u64;&#125; epoll_data_t;struct epoll_event &#123; uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125;; events可以是以下几个宏的集合： EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）； EPOLLOUT：表示对应的文件描述符可以写； EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）； EPOLLERR：表示对应的文件描述符发生错误； EPOLLHUP：表示对应的文件描述符被挂断； EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。 EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 epoll_wait 12int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 等待事件的产生，类似于select()调用。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个 maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 ET、LT两种工作模式 EPOLLLT：完全靠Linux-kernel-epoll驱动，应用程序只需要处理从epoll_wait返回的fds， 这些fds我们认为它们处于就绪状态。此时epoll可以认为是更快速的poll。 EPOLLET：此模式下，系统仅仅通知应用程序哪些fds变成了就绪状态，一旦fd变成就绪状态，epoll将不再关注这个fd的任何状态信息(从epoll队列移除), 直到应用程序通过读写操作（非阻塞）触发EAGAIN状态，epoll认为这个fd又变为空闲状态，那么epoll又重新关注这个fd的状态变化(重新加入epoll队列)。 随着epoll_wait的返回，队列中的fds是在减少的，所以在大并发的系统中，EPOLLET更有优势，但是对程序员的要求也更高。 举例 假设现在对方发送了2k的数据，而我们先读取了1k，然后这时调用了epoll_wait，如果是边沿触发ET，那么这个fd变成就绪状态就会从epoll 队列移除，则epoll_wait 会一直阻塞，忽略尚未读取的1k数据; 而如果是水平触发LT，那么epoll_wait 还会检测到可读事件而返回，我们可以继续读取剩下的1k 数据。 总结: LT模式可能触发的次数更多, 一旦触发的次数多, 也就意味着效率会下降; 但这样也不能就说LT模式就比ET模式效率更低, 因为ET的使用对编程人员提出了更高更精细的要求,一旦使用者编程水平不够, 那ET模式还不如LT模式。 ET模式仅当状态发生变化的时候才获得通知,这里所谓的状态的变化并不包括缓冲区中还有未处理的数据,也就是说,如果要采用ET模式,需要一直read/write直到出错为止,很多人反映为什么采用ET模式只接收了一部分数据就再也得不到通知了,大多因为这样;而LT模式是只要有数据没有处理就会一直通知下去的. epoll IO多路复用模型实现机制设想一下如下场景：有100万个客户端同时与一个服务器进程保持着TCP连接。而每一时刻，通常只有几百上千个TCP连接是活跃的(事实上大部分场景都是这种情况)。如何实现这样的高并发？ 在select/poll时代，服务器进程每次都把这100万个连接告诉操作系统(从用户态复制句柄数据结构到内核态)，让操作系统内核去查询这些套接字上是否有事件发生，轮询完后，再将句柄数据复制到用户态，让服务器应用程序轮询处理已发生的网络事件，这一过程资源消耗较大，因此，select/poll一般只能处理几千的并发连接。 epoll的设计和实现与select完全不同。epoll通过在Linux内核中申请一个简易的文件系统，把原先的select/poll调用分成了3个部分： 调用epoll_create()建立一个epoll对象(在epoll文件系统中为这个句柄对象分配资源) 调用epoll_ctl向epoll对象中添加这100万个连接的套接字 调用epoll_wait收集发生的事件的连接 只需要在进程启动时建立一个epoll对象，然后在需要的时候向这个epoll对象中添加或者删除连接。同时，epoll_wait的效率也非常高，因为调用epoll_wait时，并没有一股脑的向操作系统复制这100万个连接的句柄数据，内核也不需要去遍历全部的连接。 Linux内核具体的epoll机制实现思路。 当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关 12345678910111213141516171819202122232425262728293031323334/* * This structure is stored inside the &quot;private_data&quot; member of the file * structure and rapresent the main data sructure for the eventpoll * interface. */struct eventpoll &#123; /* Protect the this structure access */ spinlock_t lock; /* * This mutex is used to ensure that files are not removed * while epoll is using them. This is held during the event * collection loop, the file cleanup path, the epoll file exit * code and the ctl operations. */ struct mutex mtx; /* Wait queue used by sys_epoll_wait() */ wait_queue_head_t wq; /* Wait queue used by file-&gt;poll() */ wait_queue_head_t poll_wait; /* List of ready file descriptors *//*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/ struct list_head rdllist;/*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/ /* RB tree root used to store monitored fd structs */ struct rb_root rbr; /* * This is a single linked list that chains all the &quot;struct epitem&quot; that * happened while transfering ready events to userspace w/out * holding -&gt;lock. */ struct epitem *ovflist; /* The user that created the eventpoll descriptor */ struct user_struct *user;&#125;; 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为树的高度)。 而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。 在epoll中，对于每一个事件，都会建立一个epitem结构体，如下所示： 12345678910111213141516171819202122232425262728293031323334/* * Each file descriptor added to the eventpoll interface will * have an entry of this type linked to the &quot;rbr&quot; RB tree. */struct epitem &#123; /* RB tree node used to link this structure to the eventpoll RB tree *///红黑树节点 struct rb_node rbn; /* List header used to link this structure to the eventpoll ready list *///双向链表节点 struct list_head rdllink; /* * Works together &quot;struct eventpoll&quot;-&gt;ovflist in keeping the * single linked chain of items. */ struct epitem *next; /* The file descriptor information this item refers to *///事件句柄信息 struct epoll_filefd ffd; /* Number of active wait queue attached to poll operations */ int nwait; /* List containing poll wait queues */ struct list_head pwqlist; /* The &quot;container&quot; of this item *///指向其所属的eventpoll对象 struct ![Uploading EPOLL_663944.jpg . . .]eventpoll *ep; /* List header used to link this item to the &quot;struct file&quot; items list */ struct list_head fllink; /* The structure that describe the interested events and the source fd */ //期待发生的事件类型 struct epoll_event event;&#125;; 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。 通过红黑树和双链表数据结构，并结合回调机制，造就了epoll的高效。 下面我们按照epoll三部曲的顺序进行分析。 epoll_create123456789101112asmlinkage long sys_epoll_create(int size)&#123; int error, fd; struct inode *inode; struct file *file; error = ep_getfd(&amp;fd, &amp;inode, &amp;file); error = ep_file_init(file); return fd;&#125; 我们发现create函数似乎很简单。 1、操作系统中，进程和文件系统是通过fd=&gt;file=&gt;node联系起来的。ep_getfd就是在建立这个联系。 1234567891011121314151617181920212223242526272829static int ep_getfd(int *efd, struct inode **einode, struct file **efile)&#123; // 获取一个file结构体 file = get_empty_filp(); // epoll在底层本身对应一个文件系统，从这个文件系统中获取一个inode inode = ep_eventpoll_inode(); // 获取一个文件描述符 fd = get_unused_fd(); sprintf(name, &quot;[%lu]&quot;, inode-&gt;i_ino); this.name = name; this.len = strlen(name); this.hash = inode-&gt;i_ino; // 申请一个entry dentry = d_alloc(eventpoll_mnt-&gt;mnt_sb-&gt;s_root, &amp;this); dentry-&gt;d_op = &amp;eventpollfs_dentry_operations; file-&gt;f_dentry = dentry; // 建立file和inode的联系 d_add(dentry, inode); // 建立fd=&gt;file的关联 fd_install(fd, file); *efd = fd; *einode = inode; *efile = file; return 0;&#125; 形成一个这种的结构。 2、通过ep_file_init建立file和epoll的关联。 1234567891011static int ep_file_init(struct file *file)&#123; struct eventpoll *ep; ep = kmalloc(sizeof(struct eventpoll), GFP_KERNEL) memset(ep, 0, sizeof(*ep)); // 一系列初始化 file-&gt;private_data = ep; return 0;&#125; epoll_create函数主要是建立一个数据结构。并返回一个文件描述符供后面使用。 epoll_ctl123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657asmlinkage longsys_epoll_ctl(int epfd, int op, int fd, struct epoll_event __user *event)&#123; int error; struct file *file, *tfile; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; error = -EFAULT; // 不是删除操作则复制用户数据到内核 if ( EP_OP_HASH_EVENT(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event)) ) goto eexit_1; // 根据一种的图，拿到epoll对应的file结构体 file = fget(epfd); // 拿到操作的文件的file结构体 tfile = fget(fd); // 通过file拿到epoll_event结构体，见上面的图 ep = file-&gt;private_data; // 看这个文件描述符是否已经存在，epoll用红黑树维护这个数据 epi = ep_find(ep, tfile, fd); switch (op) &#123; // 新增 case EPOLL_CTL_ADD: // 还没有则新增，有则报错 if (!epi) &#123; epds.events |= POLLERR | POLLHUP; // 插入红黑树 error = ep_insert(ep, &amp;epds, tfile, fd); &#125; else error = -EEXIST; break; // 删除 case EPOLL_CTL_DEL: // 存在则删除，否则报错 if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; // 修改 case EPOLL_CTL_MOD: // 存在则修改，否则报错 if (epi) &#123; epds.events |= POLLERR | POLLHUP; error = ep_modify(ep, epi, &amp;epds); &#125; else error = -ENOENT; break; &#125;&#125; epoll_ctl函数看起来也没有很复杂，就是根据用户传进来的信息去操作红黑树。对于红黑树的增删改查，查和删除就不分析了。就是去操作红黑树。增和改是类似的逻辑，所以我们只分析增操作就可以了。在此之前，我们先了解一些epoll中其他的数据结构。 当我们新增一个需要监听的文件描述符的时候，系统会申请一个epitem去表示。epitem是保存了文件描述符、事件等信息的结构体。然后把epitem插入到eventpoll结构体维护的红黑树中。 12345678910111213141516171819202122232425262728293031323334353637static int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd)&#123; int error, revents, pwake = 0; unsigned long flags; struct epitem *epi; struct ep_pqueue epq; // 申请一个epitem epi = EPI_MEM_ALLOC() // 省略一系列初始化工作 // 记录所属的epoll epi-&gt;ep = ep; // 在epitem中保存文件描述符fd和file EP_SET_FFD(&amp;epi-&gt;ffd, tfile, fd); // 监听的事件 epi-&gt;event = *event; epi-&gt;nwait = 0; epq.epi = epi; init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); revents = tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt); // 把epitem插入红黑树 ep_rbtree_insert(ep, epi); // 如果监听的事件在新增的时候就已经触发，则直接插入到epoll就绪队列 if ((revents &amp; event-&gt;events) &amp;&amp; !EP_IS_LINKED(&amp;epi-&gt;rdllink)) &#123; // 把epitem插入就绪队列rdllist list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); // 有事件触发，唤醒阻塞在epoll_wait的进程队列 if (waitqueue_active(&amp;ep-&gt;wq)) wake_up(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125;&#125; 新增操作的大致流程是： 申请了一个新的epitem表示待观察的实体。他保存了文件描述符、感兴趣的事件等信息。 插入红黑树 判断新增的节点中对应的文件描述符和事件是否已经触发了，是则加入到就绪队列（由eventpoll-&gt;rdllist维护的一个队列） 下面具体看一下如何判断感兴趣的事件在对应的文件描述符中是否已经触发。相关代码在ep_insert中。下面单独拎出来。 1234567891011121314151617/* struct ep_pqueue &#123; // 函数指针 poll_table pt; // epitem struct epitem *epi; &#125;;*/struct ep_pqueue epq;epq.epi = epi;init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc);revents = tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt);static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)&#123; pt-&gt;qproc = qproc;&#125; 上面的代码是定义了一个struct ep_pqueue 结构体，然后设置他的一个字段为ep_ptable_queue_proc。然后执行tfile-&gt;f_op-&gt;poll。poll函数由各个文件系统或者网络协议实现。我们以管道为例。 12345678910111213141516171819202122232425262728293031323334static unsigned intpipe_poll(struct file *filp, poll_table *wait)&#123; unsigned int mask; // 监听的文件描述符对应的inode struct inode *inode = filp-&gt;f_dentry-&gt;d_inode; struct pipe_inode_info *info = inode-&gt;i_pipe; int nrbufs; /* static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p) &#123; if (p &amp;&amp; wait_address) p-&gt;qproc(filp, wait_address, p); &#125; */ poll_wait(filp, PIPE_WAIT(*inode), wait); // 判断哪些事件触发了 nrbufs = info-&gt;nrbufs; mask = 0; if (filp-&gt;f_mode &amp; FMODE_READ) &#123; mask = (nrbufs &gt; 0) ? POLLIN | POLLRDNORM : 0; if (!PIPE_WRITERS(*inode) &amp;&amp; filp-&gt;f_version != PIPE_WCOUNTER(*inode)) mask |= POLLHUP; &#125; if (filp-&gt;f_mode &amp; FMODE_WRITE) &#123; mask |= (nrbufs &lt; PIPE_BUFFERS) ? POLLOUT | POLLWRNORM : 0; if (!PIPE_READERS(*inode)) mask |= POLLERR; &#125; return mask;&#125; 我们看到具体的poll函数里会首先执行poll_wait函数。这个函数只是简单执行struct ep_pqueue epq结构体中的函数，即刚才设置的ep_ptable_queue_proc。 1234567891011121314151617181920212223// 监听的文件描述符对应的file结构体，whead是等待监听的文件描述符对应的inode可用的队列static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead, poll_table *pt)&#123; struct epitem *epi = EP_ITEM_FROM_EPQUEUE(pt); struct eppoll_entry *pwq; if (epi-&gt;nwait &gt;= 0 &amp;&amp; (pwq = PWQ_MEM_ALLOC())) &#123; pwq-&gt;wait-&gt;flags = 0; pwq-&gt;wait-&gt;task = NULL; // 设置回调 pwq-&gt;wait-&gt;func = ep_poll_callback; pwq-&gt;whead = whead; pwq-&gt;base = epi; // 插入等待监听的文件描述符的inode可用的队列，回调函数是ep_poll_callback add_wait_queue(whead, &amp;pwq-&gt;wait); list_add_tail(&amp;pwq-&gt;llink, &amp;epi-&gt;pwqlist); epi-&gt;nwait++; &#125; else &#123; /* We have to signal that an error occurred */ epi-&gt;nwait = -1; &#125;&#125; 主要的逻辑是把当前进程插入监听的文件的等待队列中，等待唤醒。 epoll_wait12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667asmlinkage long sys_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, int timeout)&#123; int error; struct file *file; struct eventpoll *ep; // 通过epoll的fd拿到对应的file结构体 file = fget(epfd); // 通过file结构体拿到eventpoll结构体 ep = file-&gt;private_data; error = ep_poll(ep, events, maxevents, timeout); return error;&#125;static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res, eavail; unsigned long flags; long jtimeout; wait_queue_t wait; // 计算超时时间 jtimeout = timeout == -1 || timeout &gt; (MAX_SCHEDULE_TIMEOUT - 1000) / HZ ? MAX_SCHEDULE_TIMEOUT: (timeout * HZ + 999) / 1000;retry: res = 0; // 就绪队列为空 if (list_empty(&amp;ep-&gt;rdllist)) &#123; // 加入阻塞队列 init_waitqueue_entry(&amp;wait, current); add_wait_queue(&amp;ep-&gt;wq, &amp;wait); for (;;) &#123; // 挂起 set_current_state(TASK_INTERRUPTIBLE); // 超时或者有就绪事件了，则跳出返回 if (!list_empty(&amp;ep-&gt;rdllist) || !jtimeout) break; // 被信号唤醒返回EINTR if (signal_pending(current)) &#123; res = -EINTR; break; &#125; // 设置定时器，然后进程挂起，等待超时唤醒（超时或者信号唤醒） jtimeout = schedule_timeout(jtimeout); &#125; // 移出阻塞队列 remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); // 设置就绪 set_current_state(TASK_RUNNING); &#125; // 是否有事件就绪，唤醒的原因有几个，被唤醒不代表就有就绪事件 eavail = !list_empty(&amp;ep-&gt;rdllist); write_unlock_irqrestore(&amp;ep-&gt;lock, flags); // 处理就绪事件返回 if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_events_transfer(ep, events, maxevents)) &amp;&amp; jtimeout) goto retry; return res;&#125; 总的来说epoll_wait的逻辑主要是处理就绪队列的节点。 如果就绪队列为空，则根据timeout做下一步处理，可能定时阻塞。 如果就绪队列非空则处理就绪队列，返回给用户。处理就绪队列的函数是ep_events_transfer。 123456789101112131415static int ep_events_transfer(struct eventpoll *ep, struct epoll_event __user *events, int maxevents)&#123; int eventcnt = 0; struct list_head txlist; INIT_LIST_HEAD(&amp;txlist); if (ep_collect_ready_items(ep, &amp;txlist, maxevents) &gt; 0) &#123; eventcnt = ep_send_events(ep, &amp;txlist, events); ep_reinject_items(ep, &amp;txlist); &#125; return eventcnt;&#125; 主要是三个函数，我们一个个看。 1、ep_collect_ready_items收集就绪事件 12345678910111213141516171819202122232425262728static int ep_collect_ready_items(struct eventpoll *ep, struct list_head *txlist, int maxevents)&#123; int nepi; unsigned long flags; // 就绪事件的队列 struct list_head *lsthead = &amp;ep-&gt;rdllist, *lnk; struct epitem *epi; for (nepi = 0, lnk = lsthead-&gt;next; lnk != lsthead &amp;&amp; nepi &lt; maxevents;) &#123; // 通过结构体字段的地址拿到结构体首地址 epi = list_entry(lnk, struct epitem, rdllink); lnk = lnk-&gt;next; /* If this file is already in the ready list we exit soon */ if (!EP_IS_LINKED(&amp;epi-&gt;txlink)) &#123; epi-&gt;revents = epi-&gt;event.events; // 插入txlist队列，然后处理完再返回给用户 list_add(&amp;epi-&gt;txlink, txlist); nepi++; // 从就绪队列中删除 EP_LIST_DEL(&amp;epi-&gt;rdllink); &#125; &#125; return nepi;&#125; 2、ep_send_events判断哪些事件触发了 1234567891011121314151617181920212223242526272829static int ep_send_events(struct eventpoll *ep, struct list_head *txlist, struct epoll_event __user *events)&#123; int eventcnt = 0; unsigned int revents; struct list_head *lnk; struct epitem *epi; // 遍历就绪队列，记录触发的事件 list_for_each(lnk, txlist) &#123; epi = list_entry(lnk, struct epitem, txlink); // 判断哪些事件触发了 revents = epi-&gt;ffd.file-&gt;f_op-&gt;poll(epi-&gt;ffd.file, NULL); epi-&gt;revents = revents &amp; epi-&gt;event.events; // 复制到用户空间 if (epi-&gt;revents) &#123; if (__put_user(epi-&gt;revents, &amp;events[eventcnt].events) || __put_user(epi-&gt;event.data, &amp;events[eventcnt].data)) return -EFAULT; // 只监听一次，触发完设置成对任何事件都不感兴趣 if (epi-&gt;event.events &amp; EPOLLONESHOT) epi-&gt;event.events &amp;= EP_PRIVATE_BITS; eventcnt++; &#125; &#125; return eventcnt;&#125; 3、ep_reinject_items重新插入就绪队列 123456789101112131415161718static void ep_reinject_items(struct eventpoll *ep, struct list_head *txlist)&#123; int ricnt = 0, pwake = 0; unsigned long flags; struct epitem *epi; while (!list_empty(txlist)) &#123; epi = list_entry(txlist-&gt;next, struct epitem, txlink); EP_LIST_DEL(&amp;epi-&gt;txlink); // 水平触发模式则一直通知，即重新加入就绪队列 if (EP_RB_LINKED(&amp;epi-&gt;rbn) &amp;&amp; !(epi-&gt;event.events &amp; EPOLLET) &amp;&amp; (epi-&gt;revents &amp; epi-&gt;event.events) &amp;&amp; !EP_IS_LINKED(&amp;epi-&gt;rdllink)) &#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ricnt++; &#125; &#125;&#125; 我们发现，并有没有在epoll_wait的时候去收集就绪事件，那么就绪队列是谁处理的呢？我们回顾一下插入红黑树的时候，做了一个事情，就是在文件对应的inode上注册一个回调。当文件满足条件的时候，就会唤醒因为epoll_wait而阻塞的进程。epoll_wait会收集事件返回给用户。 123456789101112131415static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)&#123; int pwake = 0; unsigned long flags; struct epitem *epi = EP_ITEM_FROM_WAIT(wait); struct eventpoll *ep = epi-&gt;ep; // 插入就绪队列 list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); // 唤醒因epoll_wait而阻塞的进程 if (waitqueue_active(&amp;ep-&gt;wq)) wake_up(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; return 1;&#125; epoll的实现涉及的内容比较多，先分析一下大致的原理。有机会再深入分析。]]></content>
      <categories>
        <category>computer-network</category>
        <category>tcp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux下深入理解TCP/IP协议的实现（基于linux1.2.13）]]></title>
    <url>%2Fcomputer-network%2Flinux-srcode-tcp%2F</url>
    <content type="text"><![CDATA[bind实现按照socket网络编程的顺序，我们这一篇来分析bind函数。我们通过socket函数拿到了一个socket结构体。bind函数的逻辑其实比较简单，他就是给socket结构体绑定一个地址，简单来说，就是给他的某些字段赋值。talk is cheap。show me the code。 1234567891011121314151617181920static int sock_bind(int fd, struct sockaddr *umyaddr, int addrlen)&#123; struct socket *sock; int i; char address[MAX_SOCK_ADDR]; int err; // 通过文件描述符找到对应的socket if (!(sock = sockfd_lookup(fd, NULL))) return(-ENOTSOCK); if((err=move_addr_to_kernel(umyaddr,addrlen,address))&lt;0) return err; if ((i = sock-&gt;ops-&gt;bind(sock, (struct sockaddr *)address, addrlen)) &lt; 0) &#123; return(i); &#125; return(0);&#125; 主要是两个函数，我们一个个来。 1、sockfd_lookup通过之前一些文章的分析，我们应该数socket和文件的内存布局比较熟悉了。下面的代码不难理解。就是根据文件描述符从pcb中找到inode节点。因为inode节点里保存了socket结构体，所以最后返回fd对应的socke结构体就行。 123456789101112131415161718192021222324// 通过fd找到file结构体，从而找到inode节点，最后找到socket结构体static inline struct socket *sockfd_lookup(int fd, struct file **pfile)&#123; struct file *file; struct inode *inode; if (fd &lt; 0 || fd &gt;= NR_OPEN || !(file = current-&gt;files-&gt;fd[fd])) return NULL; inode = file-&gt;f_inode; if (!inode || !inode-&gt;i_sock) return NULL; if (pfile) *pfile = file; return socki_lookup(inode);&#125;// inode和socket互相引用inline struct socket *socki_lookup(struct inode *inode)&#123; return &amp;inode-&gt;u.socket_i;&#125; 2、sock-&gt;ops-&gt;bind我们回顾socket那篇文章可以知道socket结构体里保存了一些列的操作函数，假设是协议簇是ipv4，那么bind函数就是inet_bind函数（省略了部分代码）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// 给socket绑定一个地址static int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)&#123; struct sockaddr_in *addr=(struct sockaddr_in *)uaddr; // 拿到底层的sock结构体 struct sock *sk=(struct sock *)sock-&gt;data, *sk2; unsigned short snum = 0 /* Stoopid compiler.. this IS ok */; int chk_addr_ret; // raw协议的这些数据由用户填充 if(sock-&gt;type != SOCK_RAW) &#123; // 已经绑定了端口 if (sk-&gt;num != 0) return(-EINVAL); snum = ntohs(addr-&gt;sin_port); // 端口无效则随机获取一个非root才能使用的端口 if (snum == 0) &#123; snum = get_new_socknum(sk-&gt;prot, 0); &#125; // 小于1024的端口需要超级用户权限 if (snum &lt; PROT_SOCK &amp;&amp; !suser()) return(-EACCES); &#125; // 判断ip chk_addr_ret = ip_chk_addr(addr-&gt;sin_addr.s_addr); // 非法地址 if (addr-&gt;sin_addr.s_addr != 0 &amp;&amp; chk_addr_ret != IS_MYADDR &amp;&amp; chk_addr_ret != IS_MULTICAST) return(-EADDRNOTAVAIL); /* Source address MUST be ours! */ // 记录ip if (chk_addr_ret || addr-&gt;sin_addr.s_addr == 0) sk-&gt;saddr = addr-&gt;sin_addr.s_addr; if(sock-&gt;type != SOCK_RAW) &#123; /* Make sure we are allowed to bind here. */ cli(); // 遍历哈希表，哈希表冲突解决法是链地址法，校验绑定的端口的合法性 for(sk2 = sk-&gt;prot-&gt;sock_array[snum &amp; (SOCK_ARRAY_SIZE -1)]; sk2 != NULL; sk2 = sk2-&gt;next) &#123; // 端口还没有绑定过，直接校验下一个 if (sk2-&gt;num != snum) continue; // 端口已经被使用，没有设置可重用标记，比如断开连接后在2msl内是否可以重用，通过setsockopt函数设置 if (!sk-&gt;reuse) &#123; sti(); return(-EADDRINUSE); &#125; // 端口一样，但是ip不一样，ok，下一个 if (sk2-&gt;saddr != sk-&gt;saddr) continue; /* socket per slot ! -FB */ // 端口和ip都一样。被监听的端口不能同时被使用 if (!sk2-&gt;reuse || sk2-&gt;state==TCP_LISTEN) &#123; sti(); return(-EADDRINUSE); &#125; &#125; sti(); // 保证该sk不在sock_array队列里 remove_sock(sk); // 挂载到sock_array里 put_sock(snum, sk); // tcp头中的源端口 sk-&gt;dummy_th.source = ntohs(sk-&gt;num); sk-&gt;daddr = 0; sk-&gt;dummy_th.dest = 0; &#125; return(0);&#125; bind函数主要是对待绑定的ip和端口做一个校验，合法的时就记录在sock结构体中。并且把sock结构体挂载到一个全局的哈希表里。 listen 实现listen函数的逻辑比bind还简单。bind主要是校验和绑定ip、端口。listen则是修改socket的状态，并记录一些设置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static int sock_listen(int fd, int backlog)&#123; struct socket *sock; if (fd &lt; 0 || fd &gt;= NR_OPEN || current-&gt;files-&gt;fd[fd] == NULL) return(-EBADF); if (!(sock = sockfd_lookup(fd, NULL))) return(-ENOTSOCK); if (sock-&gt;state != SS_UNCONNECTED) &#123; return(-EINVAL); &#125; if (sock-&gt;ops &amp;&amp; sock-&gt;ops-&gt;listen) sock-&gt;ops-&gt;listen(sock, backlog); // 设置socket的监听属性，accept函数时用到 sock-&gt;flags |= SO_ACCEPTCON; return(0);&#125;static int inet_listen(struct socket *sock, int backlog)&#123; struct sock *sk = (struct sock *) sock-&gt;data; // 如果没有绑定端口则绑定一个，并把sock加到sock_array中 if(inet_autobind(sk)!=0) return -EAGAIN; if ((unsigned) backlog &gt; 128) backlog = 128; // tcp接收队列的长度上限，不同系统实现不一样，具体参考tcp.c的使用 sk-&gt;max_ack_backlog = backlog; // 修改socket状态，防止多次调用listen if (sk-&gt;state != TCP_LISTEN) &#123; sk-&gt;ack_backlog = 0; sk-&gt;state = TCP_LISTEN; &#125; return(0);&#125;// 绑定一个随机的端口，更新sk的源端口字段，并把sk挂载到端口对应的队列中，见bind函数的分析static int inet_autobind(struct sock *sk)&#123; /* We may need to bind the socket. */ if (sk-&gt;num == 0) &#123; sk-&gt;num = get_new_socknum(sk-&gt;prot, 0); if (sk-&gt;num == 0) return(-EAGAIN); put_sock(sk-&gt;num, sk); sk-&gt;dummy_th.source = ntohs(sk-&gt;num); &#125; return 0;&#125; accept 实现我们继续分析tcp/ip协议的实现，这一篇讲一下accept，accept就是从已完成三次握手的连接队列里，摘下一个节点。我们可以了解到三次握手的实现和过程。很多同学都了解三次握手是什么，但是可能很少同学会深入思考或者看他的实现，众所周知，一个服务器启动的时候，会监听一个端口。其实就是新建了一个socket。那么如果有一个连接到来的时候，我们通过accept就能拿到这个新连接对应的socket。那么这个socket和监听的socket是不是同一个呢？其实socket分为监听型和通信型的。表面上，服务器用一个端口实现了多个连接，但是这个端口是用于监听的，底层用于和客户端通信的其实是另一个socket。所以每一个连接过来，负责监听的socket发现是一个建立连接的包（syn包），他就会生成一个新的socket与之通信（accept的时候返回的那个）。我们将会从代码中看到这个实现。我们从accept函数开始，详细分析这个过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758static int sock_accept(int fd, struct sockaddr *upeer_sockaddr, int *upeer_addrlen)&#123; struct file *file; struct socket *sock, *newsock; int i; char address[MAX_SOCK_ADDR]; int len; if (fd &lt; 0 || fd &gt;= NR_OPEN || ((file = current-&gt;files-&gt;fd[fd]) == NULL)) return(-EBADF); // 根据文件描述符找到对应的file结构体和socket结构 if (!(sock = sockfd_lookup(fd, &amp;file))) return(-ENOTSOCK); if (sock-&gt;state != SS_UNCONNECTED) &#123; return(-EINVAL); &#125; // socket没有调用过listen，报错，该标记位在listen中设置 if (!(sock-&gt;flags &amp; SO_ACCEPTCON)) &#123; return(-EINVAL); &#125; // 分配一个新的socket结构体 if (!(newsock = sock_alloc())) &#123; printk(&quot;NET: sock_accept: no more sockets\n&quot;); return(-ENOSR); /* Was: EAGAIN, but we are out of system resources! */ &#125; newsock-&gt;type = sock-&gt;type; newsock-&gt;ops = sock-&gt;ops; // 创建一个底层的sock结构体和新的socket结构体互相关联 if ((i = sock-&gt;ops-&gt;dup(newsock, sock)) &lt; 0) &#123; sock_release(newsock); return(i); &#125; // accept返回一个新的sock和socket关联 i = newsock-&gt;ops-&gt;accept(sock, newsock, file-&gt;f_flags); if ( i &lt; 0) &#123; sock_release(newsock); return(i); &#125; // 返回一个新的文件描述符 if ((fd = get_fd(SOCK_INODE(newsock))) &lt; 0) &#123; sock_release(newsock); return(-EINVAL); &#125; // 是否需要获取socket对应的地址 if (upeer_sockaddr) &#123; newsock-&gt;ops-&gt;getname(newsock, (struct sockaddr *)address, &amp;len, 1); move_addr_to_user(address,len, upeer_sockaddr, upeer_addrlen); &#125; return(fd);&#125; 我们一步步来分析这个函数。1 通过fd找到对应的socket结构体，然后申请一个新的socket结构体和sock结构体，并且把他们两互相关联。这个在前面的文章分析过。2 然后把监听的socket和准备用于通信的结构体作为参数，调用accept函数。3 最后返回通信socket对应的文件描述符。 下面我们开始分析accept函数的真正实现。 123456789101112131415static int inet_accept(struct socket *sock, struct socket *newsock, int flags)&#123; struct sock *sk1, *sk2; int err; sk1 = (struct sock *) sock-&gt;data; // 返回一个新的sock结构体 sk2 = sk1-&gt;prot-&gt;accept(sk1,flags); // 互相关联 newsock-&gt;data = (void *)sk2; sk2-&gt;socket = newsock; newsock-&gt;conn = NULL; // 设置sock为已经建立连接状态 newsock-&gt;state = SS_CONNECTED; return(0);&#125; 这个函数主要是调底层的accept函数，底层accept函数会返回一个新的sock结构体，socket和sock结构体的区别和背景在之前的文章里已经分析过。总的来说，accept函数就是申请一个新的通信socket，这个socket关联了一个新的sock结构体。下面我们看看tcp层的accept函数。 123456789101112131415161718192021222324252627282930313233343536static struct sock *tcp_accept(struct sock *sk, int flags)&#123; struct sock *newsk; struct sk_buff *skb; // 是一个listen的套接字 if (sk-&gt;state != TCP_LISTEN) &#123; sk-&gt;err = EINVAL; return(NULL); &#125; cli(); // 从sock的receive_queue队列摘取已建立连接的节点， while((skb = tcp_dequeue_established(sk)) == NULL) &#123; // 没有已经建立连接的节点，但是设置了非阻塞模式，直接返回 if (flags &amp; O_NONBLOCK) &#123; sti(); release_sock(sk); sk-&gt;err = EAGAIN; return(NULL); &#125; release_sock(sk); //阻塞进程，如果后续建立了连接，则进程被唤醒的时候，就会跳出while循环 interruptible_sleep_on(sk-&gt;sleep); &#125; sti(); // 拿到一个新的sock结构，由建立连接的时候创建的 newsk = skb-&gt;sk; // 返回新的sock结构体 return(newsk);&#125; 这个函数主要的逻辑是从监听型socket的已完成三次握手的队列里摘下一个节点。这个节点是一个sk_buff结构体，sk_buff是一个表示网络数据包的数据结构。 accept函数就分析完了。下一篇我们分析三次握手。看看accept函数摘下的这个节点是如果生成的。 connect 实现分析完了服务器端，我们继续分析客户端，在socket编程中，客户端的流程是比较简单的，申请一个socket，然后调connect去发起连接就行。我们先看一下connect函数的定义。 12345/* socket 通过socket函数申请的结构体 address 需要连接的目的地地址信息*/int connect(int socket, const struct sockaddr *address,socklen_t address_len); 我们通过层层调用揭开connect的迷雾。 1234567891011121314151617181920static int sock_connect(int fd, struct sockaddr *uservaddr, int addrlen)&#123; struct socket *sock; struct file *file; int i; char address[MAX_SOCK_ADDR]; int err; if (fd &lt; 0 || fd &gt;= NR_OPEN || (file=current-&gt;files-&gt;fd[fd]) == NULL) return(-EBADF); if (!(sock = sockfd_lookup(fd, &amp;file))) return(-ENOTSOCK); i = sock-&gt;ops-&gt;connect(sock, (struct sockaddr *)address, addrlen, file-&gt;f_flags); if (i &lt; 0) &#123; return(i); &#125; return(0);&#125; 没有太多逻辑，通过fd找到关联的socket结构体。然后调底层函数。底层的函数是inet_connect，这个函数逻辑比较多，我们分开分析。 123456789if (sock-&gt;state == SS_CONNECTING &amp;&amp; sk-&gt;protocol == IPPROTO_TCP &amp;&amp; (flags &amp; O_NONBLOCK)) &#123; if (sk-&gt;err != 0) &#123; err=sk-&gt;err; sk-&gt;err=0; return -err; &#125; return -EALREADY; /* Connecting is currently in progress */&#125; 正在连接，并且是非阻塞的，直接返回。 123456789101112if (sock-&gt;state != SS_CONNECTING) &#123; // 如果绑过就不需要绑了 if(inet_autobind(sk)!=0) return(-EAGAIN); // 调用底层的连接函数，发一个syn包 err = sk-&gt;prot-&gt;connect(sk, (struct sockaddr_in *)uaddr, addr_len); if (err &lt; 0) return(err); // 发送成功设置状态为连接中 sock-&gt;state = SS_CONNECTING; &#125; 继续调用底层的函数，这里是tcp，所以是发送一个sync包（一会分析）。然后把socket状态修改为连接中。 12if (sk-&gt;state != TCP_ESTABLISHED &amp;&amp;(flags &amp; O_NONBLOCK)) return(-EINPROGRESS); 还没建立连接成功并且是非阻塞的方式，直接返回。 123456789101112131415161718192021// 连接建立中，阻塞当前进程while(sk-&gt;state == TCP_SYN_SENT || sk-&gt;state == TCP_SYN_RECV) &#123; // 可中断式睡眠，即可被信号唤醒 interruptible_sleep_on(sk-&gt;sleep); // 被唤醒后，判断是因为被信号唤醒的还是因为建立建立了。 if (current-&gt;signal &amp; ~current-&gt;blocked) &#123; sti(); return(-ERESTARTSYS); &#125; // 连接失败 if(sk-&gt;err &amp;&amp; sk-&gt;protocol == IPPROTO_TCP) &#123; sti(); sock-&gt;state = SS_UNCONNECTED; err = -sk-&gt;err; sk-&gt;err=0; return err; /* set by tcp_err() */ &#125;&#125; connect的时候如果没有设置阻塞标记，则进程会被挂起。tcp层建立连接后会唤醒进程。 12345678910// 连接建立 sock-&gt;state = SS_CONNECTED; if (sk-&gt;state != TCP_ESTABLISHED &amp;&amp; sk-&gt;err) &#123; sock-&gt;state = SS_UNCONNECTED; err=sk-&gt;err; sk-&gt;err=0; return(-err); &#125; 最后被连接建立唤醒后，设置socket的状态。connect就完成了。 下面我们看一下tcp层的connect的实现，其实就是从客户端视角看三次握手的过程。代码比较多，只看一下核心的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495static int tcp_connect(struct sock *sk, struct sockaddr_in *usin, int addr_len)&#123; struct sk_buff *buff; struct device *dev=NULL; unsigned char *ptr; int tmp; int atype; struct tcphdr *t1; struct rtable *rt; if (usin-&gt;sin_family &amp;&amp; usin-&gt;sin_family != AF_INET) return(-EAFNOSUPPORT); // 不传ip则取本机ip if(usin-&gt;sin_addr.s_addr==INADDR_ANY) usin-&gt;sin_addr.s_addr=ip_my_addr(); // 禁止广播和多播 if ((atype=ip_chk_addr(usin-&gt;sin_addr.s_addr)) == IS_BROADCAST || atype==IS_MULTICAST) return -ENETUNREACH; sk-&gt;inuse = 1; // 连接的远端地址 sk-&gt;daddr = usin-&gt;sin_addr.s_addr; // 第一个字节的序列号 sk-&gt;write_seq = tcp_init_seq(); sk-&gt;window_seq = sk-&gt;write_seq; sk-&gt;rcv_ack_seq = sk-&gt;write_seq -1; sk-&gt;err = 0; // 远端端口 sk-&gt;dummy_th.dest = usin-&gt;sin_port; release_sock(sk); // 分配一个skb buff = sk-&gt;prot-&gt;wmalloc(sk,MAX_SYN_SIZE,0, GFP_KERNEL); sk-&gt;inuse = 1; // tcp头和选项，告诉对方自己的接收窗口大小1 buff-&gt;len = 24; buff-&gt;sk = sk; buff-&gt;free = 0; buff-&gt;localroute = sk-&gt;localroute; t1 = (struct tcphdr *) buff-&gt;data; // 查找路由 rt=ip_rt_route(sk-&gt;daddr, NULL, NULL); // 构建ip和mac头 tmp = sk-&gt;prot-&gt;build_header(buff, sk-&gt;saddr, sk-&gt;daddr, &amp;dev, IPPROTO_TCP, NULL, MAX_SYN_SIZE,sk-&gt;ip_tos,sk-&gt;ip_ttl); buff-&gt;len += tmp; t1 = (struct tcphdr *)((char *)t1 +tmp); memcpy(t1,(void *)&amp;(sk-&gt;dummy_th), sizeof(*t1)); // 序列号为初始化的序列号 t1-&gt;seq = ntohl(sk-&gt;write_seq++); // 下一个数据包中第一个字节的序列号 sk-&gt;sent_seq = sk-&gt;write_seq; buff-&gt;h.seq = sk-&gt;write_seq; t1-&gt;ack = 0; t1-&gt;window = 2; t1-&gt;res1=0; t1-&gt;res2=0; t1-&gt;rst = 0; t1-&gt;urg = 0; t1-&gt;psh = 0; // 是一个syn包 t1-&gt;syn = 1; t1-&gt;urg_ptr = 0; // TCP头包括24个字节，因为还有4个字节的选项 t1-&gt;doff = 6; // 执行tcp头后面的第一个字节 ptr = (unsigned char *)(t1+1); // 选项的类型是2，通知对方TCP报文中数据部分的最大值 ptr[0] = 2; // 选项内容长度是4个字节 ptr[1] = 4; // 组成MSS ptr[2] = (sk-&gt;mtu) &gt;&gt; 8; ptr[3] = (sk-&gt;mtu) &amp; 0xff; // tcp头的校验和 tcp_send_check(t1, sk-&gt;saddr, sk-&gt;daddr,sizeof(struct tcphdr) + 4, sk); // 设置套接字为syn_send状态 tcp_set_state(sk,TCP_SYN_SENT); // 设置数据包往返时间需要的时间 sk-&gt;rto = TCP_TIMEOUT_INIT; // 设置超时回调 sk-&gt;retransmit_timer.function=&amp;retransmit_timer; sk-&gt;retransmit_timer.data = (unsigned long)sk; // 设置超时时间 reset_xmit_timer(sk, TIME_WRITE, sk-&gt;rto); // 设置syn包的重试次数 sk-&gt;retransmits = TCP_SYN_RETRIES; // 发送 sk-&gt;prot-&gt;queue_xmit(sk, dev, buff, 0); reset_xmit_timer(sk, TIME_WRITE, sk-&gt;rto); release_sock(sk); return(0);&#125; 代码很长，主要是构建一个sync包发出去。在这个代码里我们大概能看到tcp协议的相关实现。上面的代码完成了第一次握手。下面再看一下第二次握手的代码。 1234567891011121314151617181920212223242526272829303132// 发送了syn包if(sk-&gt;state==TCP_SYN_SENT)&#123; // 发送了syn包，收到ack包说明可能是建立连接的ack包 if(th-&gt;ack) &#123; // 尝试连接但是对端回复了重置包 if(th-&gt;rst) return tcp_std_reset(sk,skb); // 建立连接的回包 syn_ok=1; // 期待收到对端下一个的序列号 sk-&gt;acked_seq=th-&gt;seq+1; sk-&gt;fin_seq=th-&gt;seq; // 发送第三次握手的ack包，进入连接建立状态 tcp_send_ack(sk-&gt;sent_seq,sk-&gt;acked_seq,sk,th,sk-&gt;daddr); tcp_set_state(sk, TCP_ESTABLISHED); // 解析tcp选项 tcp_options(sk,th); // 记录对端地址 sk-&gt;dummy_th.dest=th-&gt;source; // 可以读取但是还没读取的序列号 sk-&gt;copied_seq = sk-&gt;acked_seq; // 唤醒阻塞在connect函数的进程 if(!sk-&gt;dead) &#123; sk-&gt;state_change(sk); sock_wake_async(sk-&gt;socket, 0); &#125; &#125;&#125; 上面代码完成了第二次握手。tcp_send_ack完成第三次握手。这里不打算深入分析tcp层的代码，后续再深入分析。]]></content>
      <categories>
        <category>computer-network</category>
        <category>tcp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线上redis blpop连接超时引发的血案]]></title>
    <url>%2Fredis%2Fredis-list-blpop%2F</url>
    <content type="text"><![CDATA[列表REDIS_LIST （列表）是 LPUSH 、 LRANGE 等命令的操作对象， 它使用 REDIS_ENCODING_ZIPLIST 和 REDIS_ENCODING_LINKEDLIST 这两种方式编码： 编码的选择创建新列表时 Redis 默认使用 REDIS_ENCODING_ZIPLIST 编码， 当以下任意一个条件被满足时， 列表会被转换成 REDIS_ENCODING_LINKEDLIST 编码： 试图往列表新添加一个字符串值，且这个字符串的长度超过 server.list_max_ziplist_value （默认值为 64 ）。 ziplist 包含的节点超过 server.list_max_ziplist_entries （默认值为 512 ）。 列表命令的实现因为两种底层实现的抽象方式和列表的抽象方式非常接近， 所以列表命令几乎就是通过一对一地映射到底层数据结构的操作来实现的。 既然这些映射都非常直观， 这里就不做赘述了， 在以下的内容中， 我们将焦点放在 BLPOP 、 BRPOP 和 BRPOPLPUSH 这个几个阻塞命令的实现原理上。 阻塞的条件BLPOP 、 BRPOP 和 BRPOPLPUSH 三个命令都可能造成客户端被阻塞， 以下将这些命令统称为列表的阻塞原语。 阻塞原语并不是一定会造成客户端阻塞： 只有当这些命令被用于空列表时， 它们才会阻塞客户端。 如果被处理的列表不为空的话， 它们就执行无阻塞版本的 LPOP 、 RPOP 或 RPOPLPUSH 命令。 作为例子，以下流程图展示了 BLPOP 决定是否对客户端进行阻塞过程： 阻塞当一个阻塞原语的处理目标为空键时， 执行该阻塞原语的客户端就会被阻塞。 阻塞一个客户端需要执行以下步骤： 将客户端的状态设为“正在阻塞”，并记录阻塞这个客户端的各个键，以及阻塞的最长时限（timeout）等数据。 将客户端的信息记录到 server.db[i]-&gt;blocking_keys 中（其中 i 为客户端所使用的数据库号码）。 继续维持客户端和服务器之间的网络连接，但不再向客户端传送任何信息，造成客户端阻塞。 步骤 2 是将来解除阻塞的关键， server.db[i]-&gt;blocking_keys 是一个字典， 字典的键是那些造成客户端阻塞的键， 而字典的值是一个链表， 链表里保存了所有因为这个键而被阻塞的客户端 （被同一个键所阻塞的客户端可能不止一个）： 在上图展示的 blocking_keys 例子中， client2 、 client5 和 client1 三个客户端就正被 key1 阻塞， 而其他几个客户端也正在被别的两个 key 阻塞。 当客户端被阻塞之后，脱离阻塞状态有以下三种方法： 被动脱离：有其他客户端为造成阻塞的键推入了新元素。 主动脱离：到达执行阻塞原语时设定的最大阻塞时间。 强制脱离：客户端强制终止和服务器的连接，或者服务器停机。 以下内容将分别介绍被动脱离和主动脱离的实现方式。 阻塞因 LPUSH 、 RPUSH 、 LINSERT 等添加命令而被取消通过将新元素推入造成客户端阻塞的某个键中， 可以让相应的客户端从阻塞状态中脱离出来 （取消阻塞的客户端数量取决于推入元素的数量）。 LPUSH 、 RPUSH 和 LINSERT 这三个添加新元素到列表的命令， 在底层都由一个 pushGenericCommand 的函数实现， 这个函数的运作流程如下图： 当向一个空键推入新元素时， pushGenericCommand 函数执行以下两件事： 检查这个键是否存在于前面提到的 server.db[i]-&gt;blocking_keys 字典里， 如果是的话， 那么说明有至少一个客户端因为这个 key 而被阻塞，程序会为这个键创建一个 redis.h/readyList 结构， 并将它添加到 server.ready_keys 链表中。 将给定的值添加到列表键中。 readyList 结构的定义如下： 1234typedef struct readyList &#123; redisDb *db; robj *key;&#125; readyList; readyList 结构的 key 属性指向造成阻塞的键，而 db 则指向该键所在的数据库。 举个例子， 假设某个非阻塞客户端正在使用 0 号数据库， 而这个数据库当前的 blocking_keys 属性的值如下： 如果这时客户端对该数据库执行 PUSH key3 value ， 那么 pushGenericCommand 将创建一个 db 属性指向 0 号数据库、 key 属性指向 key3 键对象的 readyList 结构 ， 并将它添加到服务器 server.ready_keys 属性的链表中： 在我们这个例子中， 到目前为止， pushGenericCommand 函数完成了以下两件事： 将 readyList 添加到服务器。 将新元素 value 添加到键 key3 。 虽然 key3 已经不再是空键， 但到目前为止， 被 key3 阻塞的客户端还没有任何一个被解除阻塞状态。 为了做到这一点， Redis 的主进程在执行完 pushGenericCommand 函数之后， 会继续调用 handleClientsBlockedOnLists 函数， 这个函数执行以下操作： 如果 server.ready_keys 不为空，那么弹出该链表的表头元素，并取出元素中的 readyList 值。 根据 readyList 值所保存的 key 和 db ，在 server.blocking_keys 中查找所有因为 key 而被阻塞的客户端（以链表的形式保存）。 如果 key 不为空，那么从 key 中弹出一个元素，并弹出客户端链表的第一个客户端，然后将被弹出元素返回给被弹出客户端作为阻塞原语的返回值。 根据 readyList 结构的属性，删除 server.blocking_keys 中相应的客户端数据，取消客户端的阻塞状态。 继续执行步骤 3 和 4 ，直到 key 没有元素可弹出，或者所有因为 key 而阻塞的客户端都取消阻塞为止。 继续执行步骤 1 ，直到 ready_keys 链表里的所有 readyList 结构都被处理完为止。 用一段伪代码描述以上操作可能会更直观一些： 12345678910111213141516171819202122232425def handleClientsBlockedOnLists(): # 执行直到 ready_keys 为空 while server.ready_keys != NULL: # 弹出链表中的第一个 readyList rl = server.ready_keys.pop_first_node() # 遍历所有因为这个键而被阻塞的客户端 for client in all_client_blocking_by_key(rl.key, rl.db): # 只要还有客户端被这个键阻塞，就一直从键中弹出元素 # 如果被阻塞客户端执行的是 BLPOP ，那么对键执行 LPOP # 如果执行的是 BRPOP ，那么对键执行 RPOP element = rl.key.pop_element() if element == NULL: # 键为空，跳出 for 循环 # 余下的未解除阻塞的客户端只能等待下次新元素的进入了 break else: # 清除客户端的阻塞信息 server.blocking_keys.remove_blocking_info(client) # 将元素返回给客户端，脱离阻塞状态 client.reply_list_item(element) 先阻塞先服务（FBFS）策略值得一提的是， 当程序添加一个新的被阻塞客户端到 server.blocking_keys 字典的链表中时， 它将该客户端放在链表的最后， 而当 handleClientsBlockedOnLists 取消客户端的阻塞时， 它从链表的最前面开始取消阻塞： 这个链表形成了一个 FIFO 队列， 最先被阻塞的客户端总是最先脱离阻塞状态， Redis 文档称这种模式为先阻塞先服务（FBFS，first-block-first-serve）。 举个例子，在下图所示的阻塞状况中， 如果客户端对数据库执行 PUSH key3 value ， 那么只有 client3 会被取消阻塞， client6 和 client4 仍然阻塞； 如果客户端对数据库执行 PUSH key3 value1 value2 ， 那么 client3 和 client4 的阻塞都会被取消， 而客户端 client6 依然处于阻塞状态： 阻塞因超过最大等待时间而被取消前面提到过， 当客户端被阻塞时， 所有造成它阻塞的键， 以及阻塞的最长时限会被记录在客户端里面， 并且该客户端的状态会被设置为“正在阻塞”。 每次 Redis 服务器常规操作函数（server cron job）执行时， 程序都会检查所有连接到服务器的客户端， 查看那些处于“正在阻塞”状态的客户端的最大阻塞时限是否已经过期， 如果是的话， 就给客户端返回一个空白回复， 然后撤销对客户端的阻塞。 可以用一段伪代码来描述这个过程： 123456789101112131415161718def server_cron_job(): # 其他操作 ... # 遍历所有已连接客户端 for client in server.all_connected_client: # 如果客户端状态为“正在阻塞”，并且最大阻塞时限已到达 if client.state == BLOCKING and \ client.max_blocking_timestamp &lt; current_timestamp(): # 那么给客户端发送空回复,脱离阻塞状态 client.send_empty_reply() # 并清除客户端在服务器上的阻塞信息 server.blocking_keys.remove_blocking_info(client) # 其他操作 ...]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink on K8s 部署]]></title>
    <url>%2Fflink%2Fflink-on-k8s%2F</url>
    <content type="text"><![CDATA[参考文档： 官网文档： https://ci.apache.org/projects/flink/flink-docs-stable/zh/ops/deployment/kubernetes.html https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/ops/deployment/native_kubernetes.html]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手与Tcpdump抓包分析过程]]></title>
    <url>%2Fcomputer-network%2Ftcpdump-tcp-3-and-4%2F</url>
    <content type="text"><![CDATA[一、TCP连接建立（三次握手）过程客户端A，服务器B，初始序号seq，确认号ack 初始状态：B处于监听状态，A处于打开状态 A -&gt; B : seq = x （A向B发送连接请求报文段，A进入同步发送状态SYN-SENT） B -&gt; A : ack = x + 1,seq = y （B收到报文段，向A发送确认，B进入同步收到状态SYN-RCVD） A -&gt; B : ack = y+1 （A收到B的确认后，再次确认，A进入连接状态ESTABLISHED） 连接后的状态：B收到A的确认后，进入连接状态ESTABLISHED 为什么要握手要三次防止失效的连接请求突然传到服务器端，让服务器端误认为要建立连接。 二、TCP连接释放（四次挥手）过程A -&gt; B : seq = u （A发出连接释放报文段，进入终止等待1状态FIN-WAIT-1） B -&gt; A : ack = u + 1,seq = v （B收到报文段，发出确认，TCP处于半关闭，B还可向A发数据，B进入关闭等待状态WAIT） B -&gt; A : ack = u + 1,seq = w （B重复发送确认号，进入最后确认状态LAST-ACK） A -&gt; B : ack = w + 1,seq = u + 1 （A发出确认，进入时间等待状态TIME-WAIT） 经过时间等待计时器设置的时间2MSL后，A才进入CLOSED状态 为什么A进入TIME-WAIT后必须等待2MSL 保证A发送的最后一个ACK报文段能达到B 防止失效的报文段出现在连接中 三、需要思考的问题问题1: 请详细描述三次握手和四次挥手的过程要求熟悉三次握手和四次挥手的机制，要求画出状态图。 问题2: 四次挥手中TIME_WAIT状态存在的目的是什么?这个问题是画出四次挥手状态图，会引申问你。不排除还会问为什么四次挥手是四次不是二次等问题。最好是把相关问题均掌握。 问题3: TCP是通过什么机制保障可靠性的?从四个方面进行回答，ACK确认机制、超时重传、滑动窗口以及流量控制，深入的话要求详细讲出流量控制的机制。 四、Tcpdump使用tcpdump是对网络上的数据包进行截获的包分析工具，它支持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来去掉无用的信息。 监视指定主机的数据包tcpdump host：截获该IP的主机收到的和发出的所有的数据包tcpdump host and：截获两个IP对应主机之间的通信 监视指定端口的数据包tcpdump port &lt;端口号&gt;：截获本机80端口的数据包 五、抓包分析握手过程抓包方法：首先使用tcpdump命令截获本机与某远程主机的数据包，然后打开某远程主机对应的网站，这里用我的域名www.fonxian.cn来做试验。 1Copyping www.fonxian.cn 得到域名对应的ip：151.101.100.133 1Copyifconfg 得到本机内网ip：192.168.0.108 -S 参数的目的是获得ack的绝对值，不加该参数，第三次握手的ack为相对值1 1Copysudo tcpdump -S host 192.168.0.108 and 151.101.100.133 得到下图 参考文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[ping通tcp不通]]></title>
    <url>%2Fcomputer-network%2Ftraceroute-icmp-and-tcp%2F</url>
    <content type="text"><![CDATA[linux 机器TraceRoute 实现原理Traceroute是用来侦测主机到目的主机之间所经路由情况的重要工具。 从源地址发出一个UDP探测包到目的地址，并将TTL设置为1； 到达路由器时，将TTL减1； 当TTL变为0时，包被丢弃，路由器向源地址发回一个ICMP超时通知（ICMP Time Exceeded Message），内含发送IP包的源地址，IP包的所有内容及路由器的IP地址； 当源地址收到该ICMP包时，显示这一跳路由信息； 重复1～5，并每次设置TTL加1； 直至目标地址收到探测数据包，并返回端口不可达通知（ICMP Port Unreachable）； 当源地址收到ICMP Port Unreachable包时停止traceroute。 Linux和Mac OS等系统使用UDP包进行探测，目标端口号默认为33434，每次探测目标端口号加1。Traceroute故意使用了一个大于 30000 的目标端口号，以保证目标地址收到数据包后能够返回一个“端口不可达”的 ICMP 报文，于是源地址就可将端口不可达报文当作跟踪结束的标志。 Traceroute每跳默认发送3个探测包（发包的数量可通过-q进行设置），探测包的返回会受到网络情况的影响。如果防火墙封掉了ICMP的返回信息，那么相应的延时位置会以*显示。如果某台网关阻塞或者某台DNS出现问题，那么相应行的延时会变长。可以加-n 参数来避免DNS解析，以IP格式输出数据。 每个探测包都有唯一的标识号，使得Traceroute能够识别返回的包。UDP数据包使用递增的目标端口号进行标识。 1traceroute [-n] -T -p &lt;目标端口号&gt; Host 参数说明 1234567891011121314151617181920212223242526272829303132333435363738394041traceroute [-46dFITUnreAV] [-f first_ttl] [-g gate,...] [-i device] [-m max_ttl] [-p port] [-s src_addr] [-q nqueries] [-N squeries] [-t tos] [-l flow_label] [-w waittime] [-z sendwait] [-UL] [-D] [-P proto] [--sport=port] [-M method] [-O mod_options] [--mtu] [--back] host [packet_len]traceroute6 [options] Host 目标服务器域名或 IP。-4, -6 指定使用 IPv4 or IPv6-I 使用icmp探测-T 通过 TCP 探测-n 直接使用 IP 地址而非主机名称（禁用 DNS 反查）。-p port 对于UDP跟踪，指定将使用的目标端口基traceroute（目标端口号将由每个探测递增）。 对于ICMP跟踪，指定初始ICMP序列值（也由每个探测递增）。 对于TCP和其他端口，只指定要连接的（常量）目标端口。-r 绕过常规路由表，直接发送到连接网络上的主机。如果主机不在直接连接的网络上，则返回错误。此选项可用于通过没有路由的接口ping本地主机。-m max_ttl 默认最多30跳-w waittime 每一跳最多等待多长时间，默认5秒-g gateway 指定包出口网关-i interface 指定网卡名-s source_addr 指定源ip（必须是某个网卡ip）-z sendwait 指定每一跳的时间间隔（默认0）-e, --extensions 显示ICMP扩展（rfc4884）。一般形式是类/类型：后跟十六进制转储。MPLS（rfc4950）以如下形式显示：MPLS:L=label，E=exp_use，S=stack_bottom，T=TTL（更多由/）分隔的对象。-A 在路由注册表中执行路径查找，并在相应地址后直接打印结果。-q num, 每个网关发送num个数据包---------------sport=port 选择一个源端口--fwmark=mark 设置一个防火墙标记-U, --udp 使用udp协议探测目标端口-UL, 使用 UDPLITE 进行探测-D, 使用DCCP（数据包拥塞控制协议）请求进行探测-P protocol，使用指定协议的原始数据包进行跟踪路由。默认协议是253--mtu 路径MTU是指一条因特网传输路径中，从源地址到目的地址所经过的“路径”上的所有IP跳的最大传输单元的最小值。或者从另外一个角度来看，就是无需进行分片处理就能穿过这条“路径”的最大传输单元的最大值。--back 响应包可能与探测包路径不同，打印响应包的跳数更多关于 traceroute 的用法，您可以通过man帮助查阅。 异常节点判定方法：如果相关端口在某一跳被阻断，则其后各跳均不会返回数据。据此就可以判定出异常节点 1234567891011121314151617181920[root@10-1-53-21 ~]# traceroute -n -T -p 443 183.3.217.36traceroute to 183.3.217.36 (183.3.217.36), 30 hops max, 60 byte packets 1 172.23.0.43 0.099 ms 0.051 ms 0.035 ms 2 172.23.0.254 4.176 ms 4.487 ms 4.789 ms 3 10.1.255.6 0.307 ms 0.425 ms 0.420 ms 4 122.115.48.65 3.080 ms 3.535 ms 3.806 ms 5 10.78.3.5 6.626 ms 7.133 ms 7.533 ms 6 172.16.0.1 1.333 ms 1.355 ms 1.454 ms 7 219.142.20.49 1.731 ms 1.738 ms 1.690 ms 8 * * * 9 * * *10 219.141.152.53 2.621 ms * *11 * * *12 * 119.147.222.122 42.294 ms 119.147.220.74 39.959 ms13 113.106.51.70 39.030 ms 38.859 ms 113.106.51.86 35.560 ms14 113.106.52.134 39.720 ms 40.552 ms 41.223 ms15 183.3.217.36 34.676 ms 36.169 ms 36.221 ms16 * * *17 * * *18 183.3.217.36 38.777 ms 39.290 ms 39.291 ms 1234567891011121314151617181920212223242526272829303132[root@10-1-53-21 ~]# traceroute -n -T -p 443 183.3.217.36traceroute to 183.3.217.36 (183.3.217.36), 30 hops max, 60 byte packets 1 172.23.0.43 0.127 ms 0.101 ms 0.084 ms 2 172.23.0.254 5.911 ms 7.257 ms 7.248 ms 3 10.1.255.6 0.451 ms 0.441 ms 0.426 ms 4 122.115.48.65 3.120 ms 3.574 ms 3.813 ms 5 10.78.3.5 6.859 ms 7.208 ms 7.575 ms 6 172.16.0.1 1.258 ms 1.364 ms 1.429 ms 7 219.142.20.49 10.966 ms 10.785 ms 10.764 ms 8 * * * 9 * * *10 219.141.152.61 3.043 ms 219.141.152.65 6.084 ms *11 * * *12 * * *13 113.106.51.86 36.000 ms 35.589 ms 113.106.51.82 36.043 ms14 113.106.52.130 41.988 ms 113.106.52.134 38.881 ms 39.337 ms15 183.3.217.36 35.303 ms 36.716 ms 36.751 ms16 * * *17 * * *18 * * *19 * * *20 * * *21 * * *22 * * *23 * * *24 * * *25 * * *26 * * *27 * * *28 * * *29 * * *30 * * * 123456789101112131415161718192021222324252627282930313233yum install -y sshpassnc 183.3.217.36 443nc -vz jzjyszdx01.jzsec.com 443nc -vz 183.3.217.36 443traceroute 183.3.217.36ping 183.3.217.36sshpass -p 'sdfsfsdfsgg' ssh root@10.198.45.139sshpass -p 'sdfsfsdfsgg' ssh root@10.198.45.141traceroute -n -T -p 443 183.3.217.36traceroute -4nTe -p 443 -m 40 183.3.217.36traceroute -4nIe -m 40 183.3.217.36traceroute -4nUe -p 443 -m 40 183.3.217.36traceroute -4nTe -m 40 -p 22 10.198.45.139nc 112.95.153.36 443nc -vz jzjyszlt01.jzsec.com 443nc -vz 112.95.153.36 443traceroute -m 40 112.95.153.36traceroute -n -T -p 443 112.95.153.36traceroute -4nTe -p 443 -m 40 112.95.153.36sshpass -p 'sdfsfsdfsgg' ssh root@10.198.45.140sshpass -p 'sdfsfsdfsgg' ssh root@10.198.45.142[root@10-1-53-21 ~]# nc 183.3.217.36 443123ewq[root@10-1-53-21 ~]# netstat -atn | grep "ES" | grep 183.3.217.36[root@10-1-53-21 ~]# netstat -atn | grep "ES" | grep 112.95.153.36[root@10-198-45-139 ~]# tcpdump 'tcp and src 122.115.48.123 and dst port 443 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) &lt; 20)' -XX[root@10-198-45-139 ~]# tcpdump 'tcp and src 43.227.140.124 and dst port 443 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) &lt; 20)' -XXtcpdump 'tcp and dst port 443 and (0 &lt; ((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2))) and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) &lt; 20)' -XX windows 机器1tracetcp &lt;目标服务器域名或 IP&gt;:&lt;待探测端口号&gt; 异常节点判定方法：如果相关端口在某一跳被阻断，则其后各跳均不会返回数据。据此就可以判定出异常节点 Tracert实现原理 从源地址发出一个ICMP请求回显（ICMP Echo Request）数据包到目的地址，并将TTL设置为1； 到达路由器时，将TTL减1； 当TTL变为0时，包被丢弃，路由器向源地址发回一个ICMP超时通知（ICMP Time Exceeded Message），内含发送IP包的源地址，IP包的所有内容及路由器的IP地址； 当源地址收到该ICMP包时，显示这一跳路由信息； 重复1～5，并每次设置TTL加1； 直至目标地址收到探测数据包，并返回ICMP回应答复（ICMPEcho Reply）； 当源地址收到ICMP Echo Reply包时停止tracert。 注： 1.Windows系统使用ICMP请求回显（ICMP Echo Request）数据包进行探测，源地址以目的地址返回的ICMP回应答复（ICMP Echo Reply）作为跟踪结束标志。 2.Traceroute每跳默认发送3个探测包。在未能到达路由器或未返回ICMP超时通知的情况下，相应的延时位置会以*显示。 3.每个探测包都有唯一的标识号，ICMP数据包使用seq进行标识。 tracetcp安装依赖 https://www.winpcap.org/ 下载tracetcp，解压后放在c:/windows下 1http://www.github.com/simulatedsimian/tracetcp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748tracetcp host [options] where host = hostName|ipAddress[:portNumber|serviceName] if portNumber or serviceName is not present then port 80 (http) is assumed.Options: -? Displays help information. -c Select condensed output mode -h start_hop Starts trace at hop specified. -m max_hops Maximum number of hops to reach target. -n No reverse DNS lookups for each node. -p num_pings # of pings per hop (default 3). -r p1 p2 Multiple traces from port p1 to p2. -t timeout Wait timeout milliseconds for each reply. -v Displays version information. -s p1 p2 Easy port scan mode. gives the same result as setting the following options: -cnr p1 p2 -h 128 -m 1 -p 1 -F Disables the Anti-flood timer. Normally tracetcp waits *at least* 0.5 seconds between sending out each packet, because if the packets are sent too fast some host seem to detect this as some form of flood and stop responding for a time. This option disables the 0.5 second timer, so the traces occur faster. -R Use raw socket interface to send/receive packets this will not work on XP sp2. (you still need winpcap installed) -g address use the specified host as a a gateway to remote systems rather than the default gateway.Examples: tracetcp www.microsoft.com:80 -m 60 tracetcp post.sponge.com:smtp tracetcp 192.168.0.1 -n -t 500 itracerttcpinghttpping]]></content>
  </entry>
  <entry>
    <title><![CDATA[Nacos 快速开始]]></title>
    <url>%2Fmicro-service%2Fspring-cloud%2Fnacos%2F</url>
    <content type="text"><![CDATA[官网：https://nacos.io/zh-cn/docs/quick-start.html Nacos Server 安装这个快速开始手册是帮忙您快速在您的电脑上，下载、安装并使用 Nacos。 0.版本选择您可以在Nacos的release notes及博客中找到每个版本支持的功能的介绍，当前推荐的稳定版本为1.1.4。 1.预备环境准备Nacos 依赖 Java 环境来运行。如果您是从代码开始构建并运行Nacos，还需要为此配置 Maven环境，请确保是在以下版本环境中安装使用: 64 bit OS，支持 Linux/Unix/Mac/Windows，推荐选用 Linux/Unix/Mac。 64 bit JDK 1.8+；下载 &amp; 配置。 Maven 3.2.x+；下载 &amp; 配置。 2.下载源码或者安装包你可以通过源码和发行包两种方式来获取 Nacos。 从 Github 上下载源码方式1234567git clone https://github.com/alibaba/nacos.gitcd nacos/mvn -Prelease-nacos -Dmaven.test.skip=true clean install -U ls -al distribution/target/// change the $version to your actual pathcd distribution/target/nacos-server-$version/nacos/bin 下载编译后压缩包方式您可以从 最新稳定版本 下载 nacos-server-$version.zip 包。 12unzip nacos-server-$version.zip 或者 tar -xvf nacos-server-$version.tar.gzcd nacos/bin 3.启动服务器Linux/Unix/Mac启动命令(standalone代表着单机模式运行，非集群模式): 1sh startup.sh -m standalone 如果您使用的是ubuntu系统，或者运行脚本报错提示[[符号找不到，可尝试如下运行： 1bash startup.sh -m standalone Windows启动命令： 1cmd startup.cmd 或者双击startup.cmd运行文件。 4.服务注册&amp;发现和配置管理服务注册1curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName&amp;ip=20.18.7.10&amp;port=8080' 服务发现1curl -X GET 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=nacos.naming.serviceName' 发布配置1curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&amp;group=test&amp;content=HelloWorld" 获取配置1curl -X GET "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&amp;group=test" 5.关闭服务器Linux/Unix/Mac1sh shutdown.sh Windows1cmd shutdown.cmd 或者双击shutdown.cmd运行文件。 6.浏览器访问 http://localhost:8848/nacos/ nacos/nacos 7.配置mysql12345create database nacos_config;show variables like 'innodb_large_prefix'show variables like 'innodb_file_format'set global innodb_large_prefix = 1;set global innodb_file_format=BARRACUDA 问题解决 12345678910[Err] 1071 - Specified key was too long; max key length is 767 bytes[Err] 1709 - Index column size too large. The maximum column size is 767 bytes.1、set global innodb_large_prefix=on2、set global variable innodb_file_format=BARRACUDA3、使用 Innodb引擎在建表语句后加入ROW_FORMAT=DYNAMIC or ROW_FORMAT=COMPRESSED ，默认是ROW_FORMAT=COMPACTvim /etc/my.cnfinnodb_large_prefix=oninnodb_file_format=BARRACUDA Nacos Docker安装1docker run --env MODE=standalone --name nacos -d -p 8848:8848 docker.io/nacos/nacos-server:latest 1https://github.com/nacos-group/nacos-docker/blob/master/build/Dockerfile 123456789101112131415161718192021cd /data/components/nacos-docker/docker-compose -f example/standalone-mysql-5.7.yaml up -ddocker-compose -f example/standalone-mysql-5.7.yaml downdocker-compose -f example/standalone-mysql.yaml up -d(base) [root@dev-10-1-171-41 env]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.1.171.1 0.0.0.0 UG 100 0 0 eth010.1.171.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0172.18.1.84 10.1.171.1 255.255.255.255 UGH 0 0 0 eth0(base) [root@dev-10-1-171-41 nacos-docker]# docker network lsNETWORK ID NAME DRIVER SCOPE4d95fdb44032 bridge bridge localb19a30233e5d example_ncosnetwork bridge local2b9e87501791 host host localdbc75370a996 none null local]]></content>
      <categories>
        <category>springcloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RPC核心原理和实战 —— 基础篇]]></title>
    <url>%2Fdistribution%2Frpc%2Frpc-core-principle%2F</url>
    <content type="text"><![CDATA[别老想着怎么用好RPC框架，你得多花时间琢磨原理 引言1、为什么要学习 RPC？只要涉及网络通信，我们就有可能使用RPC。 在我看来，RPC 是解决分布式系统通信问题的一大利器。分布式系统中的网络通信一般都会采用四层的 TCP 协议或七层的 HTTP 协议，在我的了解中，前者占大多数，这主要得益于 TCP 协议的稳定性和高效性。网络通信说起来简单，但实际上是一个非常复杂的过程，这个过程主要包括：对端节点的查找、网络连接的建立、传输数据的编码解码以及网络连接的管理等等，每一项都很复杂。 2、如何学习 RPC？逐步深入 使用 RPC 就可以像调用本地一样发起远程调用，用它可以解决通信问题，这时候我们肯定要去学序列化、编解码以及网络传输这些内容。 把这些内容掌握后，你就会发现，原来这些只是 RPC 的基础，RPC 还有更吸引人的点，它真正强大的地方是它的治理功能，比如连接管理、健康检测、负载均衡、优雅启停机、异常重试、业务分组以及熔断限流等等。突然间，你会感觉自己走进了一个新世界，这些内容会成为你今后学习 RPC 的重点和难点。 对 RPC 活学活用，学会提升 RPC 的性能以及它在分布式环境下如何定位问题等等 1、核心原理：能否画张图解释RPC通信流程？ 1.1、什么是RPC？RPC 的全称是 Remote Procedure Call，即远程过程调用。简单解读字面上的意思，远程肯定是指要跨机器而非本机，所以需要用到网络编程才能实现。 RPC 的作用就是体现在这样两个方面： 屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法； 隐藏底层网络通信的复杂性，让我们更专注于业务逻辑； 1.2、RPC通信流程1)、一个完成的RPC会涉及哪些步骤呢？ TCP通信：RPC 是一个远程调用，那肯定就需要通过网络来传输数据，并且 RPC 常用于业务系统之间的数据交互，需要保证其可靠性，所以 RPC 一般默认采用 TCP 来传输。 序列化：网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是肯定没法直接在网络中传输的，需要提前把它转成可传输的二进制，并且要求转换算法是可逆的，这个过程我们一般叫做“序列化”。 协议：在这里我们可以想想高速公路，它上面有很多出口，为了让司机清楚地知道从哪里出去，管理部门会在路上建立很多指示牌，并在指示牌上标明下一个出口是哪里、还有多远。那回到数据包识别这个场景，我们是不是也可以建立一些“指示牌”，并在上面标明数据包的类型和长度，这样就可以正确的解析数据了。确实可以，并且我们把数据格式的约定内容叫做“协议”。大多数的协议会分成两部分，分别是数据头和消息体。数据头一般用于身份识别，包括协议标识、数据大小、请求类型、序列化类型等信息；消息体主要是请求的业务参数信息和扩展属性等 反序列化：根据协议格式，服务提供方就可以正确地从二进制数据中分割出不同的请求来，同时根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象。这个过程叫作“反序列化”。 执行：服务提供方再根据反序列化出来的请求对象找到对应的实现类，完成真正的方法调用，然后把执行结果序列化后，回写到对应的 TCP 通道里面。调用方获取到应答的数据包后，再反序列化成应答对象，这样调用方就完成了一次 RPC 调用 2)、上述构成一个完成的RPC吗？缺点：对于研发人员来说，这样做需要掌握太多的RPC底层细节，需要手动写代码去构造请求、调用徐磊好、并进行网络调用，整个API非常不友好； 如何简单 API，屏蔽 RPC 细节？ 如果你了解 Spring，一定对其 AOP 技术很佩服，其核心是采用动态代理的技术，通过字节码增强对方法进行拦截增强，以便于增加需要的额外处理逻辑。其实这个技术也可以应用到 RPC 场景来解决我们刚才面临的问题。 由服务提供者给出业务接口声明，在调用方的程序里面，RPC 框架根据调用的服务接口提前生成动态代理实现类，并通过依赖注入等技术注入到声明了该接口的相关业务逻辑里面。该代理实现类会拦截所有的方法调用，在提供的方法处理逻辑里面完成一整套的远程调用，并把远程调用结果返回给调用方，这样调用方在调用远程方法的时候就获得了像调用本地接口一样的体验。 到这里，一个简单版本的 RPC 框架就实现了。我把整个流程都画出来了，供你参考： 1.3、RPC在架构中的位置RPC是解决应用间通信的一种方式； RPC 框架能够帮助我们解决系统拆分后的通信问题，并且能让我们像调用本地一样去调用远程方法。利用 RPC 我们不仅可以很方便地将应用架构从“单体”演进成“微服务化”，而且还能解决实际开发过程中的效率低下、系统耦合等问题，这样可以使得我们的系统架构整体清晰、健壮，应用可运维度增强。 当然 RPC 不仅可以用来解决通信问题，它还被用在了很多其他场景，比如：发 MQ、分布式缓存、数据库等。下图是我之前开发的一个应用架构图： 1.4、总结本讲我主要讲了下 RPC 的原理，RPC 就是提供一种透明调用机制，让使用者不必显式地区分本地调用和远程调用。RPC 虽然可以帮助开发者屏蔽远程调用跟本地调用的区别，但毕竟涉及到远程网络通信，所以这里还是有很多使用上的区别，比如： 调用过程中超时了怎么处理业务？ 什么场景下最适合使用 RPC？ 什么时候才需要考虑开启压缩？ 无论你是一个初级开发者还是高级开发者，RPC 都应该是你日常开发过程中绕不开的一个话题，所以作为软件开发者的我们，真的很有必要详细地了解 RPC 实现细节。只有这样，才能帮助我们更好地在日常工作中使用 RPC。课后思考 2、RPC协议：怎么设计可扩展向后兼容的协议？一提到协议，你最先想到的可能是 TCP 协议、UDP 协议等等； 那 HTTP 协议跟 RPC 协议又有什么关系呢？看起来他俩好像不搭边，但他们有一个共性就是都属于应用层协议； 我们今天要讲的 RPC 协议就是围绕应用层协议展开的。我们可以先了解下 HTTP 协议，我们先看看它的协议格式是什么样子的。回想一下我们在浏览器里面输入一个 URL 会发生什么？抛开 DNS 解析暂且不谈，浏览器收到命令后会封装一个请求，并把请求发送到 DNS 解析出来的 IP 上，通过抓包工具我们可以抓到请求的数据包，如下图所示： 2.1、协议的作用看完 HTTP 协议之后，你可能会有一个疑问，我们为什么需要协议这个东西呢？没有协议就不能通信吗？ 我们知道只有二进制才能在网络中传输，所以 RPC 请求在发送到网络中之前，他需要把方法调用的请求参数转成二进制；转成二进制后，写入本地 Socket 中，然后被网卡发送到网络设备中。 但在传输过程中，RPC 并不会把请求参数的所有二进制数据整体一下子发送到对端机器上，中间可能会拆分成好几个数据包，也可能会合并其他请求的数据包（合并的前提是同一个 TCP 连接上的数据），至于怎么拆分合并，这其中的细节会涉及到系统参数配置和 TCP 窗口大小。对于服务提供方应用来说，他会从 TCP 通道里面收到很多的二进制数据，那这时候怎么识别出哪些二进制是第一个请求的呢？ 所以呢，为了避免语义不一致的事情发生，我们就需要在发送请求的时候设定一个边界，然后在收到请求的时候按照这个设定的边界进行数据分割。这个边界语义的表达，就是我们所说的协议。 2.2、如何设计协议？有了现成的 HTTP 协议，为啥不直接用，还要为 RPC 设计私有协议呢？ 这还要从 RPC 的作用说起，相对于 HTTP 的用处，RPC 更多的是负责应用间的通信，所以性能要求相对更高。但 HTTP 协议的数据包大小相对请求数据本身要大很多，又需要加入很多无用的内容，比如换行符号、回车符等；还有一个更重要的原因是，HTTP 协议属于无状态协议，客户端无法对请求和响应进行关联，每次请求都需要重新建立连接，响应完成后再关闭连接。因此，对于要求高性能的 RPC 来说，HTTP 协议基本很难满足需求，所以 RPC 会选择设计更紧凑的私有协议。 那怎么设计一个私有 RPC 协议呢？ 消息边界：消息长度，实现消息间正确断句 在协议头里面，我们除了会放协议长度、序列化方式，还会放一些像协议标示、消息 ID、消息类型这样的参数，而协议体一般只放请求接口方法、请求的业务参数值和一些扩展属性。这样一个完整的 RPC 协议大概就出来了，协议头是由一堆固定的长度参数组成，而协议体是根据请求接口和参数构造的，长度属于可变的，具体协议如下图所示： 2.3、可扩展的协议刚才讲的协议属于定长协议头，那也就是说往后就不能再往协议头里加新参数了，如果加参数就会导致线上兼容问题。举个具体例子，假设你设计了一个 88Bit 的协议头，其中协议长度占用 32bit，然后你为了加入新功能，在协议头里面加了 2bit，并且放到协议头的最后。升级后的应用，会用新的协议发出请求，然而没有升级的应用收到的请求后，还是按照 88bit 读取协议头，新加的 2 个 bit 会当作协议体前 2 个 bit 数据读出来，但原本的协议体最后 2 个 bit 会被丢弃了，这样就会导致协议体的数据是错的。 可能你会想：“那我把参数加在不定长的协议体里面行不行？而且刚才你也说了，协议体里面会放一些扩展属性。” 没错，协议体里面是可以加新的参数，但这里有一个关键点，就是协议体里面的内容都是经过序列化出来的，也就是说你要获取到你参数的值，就必须把整个协议体里面的数据经过反序列化出来。但在某些场景下，这样做的代价有点高啊！ 比如说，服务提供方收到一个过期请求，这个过期是说服务提供方收到的这个请求的时间大于调用方发送的时间和配置的超时时间，既然已经过期，就没有必要接着处理，直接返回一个超时就好了。那要实现这个功能，就要在协议里面传递这个配置的超时时间，那如果之前协议里面没有加超时时间参数的话，我们现在把这个超时时间加到协议体里面是不是就有点重了呢？显然，会加重 CPU 的消耗。 所以为了保证能平滑地升级改造前后的协议，我们有必要设计一种支持可扩展的协议。其关键在于让协议头支持可扩展，扩展后协议头的长度就不能定长了。那要实现读取不定长的协议头里面的内容，在这之前肯定需要一个固定的地方读取长度，所以我们需要一个固定的写入协议头的长度。整体协议就变成了三部分内容：固定部分、协议头内容、协议体内容，前两部分我们还是可以统称为“协议头”，具体协议如下： 最后，我想说，设计一个简单的 RPC 协议并不难，难的就是怎么去设计一个可“升级”的协议。不仅要让我们在扩展新特性的时候能做到向下兼容，而且要尽可能地减少资源损耗，所以我们协议的结构不仅要支持协议体的扩展，还要做到协议头也能扩展。上述这种设计方法来源于我多年的线上经验，可以说做好扩展性是至关重要的，期待这个协议模版能帮你避掉一些坑。 3、序列化：对象怎么在网络中传输​ 那么承接上一讲的一个重点，今天我会讲解下 RPC 框架中的序列化。要知道，在不同的场景下合理地选择序列化方式，对提升 RPC 框架整体的稳定性和性能是至关重要的 3.1、为什么需要序列化？​ 网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是不能直接在网络中传输的，所以我们需要提前把它转成可传输的二进制 ​ 总结来说，序列化就是将对象转换成二进制数据的过程，而反序列就是反过来将二进制转换为对象的过程。 ​ 那么 RPC 框架为什么需要序列化呢？还是请你回想下 RPC 的通信流程： ​ 不妨借用个例子帮助你理解，比如发快递，我们要发一个需要自行组装的物件。发件人发之前，会把物件拆开装箱，这就好比序列化；这时候快递员来了，不能磕碰呀，那就要打包，这就好比将序列化后的数据进行编码，封装成一个固定格式的协议；过了两天，收件人收到包裹了，就会拆箱将物件拼接好，这就好比是协议解码和反序列化。 ​ 所以现在你清楚了吗？因为网络传输的数据必须是二进制数据，所以在 RPC 调用中，对入参对象与返回值对象进行序列化与反序列化是一个必须的过程。 3.2、有哪些常用的序列化1)、JDK原生序列化​ JDK 自带的序列化机制对使用者而言是非常简单的。序列化具体的实现是由 ObjectOutputStream 完成的，而反序列化的具体实现是由 ObjectInputStream 完成的。 ​ 那么 JDK 的序列化过程是怎样完成的呢？我们看下下面这张图： 序列化过程就是在读取对象数据的时候，不断加入一些特殊分隔符，这些特殊分隔符用于在反序列化过程中截断用。 头部数据用来声明序列化协议、序列化版本，用于高低版本向后兼容 对象数据主要包括类名、签名、属性名、属性类型及属性值，当然还有开头结尾等数据，除了属性值属于真正的对象值，其他都是为了反序列化用的元数据 存在对象引用、继承的情况下，就是递归遍历“写对象”逻辑 2)、JSON一种文本型序列化框架。无论是前台 Web 用 Ajax 调用、用磁盘存储文本类型的数据，还是基于 HTTP 协议的 RPC 框架通信，都会选择 JSON 格式。 但用 JSON 进行序列化有这样两个问题，你需要格外注意： JSON 进行序列化的额外空间开销比较大，对于大数据量服务这意味着需要巨大的内存和磁盘开销； JSON 没有类型，但像 Java 这种强类型语言，需要通过反射统一解决，所以性能不会太好。 所以如果 RPC 框架选用 JSON 序列化，服务提供者与服务调用者之间传输的数据量要相对较小，否则将严重影响性能。 3)、HessianHessian 是动态类型、二进制、紧凑的，并且可跨语言移植的一种序列化框架。Hessian 协议要比 JDK、JSON 更加紧凑，性能上要比 JDK、JSON 序列化高效很多，而且生成的字节数也更小。 12345678910111213141516171819Student student = new Student();student.setNo(101);student.setName("HESSIAN");//把student对象转化为byte数组ByteArrayOutputStream bos = new ByteArrayOutputStream();Hessian2Output output = new Hessian2Output(bos);output.writeObject(student);output.flushBuffer();byte[] data = bos.toByteArray();bos.close();//把刚才序列化出来的byte数组转化为student对象ByteArrayInputStream bis = new ByteArrayInputStream(data);Hessian2Input input = new Hessian2Input(bis);Student deStudent = (Student) input.readObject();input.close();System.out.println(deStudent); 相对于 JDK、JSON，由于 Hessian 更加高效，生成的字节数更小，有非常好的兼容性和稳定性，所以 Hessian 更加适合作为 RPC 框架远程通信的序列化协议。 但 Hessian 本身也有问题，官方版本对 Java 里面一些常见对象的类型不支持，比如： Linked 系列，LinkedHashMap、LinkedHashSet 等，但是可以通过扩展 CollectionDeserializer 类修复； Locale 类，可以通过扩展 ContextSerializerFactory 类修复； Byte/Short 反序列化的时候变成 Integer。以上这些情况，你在实践时需要格外注意。 4)、ProtobufProtobuf 是 Google 公司内部的混合语言数据标准，是一种轻便、高效的结构化数据存储格式，可以用于结构化数据序列化，支持 Java、Python、C++、Go 等语言。Protobuf 使用的时候需要定义 IDL（Interface description language），然后使用不同语言的 IDL 编译器，生成序列化工具类，它的优点是： 序列化后体积相比 JSON、Hessian 小很多； IDL 能清晰地描述语义，所以足以帮助并保证应用程序之间的类型不会丢失，无需类似 XML 解析器； 序列化反序列化速度很快，不需要通过反射获取类型； 消息格式升级和兼容性不错，可以做到向后兼容 1234567891011121314151617181920212223242526272829/** * * // IDl 文件格式 * synax = "proto3"; * option java_package = "com.test"; * option java_outer_classname = "StudentProtobuf"; * * message StudentMsg &#123; * //序号 * int32 no = 1; * //姓名 * string name = 2; * &#125; * */ StudentProtobuf.StudentMsg.Builder builder = StudentProtobuf.StudentMsg.newBuilder();builder.setNo(103);builder.setName("protobuf");//把student对象转化为byte数组StudentProtobuf.StudentMsg msg = builder.build();byte[] data = msg.toByteArray();//把刚才序列化出来的byte数组转化为student对象StudentProtobuf.StudentMsg deStudent = StudentProtobuf.StudentMsg.parseFrom(data);System.out.println(deStudent); Protobuf 非常高效，但是对于具有反射和动态能力的语言来说，这样用起来很费劲，这一点就不如 Hessian，比如用 Java 的话，这个预编译过程不是必须的，可以考虑使用 Protostuff。 Protostuff 不需要依赖 IDL 文件，可以直接对 Java 领域对象进行反 / 序列化操作，在效率上跟 Protobuf 差不多，生成的二进制格式和 Protobuf 是完全相同的，可以说是一个 Java 版本的 Protobuf 序列化框架。但在使用过程中，我遇到过一些不支持的情况，也同步给你： 不支持 null； ProtoStuff 不支持单纯的 Map、List 集合对象，需要包在对象里面。 3.3、RPC框架中如何选择序列化我刚刚简单地介绍了几种最常见的序列化协议，其实远不止这几种，还有 Message pack、kryo 等。那么面对这么多的序列化协议，在 RPC 框架中我们该如何选择呢？ 性能和效率 空间开销 序列化协议的通用性和兼容性（版本升级后的兼容性是否很好，是否支持更多的对象类型，是否是跨平台、跨语言的） 安全性（序列化存在安全漏洞，那么线上的服务就很可能被入侵） 易于调试 我们首选的还是 Hessian 与 Protobuf，因为他们在性能、时间开销、空间开销、通用性、兼容性和安全性上，都满足了我们的要求。其中 Hessian 在使用上更加方便，在对象的兼容性上更好；Protobuf 则更加高效，通用性上更有优势。 3.4、RPC框架在使用时要注意哪些问题？对象构造得过于复杂：属性很多，并且存在多层的嵌套 对象过于庞大：我经常遇到业务过来咨询，为啥他们的 RPC 请求经常超时，排查后发现他们的入参对象非常得大 用序列化框架不支持的类作为入参类：比如 Hessian 框架，他天然是不支持 LinkHashMap、LinkedHashSet 等，而且大多数情况下最好不要使用第三方集合类，Guava 中的集合类，很多开源的序列化框架都是优先支持编程语言原生的对象。因此如果入参是集合类，应尽量选用原生的、最为常用的集合类，如 HashMap、ArrayList 对象有复杂的继承关系：大多数序列化框架在序列化对象时都会将对象的属性一一进行序列化，当有继承关系时，会不停地寻找父类，遍历属性 3.5、总结在使用 RPC 框架的过程中，我们构造入参、返回值对象，主要记住以下几点： 对象要尽量简单，没有太多的依赖关系，属性不要太多，尽量高内聚； 入参对象与返回值对象体积不要太大，更不要传太大的集合； 尽量使用简单的、常用的、开发语言原生的对象，尤其是集合类； 对象不要有复杂的继承关系，最好不要有父子类的情况。]]></content>
      <categories>
        <category>rpc</category>
      </categories>
      <tags>
        <tag>rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何向rocketmq提交pr]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2Frocketmq-how-to-submit-pr%2F</url>
    <content type="text"><![CDATA[1、fork仓库到自己账号下官方仓库：https://github.com/apache/rocketmq 2、clone代码到本地12345$ cd /d/github# 测试是否可以连接$ ssh -T git@github.com$ cd ~/.ssh$ ssh-keygen -t rsa 将公钥添加到 github github -&gt; Settings -&gt; SSH and GPG keys -&gt; New SSH key 再测试一下 12$ ssh -T git@github.comHi carlo-z! You've successfully authenticated, but GitHub does not provide shell access. clone 代码 1$ git clone git@github.com:carlo-z/rocketmq.git 查看仓库状态，提示现在是 master 分支 12345$ cd rocketmq/$ git statusOn branch masterYour branch is up to date with 'origin/master' 用git remote -v命令，可以看到此时只与自己的远程仓库建立了连接 123$ git remote -vorigin git@github.com:carlo-z/rocketmq.git (fetch)origin git@github.com:carlo-z/rocketmq.git (push) 3、与官方仓库建立连接12345678$ git remote add upstream https://github.com/apache/rocketmq.git$ git remote -vorigin git@github.com:carlo-z/rocketmq.git (fetch)origin git@github.com:carlo-z/rocketmq.git (push)upstream https://github.com/apache/rocketmq.git (fetch)upstream https://github.com/apache/rocketmq.git (push) 再用git remote -v可以看到 已经与官方仓库建立了连接 4、创建新分支用于修改接着上面的运行命令：git checkout -b rocketmq-read，这个命令的意思是创建一个叫 rocketmq-read 的分支，运行这个命令后bash将自动切换到新的分支下 12$ git checkout -b rocketmq-readSwitched to a new branch 'rocketmq-read' 修改代码提交5、提交pr 然后点Create pull request 写好名字，写好说明，提交，就OK啦。 6、后期同步代码所以每次提交pr前，都要先从做代码同步。过程如下： 先fetch 1$ git fetch upstream 再 rebase 1$ git rebase upstream/master 再 push master 1$ git push origin master push完后，远程仓库便可看到你的branch版本和master分支一致了，否则这个位置会显示与master相差了多少次commit。]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[arthas手册]]></title>
    <url>%2Fjava%2Fjvm%2Farthas-docs%2F</url>
    <content type="text"><![CDATA[官方文档：https://alibaba.github.io/arthas/ 下载安装https://alibaba.github.io/arthas/manual-install.html 下载 https://maven.aliyun.com/repository/public/com/taobao/arthas/arthas-packaging/3.x.x/arthas-packaging-3.x.x-bin.zip 解压 启动1java -jar arthas-boot.jar 查看dashboard [arthas@14628]$ dashboard 查看线程 [arthas@14628]$ thread // 拿到最忙的3个线程 [arthas@14628]$ thread -n 3 dump内存数据 [arthas@14628]$ heapdump d:/data/dump.hprof jhat分析dump文件 jhat d:/data/dump.hprof 生成一个web，通过浏览器访问 MAT分析dump文件]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
        <category>arthas</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sonarqube-8.3 安装]]></title>
    <url>%2Fdevops%2Fsonar%2Fsonarqube-server-setup%2F</url>
    <content type="text"><![CDATA[版本要求 https://docs.sonarqube.org/latest/requirements/requirements/ 安装PostgreSQL 123456docker pull postgresdocker run --name postgres -e POSTGRES_USER=sonar -e POSTGRES_PASSWORD=sonar -p 5432:5432 -d postgresfirewall-cmd --permanent --add-port=5432/tcpfirewall-cmd --reloadfirewall-cmd --list-alldocker exec -it postgres /bin/bash 修改host 123456vi /etc/hosts10.1.171.41 dev.mypostgres.comC:\Windows\System32\drivers\etc\hosts10.1.171.41 dev.mypostgres.com 通过Navicat for Postgres连接 dev.mypostgres.com 5432 sonar/sonar 安装jdk11 12345678910$ vi /etc/profileexport JAVA_HOME=/data/components/jdk-11.0.7export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar$ source /etc/profile$ java -versionjava version "11.0.7" 2020-04-14 LTSJava(TM) SE Runtime Environment 18.9 (build 11.0.7+8-LTS)Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.7+8-LTS, mixed mode) SonarQube安装配置使用zip安装下载zip文件（http://www.sonarqube.org/downloads/） 123unzip sonarqube-8.3.1.34397.zipln -s /data/sonarqube-8.3.1.34397 /data/sonarqubecd /data/sonarqube 设置DB在pg中建立数据库 12345create database sonar;create user sonar;atler user sonar with password 'sonar';alter role sonar createdb;alter role sonar superuser;alter role sonar createrole;alter database sonar owner to sonar 123456789101112131415vi /data/sonarqube/conf/sonar.properties# DATABASEsonar.jdbc.username=sonarsonar.jdbc.password=sonarsonar.jdbc.url=jdbc:postgresql://dev.mypostgres.com:5432/sonarsonar.jdbc.maxActive=60sonar.jdbc.maxIdle=5sonar.jdbc.minIdle=2sonar.jdbc.maxWait=5000sonar.jdbc.minEvictableIdleTimeMillis=600000sonar.jdbc.timeBetweenEvictionRunsMillis=30000# sonar.sorceEncoding=UTF-8 添加JDBC驱动程序已经提供了支持的数据库（Oracle除外）的驱动程序。不要更换提供的驱动程序；他们是唯一受支持的。 123456/data/sonarqube/lib/jdbc(base) [root@dev-10-1-171-41 jdbc]# lltotal 12drwxr-xr-x 2 root root 4096 May 7 13:41 h2drwxr-xr-x 2 root root 4096 May 7 13:41 mssqldrwxr-xr-x 2 root root 4096 May 7 13:41 postgresql oracle 数据库需要单独添加驱动，其他数据库已经默认提供 1234/data/sonarqube/extensions/jdbc-driver(base) [root@dev-10-1-171-41 jdbc-driver]# lltotal 4drwxr-xr-x 2 root root 4096 May 7 13:22 oracle 配置Elasticsearch存储路径默认情况下，Elasticsearch数据存储在$ SONARQUBE-HOME / data中，但不建议将其用于生产实例。相反，您应该将此数据存储在其他位置，最好是在具有快速I / O的专用卷中。除了保持可接受的性能外，这样做还可以简化SonarQube的升级。 123vi /data/sonarqube/conf/sonar.properties#sonar.path.data=/var/sonarqube/data#sonar.path.temp=/var/sonarqube/temp 特别配置jdk路径12vi /data/sonarqube/conf/wrapper.confwrapper.java.command=/data/components/jdk-11.0.7/bin/java linux系统参数优化1234567891011vi /etc/sysctl.d/99-sonarqube.conf（或/etc/sysctl.conf文件）vm.max_map_count=262144fs.file-max=65536sysctl -w vm.max_map_count=262144sysctl -w fs.file-max=65536ulimit -n 65536sysctl vm.max_map_count sysctl fs.file-max ulimit -n 启动Web服务器12345678vi /data/sonarqube/conf/sonar.propertiessonar.web.host=0.0.0.0sonar.web.port=9011sonar.web.context=firewall-cmd --permanent --add-port=9011/tcpfirewall-cmd --reloadfirewall-cmd --list-all sonar不能使用root启动 123useradd sonarpasswd sonarchown -R sonar:sonar /data/sonarqube/ 12345su - sonar/data/sonarqube/bin/linux-x86-64/sonar.sh start/data/sonarqube/bin/linux-x86-64/sonar.sh restartjps -l 设置域名 1234567vi /etc/hosts10.1.171.41 dev.mysonar.comecho '10.1.171.41 dev.mysonar.com' &gt;&gt; /etc/hostsC:\Windows\System32\drivers\etc\hosts10.1.171.41 dev.mysonar.com 访问： http://dev.mysonar.com:9011/ 默认账号：admin/admin 安装中文插件install -&gt; restart 使用systemd管理启动 SonarScanner安装配置 文档：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/ 下载zip包安装 123$ unzip sonar-scanner-cli-4.3.0.2102-linux.zip$ ln -s /data/downloads/sonar-scanner-4.3.0.2102-linux /data/sonar-scanner$ ln -s /data/sonar-scanner/bin/sonar-scanner /usr/bin/sonar-scanner 配置123$ vi /data/sonar-scanner/conf/sonar-scanner.propertiessonar.host.url=http://dev.mysonar.com:9011sonar.sourceEncoding=UTF-8 扫描maven工程示例1234567891011121314$ cd /data/components/leancloud-proxy$ vi sonar-project.propertiessonar.projectKey=leancloud-proxysonar.projectName=leancloud-proxysonar.projectVersion=1.0sonar.sources=./sonar.language=javasonar.sources=srcsonar.java.binaries=targetsonar.sourceEncoding=UTF-8sonar.dynamicAnalysis=falsesonar.scm.disabled=truesonar.scm.provider=svnsonar.host.url=http://dev.mysonar.com:9011 执行扫描 1$ sonar-scanner http://dev.mysonar.com:9011/projects 扫描php工程示例123456789101112$ cd /data/public_html$ vi sonar-project.propertiessonar.projectKey=php-trunksonar.projectName=php-trunksonar.projectVersion=1.0sonar.sources=./sonar.language=phpsonar.sourceEncoding=UTF-8sonar.dynamicAnalysis=falsesonar.scm.disabled=truesonar.scm.provider=svnsonar.host.url=http://dev.mysonar.com:9011 1$ sonar-scanner http://dev.mysonar.com:9011/projects jenkins + SonarScanner安装SonarQube Scanner插件Jenkins → 系统管理 → 管理插件 → 可选插件 → 🔍SonarQueue Scanner 安装以后重启 配置SonarQube首先，在SonarQube中生成一个Token（PS：用token代替输入用户名和密码） 1bb6ad69138f3607af1082565551846a5b23eb310 然后，在Jenkins中配置连接sonarqube服务器的地址，这里用到的token就是刚才在sonarqube中创建的那个token 系统设置 最后，配置全局工具配置 创建任务 12345678910sonar.projectKey=php-trunksonar.projectName=php-trunksonar.projectVersion=1.0sonar.sources=$WORKSPACEsonar.language=phpsonar.sourceEncoding=UTF-8sonar.dynamicAnalysis=falsesonar.scm.disabled=truesonar.scm.provider=svnsonar.host.url=http://dev.mysonar.com:9011 开始构建任务]]></content>
      <categories>
        <category>devops</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7 下 rocketmq 单机版安装]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2Flinux-rocketmq-setup%2F</url>
    <content type="text"><![CDATA[文档：http://rocketmq.apache.org/docs/quick-start/ 先决条件假定已安装以下软件： 建议使用64位操作系统，建议使用Linux / Unix / Mac； 64位JDK 1.8+; Maven 3.2.x; Git; 适用于Broker服务器的4g +可用磁盘 下载安装123456789101112131415161718[root@centos7cz ~]# cd /data/download/[root@centos7cz download]# wget http://mirror.bit.edu.cn/apache/rocketmq/4.6.0/rocketmq-all-4.6.0-bin-release.zip[root@centos7cz download]# mkdir -p /data/tools/[root@centos7cz download]# unzip rocketmq-all-4.6.0-bin-release.zip -d /data/tools[root@centos7cz download]# tar -zxf rocketmq-all-4.6.0.tar.gz -C /data/tools[root@centos7cz download]# cd /data/tools/[root@centos7cz tools]# mv rocketmq-all-4.6.0-bin-release/ rocketmq-all-4.6.0[root@centos7cz tools]# cd rocketmq-all-4.6.0/[root@centos7cz rocketmq-all-4.6.0]# lltotal 40drwxr-xr-x. 2 root root 83 Nov 20 11:04 benchmarkdrwxr-xr-x. 3 root root 4096 Aug 19 15:31 bindrwxr-xr-x. 6 root root 211 Aug 6 16:23 confdrwxr-xr-x. 2 root root 4096 Nov 20 11:04 lib-rw-r--r--. 1 root root 17336 Aug 6 16:23 LICENSE-rw-r--r--. 1 root root 1338 Aug 6 16:23 NOTICE-rw-r--r--. 1 root root 4225 Nov 1 16:54 README.md[root@centos7cz rocketmq-all-4.6.0]# 启动和关闭1234# 安装nohup[root@centos7cz ~]# yum install coreutils -y[root@centos7cz ~]# mkdir -p /data/tools/rocketmq-all-4.6.0/logs/ NameSrv1234567[root@centos7cz ~]# cd /data/tools/rocketmq-all-4.6.0[root@centos7cz rocketmq-all-4.6.0]# nohup sh /data/tools/rocketmq-all-4.6.0/bin/mqnamesrv &gt; /data/tools/rocketmq-all-4.6.0/logs/namesrv.log 2&gt;&amp;1 &amp;[root@centos7cz rocketmq-all-4.6.0]# tail -f /data/tools/rocketmq-all-4.6.0/logs/namesrv.log[root@centos7cz rocketmq-all-4.6.0]# /data/tools/rocketmq-all-4.6.0/bin/mqshutdown namesrv Broker1234567[root@centos7cz ~]# cd /data/tools/rocketmq-all-4.6.0[root@centos7cz rocketmq-all-4.6.0]# nohup sh /data/tools/rocketmq-all-4.6.0/bin/mqbroker &gt; /data/tools/rocketmq-all-4.6.0/logs/broker.log 2&gt;&amp;1 &amp;[root@centos7cz rocketmq-all-4.6.0]# tail -f /data/tools/rocketmq-all-4.6.0/logs/broker.log[root@centos7cz rocketmq-all-4.6.0]# /data/tools/rocketmq-all-4.6.0/bin/mqshutdown broker 收发消息1234567[root@centos7cz ~]# cd /data/tools/rocketmq-all-4.6.0[root@centos7cz rocketmq-all-4.6.0]# export NAMESRV_ADDR=localhost:9876[root@centos7cz rocketmq-all-4.6.0]# sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer[root@centos7cz rocketmq-all-4.6.0]# sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer]]></content>
      <categories>
        <category>rocketmq</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TPC,TPCC,TPMC(数据库性能衡量指标)]]></title>
    <url>%2Fmysql%2Fmysql-tpc-tpcc-tpmc%2F</url>
    <content type="text"><![CDATA[第一章 什么是TPC和tpmC?1 TPCTPC(Transaction Processing Performance Council，事务处理性能委员会)是由数10家会员公司创建的非盈利组织，总部设在美国。该组织对全世界开放，但迄今为止，绝大多数会员都是美、日、西欧的大公司。TPC的成员主要是计算机软硬件厂家，而非计算机用户，它的功 能是制定商务应用基准程序(Benchmark)的标准规范、性能和价格度量，并管理测 试结果的发布。 TPC的出版物是开放的，可以通过网络获取(http://www.tpc.org)。TPC不给出基准程序的代码，而只给出基准程序的标准规范(Standard Specification)。任何厂家或其它测试者都可以根据规范，最优地构造出自己的系统(测试平台和测试程序)。为保证测试结果的客观性，被测试者(通常是厂家)必须提交给TPC一套完整的报告(Full Disclosure Report)，包括被测系统的详细配置、分类价格和包含五年维护费用在内的总价 格。该报告必须由TPC授权的审核员核实(TPC本身并不做审计)。现在全球只有几个审核员，全部在美国。 TPC已经推出了四套基准程序，被称为TPC－A、TPC－B、TPC－C和TPC－D。其中A和B已经过时，不再使用了。TPC－C是在线事务处理(OLTP)的基准程序，TPC－D是决策支持(Decision Support) 的基准程序。TPC即将推TPC－E，作为大型企业(Enterprise)信息服务的基准程序。 2 tpmCtpmC值在国内外被广 泛用于衡量计算机系统的事务处理能力。但究竟什么是tpmC值呢?作者曾向一些 用户、推销人员乃至某些国外大公司的技术人员问过这个问题，但回答的精确度 与tpmC值的流行程度远非相称。tpmC这一度量也常被误写为TPM或TPMC。 TPC-C模拟一个批发商的货物管理环境。该批发公司有N个仓库，每个仓库供应10个地区，其中每个地区为3000名顾客服务。在每个仓库中有10个终端，每一个终端用于一个地区。在运行时，10×N个终端操作员向公司的数据库发出5类请求。由于一个仓库中不可能存储公司所有的货物，有一些请求必须发往其它仓库，因此，数据库在逻辑上是 分布的。N是一个可变参数，测试者可以随意改变N，以获得最佳测试效果。 TPC-C使用三种性能和价格度量，其中性能由TPC-C吞吐率衡量，单位是tpmC。tpm是transactions per minute的简称；C指TPC中的C基准程序。它的定义是每分钟内系统处理的新订单个数。要注意的是，在处理新订单的同时，系统还要按表1的要求处理其它4类事务 请求。从表1可以看出，新订单请求不可能超出全部事务请求的45％，因此，当一个 系统的性能为1000tpmC时，它每分钟实际处理的请求数是2000多个。价格是指系 统的总价格，单位是美元，而价格性能比则定义为总价格÷性能，单位是＄/tpmC。 tpmC定义: TPC-C的吞吐量，按有效TPC-C配置期间每分钟处理的平均交易次数测量，至少要运行12分钟。 （吞吐量测试结果以比特/秒或字节/秒表示。） 第二章 TPCC1 基准测试TPCC值被广泛用于衡量C/S环境下,由服务器和客户端构筑的整体系统的性能,它由事物处理性能委员会（TPC，Transaction Processing Corp）制定,TPC为非赢利性国际组织。 TPCC值可以反映出系统的性能价格比。TPCC测试系统每分钟处理的任务数,单位为tpm,(transactions per minute)。系统的总体价格(单位为美元)除以TPCC值,就可以衡量出系统的性价比,系统的性价比值越大,系统的性价比越好。 需要注意的是,TPC-C值描述的是C/S整体系统的性能,它与系统的服务器和客户机的性能都有关系,也就是说,同样的服务器配置不同的客户端将会影响TPCC值,任何厂商和测试者都可以根据TPC提供的测试规范构造出自己最优的系统,当然测试的结果要经过TPC审核。 2 性能测试指标介绍TPC-C 作为一家非盈利性机构，事务处理性能委员会（TPC）负责定义诸如TPC-C、TPC-H和TPC-W基准测试之类的事务处理与数据库性能基准测试，并依据这些基准测试项目发布客观性能数据。TPC基准测试采用极为严格的运行环境，并且必须在独立审计机构监督下进行。委员会成员包括大多数主要数据库产品厂商以及服务器硬件系统供应商。 相关企业参与TPC基准测试以期在规定运行环境中获得客观性能验证，并通过应用测试过程中所使用的技术开发出更加强健且更具伸缩性的软件产品及硬件设备。 TPC-C是一种旨在衡量联机事务处理（OLTP）系统性能与可伸缩性的行业标准基准测试项目。这种基准测试项目将对包括查询、更新及队列式小批量事务在内的广泛数据库功能进行测试。许多IT专业人员将TPC-C视为衡量“真实”OLTP系统性能的有效指示器。 TPC-C基准测试针对一种模拟订单录入与销售环境测量每分钟商业事务（tpmC）吞吐量。特别值得一提的是，它将专门测量系统在同时执行其它四种事务类型（如支付、订单状态更新、交付及证券级变更）时每分钟所生成的新增订单事务数量。独立审计机构将负责对基准测试结果进行公证，同时，TPC将出据一份全面彻底的测试报告。这份测试报告可以从TPC Web站点(http://www.tpc.org)上获得。 3 TPC-C规范概要TPC-C是专门针对联机交易处理系统（OLTP系统）的，一般情况下我们也把这类系统称为业务处理系统。 TPC-C测试规范中模拟了一个比较复杂并具有代表意义的OLTP应用环境:假设有一个大型商品批发商，它拥有若干个分布在不同区域的商品库；每个仓库负责为10个销售点供货；每个销售点为3000个客户提供服务；每个客户平均一个订单有10项产品;所有订单中约1%的产品在其直接所属的仓库中没有存货，需要由其他区域的仓库来供货。同时，每个仓库都要维护公司销售的100000种商品的库存记录。 该系统需要处理的交易为以下几种： New-Order：客户输入一笔新的订货交易； Payment:更新客户账户余额以反映其支付状况; Delivery:发货(模拟批处理交易); Order-Status:查询客户最近交易的状态； Stock-Level:查询仓库库存状况，以便能够及时补货。 对于前四种类型的交易，要求响应时间在5秒以内；对于库存状况查询交易，要求响应时间在20秒以内。 4评测指标TPC-C测试规范经过两年的研制，于1992年7月发布。几乎所有在OLTP市场提供软硬件平台的厂商都发布了相应的TPC-C测试结果，随着计算机技术的不断发展，这些测试结果也在不断刷新。 TPC-C的测试结果主要有两个指标： ①流量指标(Throughput，简称tpmC)按照TPC的定义，流量指标描述了系统在执行Payment、Order-status、Delivery、Stock-Level这四种交易的同时，每分钟可以处理多少个New-Order交易。所有交易的响应时间必须满足TPC-C测试规范的要求。 流量指标值越大越好！ ②性价比(Price/Performance，简称Price/tpmC)即测试系统价格（指在美国的报价）与流量指标的比值。 性价比越大越好！ 第三章（TPCC）如何衡量计算机系统的性能和价格在系统选型时，我们一定不要忘记我们是为特定用户环境中的特定应用选择系统。切忌为了“与国际接轨”而盲目套用“国际通用”的东西。在性能评价领域，越是通用的度量常常越是不准确的。据我所知，美国的一些大用户从不相信任何“国际通用”的度量，而是花相当精力，比如预算的5％，使用自己的应用来测试系统，决定选型。在使用任何一种性能和价格度量时，一定要弄明白该度量的定义，以及它是在什么系统配置和运行环境下得到的，如何解释它的意义等。下面我们由好到差讨论三种方式。 1在真实环境中运行 实际应用最理想的方式是搞一个试点，要求制造商或系统集成商配合将系统(含平台、软件和操作流程)在一个 实际用户点真正试运行一段时间。这样，用户不仅能看到实际性能，也能观察到系统是否稳定可靠、使用是否方便、服务是否周到、配置是否足够、全部价格是否合理。如果一个部门需要购买一批同类的系统，这种方式应列为首选，因为它不仅最精确、稳妥，也常常最有效率，用户还可先租一套系统作为试点。用这种方式得到的度量值常常具有很明确和实际的含义。 2使用用户定义的基准程序如果由于某种原因第一种方式不可行，用户可以定义一组含有自己实际应用环境特征的应用基准程序。 我举两个例子：近年来，由于R/3软件是应用层软件，SAP公司的基准程序获得了越来越多国外企业的认可；中国税务总局最近也开发了自己的基准程序，以帮助税务系统进行计算机选型。这种方式在中国尤其重要，因为中国的信息系统有其特殊性。 3使用通用基准程序如果第1种和第2种方式都不行，则使用如TPC-C之类的通用基准程序，这是不得已的一种近似方法。因 此，tpmC值只能用作参考。我们应当注意以下几点： ①实际应用是否与基准程序相符绝大多数基准程序都是在美国制订的，而中国的企事业单位与美国的运作方式常常不一样(恐怕也不应该或不可能一样)。在使用TPC－C时，我们应该清楚地知道：我的应用是否符合批发商模式?事务请求是否与表1近似?对响应时间的要求是否满足表1?如果都不是，则tpmC值的参考价值就不太大了。 ②TPC度量的解释TPC基准程序是用来测系统而不是测主机的，厂家肯定要充分优化他们的被测系统。此处的“系统”包括主机、外设(如硬盘或RAID)、主机端操作系统、数据库软件、客户端计算机及其 操作系统、数据库软件和网络连接等。在很多厂家的TPC测试系统中，主机的价格只是系统总价格的1/4或更小，而硬盘的价格有可能占到总价格的1/3以上，因为TPC－C要求被测系统必须保存180天的事务记录。如果同样的主机被用到用户的环境中，厂家报的tpmC值就意义不大，因为用户的实际系统与厂家原来用于TPC测试的系统大不一样。当同样的主机用在不同的系统中时，tpmC值可能有相当大的变化，现在很多用户还没有意识到这一点。 我举一个例子。假设用 户希望购买一批同类系统，每一系统至少需要1GB的内存和50GB的硬盘。厂家A、B、C 各报了三个价格相当的系统，tpmC值分别为3000、2800、2600。用户是否应该选厂家A的产品呢?答案是：不一定。厂家用于测试tpmC值的系统与实际提供给用户的系统配置大不一样。tpmC最低的厂家C提供给用户的系统反而有可能性能最好，不 论是以实际系统的tpmC值还是以用户的实际应用性能来衡量。 ③TPC测试的成本TPC－C和TPC－D都是很复杂的基准程序，做一个严格的测试是很消耗资源的，厂家当然不会说出他们花费了多少钱和时间。但据国外知情人士透露，一个厂家做第一个TPC－C测试需 要几十万到上百万美元的资金和半年左右的时间投入。因此，很多TPC的度量值都 是估计的。由于计算机系统换代频繁，如果用户一定要用通过审核的度量值，就必 须多等待半年时间，因此而不能用最先进的系统。中国的厂家通过审核的时间则更长。 综上所述，我们对中国 用户(尤其是大用户)在计算机系统的选型方面有如下建议： 最好建立一个真实的试点，因为实际应用环境是检验计算机系统的最好标准。 中国的行业应该建立符合自己实际应用的基准程序和测试标准。中国税务总局的做法值得提倡。国家有关部门应该建立独立的测试中心，制定跨行业、符合中国企事业运作模式的性能测试标准。 “国际通用”的度量可以作为参考值，而不应作为必要条件。尤其是一定要弄清这些流行度量有什么含义，是在什么样的系统环境中测得的，以及基准程序是否符合企业真实的业务流程和运作模式。 作为一家非盈利性机构，事务处理性能委员会（TPC）负责定义诸如TPC-C、TPC-H和TPC-W基准测试之类的事务处理与数据库性能基准测试，并依据这些基准测试项目发布客观性能数据。TPC基准测试采用极为严格的运行环境，并且必须在独立审计机构监督下进行。委员会成员包括大多数主要数据库产品厂商以及服务器硬件系统供应商。 相关企业参与TPC基准测试以期在规定运行环境中获得客观性能验证，并通过应用测试过程中所使用的技术开发出更加强健且更具伸缩性的软件产品及硬件设备。 TPC-C是一种旨在衡量联机事务处理（OLTP）系统性能与可伸缩性的行业标准基准测试项目。这种基准测试项目将对包括查询、更新及队列式小批量事务在内的广泛数据库功能进行测试。许多IT专业人员将TPC-C视为衡量“真实”OLTP系统性能的有效指示器。 TPC-C基准测试针对一种模拟订单录入与销售环境测量每分钟商业事务（tpmC）吞吐量。特别值得一提的是，它将专门测量系统在同时执行其它四种事务类型（如支付、订单状态更新、交付及证券级变更）时每分钟所生成的新增订单事务数量。独立审计机构将负责对基准测试结果进行公证，同时，TPC将出据一份全面彻底的测试报告。这份测试报告可以从TPC Web站点(http://www.tpc.org)上获得。 第四章 TPCC计算原则1建议不管是TPC-C还是SPECjbb2000，计算结果都只能作为一个横向比较的参考。在实际应用中，决定系统性能的因素除了硬件、系统软件外，与应用软件的设计也是有很大关系的，此外，基于系统可扩展性的考虑，更多时候也倾向于一次性的采购。从长远考虑，以政府信息化主管部门的角度考虑，建立一套评估机制是非常有用的，这其中包括：1、 通过对各单位业务系统运行情况的调查，进行历史数据的收集分析，按分类建立基准指标库。收集的信息包括：服务器的配置、并发用户数（每天业务量）、CPU负荷等；2、 由厂商定期提供基准值，更新基准指标库；有了基准指标库的信息参照，不仅可以用于评估项目建设方案中服务器选型，也可以对各部门进行系统架构设计的优化提供指导。如以下是一些指导原则：1、 数据库服务器选型：采购两台相同配置的小型机，进行虚拟分区和并行处理，以提高系统资源的利用率；日后扩容时采取垂直扩展的方式进行升级；2、 应用服务器：采用负载均衡的方式提高并发处理能力，一般可配置2台以上，每台的硬件配置完全可以不同，应首先考虑使用旧的数据库服务器（利旧），如需采购新的服务器，应采用水平扩展的方式逐步升级；3、 WEB服务器，可以考虑采用刀片服务器，提高扩展性和可管理性。 2参考：某项目计算实例参考1为了方便计算数据库服务器的造型，我们约定：“ 系统同时在线用户数为1500人（U1）；“ 平均每个用户每分钟发出2次业务请求（N1）；“ 系统发出的业务请求中，更新、查询、统计各占1/3；“ 平均每次更新业务产生3个事务（T1）；“ 平均每次查询业务产生8个事务（T2）；“ 平均每次统计业务产生13个事务（T3）；“ 一天内忙时的处理量为平均值的5倍；“ 经验系数为1.6；(实际工程经验)“ 考虑服务器保留30％的冗余；服务器需要的处理能力为：TPC-C=U1N1（T1+T2+T3）/33经验系数/冗余系数则应用服务器的处理性能估算为：TPC-C= 15002（3+8+13）/351.6/0.7= 274,285 tpmC 数据库服务器关系到整个系统的稳定运行，考虑到高可靠性和高可用性，并注重设备的可扩展性和性价比，系统将配置两台TPC-C值不小于28万的高性能数据库服务器。 参考2以单台服务器性能进行计算，即确保单台服务器工作的时候可以满足系统正常运行的需要； 假设每天有1万人次来窗口办理业务，每人次办理一项业务。即以每日1万笔前台交易为例进行\综合系数**的推导： \1. 假设每月前台交易数（未来5年内的设计指标）为220,000 （有些业务在月初、月末的处理量比较高，按月统计可以平衡此项差异）;\2. 每日前台交易数=220000/22=10,000 ，即每日 1万笔；\3. 忙时处理能力：每日交易的80%在4个小时内完成，即1000080%/4=2000（笔/小时）\4. 峰值处理能力：20002=4000（笔/小时），即峰值处理能力为每小时4000笔，或 67笔/分，假设业务人员同时在线为100人，即每人每分钟处理0.7笔）\5. 假设每笔交易对应数据库事务数=20（这个是不是有点夸张了，有这么复杂的交易，TPC的标准交易都不算简单了吧，这只能说明数据库设计的有问题，按这样，支付宝咋弄），基准TPC指标值对应的比例=8，cpu保留30%的处理能力冗余，计算值与公布值（最优值）的偏差经验值为4 （这几个参数估算的依据不足，更多的是经验值） 则 tpmC值为：tpmC= 672084/(1-30%)= 61257[颠峰处理能力时（笔/分）每笔交易对应数据库事务数基准TPC指标值对应的比例计算值与公布值（最优值）的偏差经验值/（1- cpu保留30%的处理能力冗余）倒算出 综合系数 = 61257/10000=6.1即数据库服务器tpmC= ***每日前台交易数 \ 6.1**** （实际计算值应不高于该值）应用服务器的 tpmC = 数据库服务器 tpmC *50% （一般） 应用服务器的 tpmC = tpmC *70% 某单位要建立一个系统，设计容量是 2008 年要达到 1500 万客户的规模，在花费巨资（将近 100 M $）建设起此系统后，运行只有几个月，客户量也只达到几十万，厂商就说此系统容量不够，要花数 M $ 要扩容，并声称不是拍脑袋是，按 TPCC 计算的结果 … 以前只大致了解 TPCC 指标的一些情况，平常也就看看 By Performace 和 By Price/Performace 的 Top 10 列表，了解一下各厂商的实力和产品的情况，这次对 TPCC 进行了一下深入了解，发现了一些容易被厂商搞猫腻的地方。 TPCC 简单的来说，就是事务处理性能委员会 TPC 组织针对联机数据库应用系统（OLTP）制定的一个综合性能考评指标，它模拟了一个批发 商的货物管理环境，有标准的数据库结构（Schema），可以很容易的扩展，在此系统中，主要有五类交易： \1. 新订单（New-Order） \2. 支付（Payment ） 43%（最小比例） \3. 订单查询（Order-Status） 4%（最小比例） \4. 交付（Delivery） 4%（最小比例） \5. 库存查询（Stock-Level） 4%（最小比例） 因为 TPCC 指标值主要是衡量“新订单”交易的数量，所以厂商都会趋向于增加新订单交易的数量，这样并不满足实际的应用场景，所以 TPCC 的规范中指定了后四种交易数量的最小比例，这样意味着 “新订单” 的数量最多只能占到45%（这个 45% 不知出于什么原因，在很多 TPCC 的指标介绍中并没有提及）。 这就说明了：如果一台机器有 1000 tpm (Transactions per Minute)，那么它实际上处理的交易（请求）为 1000/45% = 2222 ，而实际上大多数厂商在面对用户估算应用系统需要多少个 TPCC 的服务器的时候，是计算所有交易量的，这样带来的评估结果是用户至少买比实际需要大一倍性能容量的机器，获得利益的谁呢？当然是服务器厂商，越高 TPCC 值的机器，价格越高，性价比也一般要偏小。 此外，服务器厂商还容易在以下方面愚弄客户： １、交易复杂度（用户的一个交易约等于多少个 TPCC 的标准交易），某国际著名的服务器厂商建议此值取 10 - 20 ，这个值也忒大了点， 在 TPCC 的场景，交易也不是很简单的，和实际的应用交易差别并不是很大。 2、按一些峰值指标来计算性能，实际上，这些峰值只是在极少数情况下出现，所以在计算时，应乘以 80% 的系数。 ​ 3、预留容量，很多厂商建议预留容量，甚至达到一半，这有点过份，在实际容量要求增高的时候，可以通过服务器的横向扩展来纵向扩展来满足应用要求，何必为未来五年的要求，要在眼下一下子要做全部的硬件投资呢？ 何况硬件的价格在不断下降呢。 具体情况具体分析，还得按中国国情来实测，国际标准有时不适合我们。而且这个标准好多年了，现在很少在纵向扩展上花心思了，都注重横向扩展，按成本购置小机，尽量优化小机的数据库性能，得出TPCM值，然后对数据库进行横向扩展（虽然横向扩展有诸多问题，但是还是要扩展），利用集群提高整体的TPS]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>tpc</tag>
        <tag>tpmc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7.5下对mysql5.6基准测试]]></title>
    <url>%2Fmysql%2F2-mysql-benchmark-test%2F</url>
    <content type="text"><![CDATA[一、测试环境准备 操作系统：centos7.5 处理器：Intel 8核 Current Speed: 2000 MHz， Max Speed: 2000 MHz，三级缓存 内存：16G 磁盘：100G SSD 数据库：mysql 5.6.28 sysbench：1.0.18 二、测试工具1、sysbench安装文档： https://github.com/akopytov/sysbench#installing-from-binary-packages 123# centos7.5使用官方镜像安装curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bashsudo yum -y install sysbench 帮助： 12345678910111213141516171819[root@10-10-135-172 benchmark]# sysbench --help[root@10-10-135-172 benchmark]# sysbench fileio helpsysbench 1.0.18 (using bundled LuaJIT 2.1.0-beta2)fileio options: --file-num=N number of files to create [128] --file-block-size=N block size to use in all IO operations [16384] --file-total-size=SIZE total size of files to create [2G] --file-test-mode=STRING test mode &#123;seqwr, seqrewr, seqrd, rndrd, rndwr, rndrw&#125; --file-io-mode=STRING file operations mode &#123;sync,async,mmap&#125; [sync] --file-async-backlog=N number of asynchronous operatons to queue per thread [128] --file-extra-flags=[LIST,...] list of additional flags to use to open files &#123;sync,dsync,direct&#125; [] --file-fsync-freq=N do fsync() after this number of requests (0 - don't use fsync()) [100] --file-fsync-all[=on|off] do fsync() after each write operation [off] --file-fsync-end[=on|off] do fsync() at the end of test [on] --file-fsync-mode=STRING which method to use for synchronization &#123;fsync, fdatasync&#125; [fsync] --file-merged-requests=N merge at most this number of IO requests if possible (0 - don't merge) [0] --file-rw-ratio=N reads/writes ratio for combined test [1.5] 2、tpcc-mysqlgithub： https://github.com/Percona-Lab/tpcc-mysql 1234567mkdir -p /var/lib/mysql/benchmarkcd /var/lib/mysql/benchmarkwget https://github.com/Percona-Lab/tpcc-mysql/archive/master.zip -O tpcc-mysql.zipunzip -d ./ tpcc-mysql.zipcd /var/lib/mysql/benchmark/tpcc-mysql-master/srcyum install mysql-develmake 编译完成后，生成 tpcc_load tpcc_start： 三、测试目标 处理器性能 内存性能 磁盘顺序读写，随机读写性能 mysql 吞吐量，响应时间 四、测试步骤1、准备获取系统性能和状态的脚本vi /root/benchmark/1-collect-system-status.sh 123456789101112131415161718192021222324#!/bin/shmkdir -p status_dataINTERVAL=5PREFIX=./status_data/$INTERVAL-sec-statusRUNFILE=/root/benchmark/runningmysql -uroot -p123456 -e 'SHOW GLOBAL VARIABLES' &gt;&gt; mysql-variableswhile test -e $RUNFILE; do tfile=$(date +%F_%I) sleep=$(date +%s.%N | awk "&#123;print $INTERVAL - (\$1 % $INTERVAL)&#125;") sleep $sleep ts="$(date +"TS %s.%N %F %T")" loadavg="$(uptime)" echo "$ts $loadavg" &gt;&gt; $PREFIX-$&#123;tfile&#125;-status mysql -uroot -p123456 -e 'SHOW GLOBAL STATUS' &gt;&gt; $PREFIX-$&#123;tfile&#125;-status &amp; echo "$ts $loadavg" &gt;&gt; $PREFIX-$&#123;tfile&#125;-innodbstatus mysql -uroot -p123456 -e 'SHOW GLOBAL STATUS\G' &gt;&gt; $PREFIX-$&#123;tfile&#125;-innodbstatus &amp; echo "$ts $loadavg" &gt;&gt; $PREFIX-$&#123;tfile&#125;-processlist mysql -uroot -p123456 -e 'SHOW FULL PROCESSLIST\G' &gt;&gt; $PREFIX-$&#123;tfile&#125;-processlist &amp; echo $tsdoneecho Exiting because $RUNFILE does not exist. 分析收集到的数据 vi /root/benchmark/2-system-status-analyzer.sh 12345678910111213141516171819#!/bin/bashawk ' BEGIN &#123; printf "#ts date time load QPS"; fmt = " %.2f"; &#125; /^TS/ &#123; ts = substr($2, 1, index($2, ".") - 1); load = NF -2; diff = ts - prev_ts; prev_ts = ts; printf "\n%s %s %s %s", ts, $3, $4, substr($load, 1, length($load) - 1); &#125; /Queries/ &#123; printf fmt, ($2-Queries)/diff; Queries=$2 &#125;' "$@" 用法示例 1sh 2-system-status-analyzer.sh status_data/5-sec-status-2019-11-19_10-status 2、使用sysbench服务器性能测试1)、cpu测试 计算20000个素数。8核cpu，分别测试 1，4，8，12 线程时的 CPU表现 1sysbench --test=cpu --threads=1 --cpu-max-prime=20000 run 1sysbench --test=cpu --threads=4 --cpu-max-prime=20000 run 1sysbench --test=cpu --threads=8 --cpu-max-prime=20000 run 结论：随着线程数增加，计算能力线性增加； 1sysbench --test=cpu --threads=12 --cpu-max-prime=20000 run 结论：当线程数超过cpu核数之后，随着线程数增加，计算能力不再增加，甚至略微下降，这是因为线程上下文切换以后的时间损耗； 2)、文件I/O测试① 准备30G的数据 1sysbench --test=fileio --file-total-size=30G prepare ② 运行阶段 seqwr：顺序写入 1sysbench --test=fileio --file-test-mode=seqwr --file-total-size=30G --time=60 run 结论：可以看到，顺序写的过程中 CPU占用很低，但是 buff、cache、block out、in、cs(上下文切换)都很高，这是因为 linux 写磁盘用了 页缓存 和 DMA。 seqrewr：顺序重写 1sysbench --test=fileio --file-test-mode=seqrewr --file-total-size=30G --time=60 run seqrd：顺序读取 1sysbench --test=fileio --file-test-mode=seqrd --file-total-size=30G --time=60 run rndrd：随机读取 1sysbench --test=fileio --file-test-mode=rndrd --file-total-size=30G --time=60 run 结论： SSD是没有寻道时间的，但是 随机读（46.95MiB/s） 仍然比顺序读（184.46MiB/s）要慢很多 rndwr：随机写入 1sysbench --test=fileio --file-test-mode=rndwr --file-total-size=30G --time=60 run 随机写（26.25MiB/s）更慢了，顺序写的速度有138.38MiB/s rndrw：混合随机读/写 1sysbench --test=fileio --file-test-mode=rndrw --file-total-size=30G --time=60 run 混合随机读写的速度更是龟速，然而我们平时业务场景中使用最多的就是这种情况，所以一定要注意，能顺序写的，一定不随机写 ③ 清除数据 sysbench –test=fileio –file-total-size=30G cleanup 3)、内存测试测试8K顺序分配 1sysbench --test=memory --memory-access-mode=seq --memory-block-size=8K --memory-total-size=100G --threads=12 --events=10000 run 结论：可以看到 CPU 占用极高，内存写入速度为 10138.32 MiB / sec 测试8K随机分配 1sysbench --test=memory --memory-access-mode=rnd --memory-block-size=8K --memory-total-size=100G --threads=12 --events=10000 run 结论：cpu占用极高，随机分配速度为 1381.41 MiB / sec，只有顺序分配的 1/7 左右，内存都有顺序写都有差距。 3、sysbench对mysql进行OLTP测试1sysbench /usr/share/sysbench/oltp_read_write.lua help 1) 、新建数据库 sbtest1mysqladmin create sbtest -uroot -p 2) 、运行状态收集脚本1sh /root/benchmark/1-collect-system-status.sh 3)、 准备1百万条数据 可以在/usr/share/sysbench/中找到需要使用的lua脚本： 12345678# 开始插入数据sysbench /usr/share/sysbench/oltp_read_write.lua --threads=64 --time=600 --histogram=on --mysql-host=10.10.135.172 --mysql-port=3306 --mysql-db=sbtest --mysql-user=root --mysql-password=123456 --table-size=10000000 prepare# 关键参数说明：--threads=64 测试时使用的线程数--time=600 测试时间--histogram=on 在报告中是否输出关于延迟的直方图，建议开启--table_size=10000000 数据表的大小 4)、 运行压测程序1sysbench /usr/share/sysbench/oltp_read_write.lua --threads=64 --time=600 --histogram=on --mysql-host=10.10.135.172 --mysql-port=3306 --mysql-db=sbtest --mysql-user=root --table-size=10000000 run 总事务数：820388 每秒事务数：1367.22 per sec 总查询数：16407760 每秒查询数：27344.47 per sec 请求分布直方图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306[root@10-10-135-172 benchmark]# sysbench /usr/share/sysbench/oltp_read_write.lua --threads=64 --time=600 --histogram=on --mysql-host=10.10.135.172 --mysql-port=3306 --mysql-db=sbtest --mysql-user=root --table-size=10000000 runsysbench 1.0.18 (using bundled LuaJIT 2.1.0-beta2)Running the test with following options:Number of threads: 64Initializing random number generator from current timeInitializing worker threads...Threads started! Latency histogram (values are in milliseconds) value ------------- distribution ------------- count 4.329 | 64 4.407 |***** 1030 4.487 |**************** 3445 4.569 |************************** 5587 4.652 |**************************** 5867 4.737 |************************* 5250 4.823 |******************** 4190 4.910 |**************** 3476 4.999 |************** 2922 5.090 |************ 2605 5.183 |*********** 2329 5.277 |********* 1991 5.373 |********* 1813 5.470 |******** 1600 5.570 |******* 1476 5.671 |****** 1279 5.774 |****** 1178 5.879 |***** 1043 5.986 |***** 974 6.095 |**** 924 6.205 |**** 843 6.318 |**** 768 6.433 |**** 786 6.550 |**** 839 6.669 |**** 943 6.790 |***** 1012 6.913 |***** 1126 7.039 |****** 1171 7.167 |****** 1310 7.297 |****** 1305 7.430 |****** 1291 7.565 |***** 1167 7.702 |***** 1072 7.842 |***** 1066 7.985 |***** 1051 8.130 |***** 1102 8.277 |***** 1081 8.428 |***** 1120 8.581 |***** 1129 8.737 |****** 1244 8.895 |******* 1444 9.057 |******** 1692 9.222 |********* 1984 9.389 |********** 2161 9.560 |********** 2057 9.734 |********* 1944 9.910 |********* 1930 10.090 |******** 1773 10.274 |******** 1753 10.460 |******** 1698 10.651 |******** 1605 10.844 |******* 1553 11.041 |******** 1601 11.242 |******** 1636 11.446 |******** 1649 11.654 |******** 1667 11.866 |******** 1673 12.081 |******** 1733 12.301 |********* 1821 12.524 |********* 1838 12.752 |********* 1844 12.984 |********** 2031 13.219 |********* 1978 13.460 |********** 2206 13.704 |************ 2449 13.953 |************* 2681 14.207 |************* 2675 14.465 |************* 2744 14.728 |************* 2694 14.995 |************* 2667 15.268 |************ 2652 15.545 |************ 2650 15.828 |************* 2671 16.115 |************ 2644 16.408 |************* 2757 16.706 |************* 2760 17.010 |************* 2845 17.319 |************** 2933 17.633 |*************** 3217 17.954 |*************** 3105 18.280 |**************** 3294 18.612 |***************** 3587 18.950 |***************** 3704 19.295 |****************** 3845 19.645 |****************** 3763 20.002 |****************** 3743 20.366 |****************** 3852 20.736 |****************** 3763 21.112 |****************** 3891 21.496 |******************* 4076 21.886 |******************* 4108 22.284 |******************** 4191 22.689 |********************* 4400 23.101 |********************** 4702 23.521 |*********************** 4863 23.948 |************************ 5072 24.384 |************************ 5043 24.827 |************************* 5224 25.278 |************************ 5196 25.737 |************************* 5285 26.205 |************************** 5442 26.681 |************************** 5452 27.165 |*************************** 5727 27.659 |**************************** 5867 28.162 |***************************** 6097 28.673 |****************************** 6367 29.194 |****************************** 6414 29.725 |****************************** 6441 30.265 |******************************* 6512 30.815 |******************************* 6479 31.375 |******************************** 6828 31.945 |******************************** 6876 32.525 |********************************** 7128 33.116 |********************************** 7241 33.718 |*********************************** 7372 34.330 |************************************ 7548 34.954 |************************************ 7653 35.589 |************************************* 7788 36.236 |************************************* 7808 36.894 |************************************* 7910 37.565 |************************************** 8106 38.247 |*************************************** 8177 38.942 |*************************************** 8311 39.650 |**************************************** 8451 40.370 |************************************** 8058 41.104 |*************************************** 8309 41.851 |*************************************** 8260 42.611 |**************************************** 8481 43.385 |**************************************** 8493 44.173 |**************************************** 8478 44.976 |**************************************** 8394 45.793 |*************************************** 8369 46.625 |**************************************** 8398 47.472 |**************************************** 8459 48.335 |**************************************** 8455 49.213 |**************************************** 8446 50.107 |*************************************** 8245 51.018 |*************************************** 8215 51.945 |*************************************** 8215 52.889 |************************************** 8087 53.850 |************************************** 8119 54.828 |************************************** 8043 55.824 |************************************* 7959 56.839 |************************************** 8043 57.871 |************************************ 7746 58.923 |************************************ 7650 59.993 |*********************************** 7522 61.083 |*********************************** 7535 62.193 |********************************** 7250 63.323 |********************************** 7254 64.474 |********************************** 7118 65.645 |********************************* 6983 66.838 |******************************** 6868 68.053 |******************************** 6890 69.289 |******************************* 6688 70.548 |******************************* 6530 71.830 |****************************** 6437 73.135 |***************************** 6127 74.464 |****************************** 6278 75.817 |**************************** 5952 77.194 |*************************** 5767 78.597 |************************** 5581 80.025 |************************** 5522 81.479 |************************* 5374 82.959 |************************* 5217 84.467 |*********************** 4957 86.002 |*********************** 4947 87.564 |********************** 4767 89.155 |********************** 4673 90.775 |********************* 4499 92.424 |********************* 4358 94.104 |******************** 4230 95.814 |******************* 4021 97.555 |****************** 3871 99.327 |****************** 3750 101.132 |***************** 3665 102.969 |**************** 3465 104.840 |**************** 3322 106.745 |*************** 3286 108.685 |************** 3019 110.659 |************** 2963 112.670 |************* 2707 114.717 |************* 2712 116.802 |************ 2528 118.924 |*********** 2379 121.085 |*********** 2269 123.285 |********** 2138 125.525 |********** 2105 127.805 |********* 1861 130.128 |********* 1820 132.492 |******** 1735 134.899 |******* 1575 137.350 |******* 1502 139.846 |******* 1412 142.387 |****** 1302 144.974 |****** 1231 147.608 |***** 1056 150.290 |***** 1015 153.021 |**** 927 155.801 |**** 879 158.632 |**** 815 161.514 |*** 685 164.449 |*** 696 167.437 |*** 613 170.479 |*** 591 173.577 |** 499 176.731 |** 457 179.942 |** 447 183.211 |** 360 186.540 |** 352 189.929 |* 298 193.380 |* 304 196.894 |* 224 200.472 |* 208 204.114 |* 204 207.823 |* 173 211.599 |* 155 215.443 |* 113 219.358 |* 129 223.344 | 106 227.402 | 82 231.534 | 67 235.740 | 66 240.024 | 43 244.385 | 53 248.825 | 30 253.346 | 31 257.950 | 26 262.636 | 19 267.408 | 25 272.267 | 12 277.214 | 11 282.251 | 11 287.379 | 11 292.601 | 5 297.917 | 11 303.330 | 5 308.842 | 6 314.453 | 14 320.167 | 10 325.984 | 16 331.907 | 11 337.938 | 2 344.078 | 8 350.330 | 10 356.695 | 11 363.176 | 10 369.775 | 15 376.494 | 12 383.334 | 14 390.299 | 10 397.391 | 5 404.611 | 11 411.963 | 2 419.448 | 11 427.069 | 7 434.829 | 6 442.730 | 6 450.774 | 5 458.964 | 2 475.794 | 1 484.439 | 2 493.242 | 1 559.501 | 1 SQL statistics: queries performed: read: 11485432 write: 3281552 other: 1640776 total: 16407760 transactions: 820388 (1367.22 per sec.) queries: 16407760 (27344.47 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 600.0372s total number of events: 820388Latency (ms): min: 4.30 avg: 46.80 max: 556.16 95th percentile: 112.67 sum: 38398062.70Threads fairness: events (avg/stddev): 12818.5625/107.50 execution time (avg/stddev): 599.9697/0.01[root@10-10-135-172 benchmark]# 5)、清理数据1sysbench /usr/share/sysbench/oltp_read_write.lua --threads=64 --time=600 --histogram=on --mysql-host=10.10.135.172 --mysql-port=3306 --mysql-db=sbtest --mysql-user=root --table-size=10000000 cleanup 6)、分析收集到的状态1234567891011121314151617181920212223242526272829303132$ sh 2-system-status-analyzer.sh status_data/5-sec-status-2019-11-19_06-status &gt; status.txt$ vi status.txtunix时间戳 时间 系统负载 QPS(数据库)1574160560 18:49:20 18:49:20 51.53 28179.801574160565 18:49:25 18:49:25 51.80 26713.201574160570 18:49:30 18:49:30 50.86 27784.601574160575 18:49:35 18:49:35 51.19 26912.401574160580 18:49:40 18:49:40 52.30 27048.201574160585 18:49:45 18:49:45 52.99 26843.001574160590 18:49:50 18:49:50 53.87 27478.801574160595 18:49:55 18:49:55 54.61 28724.601574160600 18:50:00 18:50:00 55.68 27960.001574160605 18:50:05 18:50:05 55.46 25874.401574160610 18:50:10 18:50:10 56.07 25936.401574160615 18:50:15 18:50:15 52.54 25914.201574160620 18:50:20 18:50:20 52.66 27237.601574160625 18:50:25 18:50:25 52.92 27507.201574160630 18:50:30 18:50:30 53.89 26005.401574160635 18:50:35 18:50:35 55.02 26745.001574160640 18:50:40 18:50:40 54.86 25643.801574160645 18:50:45 18:50:45 55.75 24751.201574160650 18:50:50 18:50:50 53.69 27608.801574160655 18:50:55 18:50:55 52.51 26590.201574160660 18:51:00 18:51:00 51.83 27411.601574160665 18:51:05 18:51:05 49.68 28504.201574160670 18:51:10 18:51:10 50.27 20451.001574160675 18:51:15 18:51:15 46.24 1.401574160680 18:51:20 18:51:20 42.62 2.201574160685 18:51:25 18:51:25 39.21 1.401574160690 18:51:30 18:51:30 36.07 1.801574160695 18:51:35 18:51:35 33.18 2.401574160700 18:51:40 18:51:40 30.52 1.00 123456789101112131415161718192021222324252627282930$ cat status_data/5-sec-status-2019-11-19_06-status | grep 'TS ' &gt; load_status.txt$ vi load_status.txtTS 1574160295.047916883 18:44:55 18:44:55 up 1 day, 2:22, 5 users, load average: 52.13, 24.94, 9.96TS 1574160300.059352736 18:45:00 18:45:00 up 1 day, 2:22, 5 users, load average: 53.08, 25.59, 10.25TS 1574160305.042101201 18:45:05 18:45:05 up 1 day, 2:22, 5 users, load average: 50.91, 25.60, 10.33TS 1574160310.052530257 18:45:10 18:45:10 up 1 day, 2:22, 5 users, load average: 51.72, 26.19, 10.60TS 1574160315.046490819 18:45:15 18:45:15 up 1 day, 2:22, 5 users, load average: 52.78, 26.83, 10.89TS 1574160320.008893450 18:45:20 18:45:20 up 1 day, 2:22, 5 users, load average: 53.68, 27.45, 11.18TS 1574160325.203247981 18:45:25 18:45:25 up 1 day, 2:22, 5 users, load average: 54.67, 28.09, 11.47TS 1574160330.118228229 18:45:30 18:45:30 up 1 day, 2:22, 5 users, load average: 55.57, 28.72, 11.77TS 1574160335.016869072 18:45:35 18:45:35 up 1 day, 2:22, 5 users, load average: 55.61, 29.17, 12.00TS 1574160340.042057908 18:45:40 18:45:40 up 1 day, 2:22, 5 users, load average: 54.84, 29.45, 12.19TS 1574160345.056853963 18:45:45 18:45:45 up 1 day, 2:23, 5 users, load average: 55.65, 30.04, 12.47TS 1574160350.059495691 18:45:50 18:45:50 up 1 day, 2:23, 5 users, load average: 55.84, 30.50, 12.72TS 1574160355.238934528 18:45:55 18:45:55 up 1 day, 2:23, 5 users, load average: 56.65, 31.09, 13.00TS 1574160360.115909774 18:46:00 18:46:00 up 1 day, 2:23, 5 users, load average: 56.76, 31.54, 13.24TS 1574160365.067470620 18:46:05 18:46:05 up 1 day, 2:23, 5 users, load average: 57.34, 32.08, 13.52TS 1574160370.111400548 18:46:10 18:46:10 up 1 day, 2:23, 5 users, load average: 55.39, 32.09, 13.62TS 1574160375.074730081 18:46:15 18:46:15 up 1 day, 2:23, 5 users, load average: 53.76, 32.14, 13.74TS 1574160380.210612654 18:46:20 18:46:20 up 1 day, 2:23, 5 users, load average: 54.34, 32.62, 13.99TS 1574160385.102457806 18:46:25 18:46:25 up 1 day, 2:23, 5 users, load average: 50.87, 32.26, 13.97TS 1574160390.117591568 18:46:30 18:46:30 up 1 day, 2:23, 5 users, load average: 50.48, 32.49, 14.14TS 1574160395.076076309 18:46:35 18:46:35 up 1 day, 2:23, 5 users, load average: 51.40, 32.98, 14.40TS 1574160400.191211227 18:46:40 18:46:40 up 1 day, 2:23, 5 users, load average: 52.09, 33.43, 14.65TS 1574160405.153420741 18:46:45 18:46:45 up 1 day, 2:24, 5 users, load average: 49.20, 33.14, 14.65TS 1574160410.013108072 18:46:50 18:46:50 up 1 day, 2:24, 5 users, load average: 49.58, 33.49, 14.87TS 1574160415.084485731 18:46:55 18:46:55 up 1 day, 2:24, 5 users, load average: 48.82, 33.59, 15.00TS 1574160420.011562959 18:47:00 18:47:00 up 1 day, 2:24, 5 users, load average: 47.55, 33.58, 15.10TS 1574160425.018923271 18:47:05 18:47:05 up 1 day, 2:24, 5 users, load average: 48.95, 34.11, 15.37 4、tpcc-mysql对mysql进行测试 Build binaries cd src ; make ( you should have mysql_config available in $PATH) Load data create database mysqladmin create tpcc1000 create tables mysql tpcc1000 &lt; create_table.sql create indexes and FK ( this step can be done after loading data) mysql tpcc1000 &lt; add_fkey_idx.sql populate data simple step tpcc_load -h127.0.0.1 -d tpcc1000 -u root -p &quot;&quot; -w 1000 |hostname:port| |dbname| |user| |password| |WAREHOUSES| ref. tpcc_load –help for all options load data in parallel check load.sh script Start benchmark ./tpcc_start -h127.0.0.1 -P3306 -dtpcc1000 -uroot -w1000 -c32 -r10 -l10800 |hostname| |port| |dbname| |user| |WAREHOUSES| |CONNECTIONS| |WARMUP TIME| |BENCHMARK TIME| ref. tpcc_start –help for all options 1)、 创建数据库和表结构12345cd tpcc-mysql-mastermysqladmin create tpcc1000 -uroot -p123456mysql -uroot -p123456 tpcc1000 &lt; create_table.sqlmysql -uroot -p123456 tpcc1000 &lt; add_fkey_idx.sql 2)、加载数据123./tpcc_load -h10.10.135.172 -P3306 -d tpcc1000 -u root -p 123456 -w 10# 1 代表1个仓库大小，10代表 10个仓库大小# sh load.sh tpcc1000 1000 加载完后查看数据量 123456789select count(*) from tpcc1000.customer;select count(*) from tpcc1000.district;select count(*) from tpcc1000.history;select count(*) from tpcc1000.item;select count(*) from tpcc1000.new_orders;select count(*) from tpcc1000.order_line;select count(*) from tpcc1000.orders;select count(*) from tpcc1000.stock;select count(*) from tpcc1000.warehouse; 3)、运行状态收集脚本1sh /root/benchmark/1-collect-system-status.sh 4)、执行测试12345678910# -w 指定仓库数量# -c 指定并发连接数# -r 指定开始测试前进行warmup的时间，进行预热后，测试效果更好# -l 指定测试持续时间# -i 指定生成报告间隔时长# -f 指定生成的报告文件名# 真实测试场景中，建议预热时间不小于5分钟，持续压测时长不小于30分钟，否则测试数据可能不具参考意义# 耗时：1800, 即 30分钟./tpcc_start -h10.10.135.172 -P3306 -dtpcc1000 -uroot -p123456 -w10 -c32 -r300 -l1800 &gt; tpcc_output.txt top vmstat 1 执行结果 5)、分析收集到的状态数据1234567891011121314151617181920212223242526272829303132$ sh 2-system-status-analyzer.sh status_data/5-sec-status-2019-11-19_06-status &gt; status.txt$ vi status.txtunix时间戳 时间 系统负载 QPS(数据库)1574177360 2019-11-19 23:29:20 22.00 37156.601574177365 2019-11-19 23:29:25 20.24 36821.801574177370 2019-11-19 23:29:30 21.26 36879.801574177375 2019-11-19 23:29:35 19.56 37217.601574177380 2019-11-19 23:29:40 20.63 37646.801574177385 2019-11-19 23:29:45 21.62 35293.201574177390 2019-11-19 23:29:50 19.89 36457.801574177395 2019-11-19 23:29:55 18.30 37464.001574177400 2019-11-19 23:30:00 19.72 36722.601574177405 2019-11-19 23:30:05 20.78 37913.601574177410 2019-11-19 23:30:10 19.12 36294.001574177415 2019-11-19 23:30:15 17.58 35279.201574177420 2019-11-19 23:30:20 18.98 39051.201574177425 2019-11-19 23:30:25 20.10 37700.201574177430 2019-11-19 23:30:30 18.49 37632.201574177435 2019-11-19 23:30:35 17.01 38068.001574177440 2019-11-19 23:30:40 15.65 38218.401574177445 2019-11-19 23:30:45 14.48 34143.401574177450 2019-11-19 23:30:50 13.40 7794.601574177455 2019-11-19 23:30:55 12.40 7744.801574177460 2019-11-19 23:31:00 11.41 613.401574177465 2019-11-19 23:31:05 10.50 2.201574177470 2019-11-19 23:31:10 9.66 1.401574177475 2019-11-19 23:31:15 8.88 1.801574177480 2019-11-19 23:31:20 8.17 1.801574177485 2019-11-19 23:31:25 7.52 1.801574177490 2019-11-19 23:31:30 6.92 2.001574177495 2019-11-19 23:31:35 6.36 2.001574177500 2019-11-19 23:31:40 5.85 1.60 vi tpcc_output.txt 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192****************************************** ###easy### TPC-C Load Generator ******************************************option h with value '10.10.135.172'option P with value '3306'option d with value 'tpcc1000'option u with value 'root'option p with value '123456'option w with value '10'option c with value '32'option r with value '300'option l with value '1800'&lt;Parameters&gt; [server]: 10.10.135.172 -- 主机 [port]: 3306 -- 端口 [DBname]: tpcc1000 -- 压测的数据库 [user]: root -- 账号 [pass]: 123456 -- 密码 [warehouse]: 10 -- 仓库数 [connection]: 32 -- 并发线程数 [rampup]: 300 (sec.) -- 数据预热时长 [measure]: 1800 (sec.) -- 压测时长RAMP-UP TIME.(300 sec.) --预热结束MEASURING START. --开始压测-- 每10秒输出一次压测数据-- 以逗号分隔，共9列-- 第1列，第N次10秒-- 第2列，New-Order在本轮取样时间内成功执行事务的次数(即吞吐量)-- 第3列，New-Order在本轮取样时间内，95%事务 在响应时间内（ms）完成-- 第4列，New-Order在本轮取样时间内，99%事务 在响应时间内完成-- 第5列，New-Order在本轮取样时间内，本轮测试最大响应时间（ms）-- 第6列，Payment 有效事务数（吞吐量） | 最大响应时间（ms）-- 第7列，Order-Status 有效事务数（吞吐量） | 最大响应时间（ms）-- 第8列，Delivery 有效事务数（吞吐量） | 最大响应时间（ms）-- 第9列，Stock-Level 有效事务数（吞吐量） | 最大响应时间（ms） 10, trx: 6322, 95%: 58.880, 99%: 80.384, max_rt: 465.298, 6319|311.356, 632|147.998, 633|518.363, 632|840.708 20, trx: 6402, 95%: 57.161, 99%: 74.165, max_rt: 108.881, 6399|102.155, 640|54.569, 640|187.683, 641|125.100 30, trx: 6215, 95%: 58.616, 99%: 83.674, max_rt: 240.631, 6224|149.658, 623|135.217, 622|332.418, 621|168.763 40, trx: 6365, 95%: 57.109, 99%: 76.190, max_rt: 136.020, 6362|140.110, 637|35.815, 637|142.510, 636|154.135 50, trx: 6426, 95%: 56.973, 99%: 76.281, max_rt: 131.508, 6427|171.652, 641|45.733, 640|145.872, 643|156.595 60, trx: 6198, 95%: 58.476, 99%: 80.384, max_rt: 229.300, 6197|238.621, 620|37.124, 621|269.572, 619|261.585 70, trx: 6418, 95%: 57.366, 99%: 76.648, max_rt: 116.301, 6421|115.735, 643|42.165, 643|172.855, 643|151.629 80, trx: 6406, 95%: 57.815, 99%: 76.877, max_rt: 115.657, 6396|99.119, 639|39.815, 640|152.090, 640|159.341 90, trx: 5841, 95%: 62.047, 99%: 87.229, max_rt: 290.416, 5836|291.379, 584|42.550, 585|301.866, 584|142.273 100, trx: 6168, 95%: 56.362, 99%: 76.717, max_rt: 125.903, 6183|115.971, 618|47.143, 619|158.824, 618|162.145 110, trx: 6249, 95%: 57.573, 99%: 78.200, max_rt: 138.199, 6232|126.211, 624|40.234, 623|155.532, 624|152.938 120, trx: 6107, 95%: 58.511, 99%: 78.694, max_rt: 121.301, 6125|146.152, 611|37.128, 608|156.622, 609|126.521 130, trx: 6076, 95%: 58.424, 99%: 84.227, max_rt: 346.854, 6074|266.228, 608|48.112, 608|361.008, 610|140.032 140, trx: 6365, 95%: 55.992, 99%: 75.599, max_rt: 115.579, 6367|110.699, 637|47.943, 637|156.809, 634|126.359....1760, trx: 6386, 95%: 56.599, 99%: 74.588, max_rt: 115.095, 6386|103.424, 638|44.586, 640|143.890, 637|139.0051770, trx: 6148, 95%: 59.039, 99%: 76.510, max_rt: 124.202, 6149|99.945, 615|37.548, 612|152.861, 615|157.2651780, trx: 5831, 95%: 59.786, 99%: 82.061, max_rt: 291.887, 5826|335.447, 583|47.130, 585|365.992, 583|156.4201790, trx: 6161, 95%: 57.676, 99%: 78.294, max_rt: 130.952, 6162|132.039, 616|46.349, 616|149.911, 615|173.0921800, trx: 6164, 95%: 56.295, 99%: 74.098, max_rt: 117.632, 6169|131.690, 616|39.105, 616|202.839, 618|126.449STOPPING THREADS................................ -- 结束压测# 成功(success,简写sc)次数，延迟(late,简写lt)次数，重试(retry,简写rt)次数，失败(failure,简写fl)次数&lt;Raw Results&gt; -- 第一次统计结果 [0] sc:35421 lt:1093057 rt:0 fl:0 avg_rt: 30.0 (5) -- New-Order，新订单业务 [1] sc:457714 lt:670742 rt:0 fl:0 avg_rt: 15.6 (5) -- Payment，支付业务统计 [2] sc:96544 lt:16304 rt:0 fl:0 avg_rt: 3.6 (5) -- Order-Status，订单状态业务统计 [3] sc:87922 lt:24927 rt:0 fl:0 avg_rt: 72.5 (80) -- Delivery，发货业务统计 [4] sc:1565 lt:111282 rt:0 fl:0 avg_rt: 62.8 (20) -- Stock-Level，库存业务统计 in 1800 sec.&lt;Raw Results2(sum ver.)&gt; -- 第二次统计结果，其他同上 [0] sc:35421 lt:1093057 rt:0 fl:0 [1] sc:457717 lt:670764 rt:0 fl:0 [2] sc:96544 lt:16304 rt:0 fl:0 [3] sc:87922 lt:24927 rt:0 fl:0 [4] sc:1565 lt:111283 rt:0 fl:0&lt;Constraint Check&gt; (all must be [OK]) -- 下面所有业务逻辑结果都必须为 OK 才行 [transaction percentage] Payment: 43.48% (&gt;=43.0%) [OK] -- 支付成功次数(上述统计结果中sc + lt)必须大于43.0%，否则结果为NG，而不是OK Order-Status: 4.35% (&gt;= 4.0%) [OK] --订单状态，其他同上 Delivery: 4.35% (&gt;= 4.0%) [OK] -- 发货，其他同上 Stock-Level: 4.35% (&gt;= 4.0%) [OK] -- 库存，其他同上 [response time (at least 90% passed)] -- 响应耗时指标必须超过90%通过才行 New-Order: 3.14% [NG] * -- 标准5ms，只有3.14% 响应时间合格 Payment: 40.56% [NG] * -- 标准5ms， Order-Status: 85.55% [NG] * -- 标准5ms， Delivery: 77.91% [NG] * -- 标准80ms， Stock-Level: 1.39% [NG] * -- 标准20ms，&lt;TpmC&gt; 37615.934 TpmC -- TpmC结果值&#123;每分钟事务数，该值是第一次统计结果中的新订单事务数除以总耗时分钟数，例如本例中是：（35421+1093057）/30 ≈ 37615.93..&#125;；1000tpmC相当于2000个事务； tpcc-mysql的业务逻辑及其相关的几个表作用如下： New-Order：新订单，一次完整的订单事务，几乎涉及到全部表 Payment：支付，主要对应 orders、history 表 Order-Status：订单状态，主要对应 orders、order_line 表 Delivery：发货，主要对应 order_line 表 Stock-Level：库存，主要对应 stock 表 其它表说明: 客户：主要对应 customer 表 地区：主要对应 district 表 商品：主要对应 item 表 仓库：主要对应 warehouse 表 6)、清理数据1drop database tpcc1000; 7)、使用gunplot绘图① 安装 gunplot 1234yum install -y gnuplotgnuplot -V# gnuplot 4.6 patchlevel 2 帮助文档： http://gnuplot.info/docs_4.6/gnuplot.pdf ② 准备分析脚本和绘图脚本 gunplot-analyze-tpcc.sh 123456789101112131415161718#!/bin/bashTIMESLOT=1if [ -n "$2" ]then TIMESLOT=$2 echo "Defined $2"ficat $1 | grep -v HY000 | grep -v payment | grep -v neword | \awk -v timeslot=$TIMESLOT '\BEGIN &#123; FS="[,():|]"; s=0; cntr=0; aggr=0 &#125; \/MEASURING START/ &#123;s=1&#125; \/STOPPING THREADS/ &#123;s=0&#125; \/0/ &#123; \ if (s==1) &#123; cntr++; aggr+=$2; &#125; \ if (cntr==timeslot ) &#123; printf ("%d %3f %d\n",$1,$5,$3) ; cntr=0; aggr=0 &#125; \&#125;' 解释： 把 140, trx: 6365, 95%: 55.992, 99%: 75.599, max_rt: 115.579, 6367|110.699, 637|47.943, 637|156.809, 634|126.359中的 “[,():|]” 字符去掉以后，就剩下了140 trx 6365 95% 55.992 99% 75.599 max_rt 115.579 6367 110.699 637 47.943 637 156.809 634 126.359。$1就是140， $5就是55.992，打印的语法就跟c语言一样 帮助文档：man awk ③ 对 tpcc-mysql 测试生成的数据文件进行提取 1sh gunplot-analyze-tpcc.sh tpcc_output.txt &gt; tpcc_output_transfer.txt ④ 使用tpcc-graph-build.sh脚本生成图表 画95%的事务响应时间图：gunplot-graph-build-rt.sh 123456789101112131415161718#!/bin/bash### goto user homedir and remove previous filerm -f '$2'gnuplot &lt;&lt; EOP### set data source filedatafile = '$1'### set graph type and sizeset terminal jpeg size 720,640### set titlesset grid x yset xlabel "Time (sec)"set ylabel "Response Times(sec)"### set output filenameset output '$2'### build graph# plot datafile using 1:2 title "95%-rt" with lines,datafile using 1:3 title "max_rt" with lines axes x1y1plot datafile using 1:2 title "95%-rt" with lines axes x1y1EOP 画间隔时间内完成的事务数的图：gunplot-graph-build-trx.sh 123456789101112131415161718#!/bin/bash### goto user homedir and remove previous filerm -f '$2'gnuplot &lt;&lt; EOP### set data source filedatafile = '$1'### set graph type and sizeset terminal jpeg size 720,640### set titlesset grid x yset xlabel "Time (sec)"set ylabel "Transaction Nums"### set output filenameset output '$2'### build graph# plot datafile using 1:2 title "95%-rt" with lines,datafile using 1:3 title "max_rt" with lines axes x1y1plot datafile using 1:3 title "Transaction Nums" with lines axes x1y1EOP 1sh gunplot-graph-build-rt.sh tpcc_output_transfer.txt tpcc_output.jpg 1sh gunplot-graph-build-trx.sh tpcc_output_transfer.txt tpcc_output.jpg 五、mysql性能调优可以看到tpcc-mysql基准测试中 rt项全部不合格，所以需要对mysql进行性能调优]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>benchmark</tag>
        <tag>基准测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 管理 buff/cache 方法]]></title>
    <url>%2Flinux%2Flinux-cache-and-buffer%2F</url>
    <content type="text"><![CDATA[介绍 Linux服务器运行一段时间后，由于其内存管理机制，会将暂时不用的内存转为buff/cache，这样在程序使用到这一部分数据时，能够很快的取出，从而提高系统的运行效率，所以这也正是linux内存管理中非常出色的一点，所以乍一看内存剩余的非常少，但是在程序真正需要内存空间时，linux会将缓存让出给程序使用，这样达到对内存的最充分利用，所以真正剩余的内存是free+buff/cache 但是有些时候大量的缓存占据空间，这时候应用程序回去使用swap交换空间，从而使系统变慢，这时候需要手动去释放内存，释放内存的时候，首先执行命令 sync 将所有正在内存中的缓冲区写到磁盘中，其中包括已经修改的文件inode、已延迟的块I/O以及读写映射文件，从而确保文件系统的完整性 linux机制 说到清理内存，那么不得不提到/proc这一个虚拟文件系统，这里面的数据和文件都是内存中的实时数据，很多参数的获取都可以从下面相应的文件中得到，比如查看某一进程占用的内存大小和各项参数，cpu和主板的详细信息，显卡的参数等等；相应的关于内存的管理方式是在/proc/sys/vm/drop_chches文件中，一定要注意这个文件中存放的并不是具体的内存内容，而是0-3这几个数字，通过文件大小只有1B也可以知道，而这些代号分别告诉系统代表不同的含义如下： 0：0是系统默认值，默认情况下表示不释放内存，由操作系统自动管理 1：释放页缓存 2：释放dentries和inodes 3：释放所有缓存 所以根据上面的说明，分别将1,2,3这3个数字重定向到drop_caches中可以实现内存的释放，一般释放内存都是重定向3到文件中，释放所有的缓存 使用方法那么下面举个例子，比如这里只释放页缓存，首先使用 1free -m 查看当前内存剩余 当前内存剩余570M左右，另外buff/cache是1.3G，根据上面说的现在真正的剩余内存应该是1.8G左右，首先写缓存到文件系统： 1sync 然后执行下面命令释放内存(页缓存buff/cache)： 1echo 1 &gt; /proc/sys/vm/drop_caches 执行完之后，再次查看内存剩余： 会发现内存被释放了，可用内存确实变为1.5G左右 到这里内存就释放完了，现在drop_caches中的值为1，如果现在想让操作系统重新分配内存，那么设置drop_caches的值为0即可： 1echo 0 &gt; /proc/sys/vm/drop_caches 但是我走到这里报错，从0可以设置为1-3 ，但是不能设置为0了。一直没有找到原因，最后直接重启服务器了，然后开机之后自己恢复成0了。 所以命令慎用！！！ 另外需要注意的是，在生产环境中的服务器我们不要频繁的去释放内存，只在必要时候清理内存即可，更重要的是我们应该从应用程序层面去优化内存的利用和释放，经常清理内存可能只是暂时屏蔽的应用程序中的一些bug，所以更重要的是程序的调优，其他的交给操作系统来管理]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>linux</tag>
        <tag>buff</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived+ip组播]]></title>
    <url>%2Fcomputer-network%2Fkeepalived-vrrp%2F</url>
    <content type="text"><![CDATA[开启ip组播centos7centos6keepalived安装配置2台主机 server hostname ip keepalived ka1 192.168.99.103 keepalived ka2 192.168.99.104 1、2台分别安装keepalived12[ka1]$ yum -y install keepalived[ka2]$ yum -y install keepalived 2、修改ka1的配置12345678910111213141516171819202122232425262728293031323334[ka1]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from root@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_skip_check_adv_addr vrrp_strict vrrp_iptables vrrp_mcast_group4 224.100.100.100 #不写默认是224.0.0.18 vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 11 priority 100 advert_int 1 #nopreempt #非抢占模式，当优先级更高的主机上线时，不会抢占为主服务器 authentication &#123; auth_type PASS auth_pass 123 &#125; virtual_ipaddress &#123; 192.168.0.100 dev eth0 label eth0:1 &#125;&#125; 3、修改ka2的配置123456789101112131415161718192021222324252627282930313233! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from root@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka2 vrrp_skip_check_adv_addr vrrp_strict vrrp_iptables vrrp_mcast_group4 224.100.100.100 #不写默认是224.0.0.18 vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 11 priority 80 advert_int 1 #nopreempt #非抢占模式，当优先级更高的主机上线时，不会抢占为主服务器 authentication &#123; auth_type PASS auth_pass 123 &#125; virtual_ipaddress &#123; 192.168.0.100 dev eth0 label eth0:1 &#125;&#125; 4、在ka1或ka2新开个终端准备抓包1234[ka1]$ tcpdump -i eth0 -nn net 224.100.100.100tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 5、启动ka2的keepalived(先启动backup) 1[ka2]$ systemctl restart keepalived 抓到包了。 VIP绑定上了 6、再启动ka1的keepalived(注意看抓包)1[ka1]$ systemctl start keepalived 可以看到，宣告的IP变成了ka1的了 VIP漂移到ka1了，因为ka1的优先级比ka2高]]></content>
      <categories>
        <category>computer-network</category>
        <category>keepalived</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql配置优化]]></title>
    <url>%2Fmysql%2Fmysql-innodb-conf-optimize%2F</url>
    <content type="text"><![CDATA[要做优化工作首先要了解 mysql 的 架构设计，针对每个功能点做优化 一、整体架构优化 1、连接管理器参数概览 123456789101112131415mysql&gt; show variables like '%conn%';+-----------------------------------------------+-----------------+| Variable_name | Value |+-----------------------------------------------+-----------------+| character_set_connection | utf8 || collation_connection | utf8_general_ci || connect_timeout | 10 || disconnect_on_expired_password | ON || init_connect | || max_connect_errors | 1000000 || max_connections | 2000 || max_user_connections | 0 || performance_schema_session_connect_attrs_size | -1 |+-----------------------------------------------+-----------------+9 rows in set (0.00 sec) open_files_limit=1000000 # 文件描述符数量限制 character_set_connection=utf8mb4 # 连接使用的编码 collation_connection=utf8mb4_general_ci # 字符集 connect_timeout=10 init_connect= max_connect_errors=100 # 连续失败的连接超过max_connect_errors会阻止这台主机后续的所有请求 max_connections=151 # MySql的最大连接数 max_user_connections=0 # 指每个数据库用户的最大连接, 默认值为：0不受限制 back_log=2000 # 当MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log open_files_limit=1000000 在/etc/my.cnf加入open_files_limit=8192后重启MySQL后查看不起作用(重点在于操作系统的文件打开数是否够) 如果my.cnf里配置了open_files_limit，open_files_limit最后取值为 /etc/my.cnf 中 open_files_limit，max_connections * 5， wanted_files=10+max_connections+table_cache_size*2 三者中的最大值。 如果my.cnf里如果没配置了open_files_limit，open_files_limit最后取值为max_connections * 5，10+max_connections+table_cache_size*2，ulimit -n中的最大者 max_connections=2000 max_connections是指MySql的最大连接数。 如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量。当然这建立在机器能支撑的情况下，因为如果连接数越多，介于MySql会为每个连接提供连接缓冲区，就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。 可以过’conn%’通配符查看当前状态的连接数量，以定夺该值的大小。 MySQL服务器允许的最大连接数16384； 查看系统当前最大连接数： show variables like ‘max_connections’; max_user_connections=0 max_user_connections是指每个数据库用户的最大连接 针对某一个账号的所有客户端并行连接到MYSQL服务的最大并行连接数。简单说是指同一个账号能够同时连接到mysql服务的最大连接数。设置为0表示不限制。 目前默认值为：0不受限制。 查看max_user_connections值 show variables like ‘max_user_connections’; max_connect_errors=100 如果MySQL服务器连续接收到了来自于同一个主机的请求，而且这些连续的请求全部都没有成功的建立连接就被中断了，当这些连续的请求的累计值大于max_connect_errors的设定值时，MySQL服务器就会阻止这台主机后续的所有请求。相信一开始你看到这些资料，也会被“many successive connection requests from a host are interrupted without a successful connection”给弄懵，其实这个就是因为由于网络异常而中止数据库连接 back_log=2000 back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。 当MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log。如果等待连接的数量超过back_log，将不被授予连接资源。将会报：unauthenticated user | xxx.xxx.xxx.xxx | NULL | Connect | NULL | login | NULL 的待连接进程时。 back_log值不能超过TCP/IP连接的侦听队列的大小。若超过则无效，查看当前系统的TCP/IP连接的侦听队列的大小命令：cat /proc/sys/net/ipv4/tcp_max_syn_backlog目前系统为1024。 查看mysql 当前系统默认back_log值，命令：show variables like ‘back_log’; 查看当前数量 thread_cache_size 默认的thread_cache_size=8 根据物理内存设置规则如下：1G —&gt; 82G —&gt; 163G —&gt; 32 &gt;3G —&gt; 64 mysql&gt; show status like ‘thread%’;+——————-+——-+| Variable_name | Value |+——————-+——-+| Threads_cached | 0 | &lt;—当前被缓存的空闲线程的数量| Threads_connected | 1 | &lt;—正在使用（处于连接状态）的线程| Threads_created | 1498 | &lt;—服务启动以来，创建了多少个线程| Threads_running | 1 | &lt;—正在忙的线程（正在查询数据，传输数据等等操作）+——————-+——-+ 查看开机起来数据库被连接了多少次？ mysql&gt; show status like ‘%connection%’;+———————-+——-+| Variable_name | Value |+———————-+——-+| Connections | 1504 | –&gt;服务启动以来，历史连接数| Max_used_connections | 2 |+———————-+——-+ 通过连接线程池的命中率来判断设置值是否合适？命中率超过90%以上,设定合理。 (Connections - Threads_created) / Connections * 100 % 2、查询缓存 查询缓存是 把 查询 sql 作为 key，查询结果作为 value 缓存；因为 sql 多变，而且 查询结果一般比较大；所以缓存的意义不大，一般关掉查询缓存 之所以查询缓存并没有能起到提升性能的做用，客观上有如下两点原因：1、把SQL语句的hash值作为键，SQL语句的结果集作为值；这样就引起了一个问题如 select user from mysql.user 和cSELECT user FROM mysql.user 这两个将会被当成不同的SQL语句，这个时候就算结果集已经有了，但是一然用不到； 2、当查询所基于的低层表有改动时与这个表有关的查询缓存都会作废、如果对于并发度比较大的系统这个开销是可观的；对于作废结果集这个操作也是要用并发访问控制的，就是说也会有锁。并发大的时候就会有Waiting for query cache lock 产生； 123456789101112mysql&gt; show variables like '%query_cache%';+------------------------------+---------+| Variable_name | Value |+------------------------------+---------+| have_query_cache | YES || query_cache_limit | 1048576 || query_cache_min_res_unit | 4096 || query_cache_size | 1048576 || query_cache_type | OFF || query_cache_wlock_invalidate | OFF |+------------------------------+---------+6 rows in set (0.01 sec) have_query_cache YES # 表示这个mysql版本是否支持查询缓存 query_cache_limit 1048576 # 表示单个结果集所被允许缓存的最大值 query_cache_min_res_unit 4096 # 每个被缓存的结果集要占用的最小内存 query_cache_size 1048576 # 设置query缓冲区的大小 query_cache_type OFF # 是否开启查询缓存 query_cache_wlock_invalidate OFF # 如果某个数据表被其他的连接锁住，是否仍然从查询缓存中返回结果 query_cache_type query_cache_type=0（OFF）关闭 query_cache_type=1（ON）缓存所有结果，除非select语句使用SQL_NO_CACHE禁用查询缓存 query_cache_type=2(DEMAND)，只缓存select语句中通过SQL_CACHE指定需要缓存的查询 修改方法： vi /etc/my.cnf,加入如下行: query_cache_type =2 query_cache_size 设置query缓冲区的大小，一般关掉 query_cache_size: 主要用来缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集。 Query Cache也有一个致命的缺陷，那就是当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query Cache可能会得不偿失 Query Cache的使用需要多个参数配合，其中最为关键的是query_cache_size和query_cache_type，前者设置用于缓存 ResultSet的内存大小，后者设置在何场景下使用Query Cache 可以通过命令：show status like ‘Qcache_%’;查看目前系统Query catch使用大小 计算缓存命中率12345678910111213mysql&gt; show status like 'Qcache%';+-------------------------+----------+| Variable_name | Value |+-------------------------+----------+| Qcache_free_blocks | 1 || Qcache_free_memory | 1031352 || Qcache_hits | 0 || Qcache_inserts | 0 || Qcache_lowmem_prunes | 0 || Qcache_not_cached | 94462672 || Qcache_queries_in_cache | 0 || Qcache_total_blocks | 1 |+-------------------------+----------+ Qcache_free_blocks: 表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片过多了，可能在一定的时间进行整理。 Qcache_free_memory: 查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，是多了，还是不够用，DBA可以根据实际情况做出调整。 Qcache_hits: 表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越理想。 Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行查询处理后把结果insert到查询缓存中。这样的情况的次数，次数越多，表示查询缓存应用到的比较少，效果也就不理想。当然系统刚启动后，查询缓存是空的，这很正常。 Qcache_lowmem_prunes: 该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的调整缓存大小。 Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。 Qcache_queries_in_cache: 当前缓存中缓存的查询数量。 Qcache_total_blocks: 当前缓存的block数量。 内存碎片的产生。当一块分配的内存没有完全使用时，MySQL会把这块内存Trim掉，把没有使用的那部分归还以重 复利用。比如，第一次分配4KB,只用了3KB，剩1KB，第二次连续操作，分配4KB，用了2KB，剩2KB，这两次连续操作共剩下的 1KB+2KB=3KB，不足以做个一个内存单元分配， 这时候，内存碎片便产生了。使用flush query cache，可以消除碎片 下面是命中率和内存使用率的一些算法 12345query_cache_min_res_unit的估计值：(query_cache_size - Qcache_free_memory) / Qcache_queries_in_cache查询缓存命中率 ≈ (Qcache_hits – Qcache_inserts) / Qcache_hits * 100%查询缓存内存使用率 ≈ (query_cache_size – Qcache_free_memory) / query_cache_size * 100% 二、存储引擎 InnoDB 优化 InnoDB 的存储引擎 总体 分为 内存结构 和 磁盘结构 1、内存结构配置1.1、Buffer Pool功能介绍Buffer Pool是主内存中的一个区域，在InnoDB访问表和索引数据时会在其中进行 缓存。Buffer Pool允许直接从内存中直接处理经常使用的数据，从而加快了处理速度。在专用服务器上，通常将多达80％的物理内存分配给缓冲池。 为了提高大容量读取操作的效率，Buffer Pool被分为多个页面，这些页面可能包含多个行。为了提高缓存管理的效率，Buffer Pool被实现为页面的链接列表。使用LRU算法的变体将很少使用的数据从缓存中老化掉 图14.2缓冲池列表 该算法将大量页面保留在新的子列表中。旧的子列表包含较少使用的页面。这些页面是驱逐的候选对象 。 默认情况下，该算法的运行方式如下： 3/8的缓冲池专用于旧的子列表。 列表的中点是新子列表的尾部与旧子列表的头相交的边界。 当InnoDB将页面读入缓冲池时，它首先将其插入中点（旧子列表的头部）。可以读取页面，因为它是用户启动的操作（例如SQL查询）所必需的，或作为的自动执行的预读操作的一部分 InnoDB。 访问旧子列表中的页面 使其变为“ 年轻 ”，将其移至新子列表的头部。如果由于用户启动的操作而需要读取页面，则将立即进行首次访问，并使页面年轻。如果由于预读操作而读取了该页面，则第一次访问不会立即发生，并且在退出该页面之前可能根本不会发生。 随着数据库的运行，通过移至列表的尾部，缓冲池中未被访问的页面将“ 老化 ”。新的和旧的子列表中的页面都会随着其他页面的更新而老化。随着将页面插入中点，旧子列表中的页面也会老化。最终，未使用的页面到达旧子列表的尾部并被逐出。 默认情况下，查询读取的页面会立即移入新的子列表，这意味着它们在缓冲池中的停留时间更长。例如，针对mysqldump操作或SELECT不带WHERE子句的 语句 执行的表扫描可以将大量数据带入缓冲池，并驱逐同等数量的旧数据，即使不再使用新数据也是如此。同样，由预读后台线程加载且仅访问一次的页面将移到新列表的开头。这些情况可能会将常用页面推送到旧的子列表，在此它们会被逐出。有关优化此行为的信息，请参见 第14.8.3.2节“使缓冲池扫描具有抵抗力”和 第14.8.3.3节“配置InnoDB缓冲池预取（预读）”。 InnoDB标准监视器输出在BUFFER POOL AND MEMORY有关缓冲池LRU算法操作的部分中包含几个字段。有关详细信息，请参阅使用InnoDB Standard Monitor监视缓冲池。 参数配置1234567891011121314mysql&gt; show variables like '%innodb_buffer%';+-------------------------------------+----------------+| Variable_name | Value |+-------------------------------------+----------------+| innodb_buffer_pool_dump_at_shutdown | OFF || innodb_buffer_pool_dump_now | OFF || innodb_buffer_pool_filename | ib_buffer_pool || innodb_buffer_pool_instances | 8 || innodb_buffer_pool_load_abort | OFF || innodb_buffer_pool_load_at_startup | OFF || innodb_buffer_pool_load_now | OFF || innodb_buffer_pool_size | 1688207360 |+-------------------------------------+----------------+8 rows in set (0.00 sec) 参数说明： innodb_buffer_pool_dump_at_shutdown：在 mysqld 关闭时把热数据dump到本地磁盘； innodb_buffer_pool_load_at_startup：在 mysqld 启动时把热数据加载到内存； innodb_buffer_pool_dump_now：在 mysqld 关闭时把热数据dump到本地磁盘； innodb_buffer_pool_load_now：采用手工方式把热数据加载到内存； innodb_buffer_pool_filename：停止MySQL服务时，MySQL将InnoDB缓冲池中的热数据保存到数据库根目录中，默认文件名为ib_buffer_pool innodb_buffer_pool_instances：表示InnoDB缓存池被划分到一个区域。适当地增加该参数（例如将该参数值设置为2），此时InnoDB被划分成为两个区域，可以提升InnoDB的并发性能。如果InnoDB缓存池被划分成多个区域，建议每个区域不小于1GB的空间； innodb_buffer_pool_load_abort：终止Buffer Pool恢复，可以指定负载运行； innodb_buffer_pool_size：缓存池大小 innodb_buffer_pool_size 只使用InnoDB引擎的MySQL服务器中，根据经验，推荐设置 innodb_buffer_pool_size 为服务器总可用内存的80%； 通过(innodb_buffer_pool_read_requests – innodb_buffer_pool_reads) / innodb_buffer_pool_read_requests * 100% 计算缓存命中率。并根据命中率来调整 innodb_buffer_pool_size 参数大小进行优化， 命中率越高越好； 设置方法 12345SET GLOBAL innodb_buffer_pool_size=size_in_bytes;# 并且$ vi my.cnfinnodb_buffer_pool_size=size_in_bytesinnodb_buffer_pool_instances=8 效果监控方法一 12345678910111213141516mysql&gt; show status like 'innodb_%_read%'; +---------------------------------------+----------------+| Variable_name | Value |+---------------------------------------+----------------+| Innodb_buffer_pool_read_ahead_rnd | 0 || Innodb_buffer_pool_read_ahead | 1125395433 || Innodb_buffer_pool_read_ahead_evicted | 4003043 || Innodb_buffer_pool_read_requests | 26830062706 || Innodb_buffer_pool_reads | 606103167 || Innodb_data_pending_reads | 0 || Innodb_data_read | 28370717659136 || Innodb_data_reads | 1731613719 || Innodb_pages_read | 1731611205 || Innodb_rows_read | 170090056704 |+---------------------------------------+----------------+10 rows in set (0.01 sec) 参数说明： Innodb_buffer_pool_reads: 表示从物理磁盘读取页的次数 Innodb_buffer_pool_read_ahead: 预读的次数 Innodb_buffer_pool_read_ahead_evicted: 预读的页，但是没有读取就从缓冲池中被替换的页的数量，一般用来判断预读的效率 Innodb_buffer_pool_read_requests: 从缓冲池中读取页的次数 Innodb_data_read: 总共读入的字节数 Innodb_data_reads: 发起读取请求的次数，每次读取可能需要读取多个页 ​ 通过(innodb_buffer_pool_read_requests – innodb_buffer_pool_reads) / innodb_buffer_pool_read_requests * 100% 计算缓存命中率。并根据命中率来调整 innodb_buffer_pool_size 参数大小进行优化， 命中率越高越好； 方法二 1mysql&gt; SHOW ENGINE INNODB STATUS\G InnoDB可以使用访问的标准监视器输出， SHOW ENGINE INNODB STATUS提供有关缓冲池操作的度量。缓冲池度量标准位于BUFFER POOL AND MEMORY“ InnoDB标准监视器”输出中的部分，其 外观类似于以下内容： 123456789101112131415161718192021----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 2197815296; in additional pool allocated 0Dictionary memory allocated 155455Buffer pool size 131071Free buffers 92158Database pages 38770Old database pages 14271Modified db pages 619Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 4, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 322, created 38448, written 420830.00 reads/s, 222.30 creates/s, 159.47 writes/sBuffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead0.00/sLRU len: 38770, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0] 1.2、Change Buffer功能介绍Change Buffer是一种特殊的数据结构，当二级索引页不在缓冲池(Buffer Pool)中时，它们 会缓存这些更改 。当页面通过其他读取操作加载到缓冲池中时，可能由INSERT， UPDATE或 DELETE操作（DML）导致的缓冲更改 将在以后合并。 图14.3Change Buffer 与聚簇索引不同，二级索引通常是非唯一的，并且二级索引中的插入以相对随机的顺序发生。同样，删除和更新可能会影响索引树中不相邻的二级索引页。当稍后通过其他操作将受影响的页读入缓冲池时，合并缓存的更改将避免从磁盘将辅助索引页读入缓冲池所需的大量随机访问I / O。 参数配置12345678mysql&gt; show variables like '%change_buffer%'; +-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| innodb_change_buffer_max_size | 25 || innodb_change_buffering | all |+-------------------------------+-------+2 rows in set (0.00 sec) 参数说明： innodb_change_buffering： all: 默认值：缓冲区插入，删除标记操作和清除。 none: 不要缓冲任何操作。 inserts: 缓冲区插入操作。 deletes: 缓冲区删除标记操作。 changes: 缓冲插入和删除标记操作。 purges: 缓冲在后台发生的物理删除操作。 innodb_change_buffer_max_size：Change Buffer的最大大小配置为Buffer Pool总大小的百分比。默认情况下， innodb_change_buffer_max_size设置为25。最大设置为50 效果监控方法一 1mysql&gt; SHOW ENGINE INNODB STATUS\G Change Buffer状态信息位于INSERT BUFFER AND ADAPTIVE HASH INDEX 标题下， 并显示类似以下内容： 12345678910-------------------------------------INSERT BUFFER AND ADAPTIVE HASH INDEX-------------------------------------Ibuf: size 1, free list len 0, seg size 2, 0 mergesmerged operations: insert 0, delete mark 0, delete 0discarded operations: insert 0, delete mark 0, delete 0Hash table size 3340871, node heap has 2 buffer(s)0.00 hash searches/s, 0.00 non-hash searches/s 方法二 12345678910mysql&gt; SELECT (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE WHERE PAGE_TYPE LIKE 'IBUF%') AS change_buffer_pages, (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE) AS total_pages, (SELECT ((change_buffer_pages/total_pages)*100)) AS change_buffer_page_percentage;+---------------------+-------------+-------------------------------+| change_buffer_pages | total_pages | change_buffer_page_percentage |+---------------------+-------------+-------------------------------+| 25 | 8192 | 0.3052 |+---------------------+-------------+-------------------------------+ 1.3、Log Buffer 配置日志缓冲区是存储区域，用于保存要写入磁盘上的日志文件的数据。日志缓冲区大小由innodb_log_buffer_size变量定义 。默认大小为16MB。日志缓冲区的内容会定期刷新到磁盘。大型的日志缓冲区使大型事务能够运行，而无需在事务提交之前将Redo Log数据写入磁盘。因此，如果您有更新，插入或删除许多行的事务，则增加日志缓冲区的大小可以节省磁盘I / O。 该 innodb_flush_log_at_trx_commit 变量控制如何将日志缓冲区的内容写入并刷新到磁盘。该 innodb_flush_log_at_timeout 变量控制日志刷新频率。 123456789101112131415mysql&gt; show variables like '%log_buffer%'; +------------------------+---------+| Variable_name | Value |+------------------------+---------+| innodb_log_buffer_size | 8388608 | 此时为8M = 8 * 1024 * 1024+------------------------+---------+1 row in set (0.01 sec)mysql&gt; show variables like '%innodb_flush_log_at_trx_commit%';+--------------------------------+-------+| Variable_name | Value |+--------------------------------+-------+| innodb_flush_log_at_trx_commit | 2 | +--------------------------------+-------+1 row in set (0.00 sec) innodb_flush_log_at_trx_commit 参数解释： 0（延迟写）： log_buff –每隔1秒–&gt; log_file —实时—&gt; disk 1（实时写，实时刷）： log_buff —实时—&gt; log_file —实时—&gt; disk 2（实时写，延迟刷）： log_buff —实时—&gt; log_file –每隔1秒–&gt; disk 0：最快减少mysql写的等待 1：最大安全性,不会丢失数据 2：折中，减少操作系统文件写入等待时间 1.4、Adaptive Hash Index功能介绍InnoDB存储引擎会监控对表上索引的查找，如果观察到建立哈希索引可以带来速度的提升，则建立哈希索引，所以称之为自适应（adaptive） 的。自适应哈希索引通过缓冲池的B+树构造而来，因此建立的速度很快。而且不需要将整个表都建哈希索引，InnoDB存储引擎会自动根据访问的频率和模式 来为某些页建立哈希索引。 根据InnoDB的官方文档显示，启用自适应哈希索引后，读取和写入速度可以提高2倍；对于辅助索引的连接操作，性能可以提高5倍。在我看来，自适应哈希索引是非常好的优化模式，其设计思想是数据库自优化（self-tuning），即无需DBA对数据库进行调整。 Adaptive Hash Index是针对B+树Search Path的优化，因此所有会涉及到Search Path的操作，均可使用此Hash索引进行优化，这些可优化的操作包括：Unique Scan/Range Scan(Locate First Key Page)/Insert/Delete/Purge等等，几乎涵盖InnoDB所有的操作类型。 通过参数innodb_adaptive_hash_index来禁用或启动此特性，默认为开启 参数配置1234567mysql&gt; show variables like '%adaptive_hash%';+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| innodb_adaptive_hash_index | ON |+----------------------------+-------+1 row in set (0.00 sec) 2、操作系统缓存 ## 3、磁盘结构配置 3.1、System Tablespace功能介绍​ 系统表空间是 InnoDB数据字典，Doublewrite Buffer，Change Buffer和 Undo Log的存储区 。如果在系统表空间中创建表，而不是在每个表文件中创建表，则它也可能包含表和索引数据。 系统表空间可以具有一个或多个数据文件。默认情况下，ibdata1在数据目录中创建一个名为的系统表空间数据文件 。系统表空间数据文件的大小和数量由innodb_data_file_path启动选项定义。有关配置信息，请参阅《 系统表空间数据文件配置》。 参数配置12345678910111213141516mysql&gt; show variables like '%innodb_data%';+-----------------------+------------------------+| Variable_name | Value |+-----------------------+------------------------+| innodb_data_file_path | ibdata1:12M:autoextend || innodb_data_home_dir | |+-----------------------+------------------------+2 rows in set (0.00 sec)mysql&gt; show variables like '%innodb_autoextend_increment%';+-----------------------------+-------+| Variable_name | Value |+-----------------------------+-------+| innodb_autoextend_increment | 8 |+-----------------------------+-------+1 row in set (0.00 sec) ibdata1:12M:autoextend 含义： innodb_data_home_dir 为空，表示在默认目录 /var/lib/mysql 下创建一个文件 ibdata1，初始大小为12MB，如果快达到12MB时，以innodb_autoextend_increment设置的增量 8MB 根据需要增加空间。 1、增大系统表空间 假设 /var/lib/mysql 所在的磁盘将满，需要扩展系统空间，方法如下： 停止 mysql 服务； vi /etc/my.cnf 12[mysqld]innodb_data_file_path=/var/lib/mysql/ibdata1:76M;/disk2/ibdata2:50M:autoextend /disk2/ 为新磁盘；autoextend 只能跟在最后一个文件后面； 启动 mysql 服务； 2、减小系统表空间 您不能从系统表空间中删除数据文件。要减小系统表空间大小，请使用以下过程： 使用mysqldump转储所有 InnoDB表，包括 模式中的InnoDB表 mysql。使用以下查询标识 模式中的InnoDB表 mysql： 1234567891011mysql&gt; SELECT TABLE_NAME from INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA='mysql' and ENGINE='InnoDB';+----------------------+| table_name |+----------------------+| innodb_index_stats || innodb_table_stats || slave_master_info || slave_relay_log_info || slave_worker_info |+----------------------+5 rows in set (0.00 sec) 停止服务器。 删除所有现有的表空间文件（*.ibd），包括 ibdata和ib_log 文件。不要忘记删除 架构*.ibd 中表的文件mysql。 删除表的所有.frm文件 InnoDB。 为新系统表空间配置数据文件。请参阅 系统表空间数据文件配置。 重新启动服务器。 导入转储文件。 3、在l Linux 和 Unix上使用原始磁盘分区 修改 /etc/my.cnf ； 123[mysqld]innodb_data_home_dir=innodb_data_file_path=/dev/hdd1:3Gnewraw;/dev/hdd2:2Gnewraw 重启 mysql； InnoDB 的 newraw关键字将初始化新分区。重启以后，不要创建任何新表。否则下次重启将再次初始化分区，丢失所有数据； 再次修改 /etc/my.cnf ； 123[mysqld]innodb_data_home_dir=innodb_data_file_path=/dev/hdd1:3Graw;/dev/hdd2:2Graw 重启 mysql 服务器。现在允许创建表。 3.2、File-Per-Table-Tablespaces功能介绍每表文件表空间包含单个InnoDB表的数据和索引 ，并存储在文件系统中自己的数据文件中。 .frm 文件：表结构信息、字典信息； .idb 文件：数据信息、索引信息； 参数配置12345678910mysql&gt; show variables like '%innodb_file%';+--------------------------+----------+| Variable_name | Value |+--------------------------+----------+| innodb_file_format | Antelope || innodb_file_format_check | ON || innodb_file_format_max | Antelope || innodb_file_per_table | ON |+--------------------------+----------+4 rows in set (0.00 sec) innodb_file_per_table： ON-启动独立表空间，OFF-启用共享表空间 12345# 临时表更mysql&gt; SET GLOBAL innodb_file_per_table=ON;# 永久变更vi /etc/my.cnfinnodb_file_per_table=1 3.3、Undo Tablespaces功能介绍回滚段 InnoDB采用回滚段的方式来维护undo log的并发写入和持久化。回滚段实际上是一种 Undo 文件组织方式，每个回滚段又有多个undo log slot。 一共128个回滚段，每个回滚段维护了一个段头页，在该page中又划分了1024个slot (TRX_RSEG_N_SLOTS)，每个slot又对应到一个undo log对象，因此理论上InnoDB最多支持 96 * 1024个普通事务。 rseg0预留在系统表空间ibdata中; rseg 1~rseg 32这32个回滚段存放于临时表的系统表空间中; rseg33~ 则根据配置存放到独立undo表空间中（如果没有打开独立Undo表空间，则存放于ibdata中） Undo Log ​ undo Log是与单个读写事务关联的 undo Log 记录的集合。undo Log 记录包含如何 撤消事务对 聚簇索引 记录的最新更改 的信息。如果另一个事务读取原始数据，就需要在 undo Log记录读取。 ​ undo Log存在于 undo log segment。默认情况下，undo log segment 实际上是 system tablespace 的一部分 ，但它们也可以驻留在 undo log tablespace 中。有关更多信息，请参见第14.6.3.3节“撤消表空间”。 InnoDB支持128个回滚段，通过参数 innodb_rollback_segments 定义； 一个 rollback segment 支持的 事务数 取决（ rollback segment 中 undo slots 的数量） 和 （每个事务所需的 undo log 数）； 一个rollback segment 中 undo slots 的数量 又会因为 innodb_page_size 不同而不同； InnoDB Page Size rollback segment 中的 undo slots 数（innodb_page_size/ 16） 4096 (4KB) 256 8192 (8KB) 512 16384 (16KB) 1024 以下每种操作类型，一个事务最多可以分配两个 undo Log： INSERT 操作 在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除； UPDATE和 DELETE 操作 产生的Undo日志被归成一类，即update_undo； undo Log根据需要分配。例如，一个事务执行INSERT， UPDATE 和 DELETE 操作被分配了两个undo Log，仅执行INSERT操作的事务被分配有一个 undo Log； 分配给事务的Undo Log在其持续时间内始终与事务相关； 给定上述因素，可以使用以下公式来估计InnoDB能够支持的并发读写事务数。 注意：在达到 InnoDB 能够支持的并发读写事务数之前，事务可能会遇到并发事务限制错误。当分配给事务的 rollback segment 用完 undo slots 时，就会发生这种情况。在这种情况下，请尝试重新运行事务 如果每个事务执行任一个 insert 或 update 或 delete 操作，InnoDB支持的 并发读-写事务的数目： 1(innodb_page_size / 16) * innodb_rollback_segments 如果每个事务执行任一个 insert 和（ update 或 delete） 操作，InnoDB支持的 并发读-写事务的数目 1(innodb_page_size / 16 / 2) * innodb_rollback_segments 在 mysql 5.6 中开始支持把 undo log 分离到独立的表空间，并放到单独的文件目录下；这给我们部署不同IO类型的文件位置带来便利，对于并发写入型负载，我们可以把undo文件部署到单独的高速存储设备上； undo log 的 i/o 模式决定 最好使用 SSD 来存储； innodb事务日志包括redo log和undo log。redo log是重做日志，提供前滚操作，undo log是回滚日志，提供回滚操作。 undo log不是redo log的逆向过程，其实它们都算是用来恢复的日志： redo log通常是物理日志，记录的是 数据页 的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。 undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据 每行记录 进行记录。 参数配置1234567891011121314151617mysql&gt; show variables like '%undo%';+-------------------------+-------+| Variable_name | Value |+-------------------------+-------+| innodb_undo_directory | . || innodb_undo_logs | 128 || innodb_undo_tablespaces | 0 |+-------------------------+-------+3 rows in set (0.00 sec)mysql&gt; show variables like '%innodb_rollback_segments%';+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| innodb_rollback_segments | 128 |+--------------------------+-------+1 row in set (0.00 sec) innodb_undo_tablespaces 用于设定创建的undo表空间的个数，仅在初始化 mysql 实例时才能配置此选项，此后无法更改； 默认值为0，表示 不独立设置undo的 tablespace，默认记录到 ibdata 中；否则，则在undo目录下创建这么多个undo文件，例如假定设置该值为16，那么就会创建命名为 undo001 ~ undo016的 undo tablespace 文件，每个文件的默认大小为 10M 修改该值可能会导致Innodb无法完成初始化； innodb_undo_logs 用于表示回滚段的个数（早期版本的命名为innodb_rollback_segments ），该变量可以动态调整，但是物理上的回滚段不会减少，只是会控制用到的回滚段的个数; 默认为128个回滚段 innodb_undo_directory 当开启独立undo表空间时，指定undo文件存放的目录 如果我们想转移undo文件的位置，只需要修改下该配置，并将undo文件拷贝过去就可以了。 3.4、Redo Loghttps://dev.mysql.com/doc/refman/5.6/en/innodb-redo-log.html 功能介绍redo log 是一种基于磁盘的，在 mysql 崩溃恢复时 用来纠正未完成事务的数据结构。 默认情况下，redo log 在磁盘上表现为两个文件 ib_logfile0 和 ib_logfile1。mysql 把受影响的数据编码后以循环方式写这两文件。 优化建议：确保 redo log 日志足够大，甚至和 buffer pool 一样大。当 InnoDB 把 redo log 文件写满时，它必须把 buffer pool 中的 数据写入磁盘 checkpoint。如果 redo log 太小 会导致很多没必要的磁盘写入。尽管 redo log 太大会导致 mysql 在启动恢复时耗时更长，不过现在的优化以后它的速度还是挺快的，可以放心使用； 参数配置1、确保 innodb_fast_shutdown 不是2 1mysql&gt; SET GLOBAL innodb_fast_shutdown=1; 2、停止 mysql 1systemctl stop mysqld 3、将 ib_logfile0 和 ib_logfile1 复制到 其他安全的空间，防止被写入脏数据； 4、删除掉 ib_logfile0 和 ib_logfile1； 5、修改 /etc/my.cnf，改变 redo log 文件的数量 或者 大小； 1234567vi /etc/my.cnf[mysqld]# redo log 文件大小innodb_log_file_size=536870912# redo log 文件个数innodb_log_files_in_group=2 6、启动 mysql，mysqld 将会创建 新的ib_logfile0 和 ib_logfile1； 和其他AICD的数据库搜索引擎一样，InnoDB会在提交一个事务之前 flush redo log。InnoDB 使用 group commit 功能 将多个事务的提交请求一起flush，而避免每个提交只有一个flush，从而提高吞吐量。 效果监控12345678mysql&gt; show variables like '%innodb_log_file%';+---------------------------+-----------+| Variable_name | Value |+---------------------------+-----------+| innodb_log_file_size | 536870912 || innodb_log_files_in_group | 2 |+---------------------------+-----------+2 rows in set (0.00 sec) 3.5、InnoDB Data DictionaryInnoDB的数据字典 有 内部系统表组成，这些表包含 tables，indexs，和 table columns。元数据在物理上位于 system tablespace。由于历史原因，数据字典元数据 在一定程度上 与 InnoDB 表的元数据文件（.frm 文件）中存储的信息重叠。 3.6、Doublewrite Buffer功能介绍​ doublewrite buffer 是 InnoDB 的 system tablespace 中的一个区域。在 把 buffer pool 中的内容写入对应的 data file 之前，先要把数据写入 doublewirte buffer 中； ​ 如果 storage 或者 mysqld 在写 page的过程中 进程崩溃，InnoDB 可以在 doublewrite buffer 中找到完成的 page 的拷贝 用来恢复； ​ 尽管数据总是写两次，但 doublewrite buffer 不需要两倍的 I/O开销 或 两倍的 I/O 操作。数据作为一个大的连续块 写入 doublwrite buffer，只需对操作系统执行一次 fsync() 调用； ​ doublewirte buffer 默认启用，如果想关掉它，设置 innodb_doublewrite=0 效果监控12345678mysql&gt; show global status like '%dblwr%';+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Innodb_dblwr_pages_written | 91756 || Innodb_dblwr_writes | 18741 |+----------------------------+-------+2 rows in set (0.00 sec) 关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes 开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。 而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在3左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。 三、常见参数介绍back_log含义： MySQL可以具有的未完成连接请求数。也受操作系统限制，默认值-1表示自动调整大小。默认值是这么计算的(50 + (max_connections / 5))，范围(1-65535) ; back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。将会报：unauthenticated user | xxx.xxx.xxx.xxx | NULL | Connect | NULL | login | NULL 的待连接进程时. back_log值不能超过TCP/IP连接的侦听队列的大小。若超过则无效，查看当前系统的TCP/IP连接的侦听队列的大小命令：cat /proc/sys/net/ipv4/tcp_max_syn_backlog，目前系统为1024。对于Linux系统推荐设置为大于512的整数; 修改方式： 1234567891011121314# 修改 TCP/IP连接的侦听队列的大小cat /proc/sys/net/ipv4/tcp_max_syn_backlogvi /etc/sysctl.conf# 添加: net.ipv4.tcp_max_syn_backlog = 2048# 使生效sysctl -pcat /proc/sys/net/ipv4/tcp_max_syn_backlogvi /etc/my.cnf[mysqld]back_log=2000在mysql中查看show variables like 'back_log'; open_files_limit含义 在/etc/my.cnf加入open_files_limit=8192后重启MySQL后查看不起作用(重点在于操作系统的文件打开数是否够) my.cnf里如果配置了open_files_limit open_files_limit最后取值为 配置文件 open_files_limit，max_connections5， wanted_files= 10+max_connections+table_cache_size2 三者中的最大值。 如果my.cnf里如果没配置了open_files_limit open_files_limit最后取值为max_connections5，10+max_connections+table_cache_size2，ulimit -n中的最大者 修改方案： 12345678910111213141516171819202122232425262728# 永久更改最大打开文件描述符数# 第一步# vi /etc/security/limits.conf* soft nofile 1000000* hard nofile 1000000 #星号表示对所有用户生效# 第二步vi /etc/my.cnf[mysqld_safe]open_files_limit=1000000# 第三步vi /etc/systemd/system/mysql.service[Service]LimitNOFILE=1000000# 第四步systemctl daemon-reload# service mysqld restartsystemctl restart mysqld.servicereboot #重启系统# 第五步（查看是否生效）ulimit -n# sysctl fs.file-max#PID是应用的进程ID，在输出结果中查看"Max open files"的显示值ps aux | grep mysqlcat /proc/$&#123;mysql-pid&#125;/limits cat /proc/${mysql-pid}/limits 直接设置生效123456789101112131415161718192021character_set_server=utf8collation_server=utf8_general_cienforce_gtid_consistency=ONexpire_logs_days=3innodb_open_files=2000open_files_limit=1000000performance_schema_digests_size=10000performance_schema_events_stages_history_long_size=10000performance_schema_events_statements_history_long_size=10000performance_schema_events_waits_history_long_size=10000performance_schema_max_cond_instances=3504performance_schema_max_file_instances=7693performance_schema_max_mutex_instances=15906performance_schema_max_rwlock_instances=9102performance_schema_max_socket_instances=322performance_schema_max_table_handles=4000performance_schema_max_table_instances=12500performance_schema_max_thread_instances=402system_time_zone=CSTtable_definition_cache=1400table_open_cache=2000 host_cache_size含义 –skip-host-cache 为加快主机名到IP解析禁用使用内部主机缓存。在这种情况，每次客户连接，服务器执行DNS查找。见 Section 8.11.5.2, “DNS Lookup Optimization and the Host Cache”。使用 –skip-host-cache类似设置系统变量host_cache_size的值为0，但host_cache_size更加灵活，因为在运行时它也可以调整大小、启用或禁用主机缓存,不只是在服务器启动事。如果使用 –skip-host-cache启动服务，它不能阻止host_cache_size的改变，但这些改变不生效和缓存是不可用，尽管host_cache_size设置大于0。 修改 123vi /etc/my.cnf[mysqld]host_cache_size=703 innodb_buffer_pool_size https://www.cnblogs.com/wanbin/p/9530833.html MyISAM使用操作系统缓存来缓存数据。InnoDB需要innodb buffer pool中处理缓存。所以非常需要有足够的InnoDB buffer pool空间 MySQL InnoDB buffer pool 里包含什么？ 数据缓存InnoDB数据页面 索引缓存索引数据 缓冲数据脏页（在内存中修改尚未刷新(写入)到磁盘的数据） 内部结构如自适应哈希索引，行锁等。 配置的innodb_buffer_pool_size是否合适？当前配置的innodb_buffer_pool_size是否合适，可以通过分析InnoDB缓冲池的性能来验证。 可以使用以下公式计算InnoDB缓冲池性能： 12&gt; Performance = innodb_buffer_pool_reads / innodb_buffer_pool_read_requests * 100&gt; innodb_buffer_pool_reads：表示InnoDB缓冲池无法满足的请求数。需要从磁盘中读取。 innodb_buffer_pool_read_requests：表示从内存中读取逻辑的请求数。 123456789101112131415&gt; root@localhost [(none)] 15:35:31&gt;show status like &apos;innodb_buffer_pool_read%&apos;;&gt; +---------------------------------------+-------------+&gt; | Variable_name | Value |&gt; +---------------------------------------+-------------+&gt; | Innodb_buffer_pool_read_ahead_rnd | 0 |&gt; | Innodb_buffer_pool_read_ahead | 0 |&gt; | Innodb_buffer_pool_read_ahead_evicted | 0 |&gt; | Innodb_buffer_pool_read_requests | 4029033624 |&gt; | Innodb_buffer_pool_reads | 91661 |&gt; +---------------------------------------+-------------+&gt; 5 rows in set (0.00 sec)&gt; &gt; &gt; Performance = 91661 / 4029033624 * 100 = 0.0022750120389663&gt; 意味着InnoDB可以满足缓冲池本身的大部分请求。从磁盘完成读取的百分比非常小。因此无需增加innodb_buffer_pool_size值。 修改 123vi /etc/my.cnf[mysqld]innodb_buffer_pool_size=9688842240 innodb_data_file_path innodb_data_file_path用来指定innodb tablespace文件，如果我们不在My.cnf文件中指定innodb_data_home_dir和innodb_data_file_path那么默认会在datadir目录下创建ibdata1 作为innodb tablespace show variables like ‘innodb_data%’; 123&gt; innodb_data_file_path ibdata1:12M:autoextend&gt; innodb_data_home_dir &gt; 修改 123vi /etc/my.cnf[mysqld]innodb_data_file_path=ibdata1:100M:autoextend 可能引起的问题 1234562019-11-15 10:58:33 18273 [ERROR] InnoDB: auto-extending data file ./ibdata1 is of a different size 768 pages (rounded down to MB) than specified in the .cnf file: initial 6400 pages, max 0 (relevant if non-zero) pages!2019-11-15 10:58:33 18273 [ERROR] InnoDB: Could not open or create the system tablespace. If you tried to add new data files to the system tablespace, and it failed here, you should now edit innodb_data_file_path in my.cnf back to what it was, and remove the new ibdata files InnoDB created in this failed attempt. InnoDB only wrote those files full of zeros, but did not yet use them in any way. But be careful: do not remove old data files which contain your precious data!2019-11-15 10:58:33 18273 [ERROR] Plugin 'InnoDB' init function returned error.2019-11-15 10:58:33 18273 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed.2019-11-15 10:58:33 18273 [ERROR] Unknown/unsupported storage engine: InnoDB2019-11-15 10:58:33 18273 [ERROR] Aborting https://blog.csdn.net/u010735147/article/details/82415868问题分析：从Error看,表空间ibdata1跑在旧的my.cnf下大小65536/64M(64个page=1M,1个page=16k)比现在my.cnf文件中配置的ibdata1值要小,导致数据文件无法打开，同时InnoDB存储引擎加载失败解决方式:1.删除现在ibdata1数据文件,引起另外麻烦,mysqld启动了，但是所有InnoDB表报废，select时提示该表不存在—慎用；2.注释该设置的参数(InnoDB_data_file_path),MySQLd启用默认设置ibdata1值大小;3.ibdata1大小扩展方式,网上搜索一把,查询是ibdata1如何瘦身,涉及参数:InnoDB_data_file_path; innodb_numa_interleave 大家都知道，在运行mysql服务的服务器上，linux系统的内存numa特性是强烈建议关闭的。因为这种特性很容易引起内存泄漏的情况：即发现物理内存还有剩余，但是系统已经开始使用swap内存。 ​ numa内存特性：比如一台机器是有2个处理器，有4个内存块。我们将1个处理器和两个内存块合起来，称为一个NUMA node，这样这个机器就会有两个NUMA node。在物理分布上，NUMA node的处理器和内存块的物理距离更小，因此访问也更快。比如这台机器会分左右两个处理器（cpu1, cpu2），在每个处理器两边放两个内存块(memory1.1, memory1.2, memory2.1,memory2.2)，这样NUMA node1的cpu1访问memory1.1和memory1.2就比访问memory2.1和memory2.2更快。所以使用NUMA的模式如果能尽量保证本node内的CPU只访问本node内的内存块，那这样的效率就是最高的。 ​ 其实由于mysql数据库服务器一般只会部署mysql一种服务在运行，而不会部署若干应用在运行。所以一般是希望mysql是独占整个服务器的资源（剔除留给操作系统运行的资源）。所以根据这种业务特性，mysql服务器上是不建议开启numa内存特性。 修改 123456# 建议关闭【关闭NUMA的方案】1、 在mysqld_safe脚本中加上“numactl –interleave all”来启动mysqld2、 Linux Kernel启动参数中加上numa=off，需要重启服务器3、 在BIOS层面关闭NUMA4、 MySQL 5.6.27/5.7.9开始引用innodb_numa_interleave选项 innodb_flush_log_at_trx_commit 一、参数解释0：log buffer将每秒一次地写入log file中，并且log file的flush(刷到磁盘)操作同时进行。该模式下在事务提交的时候，不会主动触发写入磁盘的操作。1：每次事务提交时MySQL都会把log buffer的数据写入log file，并且flush(刷到磁盘)中去，该模式为系统默认。2：每次事务提交时MySQL都会把log buffer的数据写入log file，但是flush(刷到磁盘)操作并不会同时进行。该模式下，MySQL会每秒执行一次 flush(刷到磁盘)操作。 二、参数修改找到mysql配置文件mysql.ini，修改成合适的值，然后重启mysql。 三、注意事项当设置为0，该模式速度最快，但不太安全，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。当设置为1，该模式是最安全的，但也是最慢的一种方式。在mysqld 服务崩溃或者服务器主机crash的情况下，binary log 只有可能丢失最多一个语句或者一个事务。。当设置为2，该模式速度较快，也比0安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 四、其他相关查找资料时候看到其他文章说innodb_flush_log_at_trx_commit和sync_binlog 两个参数是控制MySQL 磁盘写入策略以及数据安全性的关键参数，当两个参数都设置为1的时候写入性能最差，推荐做法是innodb_flush_log_at_trx_commit=2，sync_binlog=500 或1000 修改 123vi /etc/my.cnf[mysqld]innodb_flush_log_at_trx_commit=2 innodb_flush_method https://blog.csdn.net/smooth00/article/details/72725941 fdatasync模式：写数据时，write这一步并不需要真正写到磁盘才算完成（可能写入到操作系统buffer中就会返回完成），真正完成是flush操作，buffer交给操作系统去flush,并且文件的元数据信息也都需要更新到磁盘。O_DSYNC模式：写日志操作是在write这步完成，而数据文件的写入是在flush这步通过fsync完成O_DIRECT模式：数据文件的写入操作是直接从mysql innodb buffer到磁盘的，并不用通过操作系统的缓冲，而真正的完成也是在flush这步,日志还是要经过OS缓冲 修改 123vi /etc/my.cnf[mysqld]innodb_flush_method=O_DIRECT 此模式，稳定性最高，直接读写磁盘，不经过buffer innodb_io_capacity可以提高 数据库 tps innodb_io_capacity 和 innodb_io_capacity_max ​ 这是一个更加高级的调优，只有当你在频繁写操作的时候才有意义（它不适用于读操作，例如 SELECTs）。若你真的需要对它进行调整，最好的方法是要了解系统可以支持多大的 IOPS。譬如，假设服务器有一块 SSD 硬盘，我们可以设置 innodb_io_capacity_max=6000 和 innodb_io_capacity=3000（最大值的一半）。运行 sysbench 或者任何其他基准工具来对磁盘吞吐量来进行基准测试是一个好方法。 然而，我们需要去担心这个选项吗？看下面这张缓冲池的”脏页“ 在这种情况下，脏页的总量很大，而且看起来 InnoDB 刷新脏页的速度跟不上脏页的速度。如果我们有一个高速的磁盘子系统（例如：SSD），可以增加 innodb_io_capacity 和 innodb_io_capacity_max 来得到更好的性能。 修改 1234vi /etc/my.cnf[mysqld]innodb_io_capacity=2000innodb_io_capacity_max=4000 innodb_log_file_size延伸阅读： https://blog.csdn.net/kai404/article/details/80242262 跟其他数据库管理系统一样，MySQL通过日志来实现数据的持久性（在使用InnoDB存储引擎的前提下）。这确保了当一个事务提交后，其相关数据在崩溃或者服务器掉电的情况下不会丢失。 MySQL的InnoDB 存储引擎使用一个指定大小的Redo log空间（一个环形的数据结构）。Redo log的空间通过innodb_log_file_size和innodb_log_files_in_group（默认2）参数来调节。将这俩参数相乘即可得到总的可用Redo log 空间。尽管技术上并不关心你是通过innodb_log_file_size还是innodb_log_files_in_group来调整Redo log空间，不过多数情况下还是通过innodb_log_file_size 来调节。 为InnoDB引擎设置合适的Redo log空间对于写敏感的工作负载来说是非常重要的。然而，这项工作是要做出权衡的。你配置的Redo空间越大，InnoDB就能更好的优化写操作；然而，增大Redo空间也意味着更长的恢复时间当出现崩溃或掉电等意外时。 恢复时间 : 每1GB的Redo log的恢复时间大约在5分钟左右来估算 ; 关于Redo 空间的使用情况，如果没有安装PMM的话，也可以通过下面的命令来观察每小时的写入量（MB）： 12&gt; a=$(mysql -uuser -p'passwd' -e "show engine innodb status\G" | grep "Log sequence number" | awk '&#123;print $4&#125;'); sleep 60; b=$(mysql -uuser -p'passwd' -e "show engine innodb status\G" | grep "Log sequence number" | awk '&#123;print $4&#125;'); let "res=($b-$a)*60/1024/1024";echo $res&gt; 修改 123vi /etc/my.cnf[mysqld]innodb_log_file_size=536870912 innodb_max_dirty_pages_pct mysql检查点事件受两个因素的制约：一个是amount,另外一个是age.amount主要由innodb_max_dirty_pages_pct参数控制;至于age,主要是由日志文件大小有关 ; innodb_io_capacity, innodb_max_dirty_pages_pct, innodb_adaptive_flushing这三个参数是为了解决SSD等大容量存储设备的出现而导致innoDB不能很好利用这类设备性能的困境而出现的，本文会结合mysql-5.5.34的源代码进行这三个参数的介绍; innodb_max_dirty_pages_pct：参数会让InnoDB Buffer Pool刷新数据而不让脏数据的百分比超过这个值； innodb_max_dirty_pages_pct_lwm：InnoDB会自动维护后台作业自动从Buffer Pool当中清除脏数据，当Buffer Pool中的脏页占用比 达到innodb_max_dirty_pages_pct_lwm的设定值的时候，就会自动将脏页清出Buffer Pool； 修改 1innodb_max_dirty_pages_pct=50 innodb_open_files ​ 限制Innodb能同时打开的表的数量，默认为300，数据库里的表特别多的情况，可以适当增大为1000。innodb_open_files的大小对InnoDB效率的影响比较小。但是在InnoDBcrash的情况下，innodb_open_files设置过小会影响recovery的效率。所以用InnoDB的时候还是把innodb_open_files放大一些比较合适。 修改 1innodb_open_files=1024 key_buffer_size key_buffer_size这个参数是用来设置索引块（index blocks）缓存的大小，它被所有线程共享，严格说是它决定了数据库索引处理的速度，尤其是索引读的速度。那我们怎么才能知道key_buffer_size的设置是否合理呢，一般可以检查状态值Key_read_requests和Key_reads，比例key_reads / key_read_requests应该尽可能的低，比如1:100，1:1000 ，1:10000。其值可以用以i下命令查得： 12345678910111213&gt; mysql&gt; show status like 'key_read%';&gt; +-------------------+------------+&gt; | Variable_name | Value |&gt; +-------------------+------------+&gt; | Key_read_requests | 3916880184 | &gt; | Key_reads | 1014261 | &gt; +-------------------+------------+&gt; 2 rows in set (0.00 sec)&gt; 3916880184/1024/1024=?M //单位为兆&gt; 我的key_buffer_size值为：&gt; key_buffer_size=536870912/1024/1024=512M,&gt; key_reads / key_read_requests=1014261: 3916880184≈1:4000，照上面来看，健康状况还行。&gt; 修改 1key_buffer_size=33554432 long_query_time慢查询阈值 1long_query_time=3.000000 max_connections 数据库最大连接数，系统支持的最大连接 1234567891011121314151617181920212223 mysql&gt; show status like 'Threads%';+-------------------+-------+| Variable_name | Value |+-------------------+-------+| Threads_cached | 58 || Threads_connected | 57 | ###这个数值指的是打开的连接数| Threads_created | 3676 || Threads_running | 4 | ###这个数值指的是激活的连接数，这个数值一般远低于connected数值+-------------------+-------+ Threads_connected 跟show processlist结果相同，表示当前连接数。准确的来说，Threads_running是代表当前并发数 这是是查询数据库当前设置的最大连接数mysql&gt; show variables like '%max_connections%';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| max_connections | 1000 |+-----------------+-------+ 可以在/etc/my.cnf里面设置数据库的最大连接数[mysqld]max_connections = 1000 设置 1max_connections=2000 max_user_connections用户能最大连接进来的数量 1max_user_connection=600 max_connect_errors阅读延伸： https://www.cnblogs.com/kerrycode/p/8405862.html If more than this many successive connection requests from a host are interrupted without a successful connection, the server blocks that host from further connections. You can unblock blocked hosts by flushing the host cache. To do so, issue a FLUSH HOSTS statement or execute a mysqladmin flush-hosts command. If a connection is established successfully within fewer than max_connect_errors attempts after a previous connection was interrupted, the error count for the host is cleared to zero. However, once a host is blocked, flushing the host cache is the only way to unblock it. The default is 100. 如上所示，翻译出来的话，大致如下：如果MySQL服务器连续接收到了来自于同一个主机的请求，而且这些连续的请求全部都没有成功的建立连接就被中断了，当这些连续的请求的累计值大于max_connect_errors的设定值时，MySQL服务器就会阻止这台主机后续的所有请求。相信一开始你看到这些资料，也会被“many successive connection requests from a host are interrupted without a successful connection”给弄懵，其实这个就是因为由于网络异常而中止数据库连接 设置 1max_connect_errors=2000 max_allowed_packet mysql根据配置文件会限制server接受的数据包大小。 有时候大的插入和更新会受max_allowed_packet 参数限制，导致写入或者更新失败。 12345678&gt; mysql&gt; show variables like 'max_allowed_packet';&gt; +--------------------+---------+&gt; | Variable_name | Value |&gt; +--------------------+---------+&gt; | max_allowed_packet | 4194304 |&gt; +--------------------+---------+&gt; 1 row in set (0.00 sec)&gt; 设置 1max_allowed_packet=16777216 max_digest_length ？？？net_buffer_length net_buffer_length选项对数据备份及恢复影响net-buffer-length 可能对mysqldump导出及恢复有影响，对此测试了一下，发现影响很小，基本可以忽略不计每个客户端连接时。用于维持连接缓冲，初始分配预设值，在有需要时，则会自动扩大到max_allowed_packet大小，然后在回收预设的net_buffer_length大小 最小1k 最大1m默认16k 设置 1net_buffer_length=8192 query_cache_size延伸阅读：https://blog.csdn.net/u014044812/article/details/78924315 1、原理MySQL查询缓存保存查询返回的完整结果。当查询命中该缓存，会立刻返回结果，跳过了解析，优化和执行阶段。查询缓存会跟踪查询中涉及的每个表，如果这写表发生变化，那么和这个表相关的所有缓存都将失效。但是随着服务器功能的强大，查询缓存也可能成为整个服务器的资源竞争单点。2、初步设置默认这个开关是关闭的，就是禁止使用query_cache，查询是否使用语句如下： 1234567891011121314151617181920&gt; mysql&gt; show variables like 'have_query_cache'; &gt; +------------------+-------+&gt; | Variable_name | Value |&gt; +------------------+-------+&gt; | have_query_cache | YES |&gt; +------------------+-------+&gt; 1 row in set (0.00 sec)&gt; &gt; mysql&gt; show variables like 'query_cache%';&gt; +------------------------------+---------+&gt; | Variable_name | Value |&gt; +------------------------------+---------+&gt; | query_cache_limit | 1048576 |&gt; | query_cache_min_res_unit | 4096 |&gt; | query_cache_size | 1048576 |&gt; | query_cache_type | OFF |&gt; | query_cache_wlock_invalidate | OFF |&gt; +------------------------------+---------+&gt; 5 rows in set (0.00 sec)&gt; 注意这个只是显示，支持query_cache功能而已，默认是关闭的 3、MYSQL如何分配query_cache_sizeMySQL用于查询的缓存的内存被分成一个个变长数据块，用来存储类型，大小，数据等信息。当服务器启动的时候，会初始化缓存需要的内存，是一个完整的空闲块。当查询结果需要缓存的时候，先从空闲块中申请一个数据块大于参数query_cache_min_res_unit的配置，即使缓存数据很小，申请数据块也是这个，因为查询开始返回结果的时候就分配空间，此时无法预知结果多大。分配内存块需要先锁住空间块，所以操作很慢，MySQL会尽量避免这个操作，选择尽可能小的内存块，如果不够，继续申请，如果存储完时有空余则释放多余的。4、如何判断是否命中缓存存放在一个引用表中，通过一个哈希值引用，这个哈希值包括查询本身，数据库，客户端协议的版本等，任何字符上的不同，例如空格，注释都会导致缓存不命中。当查询中有一些不确定的数据时，是不会缓存的，比方说now(),current_date(),自定义函数，存储函数，用户变量，字查询等。所以这样的查询也就不会命中缓存，但是还会去检测缓存的，因为查询缓存在解析SQL之前，所以MySQL并不知道查询中是否包含该类函数，只是不缓存，自然不会命中。 打开Qcache对读和写都会带来额外的消耗：a、读查询开始之前必须检查是否命中缓存。b、如果读查询可以缓存，那么执行完之后会写入缓存。c、当向某个表写入数据的时候，必须将这个表所有的缓存设置为失效，如果缓存空间很大，则消耗也会很大，可能使系统僵死一段时间，因为这个操作是靠全局锁操作来保护的。对InnoDB表，当修改一个表时，设置了缓存失效，但是多版本特性会暂时将这修改对其他事务屏蔽，在这个事务提交之前，所有查询都无法使用缓存，直到这个事务被提交，所以长时间的事务，会大大降低查询缓存的命中 一个表可以被许多类型的语句更改，例如INSERT、UPDATE、DELETE、TRUNCATE、ALTER TABLE、DROP TABLE或DROP DATABASE。对于InnoDB而言，事物的一些特性还会限制查询缓存的使用。当在事物A中修改了B表时，因为在事物提交之前，对B表的修改对其他的事物而言是不可见的。为了保证缓存结果的正确性，InnoDB采取的措施让所有涉及到该B表的查询在事物A提交之前是不可缓存的。如果A事物长时间运行，会严重影响查询缓存的命中率查询缓存的空间不要设置的太大。因为查询缓存是靠一个全局锁操作保护的，如果查询缓存配置的内存比较大且里面存放了大量的查询结果，当查询缓存失效的时候，会长时间的持有这个全局锁。因为查询缓存的命中检测操作以及缓存失效检测也都依赖这个全局锁，所以可能会导致系统僵死的情况 5、 123456789101112131415&gt; mysql&gt; show status like 'Qcache%'; &gt; +-------------------------+---------+&gt; | Variable_name | Value |&gt; +-------------------------+---------+&gt; | Qcache_free_blocks | 1 |&gt; | Qcache_free_memory | 1031352 |&gt; | Qcache_hits | 0 |&gt; | Qcache_inserts | 0 |&gt; | Qcache_lowmem_prunes | 0 |&gt; | Qcache_not_cached | 7 |&gt; | Qcache_queries_in_cache | 0 |&gt; | Qcache_total_blocks | 1 |&gt; +-------------------------+---------+&gt; 8 rows in set (0.00 sec)&gt; 解析：Qcache_free_blocks:表示查询缓存中目前还有多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内存碎片过多了，可能在一定的时间进行整理。 减少碎片：合适的query_cache_min_res_unit可以减少碎片，这个参数最合适的大小和应用程序查询结果的平均大小直接相关，可以通过内存实际消耗（query_cache_size - Qcache_free_memory）除以Qcache_queries_in_cache计算平均缓存大小。可以通过Qcache_free_blocks来观察碎片，这个值反应了剩余的空闲块，如果这个值很多，但是Qcache_lowmem_prunes却不断增加，则说明碎片太多了。可以使用flush query cache整理碎片，重新排序，但不会清空，清空命令是reset query cache。整理碎片期间，查询缓存无法被访问，可能导致服务器僵死一段时间，所以查询缓存不宜太大。Qcache_free_memory:查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，是多了，还是不够用，DBA可以根据实际情况做出调整。Qcache_hits:表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越理想。Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行查询处理后把结果insert到查询缓存中。这样的情况的次 数，次数越多，表示查询缓存应用到的比较少，效果也就不理想。当然系统刚启动后，查询缓存是空的， 这很正常。Qcache_lowmem_prunes:该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的调整缓存大小。Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。Qcache_queries_in_cache:当前缓存中缓存的查询数量。Qcache_total_blocks:当前缓存的block数量。 设置 12query_cache_size=0query_cache_type=OFF 提高查询缓存的使用率：如果碎片不是问题，命中率却非常低，可能是内存不足，可以通过 Qcache_free_memory 参数来查看没有使用的内存。如果2者都没有问题，命中率依然很低，那么说明缓存不适合你的当前系统。可以通过设置query_cache_size = 0或者query_cache_type 来关闭查询缓存。 read_buffer_size key_buffer_size + (read_buffer_size + sort_buffer_size)*max_connections = 458624 K read_buffer_size：是MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。 read_buffer_size变量控制这一缓冲区的大小。如果对表的顺序扫描请求非常频繁，并且你认为频繁扫描进行得太慢，可以通过增加该变量值以及内存缓冲区大小提高其性能。 read_rnd_buffer_size MySQL手册里关于read_rnd_buffer_size的解释如下： [ mrr order by ] sort后，得到的是行数据指针，通过key-value的形式存在，对于MyISAM是数据的偏移量，对于innodb是主键或存储重新查询的全量数据（对于小片的数据是有益的）。 假设sort后的数据使用的是行指针，并且行中的字段能够被转换成固定的大小（除了BLOB/TEXT字段外），MySQL能够使用read_rnd_buffer_size优化数据读取。 因为sort后的数据是以key-value的形式存在的，使用这些行指针去读取数据，将是以指针数据物理的顺序去读取，很大程度上是随机的方式读取数据的。MySQL从 sort_buffer中读取这些行指针数据，然后通过指针排序后存入read_rnd_buffer中，之后再通过指针读取数据时，基本上都是顺序读取了。 read_rnd_buffer_size是很重要的参数，尤其工作在如下场景： sort_buffer中存的是行指针而不是要查询的数据。 查询的字段中包含Blob/Text字段。 sort后有大量的数据行（limit 10并不能帮助你，因为MySQL是通过指针获取行数据的） 如果你取出很少字段的数据（小于max_length_for_sort_data）,行数据将会全部存储在sort buffer里，因此将不需要read_rnd_buffer_size这个参数。 而如果你查询的字段数据很长（这些字段很可能含有Text/Blob字段），比max_length_for_sort_data还长，read_rnd_buffer_size这个参数将派上用场。 table_open_cache table_open_cache指定表高速缓存的大小。每当MySQL访问一个表时，如果在表缓冲区中还有空间，该表就被打开并放入其中，这样可以更快地访问表内容。 通过检查峰值时间的状态值Open_tables和Opened_tables，可以决定是否需要增加table_open_cache的值。 如果你发现open_tables等于table_open_cache，并且opened_tables在不断增长，那么你就需要增加table_open_cache的值了(上述状态值可通过SHOW GLOBAL STATUS LIKE ‘Open%tables’获得)。 注意，不能盲目地把table_open_cache设置成很大的值，设置太大超过了shell的文件描述符(通过ulimit -n查看)，造成文件描述符不足，从而造成性能不稳定或者连接失败。 修改方式： 123vi /etc/my.cnf[mysqld]table_open_cache=2000 table_definition_cache延伸阅读： http://ju.outofmemory.cn/entry/332959 来理解一下 table_open_cache到底是来干嘛的，文档里或者网上的文章，通通解释是“用于控制MySQL Server能同时打开表的最大个数”。如果继续问这个个数怎么算呢？ 我来尝试解答一下。MySQL是多线程的，多个会话上有可能会同时访问同一个表，mysql是允许这些会话各自独立的打开这个表，而表最终都是磁盘上的数据文件。(默认假设innodb_file_per_table=1)，打开文件需要获取文件描述符(File Descriptor)，为了加快这个open table的速度，MySQL在Server层设计了这个cache： The idea behind this cache is that most statements don’t need to go to a central table definition cache to get a TABLE object and therefore don’t need to lock LOCK_open mutex. Instead they only need to go to one Table_cache instance (the specific instance is determined by thread id) and only lock the mutex protecting this cache. DDL statements that need to remove all TABLE objects from all caches need to lock mutexes for all Table_cache instances, but they are rare. table_cache 减少了表级别 LOCK_open 这个互斥量的获取，改用获取 表对象缓存实例 列表的mutex。简化成如下过程： 假设当前并发200个连接，table_open_cache=200，其中有50连接都在访问同一张表 mysql内部维护了一个 unused_table_list，在a表上的请求结束后，会把这个thread刚才用过的 table object 放入unused_table_list 每个表有个key，可以通过hash快速定位到表a的所有可用object，如果后面一下子100个连接上来访问表a，内部会先从 unused_table_list 去找这个表已经缓存过的对象(get_table)，比如前50个可以直接拿来用(unlink_unused_table) 后50个则需要调用系统内核，拿到文件描述符。 用完之后会，放回到unused_table_list，并将这个表的key放到hash表的前面。 如果缓存的对象个数超过了 table_open_cache，则会通过LRU算法，把认为不用的表对象逐出。 从上面的过程应该很容易理解 table_open_cache 与 table_definition_cache 的区别。 table_defintion_cache也是一个key/value形式的hash表，但每个表只有一个值，值/对象的内容就是表的元数据信息(Data Dictionay，frm文件里面的信息)，如表结构、字段、索引，它是一个全局的结构，并且不占用文件描述符。 而 table_open_cache的key/value的值是一个列表，表示这个表的多个 Table_cache_element，他们共用这个表的 definition (代码层定义为TABLE_SHARE对象)。 (注：我们在row格式的binlog里面看到的 table_map_id 就是在 TABLE_SHARE 里面定义的，表结构变更、缓存被逐出，都会导致 table_map_id 递增。) thread_cache_size thread_cache_size：当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁(前提是缓存数未达上限) 即可以重新利用保存在缓存中线程的数量，当断开连接时如果缓存中还有空间，那么客户端的线程将被放到缓存中，如果线程重新被请求，那么请求将从缓存中读取，如果缓存中是空的或者是新的请求，那么这个线程将被重新创建，如果有很多新的线程，增加这个值可以改善系统性能。 thread_cache_size大小的设置： 如果是短连接，适当设置大一点，因为短连接往往需要不停创建，不停销毁，如果大一点，连接线程都处于取用状态，不需要重新创建和销毁，所以对性能肯定是比较大的提升。对于长连接，不能保证连接的稳定性，所以设置这参数还是有一定必要，可能连接池的问题，会导致连接数据库的不稳定性，也会出现频繁的创建和销毁，但这个情况比较少，如果是长连接，可以设置成小一点，一般在50-100左右。 设置 1thread_cache_size=50 show variables like ‘thread_cache_size’; performance_schema1234567891011121314151617181920212223242526272829303132333435363738mysql&gt; show variables like 'performance_schema%'; +--------------------------------------------------------+-------+| Variable_name | Value |+--------------------------------------------------------+-------+| performance_schema | ON || performance_schema_accounts_size | 100 || performance_schema_digests_size | 10000 || performance_schema_events_stages_history_long_size | 10000 || performance_schema_events_stages_history_size | 10 || performance_schema_events_statements_history_long_size | 10000 || performance_schema_events_statements_history_size | 10 || performance_schema_events_waits_history_long_size | 10000 || performance_schema_events_waits_history_size | 10 || performance_schema_hosts_size | 100 || performance_schema_max_cond_classes | 80 || performance_schema_max_cond_instances | 3504 || performance_schema_max_digest_length | 1024 || performance_schema_max_file_classes | 50 || performance_schema_max_file_handles | 32768 || performance_schema_max_file_instances | 7693 || performance_schema_max_mutex_classes | 200 || performance_schema_max_mutex_instances | 15906 || performance_schema_max_rwlock_classes | 40 || performance_schema_max_rwlock_instances | 9102 || performance_schema_max_socket_classes | 10 || performance_schema_max_socket_instances | 322 || performance_schema_max_stage_classes | 150 || performance_schema_max_statement_classes | 168 || performance_schema_max_table_handles | 4000 || performance_schema_max_table_instances | 12500 || performance_schema_max_thread_classes | 50 || performance_schema_max_thread_instances | 402 || performance_schema_session_connect_attrs_size | 512 || performance_schema_setup_actors_size | 100 || performance_schema_setup_objects_size | 100 || performance_schema_users_size | 100 |+--------------------------------------------------------+-------+32 rows in set (0.00 sec) 修改 12performance_schema=OFFperformance_schema_max_table_instances=200 按硬件优化一、connection相关二、timeout相关1、参数概览 show variables like ‘%timeout%’ 12345678910111213connect_timeout=5 # 连接超时wait_timeout=28800 # 关闭闲置连接前等待的秒数interactive_timeout=28800 # 关闭交互式连接前等待的秒数net_read_timeout=30 # 读数据的时的等待超时net_write_timeout=60 # 等待将一个block发送给客户端的超时delayed_insert_timeout=300 # 这是为MyISAM INSERT DELAY设计的超时参数，在INSERT DELAY中止前等待INSERT语句的时间。innodb_flush_log_at_timeout=1 # 这个是5.6中才出现的，是InnoDB特有的参数，每次日志刷新时间。innodb_lock_wait_timeout=50 # 没有死锁的情况下一个事务持有另一个事务需要的锁资源，被回滚的肯定是请求锁的那个Queryinnodb_rollback_on_timeout=OFF # 这个参数关闭或不存在的话遇到超时只回滚事务最后一个Query，打开的话事务遇到超时就回滚整个事务lock_wait_timeout=31536000rpl_stop_slave_timeout=31536000 # 控制stop slave 的执行时间，在重放一个大的事务的时候,突然执行stop slave,命令 stop slave会执行很久,这个时候可能产生死锁或阻塞,严重影响性能，mysql 5.6可以通过rpl_stop_slave_timeout参数控制stop slave 的执行时间slave_net_timeout=3600 # 这是Slave判断主机是否挂掉的超时设置，在设定时间内依然没有获取到Master的回应就认为Master挂掉了 参考：http://www.ttlsa.com/mysql/research-and-measurement-of-timeout-mysql/ 2、connect_timeout=5 这个比较好理解，字面上看意思是连接超时。”The number of seconds that the mysqld server waits for a connect packet before responding with Bad handshake”。 MySQL连接一次连接需求经过6次“握手”方可成功，任意一次“握手”失败都有可能导致连接失败，如下图所示。 前三次握手可以简单理解为TCP建立连接所必须的三次握手，MySQL无法控制，更多的受制于不TCP协议的不同实现，后面的三次握手过程超时与connect_timeout有关。简单的测试方法： 123456789101112&gt; e telnet mysql_ip_addr port&gt; $ time telnet 127.0.0.1 5051&gt; Trying 127.0.0.1...&gt; Connected to 127.0.0.1.&gt; Escape character is '^]'.&gt; ?&gt; Connection closed by foreign&gt; host.&gt; real 0m5.005s #这里的5秒即mysql默认的连接超时&gt; user 0m0.000s&gt; sys 0m0.000s&gt; Telnet未退出前通过show processlist查看各线程状态可见，当前该连接处于授权认证阶段，此时的用户为“unauthenticated user”。细心的你懂得，千万不要干坏事。 3、wait_timeout=1800 MySQL客户端的数据库连接闲置最大时间值。当MySQL连接闲置超过一定时间后将会被强行关闭。 MySQL默认的wait-timeout 值为8个小时，可以通过命令show variables like ‘wait_timeout’查看结果值。 interactive_timeout：服务器关闭交互式连接前等待活动的秒数。 wait_timeout:服务器关闭非交互连接之前等待活动的秒数。 这两个参数必须配合使用。否则单独设置wait_timeout无效。 等待超时，那mysql等什么呢？确切的说是mysql在等用户的请求(query)，如果发现一个线程已经sleep的时间超过wait_timeout了那么这个线程将被清理掉，无论是交换模式或者是非交换模式都以此值为准。 注意：wait_timeout是session级别的变量哦，至于session和global变量的区别是什么我不说您也知道。手册上不是明明说wait_timeout为not interactive模式下的超时么？为什么你说无论是交换模式或者非交换模式都以此值为准呢？简单的测试例子如下 那为什么手册上说在交互模式下使用的是interactive_timeout呢，原因如下： check_connection函数在建立连接初期，如果为交互模式则将interactive_timeout值赋给wait_timeout，骗您误以为交互模式下等待超时为interactive_timeout 代码如下: 1234&gt; if (thd-&gt;client_capabilities &amp; CLIENT_INTERACTIVE) &#123;&gt; thd-&gt;variables.net_wait_timeout=thd-&gt;variables.net_interactive_timeout;&gt; &#125;&gt; 4、interactive_timeout上面说了那么多，这里就不再多做解释了。我理解mysql之所以多提供一个目的是提供给用户更灵活的设置空间。 5、net_write_timeout=60 看到这儿如果您看累了，那下面您得提提神了，接下来的两个参数才是我们遇到的最大的难题。 “The number of seconds to wait for a block to be written to a connection before aborting the write.” 等待将一个block发送给客户端的超时，一般在网络条件比较差的时，或者客户端处理每个block耗时比较长时，由于net_write_timeout导致的连接中断很容易发生。下面是一个模拟的例子： 6、net_read_timeout=30 “The number of seconds to wait fprintfor more data from a connection before aborting the read.”。Mysql读数据的时的等待超时，可能的原因可能为网络异常或客户端or服务器端忙无法及时发送或接收处理包。这里我用的是iptables来模拟网络异常，生成一个较大的数据以便于给我充足的时间在load data的过程中去配置iptables规则。 执行完iptables命令后show processlist可以看到load data的连接已经被中断掉了 7、net_retry_count “超时”的孪生兄弟“重试”，时间原因这个我没有进行实际的测试，手册如是说，估且先信它一回。 If a read or write on a communication port is interrupted, retry this many times before giving up. This value should be set quite high on FreeBSD because internal interrupts are sent to all threads. On Linux, the “NO_ALARM” build flag (-DNO_ALARM) modifies how the binary treats both net_read_timeout and net_write_timeout. With this flag enabled, neither timer cancels the current statement until after the failing connection has been waited on an additional net_retry_count times. This means that the effective timeout value becomes” (timeout setting) × (net_retry_count+1)”. FreeBSD中有效，Linux中只有在build的时候指定NO_ALARM参数时net_retry_count才会起作用。 说明：目前线上使用的版本都未指定NO_ALARM。 8、innodb_lock_wait_timeout The timeout in seconds an InnoDB transaction may wait for a row lock before giving up. The default value is 50 seconds. A transaction that tries to access a row that is locked by another InnoDB transaction will hang for at most this many seconds before issuing the following error: ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction When a lock wait timeout occurs, the current statement is not executed. The current transaction is not rolled back. (To have the entire transaction roll back, start the server with the –innodb_rollback_on_timeout option, available as of MySQL 5.1.15. See also Section 13.6.12, “InnoDB Error Handling”.) innodb_lock_wait_timeout applies to InnoDB row locks only. A MySQL table lock does not happen inside InnoDB and this timeout does not apply to waits for table locks. InnoDB does detect transaction deadlocks in its own lock table immediately and rolls back one transaction. The lock wait timeout value does not apply to such a wait. For the built-in InnoDB, this variable can be set only at server startup. For InnoDB Plugin, it can be set at startup or changed at runtime, and has both global and session values. 解释：描述很长，简而言之，就是事务遇到锁等待时的Query超时时间。跟死锁不一样，InnoDB一旦检测到死锁立刻就会回滚代价小的那个事务，锁等待是没有死锁的情况下一个事务持有另一个事务需要的锁资源，被回滚的肯定是请求锁的那个Query。 9、innodb_rollback_on_timeout In MySQL 5.1, InnoDB rolls back only the last statement on a transaction timeout by default. If –innodb_rollback_on_timeout is specified, a transaction timeout causes InnoDB to abort and roll back the entire transaction (the same behavior as in MySQL 4.1). This variable was added in MySQL 5.1.15. 解释：这个参数关闭或不存在的话遇到超时只回滚事务最后一个Query，打开的话事务遇到超时就回滚整个事务。 10、innodb_flush_log_at_timeout master线程刷写日志的频率。可以增大此参数设置来减少刷写次数，避免对binlog group commit带来影响。默认值是1 四、buffer相关参数概览 show variables like ‘%buffer%’ 123456789101112131415161718192021222324bulk_insert_buffer_size=8388608 # MyISAM，用来缓存批量插入数据的时候临时缓存写入数据innodb_buffer_pool_dump_at_shutdown=OFFinnodb_buffer_pool_dump_now=OFFinnodb_buffer_pool_filename=ib_buffer_poolinnodb_buffer_pool_instances=8innodb_buffer_pool_load_abort=OFFinnodb_buffer_pool_load_at_startup=OFFinnodb_buffer_pool_load_now=OFFinnodb_buffer_pool_size=134217728innodb_change_buffer_max_size=25innodb_change_buffering=allinnodb_log_buffer_size=8388608innodb_sort_buffer_size=1048576join_buffer_size=262144 # 每个线程使用连接缓存区大小，最好是添加适当的索引而不是纯粹加大 join_buffer_sizekey_buffer_size=8388608 # MyISAM，索引块的缓冲区大小myisam_sort_buffer_size=8388608 # MyISAM，排序的缓冲区大小net_buffer_length=16384 # net-buffer-length 可能对mysqldump导出及恢复有影响preload_buffer_size=32768 # 预加载索引时分配的缓冲区大小read_buffer_size=131072 # 对表进行顺序扫描的请求将分配一个读入缓冲区read_rnd_buffer_size=262144 # 当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区sort_buffer_size=262144 # 每个线程排序缓存区的大小sql_buffer_result=OFF # 通过SQL_BUFFER_RESULT.选项强制将结果集放到临时表, 释放表锁 key_buffer_size 用于索引块的缓冲区大小，增加它可得到更好处理的索引(对所有读和多重写)，对MyISAM表性能影响最大的一个参数。 如果太大，系统将开始换页并且真的变慢了。 严格说是它决定了数据库索引处理的速度，尤其是索引读的速度。 对于内存在4GB左右的服务器该参数可设置为256M或384M。 如何判断key_buffer_size的设置是否合理？检查状态值Key_read_requests和Key_reads，key_reads / key_read_requests应该尽可能的低，比如1:100，1:1000 ，1:10000。其值可以用以下命令查得：show status like ‘key_read%’; read_buffer_size read_buffer_size 是 控制 MySql读入缓冲区大小。磁盘 -&gt; 内存 的缓冲区。 对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。 如果对表的顺序扫描请求非常频繁，并且你认为频繁扫描进行得太慢，可以通过增加该变量值以及内存缓冲区大小提高其性能. read_rnd_buffer_size read_rnd_buffer_size 是MySql的随机读缓冲区大小。 当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。 进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。 但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 sort_buffer_size sort_buffer_size是MySql执行排序使用的缓冲大小。 如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。 如果不能，可以尝试增加sort_buffer_size变量的大小 tmp_table_size tmp_table_size是MySql的heap （堆积）表缓冲大小。所有联合在一个DML指令内完成，并且大多数联合甚至可以不用临时表即可以完成。大多数临时表是基于内存的(HEAP)表。具有大的记录长度的临时表 (所有列的长度的和)或包含BLOB列的表存储在硬盘上。如果某个内部heap（堆积）表大小超过tmp_table_size，MySQL可以根据需要自动将内存中的heap表改为基于硬盘的MyISAM表。还可以通过设置tmp_table_size选项来增加临时表的大小。也就是说，如果调高该值，MySql同时将增加heap表的大小，可达到提高联接查询 join 速度的效果。 join_buffer_size 应用经常会出现一些两表（或多表）join的操作需求，MySQL在完成某些join需求的时候（all row join/all index /scan join）为了减少参与join的“被驱动表”的读取次数以提高性能，需要使用到join buffer来协助完成join操作当join buffer 太小，MySQL不会将该buffer存入磁盘文件而是先将join buffer中的结果与需求join的表进行操作，然后清空join buffer中的数据，继续将剩余的结果集写入次buffer中，如此往复，这势必会造成被驱动表需要被多次读取，成倍增加IO访问，降低效率（执行计划中如果现实using join buffer） 两个表关联的时候 减少参与被驱动表的join操作（没办法有效利用索引的时候） 多表join时，就需要用到join buffer的三种情况 All row join do not user indexes nad thus perform full table scans（没有索引的全表扫描） All index join plain index scans（普通索引扫描） Range index scan join=rangeindex scans（范围索引扫描） 最好是添加适当的索引而不是纯粹加大 join_buffer_size 任何来个表间的全表join就会分配一次join_buffer，也就是说，如果3个表join就会分配2次joinbuffer（而不是一个session只分配一次） sql_buffer_result SELECT SQL_BUFFER_RESULT * FROM TABLE1 WHERE … 当我们查询的结果集中的数据比较多时，可以通过SQL_BUFFER_RESULT.选项强制将结果集放到临时表中，这样就可以很快地释放MySQL的表锁(这样其它的SQL语句就可以对这些记录进行查询了)，并且可以长时间地为客户端提供大记录集。 innodb_buffer_pool_* MySQL InnoDB buffer pool 里包含什么？ 数据缓存：InnoDB数据页面 索引缓存：索引数据 缓冲数据：脏页（在内存中修改尚未刷新(写入)到磁盘的数据） 内部结构：如自适应哈希索引，行锁等。 1&gt;.mysqld重启之后，innodb_buffer_pool几乎是空的，没有任何的缓存数据。随着sql语句的执行，table中的数据以及index 逐渐被填充到buffer pool里面，之后的查询语句只需要在内存中操作（理想状态下），大幅度提升了mysql的性能。 这个逐渐填充的过程可能需要1-2个小时，甚至更久也说不准。在此过程中，mysql性能一般，因为需要大量的硬盘读操作 2&gt;.innodb在内存中维护一个缓冲池用来缓存数据和索引，缓存池管理一个数据块列表，该列表又分为2个字列，一个子列存放new blocks，另一个子列存放old blocks。old blocks默认占整个列大小的3/8（可通过innodb_old_blocks_pct改变默认值，该值范围在5-95之间，这是一个百分比），其余大小为new blocks占用 手工导出的话，可以用这个命令： 12&gt; mysql&gt; SET innodb_buffer_pool_dump_now=ON;&gt; 然后mysql会在innodb的数据目录中生成一个文件：ib_buffer_pool 关闭mysql的时候，自动导出： 12&gt; mysql&gt; SET innodb_buffer_pool_dump_at_shutdown=ON;&gt; 在my.cnf中加上 innodb_buffer_pool_load_at_startup=ON 就会在mysqld启动之后，重新加载buffer pool。 3&gt;.innodb_buffer_pool_instances：主要用于将innodb_buffer_pool进行划分，通过划分innodb_buffer_pool为多个实例，可以提高并发能力，并且减少了不同线程读写造成的缓冲页;但是不必设置过大，该值对mysql性能提升不大，有待测试； innodb_buffer_pool_size innodb_buffer_pool_size主要针对InnoDB表性能影响最大的一个参数。功能与Key_buffer_size一样。 ​ 另外InnoDB和 MyISAM 存储引擎不同。MyISAM 的 key_buffer_size 只能缓存索引键，而 innodb_buffer_pool_size 却可以缓存数据块和索引键。适当的增加这个参数的大小，可以有效的减少 InnoDB 类型的表的磁盘 I/O 。 show status like ‘Innodb_buffer_pool_read%’; 通过(innodb_buffer_pool_read_requests – innodb_buffer_pool_reads) / innodb_buffer_pool_read_requests * 100% 计算缓存命中率。并根据命中率来调整 innodb_buffer_pool_size 参数大小进行优化， 命中率越高越好。 物理内存争用可能导致操作系统频繁的paging InnoDB为缓冲区和control structures保留了额外的内存，因此总分配空间比指定的缓冲池大小大约大10％。 缓冲池的地址空间必须是连续的，这在带有在特定地址加载的DLL的Windows系统上可能是一个问题。 初始化缓冲池的时间大致与其大小成比例。在具有大缓冲池的实例上，初始化时间可能很长。要减少初始化时间，可以在服务器关闭时保存缓冲池状态，并在服务器启动时将其还原。 innodb_buffer_pool_dump_pct：指定每个缓冲池最近使用的页面读取和转储的百分比。 范围是1到100。默认值是25。例如，如果有4个缓冲池，每个缓冲池有100个page，并且innodb_buffer_pool_dump_pct设置为25，则dump每个缓冲池中最近使用的25个page。 innodb_buffer_pool_dump_at_shutdown：默认启用。指定在MySQL服务器关闭时是否记录在InnoDB缓冲池中缓存的页面，以便在下次重新启动时缩短预热过程。 innodb_buffer_pool_load_at_startup：默认启用。指定在MySQL服务器启动时，InnoDB缓冲池通过加载之前保存的相同页面自动预热。 通常与innodb_buffer_pool_dump_at_shutdown结合使用。 增大或减小缓冲池大小时，将以chunk的形式执行操作。chunk大小由innodb_buffer_pool_chunk_size配置选项定义，默认值为128 MB。 缓冲池大小必须始终等于或者是innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances的倍数。如果将缓冲池大小更改为不等于或等于innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances的倍数的值，则缓冲池大小将自动调整为等于或者是innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances的倍数的值。 innodb_buffer_pool_size可以动态设置，允许在不重新启动服务器的情况下调整缓冲池的大小。 可以通过状态变量Innodb_buffer_pool_resize_status报告在线调整缓冲池大小操作的状态。 五、cache相关 show variables like ‘%cache%’ 1234567891011121314151617181920binlog_cache_size=32768binlog_stmt_cache_size=32768have_query_cache=YEShost_cache_size=279innodb_disable_sort_file_cache=OFFinnodb_ft_cache_size=8000000innodb_ft_result_cache_limit=2000000000innodb_ft_total_cache_size=640000000key_cache_age_threshold=300key_cache_block_size=1024key_cache_division_limit=100max_binlog_cache_size=18446744073709547520max_binlog_stmt_cache_size=18446744073709547520metadata_locks_cache_size=1024stored_program_cache=256table_definition_cache=1400table_open_cache=2000table_open_cache_instances=1thread_cache_size=9 六、thread相关 show variables like ‘%thread%’ 123456789101112131415innodb_purge_threads=1innodb_read_io_threads=4innodb_thread_concurrency=0innodb_thread_sleep_delay=10000innodb_write_io_threads=4max_delayed_threads=20max_insert_delayed_threads=20myisam_repair_threads=1performance_schema_max_thread_classes=50performance_schema_max_thread_instances=402pseudo_thread_id=5thread_cache_size=9thread_concurrency=10 # CPU核数的2倍,对性能影响很大thread_handling=one-thread-per-connectionthread_stack=262144 thread_concurrency thread_concurrency的值的正确与否, 对mysql的性能影响很大。在多个cpu(或多核)的情况下，错误设置了thread_concurrency的值, 会导致mysql不能充分利用多cpu(或多核), 出现同一时刻只能一个cpu(或核)在工作的情况。 thread_concurrency应设为CPU核数的2倍. 比如有一个双核的CPU, 那thread_concurrency 的应该为4; 2个双核的cpu, thread_concurrency的值应为8. 查看系统当前thread_concurrency默认配置命令： show variables like ‘thread_concurrency’; 七、table相关 show variables like ‘%table%’ 123456789101112131415161718big_tables=OFFinnodb_file_per_table=ONinnodb_ft_aux_table=innodb_ft_server_stopword_table=innodb_ft_user_stopword_table=innodb_table_locks=ONinnodb_undo_tablespaces=0lower_case_table_names=0max_heap_table_size=16777216max_tmp_tables=32old_alter_table=OFFperformance_schema_max_table_handles=4000performance_schema_max_table_instances=12500table_definition_cache=1400table_open_cache=2000table_open_cache_instances=1tmp_table_size=16777216updatable_views_with_limit=YES a 十、InnoDB相关1、参数概览 show variables like ‘%innodb%’ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123ignore_builtin_innodb=OFFinnodb_adaptive_flushing=ONinnodb_adaptive_flushing_lwm=10innodb_adaptive_hash_index=ONinnodb_adaptive_max_sleep_delay=150000innodb_additional_mem_pool_size=8388608innodb_api_bk_commit_interval=5innodb_api_disable_rowlock=OFFinnodb_api_enable_binlog=OFFinnodb_api_enable_mdl=OFFinnodb_api_trx_level=0innodb_autoextend_increment=64innodb_autoinc_lock_mode=1innodb_buffer_pool_dump_at_shutdown=OFFinnodb_buffer_pool_dump_now=OFFinnodb_buffer_pool_filename=ib_buffer_poolinnodb_buffer_pool_instances=8innodb_buffer_pool_load_abort=OFFinnodb_buffer_pool_load_at_startup=OFFinnodb_buffer_pool_load_now=OFFinnodb_buffer_pool_size=134217728innodb_change_buffer_max_size=25innodb_change_buffering=allinnodb_checksum_algorithm=innodbinnodb_checksums=ONinnodb_cmp_per_index_enabled=OFFinnodb_commit_concurrency=0innodb_compression_failure_threshold_pct=5innodb_compression_level=6innodb_compression_pad_pct_max=50innodb_concurrency_tickets=5000innodb_data_file_path=ibdata1:12M:autoextend # 要注释掉，否则会创建一个新的把原来的替换的 innodb_data_home_dir=innodb_disable_sort_file_cache=OFFinnodb_doublewrite=ONinnodb_fast_shutdown=1innodb_file_format=Antelopeinnodb_file_format_check=ONinnodb_file_format_max=Antelopeinnodb_file_per_table=ONinnodb_flush_log_at_timeout=1innodb_flush_log_at_trx_commit=1innodb_flush_method=innodb_flush_neighbors=1innodb_flushing_avg_loops=30innodb_force_load_corrupted=OFFinnodb_force_recovery=0innodb_ft_aux_table=innodb_ft_cache_size=8000000innodb_ft_enable_diag_print=OFFinnodb_ft_enable_stopword=ONinnodb_ft_max_token_size=84innodb_ft_min_token_size=3innodb_ft_num_word_optimize=2000innodb_ft_result_cache_limit=2000000000innodb_ft_server_stopword_table=innodb_ft_sort_pll_degree=2innodb_ft_total_cache_size=640000000innodb_ft_user_stopword_table=innodb_io_capacity=200innodb_io_capacity_max=2000innodb_large_prefix=OFFinnodb_lock_wait_timeout=50innodb_locks_unsafe_for_binlog=OFFinnodb_log_buffer_size=8388608innodb_log_compressed_pages=ONinnodb_log_file_size=50331648innodb_log_files_in_group=2innodb_log_group_home_dir=./innodb_lru_scan_depth=1024innodb_max_dirty_pages_pct=75innodb_max_dirty_pages_pct_lwm=0innodb_max_purge_lag=0innodb_max_purge_lag_delay=0innodb_mirrored_log_groups=1innodb_monitor_disable=innodb_monitor_enable=innodb_monitor_reset=innodb_monitor_reset_all=innodb_numa_interleave=OFFinnodb_old_blocks_pct=37innodb_old_blocks_time=1000innodb_online_alter_log_max_size=134217728innodb_open_files=2000innodb_optimize_fulltext_only=OFFinnodb_page_size=16384innodb_print_all_deadlocks=OFFinnodb_purge_batch_size=300innodb_purge_threads=1innodb_random_read_ahead=OFFinnodb_read_ahead_threshold=56innodb_read_io_threads=4innodb_read_only=OFFinnodb_replication_delay=0innodb_rollback_on_timeout=OFFinnodb_rollback_segments=128innodb_sort_buffer_size=1048576innodb_spin_wait_delay=6innodb_stats_auto_recalc=ONinnodb_stats_include_delete_marked=OFFinnodb_stats_method=nulls_equalinnodb_stats_on_metadata=OFFinnodb_stats_persistent=ONinnodb_stats_persistent_sample_pages=20innodb_stats_sample_pages=8innodb_stats_transient_sample_pages=8innodb_status_output=OFFinnodb_status_output_locks=OFFinnodb_strict_mode=OFFinnodb_support_xa=ONinnodb_sync_array_size=1innodb_sync_spin_loops=30innodb_table_locks=ONinnodb_thread_concurrency=0innodb_thread_sleep_delay=10000innodb_tmpdir=innodb_undo_directory=.innodb_undo_logs=128innodb_undo_tablespaces=0innodb_use_native_aio=ONinnodb_use_sys_malloc=ONinnodb_version=5.6.45innodb_write_io_threads=4 2、default-storage-engine=InnoDB1234567891011121314备注：设置完后把以下几个开启：# Uncomment the following if you are using InnoDB tablesinnodb_data_home_dir =/usr/local/mysql#innodb_data_file_path = ibdata1:1024M;ibdata2:10M:autoextend（要注释掉，否则会创建一个新的把原来的替换的。）innodb_log_group_home_dir = /usr/local/mysql# You can set .._buffer_pool_size up to 50 - 80 %# of RAM but beware of setting memory usage too highinnodb_buffer_pool_size = 1000Minnodb_additional_mem_pool_size = 20M# Set .._log_file_size to 25 % of buffer pool sizeinnodb_log_file_size = 500Minnodb_log_buffer_size = 20Minnodb_flush_log_at_trx_commit = 0innodb_lock_wait_timeout = 50 3、innodb_additional_mem_pool_size innodb_additional_mem_pool_size 设置了InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，所以当MySQL Instance中的数据库对象非常多的时候，是需要适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率的。 这个参数大小是否足够还是比较容易知道的，因为当过小的时候，MySQL会记录Warning信息到数据库的error log中，这时候就该调整这个参数大小了。 查看当前系统mysql的error日志 cat /var/lib/mysql/机器名.error 发现有很多waring警告。所以要调大为20M。根据MySQL手册，对于2G内存的机器，推荐值是20M。 32G内存的 100M 4、innodb_log_buffer_size innodb_log_buffer_size 这是InnoDB存储引擎的事务日志所使用的缓冲区。类似于Binlog Buffer，InnoDB在写事务日志的时候，为了提高性能，也是先将信息写入Innofb Log Buffer中，当满足innodb_flush_log_trx_commit 参数所设置的相应条件(或者日志缓冲区写满)之后，才会将日志写到文件 (或者同步到磁盘)中。可以通过innodb_log_buffer_size 参数设置其可以使用的最大内存空间。 InnoDB 将日志写入日志磁盘文件前的缓冲大小。理想值为 1M 至 8M。大的日志缓冲允许事务运行时不需要将日志保存入磁盘而只到事务被提交(commit)。因此，如果有大的事务处理，设置大的日志缓冲可以减少磁盘I/O。 在 my.cnf中以数字格式设置。 默认是8MB，系的如频繁的系统可适当增大至4MB～8MB。当然如上面介绍所说，这个参数实际上还和另外的flush参数相关。一般来说不建议超过32MB Creating index ‘GEN_CLUST_INDEX’ required more than ‘innodb_online_alter_log_max_size’ bytes of modification log. Please try again. 虽然optimize对innodb表没什么用，但是仍然会抛出该错误 那么查看一下手册，可以得知： innodb_online_alter_log_max_size控制在用于在Online DDL操作时的一个临时的日志文件的上限值大小。 该临时的日志文件存储了在DDL时间内，dml操作的记录。这个临时的日志文件依照innodb_sort_buffer_size的值做扩展。 如果该日志超过了innodb_online_alter_log_max_size的最大上限，DDL操作则会抛出失败，并且回滚所有未提交的DML操作。 反过来说，该值如果设置更高，则可以允许在做Online DDL时，有更多的DML操作发生。 但因此带来的问题就是，在DDL做完之后，需要更多时间来锁表和应用这些日志。 另外对于某些DDL操作，比如ADD INDEX/COLUMN，则可以通过调整innodb_sort_buffer_size的大小来加快操作速度。但是实际上分配的内存为3倍的innodb_sort_buffer_size值。 innodb_online_alter_log_max_size和innodb_sort_buffer_size均为5.6 Online DDL的新参数。 【解决方案】： 知道这个参数控制的是什么东西，就好解决了。临时调大该值，此处改成了256MB： mysql&gt; SET GLOBAL innodb_online_alter_log_max_size=25610241024;Query OK, 0 rows affected (0.03 sec) 该值默认为128MB，还是建议在做完DDL之后再将其改为默认值，也就是134217728。 十一、Master-Slave相关1、参数概览1234567gtid_mode=onenforce_gtid_consistency=onlog_slave_updates=1slave_parallel_type=logical_clockslave_parallel_workers=8innodb_flush_log_at_trx_commit=1sync_binlog=1 #每一个transaction commit都会调用一次fsync()，此时能保证数据最安全但是性能影响较大。 十二、net相关 show variables like ‘%net%’ 123456net_buffer_length=16384net_read_timeout=30net_retry_count=10net_write_timeout=60skip_networking=OFF # 开启该选项可以彻底关闭MySQL的TCP/IP连接方式。slave_net_timeout=3600 skip-name-resolve skip-name-resolve：禁止MySQL对外部连接进行DNS解析。 使用这一选项可以消除MySQL进行DNS解析的时间。 但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ skip-networking 建议被注释掉，不要开启 开启该选项可以彻底关闭MySQL的TCP/IP连接方式。 如果WEB服务器是以远程连接的方式访问MySQL数据库服务器则不要开启该选项，否则将无法正常连接。 十三、lock相关 show variables like ‘%lock%’ 1234567891011121314151617181920212223block_encryption_mode=aes-128-ecbinnodb_api_disable_rowlock=OFFinnodb_autoinc_lock_mode=1innodb_lock_wait_timeout=50innodb_locks_unsafe_for_binlog=OFFinnodb_old_blocks_pct=37innodb_old_blocks_time=1000innodb_print_all_deadlocks=OFFinnodb_status_output_locks=OFFinnodb_table_locks=ONkey_cache_block_size=1024lock_wait_timeout=31536000locked_in_memory=OFFmax_write_lock_count=18446744073709551615metadata_locks_cache_size=1024metadata_locks_hash_instances=8performance_schema_max_rwlock_classes=40performance_schema_max_rwlock_instances=9102query_alloc_block_size=8192query_cache_wlock_invalidate=OFFrange_alloc_block_size=4096skip_external_locking=ONtransaction_alloc_block_size=8192 十四、transaction 相关 show variables like ‘%transaction%’ 12345binlog_direct_non_transactional_updates=OFFslave_transaction_retries=10transaction_alloc_block_size=8192transaction_allow_batching=OFFtransaction_prealloc_size=4096 show variables like ‘%trx%’ 12innodb_api_trx_level=0innodb_flush_log_at_trx_commit=1 按功能优化查询优化sql_cache意思是说，查询的时候使用缓存。 sql_no_cache意思是查询的时候不适用缓存。 sql_buffer_result意思是说，在查询语句中，将查询结果缓存到临时表中。 这三者正好配套使用。sql_buffer_result将尽快释放表锁，这样其他sql就能够尽快执行。 使用 FLUSH QUERY CACHE 命令，你可以整理查询缓存，以更好的利用它的内存。这个命令不会从缓存中移除任何查询。FLUSH TABLES 会转储清除查询缓存。RESET QUERY CACHE 使命从查询缓存中移除所有的查询结果。 -————————————————- 那么mysql到底是怎么决定到底要不要把查询结果放到查询缓存中呢？ 是根据query_cache_type这个变量来决定的。 这个变量有三个取值：0,1,2，分别代表了off、on、demand。 意思是说，如果是0，那么query cache 是关闭的。如果是1，那么查询总是先到查询缓存中查找，除非使用了sql_no_cache。如果是2，那么，只有使用 sql_cache的查询，才会去查询缓存中查找。 三、index索引相关1、参数概览 show variables like ‘%key%’ 123456789delay_key_write=ONforeign_key_checks=ONhave_rtree_keys=YESkey_buffer_size=8388608key_cache_age_threshold=300key_cache_block_size=1024key_cache_division_limit=100max_seeks_for_key=18446744073709551615ssl_key= 八、sort相关1、参数概览 show variables like ‘%sort%’ 12345678innodb_disable_sort_file_cache=OFFinnodb_ft_sort_pll_degree=2innodb_sort_buffer_size=1048576max_length_for_sort_data=1024max_sort_length=1024myisam_max_sort_file_size=9223372036853727232myisam_sort_buffer_size=8388608sort_buffer_size=262144 九、log 相关1、参数概览 show variables like ‘%log%’ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061back_log=80binlog_cache_size=32768binlog_checksum=CRC32binlog_direct_non_transactional_updates=OFFbinlog_error_action=IGNORE_ERRORbinlog_format=STATEMENTbinlog_gtid_simple_recovery=OFFbinlog_max_flush_queue_time=0binlog_order_commits=ONbinlog_row_image=FULLbinlog_rows_query_log_events=OFFbinlog_stmt_cache_size=32768binlogging_impossible_mode=IGNORE_ERRORexpire_logs_days=0general_log=OFFgeneral_log_file=/var/lib/mysql/d27fc289bc0c.loginnodb_api_enable_binlog=OFFinnodb_flush_log_at_timeout=1innodb_flush_log_at_trx_commit=1innodb_locks_unsafe_for_binlog=OFFinnodb_log_buffer_size=8388608innodb_log_compressed_pages=ONinnodb_log_file_size=50331648innodb_log_files_in_group=2innodb_log_group_home_dir=./innodb_mirrored_log_groups=1innodb_online_alter_log_max_size=134217728innodb_undo_logs=128log_bin=OFFlog_bin_basename=log_bin_index=log_bin_trust_function_creators=OFFlog_bin_use_v1_row_events=OFFlog_error=log_output=FILElog_queries_not_using_indexes=OFFlog_slave_updates=OFFlog_slow_admin_statements=OFFlog_slow_slave_statements=OFFlog_throttle_queries_not_using_indexes=0log_warnings=1max_binlog_cache_size=18446744073709547520max_binlog_size=1073741824max_binlog_stmt_cache_size=18446744073709547520max_relay_log_size=0relay_log=relay_log_basename=relay_log_index=relay_log_info_file=relay-log.inforelay_log_info_repository=FILErelay_log_purge=ONrelay_log_recovery=OFFrelay_log_space_limit=0simplified_binlog_gtid_recovery=OFFslow_query_log=OFFslow_query_log_file=/var/lib/mysql/d27fc289bc0c-slow.logsql_log_bin=ONsql_log_off=OFFsync_binlog=0sync_relay_log=10000sync_relay_log_info=10000 undo_logredo_logbin_log]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 实现CPU亲和性_未完]]></title>
    <url>%2Fjava%2Fcpu%2Fjava-cpu-affinity%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>cpu亲和性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j2源码——架构设计]]></title>
    <url>%2Fjava%2Flog4j2%2Flog4j2-srcode%2F</url>
    <content type="text"><![CDATA[从github导入idea下载地址： https://github.com/apache/logging-log4j2.git]]></content>
      <tags>
        <tag>disk-io</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka Q&A]]></title>
    <url>%2Fmessage-queue%2Fkafka%2Fkafka-qa%2F</url>
    <content type="text"><![CDATA[kafka多个partition 之间怎么保证 消息全局顺序性]]></content>
  </entry>
  <entry>
    <title><![CDATA[flink checkpoint 分类]]></title>
    <url>%2Fflink%2Fflink-checkpoint%2F</url>
    <content type="text"><![CDATA[Keyed StateOperator State增量 checkpoint持久化方式]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>checkpoint</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink 从mysql抽取写入kafka和文本文件]]></title>
    <url>%2Fflink%2Fflink-mysql-to-kafka%2F</url>
    <content type="text"><![CDATA[一、需求说明登录日志放在mysql里 ==&gt; 现在需要将 mysql中的数据 转换格式以后写入kafka ==&gt; 数据中心 从 kafka里取数据进行 日志分析和监管审查； 首先考虑有flume读取数据，但是在做格式转换时，需要重写 source 和 interceptor，得不偿失，还不如用flink。正好 阿里发布了全新的 flink 1.9，尝鲜使用； 二、环境配置 flink-1.9.0（The only requirements are working Maven 3.0.4 (or higher) and Java 8.x installations.） mysql-5.6 kafka_2.11-0.11 1、安装 jdk 1.8+ 2、下载启动flink-1.9 下载地址： https://flink.apache.org/zh/downloads.html 说明文档： https://ci.apache.org/projects/flink/flink-docs-release-1.9/getting-started/tutorials/local_setup.html 1234567891011wget http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.9.1/flink-1.9.1-bin-scala_2.11.tgztar zxf flink-1.9.1-bin-scala_2.11.tgzcd flink-1.9.1# 启动flink(Standalone形式)/data/flink/flink-1.9.1/bin/start-cluster.sh# 停止flink/data/flink/flink-1.9.1/bin/stop-cluster.sh开启端口firewall-cmd --permanent --add-port=8081/tcpfirewall-cmd --reloadfirewall-cmd --query-port 8081/tcp 验证启动成功： jps -l netstat -nltp | grep 8081 访问管理页面： http://172.18.1.51:8081 三、创建 Flink工程官网文档：https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/projectsetup/java_api_quickstart.html#requirements 12curl https://flink.apache.org/q/quickstart.sh | bash -s 1.9.0mv quickstart/ flink-log-process 改造后工程结构如下： 1、定义 Mysql Datasource1.1、在上述工程的 pom.xml 中添加 mysql 依赖 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt;&lt;/dependency&gt; 1.2、定义 CCustLoginHisSource 读取 mysql 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.jzsec.tzdslog;import org.apache.commons.collections.IteratorUtils;import org.apache.flink.api.common.state.ListState;import org.apache.flink.api.common.state.ListStateDescriptor;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.configuration.Configuration;import org.apache.flink.runtime.state.FunctionInitializationContext;import org.apache.flink.runtime.state.FunctionSnapshotContext;import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;import org.apache.flink.streaming.api.functions.source.RichSourceFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.util.List;public class CCustLoginHisSource extends RichSourceFunction&lt;CCustLoginHisEntity&gt; implements CheckpointedFunction &#123; private PreparedStatement ps; private Connection connection; private String url, username, password; private int year; private Date beginDate, endDate; public CCustLoginHisSource(String url, String username, String password, LocalDate localDate) &#123; this.url = url; this.username = username; this.password = password; year = localDate.getYear(); beginDate = new java.sql.Date(localDate.atStartOfDay().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli()); endDate = new java.sql.Date(localDate.plusDays(1).atStartOfDay().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli()); &#125; private Connection getConnection() &#123; Connection conn = null; try &#123; Class.forName("com.mysql.jdbc.Driver"); conn = DriverManager.getConnection(url, username, password); &#125; catch (Exception e) &#123; System.out.println("-----------mysql getConnection() has exception , msg = " + e.getMessage()); &#125; return conn; &#125; @Override public void open(Configuration parameters) throws Exception &#123; // 增量抽取数据 super.open(parameters); connection = getConnection(); String sql = "select * from db.tablename_" + year + " where create_time between ? and ?;"; ps = this.connection.prepareStatement(sql); ps.setDate(1, beginDate); ps.setDate(2, endDate); &#125; @Override public void close() throws Exception &#123; super.close(); if (null != connection) &#123; connection.close(); &#125; if (null != ps) &#123; ps.close(); &#125; &#125; @Override public void cancel() &#123;&#125; @Override public void run(SourceContext&lt;CCustLoginHisEntity&gt; sourceContext) throws Exception &#123; ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) &#123; CCustLoginHisEntity entity = new CCustLoginHisEntity(); entity.setId(resultSet.getInt("id")); entity.setEnv_type(resultSet.getString("env_type")); entity.setApp_type(resultSet.getString("app_type")); entity.setUser_ip(resultSet.getString("user_ip")); entity.setUser_agent(resultSet.getString("user_agent")); entity.setIp_addr(resultSet.getString("ip_addr")); entity.setMac(resultSet.getString("mac")); entity.setPhone(resultSet.getString("phone")); entity.setImei(resultSet.getString("imei")); entity.setRequest_time(resultSet.getDate("request_time")); entity.setCreate_time(resultSet.getDate("create_time")); lastIndex = entity.getId(); sourceContext.collect(entity); &#125; &#125;&#125; 1.3、写入kafka 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;import java.util.concurrent.ExecutionException;import java.util.concurrent.atomic.AtomicInteger;public class SinkToKafka011 extends RichSinkFunction&lt;String&gt; &#123; private Properties props; private String topic; private KafkaProducer&lt;Integer, byte[]&gt; producerB; private AtomicInteger messageNo = new AtomicInteger(0); public SinkToKafka011(Properties props, String topic) &#123; this.props = props; this.topic = topic; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); producerB = new KafkaProducer(props); &#125; @Override public void invoke(String value, Context context) &#123; try &#123; producerB.send(new ProducerRecord( topic, messageNo.getAndIncrement(), value.getBytes())) .get(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void close() throws Exception &#123; if (null != producerB) producerB.close(); super.close(); &#125;&#125; 1.4、写入文本文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.io.BufferedWriter;import java.io.File;import java.io.FileOutputStream;import java.io.OutputStreamWriter;public class SinkToTextFile extends RichSinkFunction&lt;String&gt; &#123; private String filepath; private String filename; private FileOutputStream fos; private OutputStreamWriter osw; private BufferedWriter bufferedWriter; public SinkToTextFile(String filepath, String filename) &#123; this.filepath = filepath; this.filename = filename; File dir = new File(filepath); if (!dir.isDirectory()) &#123; dir.mkdir(); &#125; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); File file = new File(filepath); if (!file.isDirectory()) &#123; file.mkdir(); &#125; fos = new FileOutputStream(new File(filepath + "/" + filename), true); osw = new OutputStreamWriter(fos); bufferedWriter = new BufferedWriter(osw); &#125; @Override public void close() throws Exception &#123; if (bufferedWriter != null) bufferedWriter.close(); if (osw != null) osw.close(); if (fos != null) fos.close(); super.close(); &#125; @Override public void invoke(String value, Context context) throws Exception &#123; bufferedWriter.write(value); &#125;&#125; 1.5、定义Job 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import org.apache.commons.lang3.StringUtils;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.kafka.clients.producer.ProducerConfig;import java.text.ParseException;import java.text.SimpleDateFormat;import java.time.LocalDate;import java.util.*;import java.util.regex.Matcher;import java.util.regex.Pattern;public class TzdsLoginLogProcessJob &#123; private static ObjectMapper mapper = new ObjectMapper(); public static void main(String[] args) throws Exception &#123; final ParameterTool argsParamTool = ParameterTool.fromArgs(args); // --logdate 2019-10-15 String logdate = argsParamTool.get("logdate", "2019-10-15"); final String propertiesFile = argsParamTool.get("conf", "conf.properties"); final ParameterTool propParamTool = ParameterTool.fromPropertiesFile(propertiesFile); // 创建上下文 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(propParamTool); DataStreamSource&lt;CCustLoginHisEntity&gt; dataStreamSource = env.addSource( new CCustLoginHisSource( propParamTool.get("mysql.jdbc.url"), propParamTool.get("mysql.jdbc.username"), propParamTool.get("mysql.jdbc.password"), LocalDate.parse(logdate)) ); SingleOutputStreamOperator&lt;String&gt; logStream = dataStreamSource.map(entity -&gt; &#123; try &#123; LogModel log = new LogModel(); log.setAppType("accessLog"); log.setCid("0x000000000001"); log.setMid("0x000000000002"); log.setName("tagname"); log.setIp(entity.getUser_ip()); log.setIndicator(entity2Map(entity)); log.setTime_stamp(Long.toString(entity.getRequest_time().getTime())); return JSONObject.toJSONString(log) + "\n"; &#125; catch (Exception e) &#123; return ""; &#125; &#125;); SingleOutputStreamOperator&lt;String&gt; unifyLogStream = dataStreamSource.map(entity -&gt; &#123; try &#123; Map unifyLog = new HashMap(); initLoginLog(unifyLog); procLoginLog(unifyLog, entity2Map(entity), Long.toString(entity.getRequest_time().getTime())); return JSONObject.toJSONString(unifyLog) + "\n"; &#125; catch (Exception e) &#123; return ""; &#125; &#125;); // 写入日志 logStream.addSink(new SinkToTextFile( propParamTool.get("log.filepath") + "/" + logdate, "login_log_" + logdate.replace("-", "") + ".log" )); // 写入kafka Properties kafkaProps = new Properties(); kafkaProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, propParamTool.get("kafka.bootstrap.servers")); kafkaProps.put("client.id", propParamTool.get("kafka.client.id")); kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.IntegerSerializer"); kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.ByteArraySerializer"); String topic = propParamTool.get("kafka.topic.login-log"); unifyLogStream.addSink(new SinkToKafka011(kafkaProps, topic)).name("sink-to-kafka").setParallelism(1); env.execute(); &#125;&#125; 2、编译 mvn clean package]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume自定义拦截器 + mysql增量同步到kafka]]></title>
    <url>%2Flog-platform%2Fflume-custom-interceptor%2F</url>
    <content type="text"><![CDATA[一、搭建可运行的flume环境0、下载 apache-flume-1.9.0-bin 1、下载 mysql 依赖包1、下载 mysql-connector-java-5.1.18.jar 放在 apache-flume-1.9.0-bin\lib 下 2、下载 flume-ng-sql-source https://github.com/keedio/flume-ng-sql-source/releases mvn package 把生成的 flume-ng-sql-source-1.5.2.jar 放在 apache-flume-1.9.0-bin\lib 下 2、创建kafka topic 1./kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic tzds-login 3、配置 flume新建 conf/mysql2kafka.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849a1.channels=ch-1a1.sources=src-1a1.sinks=k1###########sql source################## For each one of the sources, the type is defineda1.sources.src-1.type=org.keedio.flume.source.SQLSourcea1.sources.src-1.hibernate.connection.url=jdbc:mysql://10.10.209.27:3306/crm# Hibernate Database connection propertiesa1.sources.src-1.hibernate.connection.user=crma1.sources.src-1.hibernate.connection.password=crm@2015a1.sources.src-1.hibernate.connection.autocommit=truea1.sources.src-1.hibernate.dialect=org.hibernate.dialect.MySQL5Dialecta1.sources.src-1.hibernate.connection.driver_class=com.mysql.jdbc.Drivera1.sources.src-1.run.query.delay=10000a1.sources.src-1.status.file.path=./a1.sources.src-1.status.file.name=sqlsource.status#sqlsource.status文件中记录了增量字段的值 $@$# Custom query=&quot;你的SQL语句&quot;a1.sources.src-1.start.from=0a1.sources.src-1.custom.query=select * from crm.c_cust_login_his_2019 where id &gt; $@$ order by id asca1.sources.src-1.batch.size=1000a1.sources.src-1.max.rows=1000a1.sources.src-1.hibernate.connection.provider_class=org.hibernate.connection.C3P0ConnectionProvidera1.sources.src-1.hibernate.c3p0.min_size=1a1.sources.src-1.hibernate.c3p0.max_size=10a1.sources.src-1.interceptors=i1a1.sources.src-1.interceptors.i1.type=com.jzsec.flume.interceptor.TzdsLoginBuilder##############################a1.channels.ch-1.type=memorya1.channels.ch-1.capacity=10000a1.channels.ch-1.transactionCapacity=10000a1.channels.ch-1.byteCapacityBufferPercentage=20a1.channels.ch-1.byteCapacity=800000#Kafka sink配置 a1.sinks.k1.topic=你的topic名字a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.topic=tzds-logina1.sinks.k1.brokerList=172.18.1.51:9092a1.sinks.k1.requiredAcks=1a1.sinks.k1.batchSize=20# combination a1.sources.src-1.channels不要漏掉sa1.sinks.k1.channel=ch-1a1.sources.src-1.channels=ch-1 4、启动flume agentlinux: 1./bin/flume-ng agent -n a1 -c conf -f conf/mysql2kafka.conf -Dflume.root.logger=INFO,console windows： 1bin\flume-ng agent -n a1 -c conf -f conf\mysql2kafka.conf 5、查看 kafka topic1234docker exec -it b6276c6648c2 /bin/bashcd /opt/kafka/bin/./kafka-console-consumer.sh --bootstrap-server 172.18.1.51:9092 --topic tzds-login --from-beginning./kafka-console-consumer.sh --bootstrap-server 172.18.1.51:9092 --topic tzds-login 二、自定义Interceptor流程 搭建flume开发环境（巧妇难为无米之炊，你没开发环境怎么玩，程序都不知道你写的类是个啥） 新建一个类，实现Interceptor接口，重写intercept(Event event)方法 新建一个类，实现Interceptor.Builder接口，重写configure(Context context)和build()方法 打成jar包放到flume的lib目录下 编写相应的flume.conf文件，将type值使用类的全限定名指定我们的拦截器。如果有自定义属性，需要配置该自定义属性。 三、搭建开发环境 1、新建一个maven工程，在 pom.xml 中引入如下依赖：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.jzsec&lt;/groupId&gt; &lt;artifactId&gt;flume-custom-interceptor&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.58&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、自定义 Interceptor 和 Builder12345678910111213141516171819202122232425262728public class TzdsLoginInterceptor implements Interceptor &#123; public void initialize() &#123;&#125; public Event intercept(Event event) &#123; //获得body的内容 Map&lt;String, String&gt; eventHeader = event.getHeaders(); String eventBody = new String(event.getBody(), Charsets.UTF_8); String fmt = "%s, %s"; try &#123; eventBody = procLog(eventBody); &#125; catch (ParseException e) &#123; eventBody = ""; e.printStackTrace(); &#125; event.setBody(String.format(fmt, JSONObject.toJSONString(eventHeader), eventBody).getBytes()); return event; &#125; public List&lt;Event&gt; intercept(List&lt;Event&gt; list) &#123; for (Event event : list) &#123; intercept(event); &#125; return list; &#125; public void close() &#123;&#125;&#125; 1234567891011import org.apache.flume.Context;import org.apache.flume.interceptor.Interceptor;public class TzdsLoginBuilder implements Interceptor.Builder &#123; public void configure(Context context) &#123;&#125; public Interceptor build() &#123; return new TzdsLoginInterceptor(); &#125;&#125; 3、打包部署mvn clean package 将 flume-custom-interceptor-1.0.jar 部署到 apache-flume-1.9.0-bin\lib 下 4、在 mysql2kafka.conf 中配置 interceptors12a1.sources.src-1.interceptors=i1a1.sources.src-1.interceptors.i1.type=com.jzsec.flume.interceptor.TzdsLoginInterceptor 5、启动flume agentlinux: 1./bin/flume-ng agent -n a1 -c conf -f conf/mysql2kafka.conf -Dflume.root.logger=INFO,console windows： 1bin\flume-ng agent -n a1 -c conf -f conf\mysql2kafka.conf 6、查看 kafka topic]]></content>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务、SpringCloud、k8s、Istio杂谈]]></title>
    <url>%2Fmicro-service%2Fsoa-springcloud-k8s-istio%2F</url>
    <content type="text"><![CDATA[一、微服务与SOA​ “微服务”是一个名词，没有这个名词之前也有“微服务”，一个朗朗上口的名词能让大家产生一个认知共识，这对推动一个事务的发展挺重要的，不然你叫微服务他叫小服务的大家很难集中到一个点上。 ​ 业界对微服务与SOA的区别争论比较多大多都是在微观上对比他们的区别什么微服务粒度更细啊、微服务没有ESB啊、微服务通讯相比SOA采用更轻量级的协议啊等等，但是从微观谈区别本身就有悖论， 这些区别只是微服务的一种”最佳实践“而已。我个人理解微服务与SOA灵魂上的不同是 ​ 微服务是互联网时代的产物而SOA是系统集成的产物，微服务是对系统的打散而SOA是对系统的整合。 二、 微服务与SpringCloud​ 因为SpringCloud的流行很多人就把SpringCloud等同于微服务，这也没有错共识的人多了就是对的。准确点说SpringCloud是适合实现微服务的一套基础框架，SpringCloud有助于讯速的落地微服务架构。SpringCloud是以Java库的形式工作所以它的工作层面是在应用层（研发层）。 ​ SpringCloud通过提供一篮子解决方案来应对微服务中的各种需求和通点，通过Eureka提供服务注册与发现，Ribbon实现客户端的负载均衡，Feign牛逼的将REST变成强类型的接口调用，Config提供方便但不灵活的配置中心，Hystrix提供熔断方案，Zuul提供网关方案等。 ​ 优点： ​ 1、提供较全的微服务治理全套解决方案 ​ 2、对开发人员友好（对代码侵入强） ​ 缺点： ​ 1、只能java平台技术栈使用，当然提供了SideCar用于集成异构技术但是限制比较大 ​ 2、对开发人员友好（对代码侵入强） 三、Kubernetes(k8s)​ k8s并不是因为微服务而生而是因为docker而生只是天时地利人和正好赶上了微服务流行的时代，docker的特性正好特别适用于微服务，而k8s进一步对docker方便的编排。 ​ 从基础设施方向来讲k8s可以比作是IDC机房和机房工作人员，对物理服务器（docker）的存放与管理，上机架、装系统、接网络等等。 ​ 从微服务的角度来讲，k8s通过基础设施的方式通过逻辑抽象出service等概念提供了对微服务的另一种实现，就好比用N台电脑联网提供了FTP服务。 ​ 优点： ​ 1、在基础层提供了抽象，对代码无侵入 ​ 缺点： ​ 1、对微服务治理比较弱，如熔断限流等，当然这也不应该是k8s做的。 四、Istio​ Istio的理论概念是Service Mesh（服务网络），我们不必纠结于概念实际也是微服务的一种落地形式有点类似上面的SideCar模式，它的主要思想是关注点分离，即不像SpringCloud一样交给研发来做，也不集成到k8s中产生职责混乱，Istio是通过为服务配 Agent代理来提供服务发现、负截均衡、限流、链路跟踪、鉴权等微服务治理手段。 ​ Istio开始就是与k8s结合设计的，Istio结合k8s可以牛逼的落地微服务架构。 ​ 优点： ​ 1、关注点分离，对代码无侵入 ​ 2、服务治理相关较全面 ​ 缺点： ​ 1、老子学不动了 五、我理想中的微服务架构]]></content>
      <categories>
        <category>micro-service</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java灵魂拷问]]></title>
    <url>%2Fjava%2Finterview%2Fjava-interview%2F</url>
    <content type="text"><![CDATA[Java集合 ArrayList 和 Vector 的区别。 说说 ArrayList,Vector, LinkedList 的存储性能和特性。 快速失败 (fail-fast) 和安全失败 (fail-safe) 的区别是什么？ hashmap 的数据结构。 HashMap 的工作原理是什么? Hashmap 什么时候进行扩容呢？ List、Map、Set 三个接口，存取元素时，各有什么特点？ Set 里的元素是不能重复的，那么用什么方法来区分重复与否呢? 是用 == 还是 equals()? 它们有何区别? 两个对象值相同 (x.equals(y) == true)，但却可有不同的 hash code，这句话对不对? heap 和 stack 有什么区别。 Java 集合类框架的基本接口有哪些？ HashSet 和 TreeSet 有什么区别？ HashSet 的底层实现是什么? LinkedHashMap 的实现原理? 为什么集合类没有实现 Cloneable 和 Serializable 接口？ 什么是迭代器 (Iterator)？ Iterator 和 ListIterator 的区别是什么？ 数组 (Array) 和列表 (ArrayList) 有什么区别？什么时候应该使用 Array 而不是 ArrayList？ Java 集合类框架的最佳实践有哪些？ Set 里的元素是不能重复的，那么用什么方法来区分重复与否呢？是用 == 还是 equals()？它们有何区别？ Comparable 和 Comparator 接口是干什么的？列出它们的区别 Collection 和 Collections 的区别。 JVM与调优 Java 类加载过程？ 描述一下 JVM 加载 Class 文件的原理机制? Java 内存分配。 GC 是什么? 为什么要有 GC？ 简述 Java 垃圾回收机制 如何判断一个对象是否存活？（或者 GC 对象的判定方法） 垃圾回收的优点和原理。并考虑 2 种回收机制 垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？ Java 中会存在内存泄漏吗，请简单描述 深拷贝和浅拷贝。 System.gc() 和 Runtime.gc() 会做什么事情？ finalize() 方法什么时候被调用？析构函数 (finalization) 的目的是什么？ 如果对象的引用被置为 null，垃圾收集器是否会立即释放对象占用的内存？ 什么是分布式垃圾回收（DGC）？它是如何工作的？ 串行（serial）收集器和吞吐量（throughput）收集器的区别是什么？ 在 Java 中，对象什么时候可以被垃圾回收？ 简述 Java 内存分配与回收策率以及 Minor GC 和 Major GC。 JVM 的永久代中会发生垃圾回收么？ Java 中垃圾收集的方法有哪些？ 什么是类加载器，类加载器有哪些？ 类加载器双亲委派模型机制？ 并发编程 Synchronized 用过吗，其原理是什么？ 你刚才提到获取对象的锁，这个“锁”到底是什么？如何确定对象的锁？ 什么是可重入性，为什么说 Synchronized 是可重入锁？ JVM 对 Java 的原生锁做了哪些优化？48 为什么说 Synchronized 是非公平锁？49 什么是锁消除和锁粗化？49 为什么说 Synchronized 是一个悲观锁？乐观锁的实现原理又是什么？什么是 CAS，它有什么特性？ 乐观锁一定就是好的吗？ 跟 Synchronized 相比，可重入锁 ReentrantLock 其实现原理有什么不同？ 那么请谈谈 AQS 框架是怎么回事儿？ 请尽可能详尽地对比下 Synchronized 和 ReentrantLock 的异同。 ReentrantLock 是如何实现可重入性的？ 除了 ReetrantLock，你还接触过 JUC 中的哪些并发工具？ 请谈谈 ReadWriteLock 和 StampedLock。 如何让 Java 的线程彼此同步？你了解过哪些同步器？请分别介绍下。 CyclicBarrier 和 CountDownLatch 看起来很相似，请对比下呢？ Java 线程池相关问题 Java 中的线程池是如何实现的？ 创建线程池的几个核心构造参数？ 线程池中的线程是怎么创建的？是一开始就随着线程池的启动创建好的吗？ 既然提到可以通过配置不同参数创建出不同的线程池，那么 Java 中默认实现好的线程池又有哪些呢？请比较它们的异同 如何在 Java 线程池中提交线程？ 什么是 Java 的内存模型，Java 中各个线程是怎么彼此看到对方的变量的？ 请谈谈 volatile 有什么特点，为什么它能保证变量对所有线程的可见性？ 既然 volatile 能够保证线程间的变量可见性，是不是就意味着基于 volatile 变量的运算就是并发安全的？ 请对比下 volatile 对比 Synchronized 的异同。 请谈谈 ThreadLocal 是怎么解决并发安全的？ 很多人都说要慎用 ThreadLocal，谈谈你的理解，使用 ThreadLocal 需要注意些什么？ spring 什么是 Spring 框架？Spring 框架有哪些主要模块？ 使用 Spring 框架能带来哪些好处？ 什么是控制反转(IOC)？什么是依赖注入？ 请解释下 Spring 框架中的 IoC？ BeanFactory 和 ApplicationContext 有什么区别？ Spring 有几种配置方式？ 如何用基于 XML 配置的方式配置 Spring？ 如何用基于 Java 配置的方式配置 Spring？ 怎样用注解的方式配置 Spring？ 请解释 Spring Bean 的生命周期？ Spring Bean 的作用域之间有什么区别？ 什么是 Spring inner beans？ Spring 框架中的单例 Beans 是线程安全的么？ 请举例说明如何在 Spring 中注入一个 Java Collection？ 如何向 Spring Bean 中注入一个 Java.util.Properties？ 请解释 Spring Bean 的自动装配？ 请解释自动装配模式的区别？ 如何开启基于注解的自动装配？ 请举例解释@Required 注解？ 请举例解释@Autowired 注解？ 请举例说明@Qualifier 注解？ 构造方法注入和设值注入有什么区别？ Spring 框架中有哪些不同类型的事件？ FileSystemResource 和 ClassPathResource 有何区别？ Spring 框架中都用到了哪些设计模式？ 设计模式 请列举出在 JDK 中几个常用的设计模式？ 什么是设计模式？你是否在你的代码里面使用过任何设计模式？ Java 中什么叫单例设计模式？请用 Java 写出线程安全的单例模式 在 Java 中，什么叫观察者设计模式（observer design pattern）？ 使用工厂模式最主要的好处是什么？在哪里使用？ 举一个用 Java 实现的装饰模式(decorator design pattern)？它是作用于对象层次还是类层次？ 在 Java 中，为什么不允许从静态方法中访问非静态变量？ 设计一个 ATM 机，请说出你的设计思路？ 在 Java 中，什么时候用重载，什么时候用重写？ 举例说明什么情况下会更倾向于使用抽象类而不是接口 springboot 什么是 Spring Boot？ Spring Boot 有哪些优点？ 什么是 JavaConfig？ 如何重新加载 Spring Boot 上的更改，而无需重新启动服务器？ Spring Boot 中的监视器是什么？ 如何在 Spring Boot 中禁用 Actuator 端点安全性？ 如何在自定义端口上运行 Spring Boot 应用程序？ 什么是 YAML？ 如何实现 Spring Boot 应用程序的安全性？ 如何集成 Spring Boot 和 ActiveMQ？ 如何使用 Spring Boot 实现分页和排序？ 什么是 Swagger？你用 Spring Boot 实现了它吗？ 什么是 Spring Profiles？ 什么是 Spring Batch？ 什么是 FreeMarker 模板？ 如何使用 Spring Boot 实现异常处理？ 您使用了哪些 starter maven 依赖项？ 什么是 CSRF 攻击？ 什么是 WebSockets？ 什么是 AOP？ 什么是 Apache Kafka？ 我们如何监视所有 Spring Boot 微服务？ Netty BIO、NIO和AIO的区别？ NIO的组成？ Netty的特点？ Netty的线程模型？ TCP 粘包/拆包的原因及解决方法？ 了解哪几种序列化协议？ 如何选择序列化协议？ Netty的零拷贝实现？ Netty的高性能表现在哪些方面？ NIOEventLoopGroup源码？ Redis 什么是redis? Reids的特点 Redis支持的数据类型 Redis是单进程单线程的 虚拟内存 Redis锁 读写分离模型 数据分片模型 Redis的回收策略 使用Redis有哪些好处？ redis相比memcached有哪些优势？4 redis常见性能问题和解决方案 MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据245 Memcache与Redis的区别都有哪些？ Redis 常见的性能问题都有哪些？如何解决？ Redis 最适合的场景]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[退避算法]]></title>
    <url>%2Falgorithm%2Fbackoff-algorighm%2F</url>
    <content type="text"><![CDATA[CSMA/CD采用二进制指数退避算法，又称为二元指数后退算法。退避算法是以冲突窗口大小为基准的，每个节点有一个冲突计数器C。退避的时间与冲突次数具有指数关系，冲突次数越多，退避的时间就可能越长，若达到限定的冲突次数，该节点就停止发送数据。 定义二进制退避技术（Binary Exponential Back off）. 指在遇到重复的冲突时，站点将重复传输，但在每一次冲突之后，随着时延的平均值将加倍。二进制指数退避算法提供了一个处理重负荷的方法。尝试传输的重复失败导致更长的退避时间，这将有助于负荷的平滑。如果没有这样的退避，以下状况可能发生：两个或多站点同时尝试传输，这将导致冲突，之后这些站点又立即尝试重传，导致一个新冲突。 应用在CSMA/CD协议中，一旦检测到冲突，为降低再冲突的概率，需要等待一个随机时间，然后再使用CSMA方法试图传输。为了保证这种退避维持稳定，采用了二进制指数退避算法的技术，其算法过程如下： 确定基本退避时间，一般为端到端的往返时间为2t，2t也称为冲突窗口或争用期。 定义参数k，k与冲突次数有关，规定k不能超过10，k=Min[冲突次数，10]。在冲突次数大于10，小于16时，k不再增大，一直取值为10。 从离散的整数集合[0,1,2，……，(2^k-1)]中随机的取出一个数r，等待的时延为r倍的基本退避时间，等于r x 2t。r的取值范围与冲突次数k有关，r可选的随机取值为2^k个、这也是称为二进制退避算法的起因。 当冲突次数大于10以后，都是从0—2^10-1个2t中随机选择一个作为等待时间。 当冲突次数超过16次后，发送失败，丢弃传输的帧，发送错误报告。 举例 如果第二次发生碰撞： n = 2 k = MIN(2,10) = 2 R = {0, 1, 2, 3) 延迟时间 = R * 512 * Bit-time 其中：Bit-time = 1 / Debit 例如： 对于传输速率Debit = 10 Mbit/s, 那么Bit-time = 0.1 us 延迟时间={0, 51.2 us, 102.4 us, 153.6 us} 其中任取一]]></content>
  </entry>
  <entry>
    <title><![CDATA[分布式计算的八大谬论]]></title>
    <url>%2Fdistribution%2Fthe-eight-fallacies-of-distributed-computing%2F</url>
    <content type="text"><![CDATA[英文原文：The Eight Fallacies of Distributed Computing 关于分布式计算有八大谬论，这是20 世纪90 年代在Sun MicroSystem 中被首次提出的，但在此之前就已经为人所熟知了。随着时间的推移，这些谬论已经被IT 从业人员所遗忘了，所以有必要提醒一下大家。它们分别是： 网络是可靠的 延迟为零 带宽是无限的 网络是安全的 拓扑不会改变 只存在一个管理员 传输代价为零 网络是同质的 我个人经历过一些大型项目，一些高级管理层会做出以上错误的假设，最终导致了灾难性的后果。许多应用设计者对他们使用的网络充满信心，但现实却往往很打脸 某些人可能认为现在的网络比他们很久之前首次接触分布式系统的时候要更快速、更便宜，所以他们认为上面的这些假设不再是一种谬论，因为技术已经把这些假设实现了。但是，尽管有市场的营销宣传，网络的现实仍然困扰着基于以上谬论设计分布式系统的人 分布式系统可以在专门设计和指定的网络中很好的工作，并受到严格的控制和监控，但分布式系统真实运行所在的生产环境可能无法满足这些要求！为什么呢？请继续看！ 网络是可靠的在局域网中，网络可能看起来坚如磐石。毕竟，现在哪还有一个网络组件经常宕机呢？而且即使一个单一的组件出现故障，还会有很多的冗余，确实是这样吗？随着网络环境越来越复杂，网络管理员很有可能犯错，尤其是在配置上。某些情况下，多达1/3 的网络更改会导致网络的可靠性故障。软件和硬件都可能出现故障，尤其是路由器，它占到所有故障的1/4 左右。“不可中断”的电源供应有可能出现断电、管理员可能进行不明智的网络设备配置更改、可能出现网络阻塞、可能遇到拒绝服务攻击，以及软件和防火墙的更新或布丁失败。网络会因为自然的和非自然的因素导致故障，设计一个能应对这些情况的网络需要很高的技巧。广域网在你的可控范围之外，很容易出错 最近几个月的Azure 事件真的是令人头疼，而且这种故障概率是主要的云服务提供商的典型特征。对于移动应用，所有的环节都可能出现问题：请求将以不可预测的间隔失败、目标不可达、请求到达目的地但返回应答失败、数据在传输过程中损坏或者不完整。移动应用必须在网络的可靠范围内具有弹性，但所有分布式应用程序必须能够应对所有这些可能性，并且网络节点必须能够应对服务器故障 延迟为零延迟和带宽不同，延迟是指花费在等待应答上的时间，原因是什么？除了明显的服务器处理延迟，还有网络延迟，包括传输延迟、节点延迟和阻塞延迟。传输延迟随着距离的增加而增加：在美国和欧洲之间的传输延迟在30ms 左右。传输路径中的节点数（译者注：路由器、网管、代理服务器等）决定了节点延迟 通常开发人员在内网中构建分布式系统，而内网中的延迟并不明显，因此频繁地进行细粒度的网络远程调用几乎不会有什么损失。这种设计错误只有在投入到真实的生产环境中才会暴露出来 高延迟的一个令人不安的影响就是它不是固定的。在一个糟糕的网络中，偶尔会是几秒。就其性质而言，无法保证网络服务单个数据包的顺序，甚至不能保证请求进程仍然存在。延迟会让事情变得更糟。此外，在应用程序通过发送多个同时请求进行补偿的情况下，可以通过对其的响应来加剧暂时高延迟 带宽是无限的虽然大多数现代电缆可以应对无限带宽，但我们还没有找到如何构建足够快的互联设备（集线器、交换机、路由器等）以保证所有连接用户的带宽，典型的企业内部网仍将具有限制带宽的区域 随着公共网络带宽的快速增加，使用网络进行视频和音频服务（而视频和音频服务所使用的是广播技术）的速度也在增加。社交媒体等新用途往往会吸收不断增加的带宽。此外，主要城市以外的许多地方存在“最后一英里”的限制，以及包丢失的可能性越来越大 所以总的来说，我们在假设高带宽是一种通用体验的时候需要谨慎。无论网络带宽如何令人印象深刻，它都无法接近共同托管进程可以通信的速度 网络是安全的奇怪的是，仍然会遇到基于网络的系统，它们具有基本的安全弱点。网络攻击逐年增加，已经远远超出了其原本的好奇心、恶意和犯罪根源，成为国际冲突和政治“行动”的一部分。网络攻击是IT 生活的一部分：对开发人员来说很无聊，但必须要防御。部分问题是网络入侵检测往往是低优先级的，所以我们只是不总是知道成功的网络入侵 传统上，漏洞通常是配置不当的防火墙的结果。大多数防火墙弱点都会经常被检测出来，因为如果你愚蠢地禁用了一个就会立即发现。然而，这只是破坏网络和防火墙的一种方式，只是防御的一部分。Wi-Fi 通常是一个弱点，使用自己的设备（BYOD）可以允许通过受损设备进行入侵，虚拟化和软件定义网络（SDN）也是如此。越来越多的DevOps 对快速变化的基础设施的需求使得更难以保持必要的控制措施。企业网络中的僵尸网络是一个持续存在的问题，以及通过业务合作伙伴的入侵也是如此 你需要假设网络是敌对的，并且安全性必须深入。这意味着要将安全性构建到分布式应用程序及其主机的基本设计中 通过纵深防御，分布式系统的任何部分都需要具有访问其他网络资源的安全方式 安全带来了自身的复杂性。这将来自维护不同用户帐户、权限、证书、帐户等的管理开销。一个主要的云网络故障是由于许可在续签之前过期 拓扑不会改变网络拓扑不断变化，速度非常快。由于“网络敏捷性”的压力越来越大，这是不可避免的，以便与快速变化的业务需求保持同步 无论你在何处部署应用程序，都必须假设大部分网络拓扑都可能无法控制。网络管理员将一次进行更改，原因可能不符合你的利益。他们将移动服务器并更改网络拓扑以获得性能或安全性，并在服务器和网络故障的情况下进行路由更改 因此，依赖特定端点或路由的持久性是错误的。必须始终从任何分布式设计中抽象出网络的物理结构 只存在一个管理员除非系统完全存在于小型LAN 中，否则将有不同的管理员与网络的各种组件相关联。他们将拥有不同程度的专业知识，不同的职责和优先事项 如果出现导致服务失败的问题，这将很重要。你的服务级别协议将要求在一定时间内做出响应。第一阶段将是确定问题。除非有问题的网络部分的管理员是你的开发团队的一部分，否则这可能并不容易。不幸的是，这不太可能。在许多网络中，问题可能完全是另一个组织的责任。如果云组件是应用程序的重要组成部分，而云出现中断，那么在确定优先级时就无能为力了。你所能做的就是等待 如果网络中有许多管理员，那么协调升级到网络或应用程序就更加困难，特别是当涉及到几个忙碌的人时。升级和部署必须协调完成，涉及的人数越多，这就变得越困难！ 传输代价为零传输成本是指通过网络传输数据的总体成本。我们可以参考时间和计算机资源，或者我们可以参考财务成本 将数据从应用程序层传输到传输层需要CPU和其他资源。需要对结构化信息进行序列化（编组）或解析以将数据传输到线路上。这种性能影响可能大于带宽和延迟时间，XML 冗长和复杂性导致其需要的时间是JSON 的两倍 金融运输成本不仅包括创建网络的硬件和安装成本，还包括监控和维护网络服务器，服务和基础设施的成本，以及如果发现带宽不足，或者你的服务器实际上无法处理足够的并发请求。我们还需要考虑租用线路和云服务的成本，这些成本由所使用的带宽支付 网络都是同质的今天的同质网络是罕见的，甚至比首次发现谬论时更为罕见！网络可能连接计算机和其他设备，每个设备具有不同的操作系统，不同的数据传输协议，并且所有设备都与来自各种供应商的网络组件相连 但是，异构网络没有什么特别的错误，除非它涉及需要专门支持，设备或驱动程序的专有数据传输协议。从应用程序的角度来看，如果数据以开放标准格式（如CSV，XML或JSON）传输，并且使用行业标准的查询数据（如ODBC）的方法，则会有很大帮助 如果所有组件都来自一个供应商，则可靠性更高，因为测试覆盖范围可能更大，但实际情况是组件的丰富组合。这意味着互操作性应该从任何分布式系统的设计开始就内置 总结即使有现代的千兆网络，数据在服务器外的传输速度也较慢，而且不那么可靠。这就是为什么我们传统上更喜欢扩展硬件而不是扩展基于网络的商品硬件。问题是，这种偏好僵化成了一条黄金法则。当它被证明可以控制网络到八种谬误更接近现实的程度时，“最佳实践”就可以被推翻，然后分布式模型就变得更有吸引力了。问题是，分布式系统、面向服务的体系结构和微服务只有在网络被驯服了所有的缺点时才有效。即使在今天，云服务的故障或“停机”发生的频率也惊人地高。当你计划或开发分布式应用程序时，在你的网络中假定那些不一定存在的属性和质量是一个坏主意：在计划时，最好假定你的网络将是昂贵的，并且偶尔会是不可靠和不安全的。假设你将面临高延迟、带宽不足和拓扑变化。当你需要更改时，你可能需要与许多管理员联系，以及各种令人眼花缭乱的网络服务器和协议。你可能会感到惊喜，但不要指望它！ 扩展阅读 Microservices: a definition of this new architectural term 微服务(翻译) Google File System MapReduce Bigtable Domain Driven Design(Quickly) 十分钟带你理解Kubernetes核心概念 技术雷达]]></content>
      <categories>
        <category>distribution</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tcpdump 捕获中文传输数据包，并手动解析]]></title>
    <url>%2Fcomputer-network%2Ftcpdump-manual-analysis-network-package%2F</url>
    <content type="text"><![CDATA[开启tcpdump抓包centos7 下 ，只抓有数据的包： 1tcpdump -i eth0 '((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0' and src 10.10.139.42 and dst 10.10.128.57 and port 21000 -XX 示例（1）输入“中国a”centos7 下 netcat 输入： 抓包结果放在 WireShark中分析 tcpdump抓包： wireshark分析抓到的文件： 可以看到 中文的16进制编码为 d6 d0 b9 fa 61 0a， 查看 控制台的编码： 可知，中文经过GBK编码以后的 16进制表示为： d6 d0 b9 fa 61 0a； GBK的编码规则如下： d6d0 b9fa 610a 按双字节表示转换为：D6D0 B9FA 610A GBK对照表：http://ff.163.com/newflyff/gbk-list/ D6D0 对应的中文为： B9FA 对应的中文为： 610A 对应的中文为： 搜索没有对应的GBK编码 ASCII码对照表：http://ascii.911cha.com/ 0A 为换行符； 示例（2）输入“asdf123”输入： 抓包： 解析：]]></content>
      <tags>
        <tag>tcpdump</tag>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcpdump抓包后，wireshark分析]]></title>
    <url>%2Fcomputer-network%2Ftcpdump-and-wireshark%2F</url>
    <content type="text"><![CDATA[来源：https://segmentfault.com/a/1190000015044878?utm_source=tag-newest 因为最近要解析 TCP 报文中 option 段的一块数据，所以不得不详细了解下 TCP/IP 报文。虽然之前看过，很长时间没这么细致地用过，导致了健忘，借着这个机会，通过 tcpdump 抓包分析，详细捋一遍 TCP/IP 报文。 报文获取如果那样干巴巴地讲这个东西比较晕，而且网上的文章一大堆，没有什么创新。我选择换一个角度来切入 TCP/IP 协议。首先通过 tcpdump 准备报文。【1】我在 192.168.1.22 这台机器的 10000 端口启一个 redis 服务。【2】通过 tcpdump 这个工具来抓取数据包，命令如下： 1tcpdump -w /tmp/logs -i eth0 port 10000 -s0 【3】在 192.168.1.26 这台机器上访问 192.168.1.22:10000 这个 redis 实例，可以用 redis-cli 客户端，也可以用 telnet，发送一个 ping, 得到对端回复 pong。【4】停止抓包，用 tcpdump 读取这个数据包（-x 以16进制形式展示，便于后面分析） 1tcpdump -r /tmp/logs -n -nn -A -x| vim - 其中有一个数据包是这样的，这也是这篇文章要分析的: 123410:54:54.270967 IP 192.168.1.26.61096 &gt; 192.168.1.22.10000: Flags [P.], seq 1041414875:1041414889, ack 658186233, win 115, options [nop,nop,TS val 2377448931 ecr 2741547141], length 140x0000: [4560 0042 7567 0000 3d06 6F3C C0A8 011A 0x0010: C0A8 0116] &#123;eea8 2710 3e12 badb 273b 1ff9 0x0020: 8018 0073 64b0 0000 0101 080a 8db4 fde3 0x0030: a368 b085&#125; 2a31 0d0a 2434 0d0a 7069 6e670x0040: 0d0a 注意：【1】之前在文章常用 shell 中介绍过抓包神器 tcpdump，还不会的小伙伴可以偷瞄一眼。【2】上面报文数据中的 [、]、{ 和 } 是为了方便区分数据，我自己加上的。[]包围的部分为本报文中的 IP 头，{}包围的部分为本报文中的 TCP 头。 报文分析IP 报文整体结构如下，因为抓到的数据包是 redis 服务，因此在传输层为 TCP 协议。 IP 层解析解析数据包之前，先把 IP 协议拿出来，如下： 可以看到，IP 报文头部采用固定长度(20B) + 可变长度构成，下面的 TCP 头部也是这样。然后下面对着抓到的数据包进行分析：【1】0x4 4bit， ip 协议版本0x4 表示 IPv4。【2】0x5 4bit，ip首部长度该字段表示单位是32bits(4字节) ，所以这个 ip 包的头部有 5*4=20B，这就可以推出，该 IP 报文头没有可选字段。4bit 可以表示最大的数为 0xF，因此，IP 头部的最大长度为 15*4=60B。该报文的 IP 头部我已经在报文中标注出来了。【3】0x60 8bit，服务类型 TOS该段数据组成为 3bit 优先权字段(现已被忽略) + 4bit TOS 字段 + 1bit 保留字段(须为0)。4bit TOS 字段分别表示自小时延、最大吞吐量、最高可用性和最小费用。只能置其中 1bit，全为 0 表示一般服务。现在大多数的TCP/IP实现都不支持TOS特性 。可以看到，本报文 TOS 字段为全 0。【4】0x0042 16bit， IP 报文总长度单位字节，换算下来，该数据报的长度为 66 字节，数一下上面的报文，恰好 66B。从占位数来算， IP 数据报最长为 2^16=65535B，但大部分网络的链路层 MTU（最大传输单元）没有这么大，一些上层协议或主机也不会接受这么大的，故超长 IP 数据报在传输时会被分片。【5】0x7567 16bit，标识唯一的标识主机发送的每一个数据报。通常每发送一个报文，它的值+1。当 IP 报文分片时，该标识字段值被复制到所有数据分片的标识字段中，使得这些分片在达到最终目的地时可以依照标识字段的内容重新组成原先的数据。【6】0x0000 3bit 标志 + 13bit 片偏移3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8）【7】0x3d 8bit 生存时间TTLIP 报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。本报文该值为 61。【8】0x06 8bit 协议指出 IP 报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。【9】0x6F3C 16bit IP 首部校验和由发送端填充。以本报文为例，先说这个值是怎么计算出来的。 123# 将校验和字段 16bit 值抹去变为 `0x0000`，然后将首部 20字节值相加0x4560 + 0x0042 + 0x7567 + 0x0000 + 0x3d06 + 0x0000 + 0xC0A8 + 0x011A + 0xC0A8 +0x0116 = 0x27B95# 将上述结果的进位 2 与低 16bit 相加0x7B95 + 0x2 = 0x7B97# 0x7B97 按位取反~(0x7B97) = 0x8468 【10】0xC0A8011A 32bit 源地址可以通过一下 python 程序将 hex 转换成我们熟悉的点分 IP 表示法 12345&gt;&gt;&gt; import socket&gt;&gt;&gt; import struct&gt;&gt;&gt; int_ip=int("0xC0A8011A",16)&gt;&gt;&gt; socket.inet_ntoa(struct.pack('I',socket.htonl(int_ip)))'192.168.1.26' 本报文中的 src addr 为 192.168.1.26，恰好就是发起请求的 IP。【11】0xC0A80116 32bit 目的地址经过计算为 192.168.1.22，恰好就是启 redis 服务那台机器的 IP。 由于该报文首部长度为 20B，因此没有可变长部分。 传输层解析本报文携带的数据使用的 TCP 协议，因此下面开始分析 TCP 协议。与上面的 IP 报文一样， TCP 报文头也才用采用固定长度(20B) + 可变长度的形式。首先还是看 TCP 协议的格式，从网上找了一张图，如下： 注： TCP 的头部必须是 4字节的倍数,而大多数选项不是4字节倍数,不足的用 NOP 填充。【1】0xeea8 16bit，源端口解析得到 61096，这与 tcpdump 读包显示的是一致的。16bit 决定了端口号的最大值为 65535.【2】0x2710 16bit，目的端口解析得到 10000。【3】0x273b1ff9 32bit，序号解析得到 1041414875，这与上面 tcpdump 显示的 seq 段是一致的。【4】0x273b1ff9 32bit，确认号解析得到 658186233，这与上面 tcpdump 显示的 ack 段是一致的。【5】0x8 4bit，TCP 报文首部长度也叫 offset，其实也就是数据从哪里开始。8 * 4 = 32B,因此该 TCP 报文的可选部分长度为 32 - 20 = 12B，这个资源还是很紧张的！ 同 IP 头部类似，最大长度为 60B。【6】0b000000 6bit, 保留位保留为今后使用，但目前应置为 0。【7】0b011000 6bit，TCP 标志位上图可以看到，从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。从抓包可以看出，该报文是带了 ack 的，所以 ACK 标志位置为 1。关于标志位的知识这里就不展开了。【8】0x0073 16bit，滑动窗口大小解析得到十进制 115，跟 tcpdump 解析的 win 字段一致。【9】0x64b0 16bit，校验和由发送端填充，接收端对 TCP 报文段执行 CRC 算法，以检验 TCP 报文段在传输过程中是否损坏，如果损坏这丢弃。检验范围包括首部和数据两部分，这也是 TCP 可靠传输的一个重要保障。【10】0x0000 16bit，紧急指针仅在 URG = 1 时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。 下面是 TCP 可选项，其格式如下： 常见的可选项如下图： 【11】0x01NOP 填充，没有 Length 和 Value 字段， 用于将TCP Header的长度补齐至 32bit 的倍数。【12】0x01同上。【13】0x080a可选项类型为时间戳，len为 10B，value 为0x8db4 0xfde3 0xa368 0xb085，加上 0x080a，恰好 10B!启用 Timestamp Option后，该字段包含2 个 32bit 的Timestamp（TSval 和 TSecr）。【14】0x8db4 0xfde3解析后得到 2377448931，恰好与 tcpdump 解析到的 TS 字段的 val一致！【15】0xa368 0xb085解析后得到 2741547141，恰好与 tcpdump 解析到的 TS 字段的 ecr一致！ 数据部分解析上面分析得知，该 IP 报文长度为 66B，IP 头长度为 20B，TCP 头部长度为 32B，因此得到数据的长度为 66 - 20 - 32 = 14B，这与 tcpdump 解析到的 len 字段一致！下面来分析这个具体的数据。这里涉及到 redis 协议，不知道的小伙伴可以查看这篇文档redis 协议说明。在抓包时，用客户端向 redis 服务端发送了一个 ping 命令，转换成 redis 协议如下： 123*1\r\n$4\r\nping\r\n 下面看抓包数据解析，这需要对照 ascii 码表来看，在 linux 下可以用 man 7 ascii 这个命令来获得，或者在这里查看ascii码表。 12340x2a31 -&gt; *10x0d0a -&gt; \r\n0x2434 -&gt; $40x0d0a -&gt; \r\n0x7069 0x6e67 -&gt; ping0x0d0a -&gt; \r\n 既然详细说到 TCP/IP 协议，那补充一下 tcpdump filter 的几点用法。filter可以简单地分为三类：type, dir 和 proto。 type 区分报文的类型，主要由 host（主机）, net（网络，支持 CIDR） 和 port(支持范围，如 portrange 21-23) 组成。dir 区分方向，主要由 src 和 dst 组成。proto 区分协议支持 tcp、udp 、icmp 等。 下面说几个 filter 表达式。proto[x:y] start at offset x into the proto header and read y bytes[x] abbreviation for [x:1]注意：单位是字节，不是位！举几个栗子：【1】打印 80 端口，有数据的 tcp 包 1tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)' ip[2:2] 从 ip 报文的第3个字节开始读2个字节，这个恰好就是 ip 包的总长度，单位是字节ip[0]&amp;0xf 取的是 ip 报文第 1 个字节的低 4 位，&lt;&lt; 2（乘以 4），为 ip 头部长度，单位是字节tcp[12]&amp;0xf0 取的是 tcp 报文第 13 个字节的高 4 位，&gt;&gt; 2 其实等价于 &gt;&gt; 4 然后 &lt;&lt; 2，为 tcp 头部长度，单位是字节。所以 ((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) 表示的数据长度。【2】打印 80 端口，长度超过 576 的 ip 包 1tcpdump 'port 80 and ip[2:2] &gt; 576' 【3】打印特定 TCP Flag 的数据包TCP Flags 在 tcpdump 抓取的报文中的体现：[S]：SYN（开始连接）[.]: 没有 Flag[P]: PSH（推送数据）[F]: FIN （结束连接）[R]: RST（重置连接）[S.] SYN-ACK，就是 SYN 报文的应答报文。 12tcpdump 'tcp[13] &amp; 16!=0'# 等价于tcpdump 'tcp[tcpflags] == tcp-ack' 打印出所有的 ACK 包. 12tcpdump 'tcp[13] &amp; 4!=0'# 等价于tcpdump 'tcp[tcpflags] == tcp-rst' 打印出所有的 RST 包，即包含 [R] 标志的包。 更多 tcpdump filter 可以查看 PCAP-FILTER 或者 man tcpdump！ 好了，这个 IP 包的解析就到此为止了，照着 TCP/IP 协议分析了一遍, 发现协议也就那么回事儿，没有想象的那么难，不要害怕协议！ 具体抓包示例 IP协议： tcp协议：]]></content>
      <tags>
        <tag>tcpdump</tag>
        <tag>tcp</tag>
        <tag>wireshark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shardingsphere-proxy 安装]]></title>
    <url>%2Fmysql%2Fshardingsphere-proxy-setup%2F</url>
    <content type="text"><![CDATA[1、官网文档源码1234# 文档https://shardingsphere.apache.org/document/current/cn/quick-start/sharding-proxy-quick-start/# 源码https://github.com/apache/incubator-shardingsphere 2、快速入门2.1、规则配置编辑%SHARDING_PROXY_HOME%\conf\config-xxx.yaml。详情请参见配置手册。 编辑%SHARDING_PROXY_HOME%\conf\server.yaml。详情请参见配置手册。 2.2、引入依赖如果后端连接PostgreSQL数据库，不需要引入额外依赖。 如果后端连接MySQL数据库，需要下载MySQL Connector/J， 解压缩后，将mysql-connector-java-5.1.47.jar拷贝到${sharding-proxy}\lib目录。 2.3、启动服务 使用默认配置项 1$&#123;sharding-proxy&#125;\bin\start.sh 配置端口 1$&#123;sharding-proxy&#125;\bin\start.sh $&#123;port&#125; 3、使用手册3.1、Proxy启动 下载Sharding-Proxy的最新发行版。 如果使用docker，可以执行docker pull shardingsphere/sharding-proxy获取镜像。详细信息请参考Docker镜像。 解压缩后修改conf/server.yaml和以config-前缀开头的文件，如：conf/config-xxx.yaml文件，进行分片规则、读写分离规则配置. 配置方式请参考配置手册。 Linux操作系统请运行bin/start.sh，Windows操作系统请运行bin/start.bat启动Sharding-Proxy。如需配置启动端口、配置文件位置，可参考快速入门 进行启动。 使用任何PostgreSQL的客户端连接。如: psql -U root -h 127.0.0.1 -p 3307 3.2、注册中心使用若想使用Sharding-Proxy的数据库治理功能，则需要使用注册中心实现实例熔断和从库禁用功能。详情请参考支持的注册中心。 Zookeeper Sharding-Proxy默认提供了Zookeeper的注册中心解决方案。您只需按照配置规则进行注册中心的配置，即可使用。 其他第三方注册中心 将Sharding-Proxy的lib目录下的sharding-orchestration-reg-zookeeper-curator-${sharding-sphere.version}.jar文件删除。 使用SPI方式实现相关逻辑编码，并将生成的jar包放到Sharding-Proxy的lib目录下。 按照配置规则进行注册中心的配置，即可使用。 3.3、使用自定义分片算法当用户需要使用自定义的分片算法类时，无法再通过简单的inline表达式在yaml文件进行配置。可通过以下方式配置使用自定义分片算法。 编码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.jzsec&lt;/groupId&gt; &lt;artifactId&gt;sharding-strategy&lt;/artifactId&gt; &lt;version&gt;4.0.0-RC3&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;sharding-sphere.version&gt;4.0.0-RC3&lt;/sharding-sphere.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-core&lt;/artifactId&gt; &lt;version&gt;$&#123;sharding-sphere.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.10&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;utf8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 1234567891011121314package com.jzsec.sharding.strategy;public class YyyyTableShardingAlgorithm implements PreciseShardingAlgorithm&lt;Integer&gt;, RangeShardingAlgorithm&lt;Integer&gt; &#123; public Collection&lt;String&gt; doSharding(Collection&lt;String&gt; availableTargetNames, RangeShardingValue&lt;Integer&gt; shardingValue) &#123; log.info("shardingValue:" + shardingValue); Range&lt;Integer&gt; valueRange = shardingValue.getValueRange(); return actualDataNodes(shardingValue.getLogicTableName(), valueRange.lowerEndpoint(), valueRange.upperEndpoint()); &#125; public String doSharding(Collection&lt;String&gt; availableTargetNames, PreciseShardingValue&lt;Integer&gt; shardingValue) &#123; log.info("shardingValue:" + shardingValue); return shardingValue.getLogicTableName() + DateFmtUtils.getYyyy(shardingValue.getValue()); &#125;&#125; 打包1mvn clean &amp;&amp; package 上传包12345/data/downloads/sharding-proxy-4.0.0-RC3/libll sharding-strategy*-rw-r--r--. 1 502 games 10437 Dec 25 19:34 sharding-strategy-4.0.0-RC3.jarchown 502:games sharding-strategy-4.0.0-RC3.jar 指定分片算法123456789shardingRule: tables: kd_logasset: actualDataNodes: ds_master.kd_logasset_$&#123;2015..2099&#125;_0$&#123;1..9&#125;, ds_master.kd_logasset_$&#123;2015..2099&#125;_$&#123;10..12&#125; tableStrategy: standard: shardingColumn: bizdate preciseAlgorithmClassName: com.jzsec.sharding.strategy.Yyyy_mmTableShardingAlgorithm rangeAlgorithmClassName: com.jzsec.sharding.strategy.Yyyy_mmTableShardingAlgorithm 3.4、分布式事务Sharding-Proxy接入的分布式事务API同Sharding-JDBC保持一致，支持LOCAL，XA，BASE类型的事务。 XA事务Sharding-Proxy原生支持XA事务，默认的事务管理器为Atomikos。 可以通过在Sharding-Proxy的conf目录中添加jta.properties来定制化Atomikos配置项。 具体的配置规则请参考Atomikos的官方文档。 BASE事务BASE目前没有打包到Sharding-Proxy中，使用时需要将实现了ShardingTransactionManagerSPI的jar拷贝至conf/lib目录，然后切换事务类型为BASE。 3.5、SCTL (Sharding-Proxy control language)SCTL为Sharding-Proxy特有的控制语句，可以在运行时修改和查询Sharding-Proxy的状态，目前支持的语法为： 语句 说明 sctl:set transaction_type=XX 修改当前TCP连接的事务类型, 支持LOCAL，XA，BASE。例：sctl:set transaction_type=XA sctl:show transaction_type 查询当前TCP连接的事务类型 sctl:show cached_connections 查询当前TCP连接中缓存的物理数据库连接个数 sctl:explain SQL语句 查看逻辑SQL的执行计划，例：sctl:explain select * from t_order; sctl:hint set MASTER_ONLY=true 针对当前TCP连接，是否将数据库操作强制路由到主库 sctl:hint set DatabaseShardingValue=yy 针对当前TCP连接，设置hint仅对数据库分片有效，并添加分片值，yy：数据库分片值 sctl:hint addDatabaseShardingValue xx=yy 针对当前TCP连接，为表xx添加分片值yy，xx：逻辑表名称，yy：数据库分片值 sctl:hint addTableShardingValue xx=yy 针对当前TCP连接，为表xx添加分片值yy，xx：逻辑表名称，yy：表分片值 sctl:hint clear 针对当前TCP连接，清除hint所有设置 sctl:hint show status 针对当前TCP连接，查询hint状态，master_only:true/false，sharding_type:databases_only/databases_tables sctl:hint show table status 针对当前TCP连接，查询逻辑表的hint分片值 Sharding-Proxy 默认不支持hint，如需支持，请在conf/server.yaml中，将props的属性proxy.hint.enabled设置为true。在Sharding-Proxy中，HintShardingAlgorithm的泛型只能是String类型。 3.6、注意事项 Sharding-Proxy默认使用3307端口，可以通过启动脚本追加参数作为启动端口号。如: bin/start.sh 3308 Sharding-Proxy使用conf/server.yaml配置注册中心、认证信息以及公用属性。 Sharding-Proxy支持多逻辑数据源，每个以config-前缀命名的yaml配置文件，即为一个逻辑数据源。 4、配置手册数据源与分片配置示例Sharding-Proxy支持多逻辑数据源，每个以config-前缀命名的yaml配置文件，即为一个逻辑数据源。以下是config-xxx.yaml的配置配置示例。 数据分片12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152schemaName: sharding_dbdataSources: ds0: url: jdbc:postgresql://localhost:5432/ds0 username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 65 ds1: url: jdbc:postgresql://localhost:5432/ds1 username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 65shardingRule: tables: t_order: actualDataNodes: ds$&#123;0..1&#125;.t_order$&#123;0..1&#125; databaseStrategy: inline: shardingColumn: user_id algorithmExpression: ds$&#123;user_id % 2&#125; tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order$&#123;order_id % 2&#125; keyGenerator: type: SNOWFLAKE column: order_id t_order_item: actualDataNodes: ds$&#123;0..1&#125;.t_order_item$&#123;0..1&#125; databaseStrategy: inline: shardingColumn: user_id algorithmExpression: ds$&#123;user_id % 2&#125; tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order_item$&#123;order_id % 2&#125; keyGenerator: type: SNOWFLAKE column: order_item_id bindingTables: - t_order,t_order_item defaultTableStrategy: none: 读写分离12345678910111213141516171819202122232425262728293031323334schemaName: master_slave_dbdataSources: ds_master: url: jdbc:postgresql://localhost:5432/ds_master username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 65 ds_slave0: url: jdbc:postgresql://localhost:5432/ds_slave0 username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 65 ds_slave1: url: jdbc:postgresql://localhost:5432/ds_slave1 username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 65masterSlaveRule: name: ds_ms masterDataSourceName: ds_master slaveDataSourceNames: - ds_slave0 - ds_slave1 全局配置示例Sharding-Proxy使用conf/server.yaml配置注册中心、认证信息以及公用属性。 1234567891011121314151617181920212223242526#省略数据分片和读写分离配置orchestration: name: orchestration_ds overwrite: true registry: type: zookeeper namespace: orchestration serverLists: localhost:2181#权限验证authentication: users: root: # 自定义用户名 password: root # 自定义用户名 sharding: # 自定义用户名 password: sharding # 自定义用户名 authorizedSchemas: sharding_db, masterslave_db # 该用户授权可访问的数据库，多个用逗号分隔。缺省将拥有root权限，可访问全部数据库。#Proxy属性#省略与Sharding-JDBC一致的配置属性props: acceptor.size: #用于设置接收客户端请求的工作线程个数，默认为CPU核数*2 proxy.transaction.type: #默认为LOCAL事务，允许LOCAL，XA，BASE三个值，XA采用Atomikos作为事务管理器，BASE类型需要拷贝实现ShardingTransactionManager的接口的jar包至lib目录中 proxy.opentracing.enabled: #是否开启链路追踪功能，默认为不开启。详情请参见[链路追踪](/cn/features/orchestration/apm/) check.table.metadata.enabled: #是否在启动时检查分表元数据一致性，默认值: false proxy.frontend.flush.threshold: # 对于单个大查询,每多少个网络包返回一次 5、排查问题1[root@localhost conf]# top 1[root@localhost conf]# top -Hp 31009 1234[root@localhost conf]# printf '%x' 3116579bd[root@localhost conf]# jstack 31009 &gt; aaa.log[root@localhost conf]# vi aaa.log 1[root@localhost conf]# iftop]]></content>
      <categories>
        <category>mysql</category>
        <category>shardingsphere-proxy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux 模拟 网络延迟、网络丢包、网络中断]]></title>
    <url>%2Fcomputer-network%2Fsimulated-network-delay-loss-interrupt%2F</url>
    <content type="text"><![CDATA[昨天，笔者讲述了如何将CPU和IO撑满，这个其实很好理解，写个CPU密集型的程序让CPU忙个不停就可以撑满CPU；弄个程序一直写就可以让IO也撑满。有兴趣的同学可以看下昨天的这篇文章《看我如何作死 | 将CPU、IO撑满》，不过里面的做法分别是使用openssl speed和linux dd工具来实现这两个功能。 面对CPU和IO时，相信大家都能很快的反应出如何实现，那么面对网络问题时，大家的反应又是如何呢？不会是拔网线吧。。。 在故障注入，或者说故障演练，甚至说混沌工程中，可以设计很多类型的故障，今天要介绍的就是网络故障。 混沌系统是在分布式系统上进行实验的学科，目的是建立对系统抵御生产环境中失控条件的能力以及信心。 在复杂的网络环境下，数据包发送和接收的时间间隔或长或短。在网络状况较差时，调用下游服务时可能要过很久才能收到返回，这时服务的反应如何，直接关系到稳定性与高可用。 我们这里索要模拟的网络故障有三类，分别是：网络延时、网络中断以及网络丢包。 ​ 一、tc工具介绍笔者也不卖关子，本文模拟的网络故障是通过linux的tc工具来实现的。Linux内核网络协议栈从2.2.x开始，就实现了对服务质量的支持模块。具体的代码位于net/sched/目录。在Linux里面，对这个功能模块的称呼是Traffic Control ,简称TC。TC是一个在上层协议处添加Qos功能的工具，原理上看，它实质是专门供用户利用内核Qos调度模块去定制Qos的中间件。 Linux操作系统中的流量控制器TC（Traffic Control）用于Linux内核的流量控制，主要是通过在输出端口处建立一个队列来实现流量控制。 接收包从输入接口（Input Interface）进来后，经过流量限制（Ingress Policing）丢弃不符合规定的数据包，由输入多路分配器（Input De-Multiplexing）进行判断选择：如果接收包的目的是本主机，那么将该包送给上层处理；否则需要进行转发，将接收包交到转发块（Forwarding Block）处理。转发块同时也接收本主机上层（TCP、UDP等）产生的包。转发块通过查看路由表，决定所处理包的下一跳。然后，对包进行排列以便将它们传送到输出接口（Output Interface）。一般我们只能限制网卡发送的数据包，不能限制网卡接收的数据包，所以我们可以通过改变发送次序来控制传输速率。Linux流量控制主要是在输出接口排列时进行处理和实现的。 tc工具的语法还是很复杂的，笔者（微信公众号：朱小厮的博客）试图想要在本文中详细的讲解一下tc的用法，最后还是放弃了，篇幅太长，难以穷尽。所以本文中只是针对前面说的三种故障简单的演示一下tc的用法以及对应故障的实现方式，希望能够能大家有个小小的印象。如果以后遇到类似问题，或者说对这个东西感兴趣，可以再深度的学习一下。 ​ 二、paping工具介绍在正式介绍如何模拟网络故障之前，还要介绍一个工具来查看模拟的效果如何。 通常我们测试数据包能否通过IP协议到达特定主机，都习惯使用Ping命令，工作时发送一个ICMP Echo，等待接受Echo响应，但是Ping使用的是ICMP协议，如果防火墙放通了此协议，依旧能够ping通，但是无法确定通过tcp传送的数据包是否正常到达对端。 而paping可以在Linux平台上测试网络的连通性及网络延时等。它的用法很简单: 1234-p, --port N 指定被测试服务的 TCP 端口（必须）；--nocolor 屏蔽彩色输出；-t, --timeout 指定超时时长，单位为毫秒，默认值为 1000；-c, --count N 指定测试次数。 比如下面的示例（记得先要开启一个以80为端口的服务, 示例中的xxx.xxx.xxx.xxx代表ip地址）: 123456789101112131415hidden@hidden$ ./paping -p 80 -c 5 xxx.xxx.xxx.xxxpaping v1.5.5 - Copyright (c) 2011 Mike LovellConnecting to xxx.xxx.xxx.xxx on TCP 80:Connected to xxx.xxx.xxx.xxx: time=27.47ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=97.83ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=37.38ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=57.62ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=71.87ms protocol=TCP port=80Connection statistics: Attempted = 5, Connected = 5, Failed = 0 (0.00%)Approximate connection times: Minimum = 27.47ms, Maximum = 97.83ms, Average = 58.43ms 可以看到平均链接时间为58.43ms。 如果你的机器上没有安装paping，那么可以采用如下的方式安装： 123wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/paping/paping_1.5.5_x86_linux.tar.gztar -zvxf paping_1.5.5_x86_linux.tar.gz ./paping -p 80 -c 5000 www.baidu.com 如果有以下的错误： ./paping: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory 可以先安装对应的库来解决： 12sudo apt-get install libstdc++6sudo apt-get install lib32stdc++6 ​ 三、模拟网络延时使用tc命令模拟延迟300ms（对应的删除命令为tc qdisc del dev eth0 root netem）： 12 tc qdisc add dev eth0 root netem delay 300ms// 该命令将网卡eth0的传输设置为延迟300ms发送 此时再次执行paping命令: 123456789101112131415hidden@hidden$ ./paping -p 80 -c 5 xxx.xxx.xxx.xxxpaping v1.5.5 - Copyright (c) 2011 Mike LovellConnecting to xxx.xxx.xxx.xxx on TCP 80:Connected to xxx.xxx.xxx.xxx : time=326.11ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=417.02ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=326.94ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=326.19ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx : time=353.51ms protocol=TCP port=80Connection statistics: Attempted = 5, Connected = 5, Failed = 0 (0.00%)Approximate connection times: Minimum = 326.11ms, Maximum = 417.02ms, Average = 349.95ms 与之前的58.43ms相比相差了291.52ms ≈ 300ms。 更真实的情况下，延迟值不会这么精确，会有一定的波动，我们可以用下面的情况来模拟出带有波动性的延迟值： 12tc qdisc add dev eth0 root netem delay 300ms 50ms//该命令将 eth0 网卡的传输设置为延迟 300ms ± 50ms (250 ~ 350 ms 之间的任意值)发送 ​ 四、模拟网络中断这次使用如下的命令： 12tc qdisc add dev eth0 root netem corrupt 10%//该命令将 eth0 网卡的传输设置为随机产生 10% 的损坏的数据包 此时再次执行paping命令: 1./paping -p 80 -c 100 xxx.xxx.xxx.xxx 注意这里的次数改成了100，为了更能清楚的看到中断的实际效果。 运行这个命令的过程中，会有“Connection timed out”字样报出，类似： 123456789&lt;snip&gt;Connected to xxx.xxx.xxx.xxx: time=26.26ms protocol=TCP port=80Connection timed outConnected to xxx.xxx.xxx.xxx: time=65.10ms protocol=TCP port=80Connection timed outConnected to xxx.xxx.xxx.xxx: time=26.50ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=25.93ms protocol=TCP port=80Connected to xxx.xxx.xxx.xxx: time=27.71ms protocol=TCP port=80&lt;snip&gt; 最终的统计结果如下： 1234Connection statistics: Attempted = 100, Connected = 90, Failed = 10 (10.00%)Approximate connection times: Minimum = 25.67ms, Maximum = 133.77ms, Average = 51.98ms 结果显而易见，验证了此次故障模拟所对应的效果。 ​ 五、模拟网络丢包使用如下命令： 123tc qdisc add dev eth0 root netem loss 7% 25%//该命令将 eth0 网卡的传输设置为随机丢掉 7% 的数据包, 成功率为 25% //如果不加上后面的25%，那么一丝就是随机丢掉7%的数据包 再次执行paping命令时，也会有Connection timed out报出，最终的统计结果如下： 1234Connection statistics: Attempted = 100, Connected = 99, Failed = 1 (1.00%)Approximate connection times: Minimum = 25.80ms, Maximum = 133.87ms, Average = 60.94ms 7%*25%的值在1%-2%之间，符合测试的结果预期。 tc还可以模拟一些其它的网络故障，比如网络包重复、网络包错序等等，有兴趣的同学可以继续深入了解一下。 ​ 六、引申混沌工程和传统测试之间的区别很多同学在进行一些故障测试的时候，会认为其正在进行混沌实验，其实混沌工程和传统的测试之间是有区别的。 混沌工程和传统测试（故障注入FIT、故障测试）在关注点和工具集上都有很大的重叠。譬如，在Netflix（如果还不知道Netflix是谁，可以先看看这篇《明星公司之Netflix》了解一下）的很多混沌工程实验研究的对象都是基于故障注入来引入的。混沌工程和这些传统测试方法的主要区别在于：混沌工程是发现新信息的实践过程，而故障注入则是对一个特定的条件、变量的验证方法。 当你希望探究复杂系统如何应对异常时，对系统中的服务注入通信故障（如超时、错误等）不失为一种很好的方法。但有时我们希望探究更多其他的非故障类的场景，如流量激增、资源竞争条件、拜占庭故障（例如性能差或有异常的节点发出有错误的响应、异常的行为、对调用者随机性的返回不同的响应，等等）、非计划中的或非正常组合的消息处理等等。因为如果一个面向公众用户的网站突然收到激增的流量，从而产生更多的收入时我们很难称之为故障，但我们仍然需要探究清楚系统在这种情况下的影响。 和故障注入类似，故障测试方法通过对预先设想到的可以破坏系统的点进行测试，但是并没能去探究上述这类更广阔领域里的、不可预知的、但很可能发生的事情。 在传统测试中，我们可以写一个断言（assertion），即我们给定一个特定的条件，产生一个特定的输出。测试一般来说只会产生二元的结果，验证一个结果是真还是假，从而判定测试是否通过。严格意义上来说，这个过程并不能让我们发掘出对于系统未知的、尚不明确的认知，它仅仅是对我们已知的系统属性可能的取值进行测验。而实验可以产生新的认知，而且通常还能开辟出一个更广袤的对复杂系统的认知空间。 混沌工程是一种帮助我们获得更多的关于系统的新认知的实验方法。它和已有的功能测试、集成测试等以测试已知属性的方法有本质上的区别。 ​ 七、后续后面还会有几篇相同主题的文章发出，不出意外，下一篇应该是《怎么让进程假死》，如果有兴趣的话，可以持续关注本公众号（朱小厮的博客）。如果你还有有什么需要进一步了解的可以在下方留言，或者也聊聊你对这一块的认知和想法。]]></content>
      <categories>
        <category>linux</category>
        <category>centos7</category>
      </categories>
      <tags>
        <tag>centos7</tag>
        <tag>混沌工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rancher 2.x 部署 管理k8s集群]]></title>
    <url>%2Fdevops%2Fk8s%2Francher-devops%2F</url>
    <content type="text"><![CDATA[中文文档：https://docs.rancher.cn/rancher2x/ https://rancher2.docs.rancher.cn/docs/releases/v2.4.5/ https://cloud.tencent.com/developer/article/1633991 Rancher 是一个全栈式的 Kubernetes 容器管理平台，也是一个可以在任何地方都能成功运行 Kubernetes 的工具。 产品介绍 Rancher Server 架构Rancher Server 由认证代理（Authentication Proxy）、Rancher API Server、集群控制器（Cluster Controller）、etcd 节点和集群 Agent（Cluster Agent） 组成。除了集群 Agent 以外，其他组件都部署在 Rancher Server 中。 图中描述的是用户通过 Rancher Server 管控 Rancher 部署的 Kubernetes 集群（RKE 集群）和托管的 Kubernetes 集群的（EKS）集群的流程。以用户下发指令为例，指令的流动路径如下： 首先，用户通过 Rancher UI（即 Rancher 控制台） Rancher 命令行工具（Rancher CLI）输入指令；直接调用 Rancher API 接口也可以达到相同的效果。 用户通过 Rancher 的代理认证后，指令会进一步下发到 Rancher Server 。 与此同时，Rancher Server 也会执行容灾备份，将数据备份到 etcd 节点。 然后 Rancher Server 把指令传递给集群控制器。集群控制器把指令传递到下游集群的 Agent，最终通过 Agent 把指令下发到指定的集群中。 如果 Rancher Server 出现问题，我们也提供了备用方案，您可以通过授权集群端点管理集群。 考虑到性能表现和安全因素，我们建议您使用两个 Kubernetes 集群，分开部署 Rancher Server 和工作负载。部署 Rancher Server 后，您可以创建或导入集群，然后在这些集群上运行您的工作负载。 通过Rancher认证代理管理 Kubernetes 集群 ​ 您可以在单个节点或高可用的 Kubernetes 集群上安装 Rancher。由于单节点安装只适用于开发和测试环境，而且单节点和高可用集群之间无法进行数据迁移，所以我们建议您从一开始就使用高可用的 Kubernetes 集群来部署 Rancher Server，而且您需要分开部署运行 Rancher Server 的集群和运行自己业务的下游集群 与下游集群交互本小节通过两个用户 Bob 和 Alice 的案例，讲解 Rancher 启动和管理下游集群的具体过程，和每个 Rancher 组件的作用。 下图演示了集群控制器、集群 Agent 和 Node Agent 是如何允许 Rancher 控制下游集群的。 与下游集群通信 Kubernetes 的安装环境我们强烈建议您使用云服务提供商的虚拟机，如 AWS EC2、Goolge Compute Enginee (GCE)、AliCloud ECS 等，安装 Kubernetes 集群，然后在集群中安装 Rancher。 为了达到最好的性能和安全条件，我们建议您为 Rancher 创建一个专用的 Kubernetes 集群，只在这个机器中部署 Rancher Server，不在这个集群中运行应用或程序。部署 Rancher 后，您可以创建新集群或导入已有集群，然后用这些集群启动您自己的应用或程序。 我们不建议在托管的 Kubernetes 集群上，如 EKS 和 GKE，安装 Rancher。 这些托管的 Kubernetes 集群不会将 etcd 暴露给 Rancher ，达到 Rancher 可以管理的程度，而且它们的特殊改动可能与 Rancher 的操作冲突。 快速入门Rancher 提供这些快速入门手册的目的是帮助您快速地建造一个 Rancher 的沙盒，您可以在这个沙盒中评估 Rancher 是否满足您的需求。请注意，快速入门手册不适用于正式的生产环境，请参考安装介绍获取适用于正式的生产环境的操作指导。 目前 Rancher 提供了以下三本快速入门手册： 部署 Rancher Server：使用对您来说最方便的方式运行 Rancher Server。 部署工作负载：部署一个简单的工作负载，然后暴露工作负载，这样您就可以从集群外部访问工作负载。 命令行工具：使用kubectl或 Rancher 命令行工具（Rancher CLI）管理 Rancher 实例。 https://rancher2.docs.rancher.cn/docs/installation/k8s-install/_index 安装 Docker1curl https://releases.rancher.com/install-docker/18.09.sh | sh 要了解某个 Docker 版本是否有可用的安装脚本，请参考这个GitHub 仓库，这里包含了 Rancher 的所有 Docker 安装脚本。 需要的 CLI 工具此安装需要以下 CLI 工具。请确保这些工具已经安装并在$PATH中可用 kubectl - Kubernetes 命令行工具. rke - Rancher Kubernetes Engine，用于构建 Kubernetes 集群的 cli。 k3s - Rancher K3s。 helm - Kubernetes 的软件包管理工具。请参阅Helm 版本要求选择 Helm 的版本来安装 Rancher。 配置基础设置 https://rancher2.docs.rancher.cn/docs/installation/k8s-install/create-nodes-lb/_index 1、Linux主机配置 3、配置负载均衡器 您还需要设置一个负载均衡器，以将流量定向到两个节点上的 Rancher 副本。这样可以在单个节点不可用时，继续保障与 Rancher 管理面的连接。 在后续步骤中配置 Kubernetes 时，K3s 工具将部署 Traefik Ingress 控制器。该控制器将侦听 worker 节点的 80 端口和 443 端口，以响应发送给特定主机名的流量。 在安装 Rancher 时（也是在后续步骤中），Rancher 系统将创建一个 Ingress 资源。该 Ingress 通知 Traefik Ingress 控制器侦听发往 Rancher 主机名的流量。Traefik Ingress 控制器在收到发往 Rancher 主机名的流量时，会将其转发到集群中正在运行的 Rancher Server Pod。 对于实现，请考虑是否要使用 4 层或 7 层负载均衡器： 4 层负载均衡器 是两种选择中相对简单的一种，它将 TCP 流量转发到您到节点。我们建议使用 4 层负载均衡器，将流量从 TCP / 80 端口和 TCP / 443 端口转发到 Rancher 管理面的集群节点上。集群上的 Ingress 控制器会将 HTTP 流量重定向到 HTTPS，并在 TCP / 443 端口上终止 SSL / TLS。Ingress 控制器会将流量转发到 Rancher Server Pod 的 TCP / 443 端口。 7 层负载均衡器 相对有些复杂，但可以提供您可能需要的功能。例如，与 Rancher 本身进行 TLS 终止相反，7 层负载均衡器能够在负载均衡器处理 TLS 终止。如果要在基础设施中进行 TLS 终止，7 层负载均衡可能会很有用。7 层负载均衡还可以为您的负载均衡器提供基于 HTTP 属性（例如 cookie 等）做出决策的能力，而 4 层负载均衡器无法提供这种功能。如果决定在 7 层负载均衡器上终止 SSL / TLS 流量，则在安装 Rancher 时（后续步骤）需要使用--set tls=external选项。有关更多信息，请参阅Rancher Helm Chart 选项。 有关如何设置 NGINX 负载均衡器的示例，请参考本页。 有关如何设置 Amazon ELB Network Load Balancer 的示例，请参考本页。 有关如何配置 F5 作为 Rancher 前端 7 层负载均衡器的示例，请参考本页。 有关如何为 F5 启动 WAF 功能的示例，请参考本页。 4、配置DNS记录 配置完负载均衡器后，您将需要创建 DNS 记录，以将流量发送到该负载均衡器。 安装 高可用 k3s 集群 在 K3s 集群中安装 Rancher 高可用，我们建议为高可用安装配置以下基础设施： 2 个 Linux 节点，通常是虚拟机，您可以自行选择的基础设施提供商，例如 Amazon EC2，阿里云，腾讯云或者 vShpere 等。 1 个外置数据库，用于存储集群数据。我们建议使用 MySQL。 1 个负载均衡器，用于将流量转发到这两个节点。 一条 DNS 记录，用于将 URL 指向负载均衡器。这将成为 Rancher Server 的 URL，下游集群需要可以访问到这个地址。 准备外置Mysql数据库12345678910111213141516171819[root@nna ~]# mkdir -p /data/rancher/ &amp;&amp; cd /data/rancher[root@nna rancher]# wget https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm[root@nna rancher]# rpm -ivh mysql57-community-release-el7-9.noarch.rpm[root@nna rancher]# yum install mysql-server -y[root@nna rancher]# systemctl start mysqld[root@nna rancher]# systemctl enable mysqld[root@nna rancher]# grep 'temporary password' /var/log/mysqld.log# tpp!e.ecj6eP[root@nna ~]# mysql -uroot -pmysql&gt; use mysql;mysql&gt; set global validate_password_policy=LOW;mysql&gt; set global validate_password_length=6;mysql&gt; set password=password("123456");mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456';mysql&gt; flush privileges;mysql&gt; create database k3s;[root@nna rancher]# yum install -y nc[root@nna rancher]# nc -vz nna 3306 安装 K3s Server 在nna和nns上安装 k3s server 1234567891011121314151617181920212223[root@nna ~]# mkdir -p /data/rancher/ &amp;&amp; cd /data/rancher[root@nna rancher]# yum install -y container-selinux selinux-policy-base[root@nna rancher]# rpm -i https://rpm.rancher.io/k3s-selinux-0.1.1-rc1.el7.noarch.rpmwarning: /var/tmp/rpm-tmp.WXgsrS: Header V4 RSA/SHA1 Signature, key ID e257814a: NOKEY[root@nna rancher]# curl -sfL https://docs.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - server --datastore-endpoint="mysql://root:123456@tcp(nna:3306)/k3s"[INFO] Finding release for channel stable[INFO] Using v1.18.4+k3s1 as release[INFO] Downloading hash https://mirror-k3s.rancher.cn/download/v1.18.4-k3s1/sha256sum-amd64.txt[INFO] Downloading binary https://mirror-k3s.rancher.cn/download/v1.18.4-k3s1/k3s[INFO] Verifying binary download[INFO] Installing k3s to /usr/local/bin/k3s[INFO] Creating /usr/local/bin/kubectl symlink to k3s[INFO] Creating /usr/local/bin/crictl symlink to k3s[INFO] Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr[INFO] Creating killall script /usr/local/bin/k3s-killall.sh[INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh[INFO] env: Creating environment file /etc/systemd/system/k3s.service.env[INFO] systemd: Creating service file /etc/systemd/system/k3s.service[INFO] systemd: Enabling k3s unitCreated symlink from /etc/systemd/system/multi-user.target.wants/k3s.service to /etc/systemd/system/k3s.service.[INFO] systemd: Starting k3s[root@nna ~]# 确认 K3s 是否创建成功要确认已成功设置 K3s，请在任一 K3s Server 节点上运行以下命令： 123456789101112131415[root@nna ~]# k3s kubectl get nodesNAME STATUS ROLES AGE VERSIONnna Ready master 72s v1.18.4+k3s1[root@nna ~]# k3s kubectl get nodesNAME STATUS ROLES AGE VERSIONnna Ready master 4m50s v1.18.4+k3s1nns Ready master 22s v1.18.4+k3s1[root@nna ~]# k3s kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system helm-install-traefik-6l8xj 0/1 ContainerCreating 0 6m43skube-system metrics-server-7566d596c8-v757d 1/1 Running 0 6m42skube-system local-path-provisioner-6d59f47c7-gt6ch 1/1 Running 0 6m42skube-system coredns-8655855d6-d9l8x 1/1 Running 0 6m42s[root@nna ~]# 启动k3s123456/usr/local/bin/k3s server --datastore-endpoint=mysql://root:123456@tcp(nna:3306)/k3ssystemctl enable k3ssystemctl status k3ssystemctl start k3ssystemctl stop k3s 卸载 k3s (备用)如果是通过 install.sh 脚本安装的k3s，默认会在服务中安装一个卸载的脚本文件： /usr/local/bin/k3s-uninstall.sh (or as k3s-agent-uninstall.sh). 1234[root@nns ~]# ll /usr/local/bin/k3s*-rwxr-xr-x. 1 root root 53604352 Jul 2 00:52 /usr/local/bin/k3s-rwxr-xr-x. 1 root root 1710 Jul 2 00:52 /usr/local/bin/k3s-killall.sh-rwxr-xr-x. 1 root root 881 Jul 2 00:52 /usr/local/bin/k3s-uninstall.sh 安装 kubectl 并配置 kubeconfig 官方文档：https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl 我的系统是centos7.5，选用如下内容 配置 kubeconfig 1\cp /etc/rancher/k3s/k3s.yaml ~/.kube/config 1234curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectlchmod +x ./kubectlmv ./kubectl /usr/local/bin/kubectlkubectl version --client 在 k3s 集群中安装 Rancher 官方文档：https://rancher2.docs.rancher.cn/docs/installation/k8s-install/helm-rancher/_index 1、安装需要的 CLI 工具以下 CLI 工具是创建 Kubernetes 集群所必需的。请确保这些工具已安装并在您的$PATH中可用。 请查看 Helm 项目提供的安装指南，来在您的平台上进行安装。 kubectl - Kubernetes 命令行工具。 helm - Kubernetes 的软件包管理工具。请参阅 Helm 版本要求以选择要安装 Rancher 的 Helm 版本 在 github（https://github.com/helm/helm/releases）下载： 1234wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gztar -zxvf helm-v3.2.4-linux-amd64.tar.gzmv linux-amd64/helm /usr/local/bin/helmhelm help 2、添加 Helm Chart 仓库添加含有 Rancher Chart 的 Helm Chart 仓库 1helm repo add rancher-stable http://rancher-mirror.oss-cn-beijing.aliyuncs.com/server-charts/stable 3、为 Rancher 创建 Namespace我们需要定义一个 Kubernetes Namespace，在 Namespace 中安装由 Chart 创建的资源。这个命名空间的名称为cattle-system： 12[root@nna rancher]# kubectl create namespace cattle-systemnamespace/cattle-system created 4、选择您的 SSL 选项12345mkdir -p /data/rancher/certs &amp;&amp; cd /data/rancher/certstouch ~/.rndyum install -y opensslcp /etc/pki/tls/openssl.cnf ./vim openssl.cnf 123456789101112131415161718[req]distinguished_name = req_distinguished_namereq_extetions = v3_reqx509_extensions = v3_ca[ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentextendedKeyUsage = clientAuth, serverAuthsubjectAltName = @alt_names[ v3_ca ]subjectKeyIdentifier=hashauthorityKeyIdentifier=keyid:always,issuer:alwaysbasicConstraints = critical,CA:truesubjectAltName = @alt_names[alt_names]DNS.1 = local-rancher 12345678910111213141516171819202122openssl genrsa -out cakey.pem 2048openssl req -x509 -new -nodes -key cakey.pem \ -days 36500 \ -out cacerts.pem \ -extensions v3_ca \ -subj "/CN=rancher.local.com" \ -config ./openssl.cnfopenssl genrsa -out server.key 2048openssl req -new -key server.key \ -out server.csr \ -subj "/CN=local-rancher" \ -config ./openssl.cnfopenssl x509 -req -in server.csr \ -CA cacerts.pem \ -CAkey cakey.pem \ -CAcreateserial -out server.crt \ -days 36500 -extensions v3_req \ -extfile ./openssl.cnfopenssl x509 -noout -in server.crt -text | grep DNScp server.crt tls.crtcp server.key tls.key ca证书密文 1234kubectl -n cattle-system create secret tls tls-rancher-ingress \ --cert=./tls.crt --key=./tls.keykubectl -n cattle-system create secret generic tls-ca \ --from-file=cacerts.pem 部署 Rancher 集群 1234567echo '192.168.145.131 local-rancher' &gt;&gt; /etc/hostshelm install rancher rancher-stable/rancher \ --namespace cattle-system \ --set hostname=local-rancher \ --set ingress.tls.source=secret \ --set privateCA=true 6、根据您选择的 SSL 选项，通过 Helm 安装 Rancher1234567echo '192.168.145.131 local-rancher' &gt;&gt; /etc/hostshelm install rancher rancher-stable/rancher \ --namespace cattle-system \ --set hostname=local-rancher \ --set ingress.tls.source=secret \ --set privateCA=true 7、验证Rancher Server 是否成功部署等待 Rancher 运行，检查 Rancher Server 是否运行成功：： 12345kubectl -n cattle-system rollout status deploy/rancherWaiting for deployment "rancher" rollout to finish: 0 of 3 updated replicas are available...deployment "rancher" successfully rolled out 如果看到以下错误： error: deployment &quot;rancher&quot; exceeded its progress deadline, 您可以通过运行以下命令来检查 deployment 的状态： 1234[root@nna ~]# kubectl -n cattle-system get deploy rancherNAME READY UP-TO-DATE AVAILABLE AGErancher 3/3 3 3 23h[root@nna ~]# DESIRED和AVAILABLE应该显示相同的个数。 8、保存您的选项请保存您使用的全部--set选项。使用 Helm 升级 Rancher 到新版本时，您将需要使用相同的选项。 安装完成现在您应该具有一个功能正常的 Rancher Server 了。 打开浏览器，访问您的 DNS，这个 DNS 会将流量转发到您的负载均衡器，您应该会看到一个色彩丰富的登录页面。 遇到了问题？查看故障排查页面。 rancher 中映射的端口无法用lsof或者netstat查看。可以通过iptables -L -t nat -n查看。 使用 Rancher 创建 真正 k8s 集群12345678mkdir -p /etc/dockervi /etc/docker/daemon.json&#123; "registry-mirrors": ["https://k8spv7nq.mirror.aliyuncs.com"]&#125;systemctl daemon-reloadsystemctl restart docker 文档：https://rancher2.docs.rancher.cn/docs/cluster-provisioning/rke-clusters/custom-nodes/_index 123456# 删除以前的安装残留rm -rf /etc/kubernetes/ &amp;&amp; rm -rf /var/run/utmp &amp;&amp; rm -rf /var/run/lock/ &amp;&amp; rm -rf /var/lib/etcd/docker rm -f xx xx xx ...# 重新安装docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.4.5 --server https://local-rancher --token 27wnwkrq6sd9t9cc7g686jc888w2p84zz4l8755nbkn8crvdstnj2b --ca-checksum 1050e7e484e48667c4d2bb94675302162b302b51157cc8a91235bf1e747f44c3 --internal-address 192.168.145.133 --etcd --controlplane --worker 成功：]]></content>
      <categories>
        <category>devops</category>
        <category>rancher</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Rancher 2.x docker安装]]></title>
    <url>%2Fdevops%2Fk8s%2Francher-docker-install%2F</url>
    <content type="text"><![CDATA[中文文档：https://docs.rancher.cn/rancher2x/ https://rancher2.docs.rancher.cn/docs/releases/v2.4.5/ 安装 Docker1curl https://releases.rancher.com/install-docker/18.09.sh | sh 要了解某个 Docker 版本是否有可用的安装脚本，请参考这个GitHub 仓库，这里包含了 Rancher 的所有 Docker 安装脚本。 安装 Rancher 2.x首先连接到主机，然后使用 shell 安装 Rancher。 使用 shell 工具（如 PuTTy 或其他连接工具）登录 Linux 主机。 执行以下命令： sudo docker run -d –restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher 结果： Rancher 已经安装在了 Linux 主机上。 登录 Rancher 界面并配置初始设置您需要先登录 Rancher，然后再开始使用 Rancher。登录以后，您需要完成一些一次性的配置。 打开浏览器，输入主机的 IP 地址：https://&lt;SERVER_IP&gt; 请使用真实的主机 IP 地址替换 &lt;SERVER_IP&gt; 。 首次登录时，请按照页面提示设置登录密码。 设置 Rancher Server URL。URL 既可以是一个 IP 地址，也可以是一个主机名称。请确保您在集群内添加的每个节点都可以连接到这个 URL。如果您使用的是主机名称，请保证主机名称可以被节点的 DNS 服务器成功解析。 结果：完成 Rancher 管理员用户的密码设置和访问地址设置。下次使用 Rancher 时，可以输入 IP 地址或主机地址访问 Rancher 界面，然后输入管理员用户名admin和您设置的密码登录 Rancher 界面。 https://local-rancher/ 设置密码：admin/123456 创建业务集群完成安装和登录 Rancher 的步骤之后，您现在可以参考以下步骤，在 Rancher 中创建第一个 Kubernetes 集群。 在这个任务中，您可以使用自定义集群选项，使用的任意 Linux 主机（云主机、虚拟机或裸金属服务器）创建集群。 访问集群页面，单击添加集群。 选择自定义选项。 输入集群名称。 跳过集群角色和集群选项。 单击下一步。 勾选主机选项 - 角色选择中的所有角色： Etcd、 Control 和 Worker。 可选： Rancher 会自动探查用于 Rancher 通信和集群通信的 IP 地址。您可以通过主机选项 &gt; 显示高级选项中的公网地址和内网地址指定 IP 地址。 跳过主机标签参数，因为对快速入门来说，这部分的参数不太重要。 复制代码框中的命令。 登录您的 Linux 主机，打开命令行工具，粘贴命令，单击回车键运命令。 运行完成后，回到 Rancher 界面，单击完成。 结果： 在 Rancher 中创建了一个 Kubernetes 集群。 1sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.4.5 --server https://local-rancher --token htxp8hrk95jjghg74zngg2j8bj2bkwxc9s7rxcv7dgs7lx42lddd9f --ca-checksum 7fb6276d3eefaf30a82c9dd0a40e0d1118863a640f37939fd5b168aa6392e7dc --etcd --controlplane --worker 部署带有 Ingress 的工作负载#先决条件已经有一个正在运行的集群，且集群中有至少一个节点 #部署工作负载参考前文完成 Rancher Server 的快速部署后，您可以创建 工作负载。工作负载即 Kubernetes 对一组 Pod 的抽象模型，用于描述业务的运行载体，包括 Deployment、Statefulset、Daemonset、Job、CronJob 等多种类型，详情请参考名词解释。 以下步骤讲解了如何在 Rancher Server 中部署带有 Ingress 的工作负载。本文部署的工作负载是一个“Hello-World”应用。 访问集群页面，选择您刚刚创建的集群，进入集群页面。 从集群页面的主菜单中选择项目/命名空间。 打开 项目：Default。 单击资源 &gt; 工作负载。如果您使用的是 v2.3.0 之前的版本，请单击 工作负载 &gt; 工作负载。 单击部署。 结果： 打开部署工作负载 页面。 输入工作负载的名称。 在Docker 镜像一栏，输入rancher/hello-world，请注意区分大小写字母。 余下的选项保持默认配置即可。 单击运行。 结果： 部署了工作负载。这个过程可能需要几分钟完成。 当您的工作负载部署完成后，它的状态将变为Active，您可以从项目的工作负载页面查看工作负载当前的状态。 #暴露服务上述步骤帮助您完成了工作负载的部署，现在您需要将服务暴露出来，让其他服务可以通过网络连接和调用这个工作负载。 访问集群页面，选择您刚刚创建的集群，进入集群页面。 从集群页面的主菜单中选择项目/命名空间。 打开 项目 &gt; Default。 单击资源 &gt; 工作负载 &gt; 负载均衡。如果您使用的是 v2.3.0 之前的版本，请单击 工作负载 &gt; 负载均衡。 单击添加 Ingress 输入 Ingress 负载均衡的名称，如 “hello”。 在目标一栏，从下拉菜单选择您服务的名称。 在端口一栏输入 80。 余下的选项保持默认配置即可，单击保存。 结果： 这个工作负载分配到了一个xip.io地址，已经暴露出去了。可能需要 1~2 分钟完成服务关联。 #查看您的应用从负载均衡页面单击目标链接hello.default.xxx.xxx.xxx.xxx.xip.io &gt; hello-world，您的应用会在一个新窗口中打开。 #结果成功部署工作负载并通过 Ingress 暴露该工作负载。 #后续操作使用完您通过快速入门搭建的 Rancher 沙盒后，您可能想要清理遗留在环境中与 Rancher 相关的资源，并删除 Rancher Server 和您的集群，请单击下方链接查看操作指导。 清理环境：Amazon AWS 清理环境：DigitalOcean 清理环境：Vagrant]]></content>
      <categories>
        <category>devops</category>
        <category>rancher</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[KubeSphere 多节点安装k8s]]></title>
    <url>%2Fdevops%2Fk8s%2Fkubesphere-devops%2F</url>
    <content type="text"><![CDATA[准备主机 主机 IP 主机名 集群角色 192.168.145.133 dn1 master，etcd 192.168.145.134 dn2 node 192.168.145.135 dn3 node 集群架构： 单 master 单 etcd 双 node 准备安装配置文件]]></content>
      <categories>
        <category>devops</category>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[arthas 解决cpu高]]></title>
    <url>%2Fjava%2Fjvm%2Farthas-cpu100%25%2F</url>
    <content type="text"><![CDATA[现象win7下，OCR服务 tesseract-ocr 运行3个小时以上时，稳定出现CPU过高。但其实此时并没有请求，也就是说服务本身没什么负载 jvisualvm观察 可以看到 GC 非常是频繁，推测是GC导致 CPU高 jmap分析 查看整个JVM内存状态jmap -heap [pid]要注意的是在使用CMS GC 情况下，jmap -heap的执行有可能会导致JAVA 进程挂起 查看JVM堆中对象详细占用情况jmap -histo [pid] 导出整个JVM 中内存信息jmap -dump:format=b,file=文件名 [pid] jhat是sun 1.6及以上版本中自带的一个用于分析JVM 堆DUMP 文件的工具，基于此工具可分析JVM HEAP 中对象的内存占用情况jhat -J-Xmx1024M [file]执行后等待console 中输入start HTTP server on port 7000 即可使用浏览器访问 IP：7000 eclipse Memory AnalyzerEclipse 提供的一个用于分析JVM 堆Dump文件的插件。借助这个插件可查看对象的内存占用状况，引用关系，分析内存泄露等。http://www.eclipse.org/mat/ 1jmap -histo:live 7304 &gt; aaaa.log 可以看到是一个 Collection 占用内存高 12jmap -dump:format=b,file=dumpocr.dump 16800jhat -J-Xmx2048M dumpocr.dump arthas分析 此时有观测到 是 两个线程各占50%的CPU，并没有GC线程，所以根源是 这两个线程导致 CPU高，而不是 GC导致的。 12345678910111213141516171819202122232425"http-nio-16080-BlockPoller" Id=17 cpuUsage=50% RUNNABLE at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method) at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296) at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278) at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked sun.nio.ch.Util$2@41003c41 - locked java.util.Collections$UnmodifiableSet@453fa0b9 - locked sun.nio.ch.WindowsSelectorImpl@3d367a at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.tomcat.util.net.NioBlockingSelector$BlockPoller.run(NioBlockingSelector.java:313)"http-nio-16080-ClientPoller" Id=28 cpuUsage=50% RUNNABLE at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method) at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296) at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278) at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked sun.nio.ch.Util$2@71d53c37 - locked java.util.Collections$UnmodifiableSet@31f7f478 - locked sun.nio.ch.WindowsSelectorImpl@9f5a097 at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:708) at java.lang.Thread.run(Thread.java:745) 查看tomcat源码NioEndpoint$Poller.run(NioEndpoint.java:708) 123456789101112131415161718192021222324252627282930313233343536@Overridepublic void run() &#123; // Loop until destroy() is called while (true) &#123; boolean hasEvents = false; try &#123; if (!close) &#123; hasEvents = events(); if (wakeupCounter.getAndSet(-1) &gt; 0) &#123; //if we are here, means we have other stuff to do //do a non blocking select keyCount = selector.selectNow(); &#125; else &#123; keyCount = selector.select(selectorTimeout); &#125; wakeupCounter.set(0); &#125; if (close) &#123; events(); timeout(0, false); try &#123; selector.close(); &#125; catch (IOException ioe) &#123; log.error(sm.getString("endpoint.nio.selectorCloseFail"), ioe); &#125; break; &#125; &#125; catch (Throwable x) &#123; ExceptionUtils.handleThrowable(x); log.error("",x); continue; &#125; &#125;&#125; 然后调用 keyCount = selector.select(selectorTimeout); 结果跟踪到了 jdk 中的调用，赶脚碰上了jdk nio selector空轮询导致cpu 100%问题 C:/Program Files/Java/jdk1.8.0_65/jre/lib/rt.jar!/sun/nio/ch/SelectorImpl.class 123456789101112131415161718private int lockAndDoSelect(long var1) throws IOException &#123; synchronized(this) &#123; if(!this.isOpen()) &#123; throw new ClosedSelectorException(); &#125; else &#123; Set var4 = this.publicKeys; int var10000; synchronized(this.publicKeys) &#123; Set var5 = this.publicSelectedKeys; synchronized(this.publicSelectedKeys) &#123; var10000 = this.doSelect(var1); &#125; &#125; return var10000; &#125; &#125;&#125; 跟踪到了native方法，再看就需要找到c的实现去看 抬头看看网友https://stackoverflow.com/questions/36284833/spring-boot-application-starts-using-all-cpu 优化tomcat使用NIO2 12345678910111213141516171819202122232425262728293031323334package com.jzsec.tesseractocr.config;import lombok.extern.slf4j.Slf4j;import org.apache.catalina.connector.Connector;import org.apache.coyote.ProtocolHandler;import org.apache.coyote.http11.AbstractHttp11Protocol;import org.springframework.boot.web.embedded.tomcat.TomcatConnectorCustomizer;import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;import org.springframework.boot.web.server.WebServerFactoryCustomizer;import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory;import org.springframework.stereotype.Component;@Component@Slf4jpublic class MyTomcatConnectorCustomizer implements WebServerFactoryCustomizer&lt;ConfigurableServletWebServerFactory&gt; &#123; @Override public void customize(ConfigurableServletWebServerFactory factory) &#123; ((TomcatServletWebServerFactory) factory).setProtocol("org.apache.coyote.http11.Http11Nio2Protocol"); ((TomcatServletWebServerFactory) factory).addConnectorCustomizers(new TomcatConnectorCustomizer() &#123; @Override public void customize(Connector connector) &#123; ProtocolHandler protocol = connector.getProtocolHandler(); log.info("Tomcat(&#123;&#125;) -- MaxConnection:&#123;&#125;;MaxThreads:&#123;&#125;;MinSpareThreads:&#123;&#125;", // protocol.getClass().getName(), // ((AbstractHttp11Protocol&lt;?&gt;) protocol).getMaxConnections(), // ((AbstractHttp11Protocol&lt;?&gt;) protocol).getMaxThreads(), // ((AbstractHttp11Protocol&lt;?&gt;) protocol).getMinSpareThreads()); &#125; &#125;); &#125;&#125; 后续观察持续运行一条后，没有再复现， 也不再有 http-nio-16080-BlockPoller 和 http-nio-16080-ClientPoller 线程]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
        <category>arthas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[oracle-jdk收费 和 openjdk下载]]></title>
    <url>%2Fjava%2Fjdk%2Fopenjdk-download%2F</url>
    <content type="text"><![CDATA[oracle-jdk License变更oracle-jdk 从 jdk 8u111更新开始收费 可以看到Oracle对于个人使用还是免费，不过商业版就要收费； 此外，还提供了openjdk的下载地址（https://jdk.java.net/）； openjdk 下载https://jdk.java.net/java-se-ri/8]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11、Elasticsearch For Hadoop]]></title>
    <url>%2Felasticsearch%2Fdoc%2F11-Elasticsearch-For-Hadoop%2F</url>
    <content type="text"><![CDATA[11.1、单机版Hadoop安装 11.2、ES-Hadoop安装 11.3、从 HDFS 到 Elasticsearch 11.4、从Elasticsearch 到 HDFS -—————————————————– 11.1、单机版Hadoop安装hadoop分为 单机模式、伪分布式模式、完全分布式模式 1、ssh免密登录 vi /etc/ssh/ssh_config 文件尾添加： 12StrictHostKeyChecking noUserKnownHostsFile /dev/null 2、hadoop下载安装[root@localhost data]# mkdir -p /data/hadoop [root@localhost data]# cd /data/hadoop [root@localhost hadoop]# wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz 3、haoop 单机模式[root@localhost hadoop]# tar -zxf hadoop-2.7.7.tar.gz [root@localhost hadoop]# mkdir input [root@localhost hadoop]# echo “hello world” &gt; input/file1.txt [root@localhost hadoop]# echo “hello hadoop” &gt; input/file2.txt [root@localhost hadoop]# ./hadoop-2.7.7/bin/hadoop jar ./hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /data/hadoop/input/ /data/hadoop/output [root@localhost hadoop]# cat output/part-r-00000 4、hadoop 伪分布式模式[root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/hadoop-env.sh 123456修改JAVA_HOME 为：export JAVA_HOME=/data/tools/jdk1.8.0_65export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"改为：export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=" [root@localhost hadoop]# mkdir -p /data/hadoop/hdfs/tmp [root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop.tmp.dir 可以自定义； fs.default.name 保存了NameNode的位置，HDFS和MapReduce组件都需要用到它； [root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/mapred-site.xml.template 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;tconfiguration&gt; mapred.job.tracker 保存JobTracker的位置，只有MapReduce需要知道； [root@localhost hadoop]# vi hadoop-2.7.7/etc/hadoop/hdfs-site.xml 配置HDFS数据库的复制次数： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 以上就配置完了，接下来格式化 namenode： [root@localhost hadoop]# ./hadoop-2.7.7/bin/hadoop namenode -format 启动hadoop： [root@localhost hadoop]# ./hadoop-2.7.7/sbin/start-all.sh 启动成功； 添加hadoop到环境变量： [root@localhost hadoop-2.7.7]# vi /etc/profile 12export HADOOP_HOME=/data/hadoop/hadoop-2.7.7export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin [root@localhost hadoop-2.7.7]# source /etc/profile 5、HDFS常用操作开放 50070 端口 [root@localhost hadoop-2.7.7]# firewall-cmd –permanent –add-port=50070/tcp success [root@localhost hadoop-2.7.7]# firewall-cmd –reload success [root@localhost hadoop-2.7.7]# firewall-cmd –query-port=50070/tcp yes 访问hdfs：http://172.18.1.51:50070/ [root@localhost hadoop]# hadoop fs -ls / [root@localhost hadoop]# hadoop fs -mkdir /work [root@localhost hadoop]# touch aa.txt [root@localhost hadoop]# hadoop fs -put aa.txt /work [root@localhost hadoop]# hadoop fs -test -e /work/aa.txt [root@localhost hadoop]# echo $? 0 [root@localhost hadoop]# echo “hello hdfs” &gt; aa.txt [root@localhost hadoop]# hadoop fs -appendToFile aa.txt /work/aa.txt [root@localhost hadoop]# hadoop fs -cat /work/aa.txt hello hdfs [root@localhost hadoop]# hadoop fs -rm /work/aa.txt 19/06/10 05:04:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes. Deleted /work/aa.txt [root@localhost hadoop]# hadoop fs -rmr /work/a rmr: DEPRECATED: Please use ‘rm -r’ instead. rmr: `/work/a’: No such file or directory [root@localhost hadoop]# hadoop dfsadmin -report [root@localhost hadoop]# ​ 11.2、引入ES-Hadoop依赖 12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-hadoop&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt; &lt;version&gt;2.7.7&lt;/version&gt; &lt;/dependency&gt; ​ 11.3、从 HDFS 到 Elasticsearch准备json文档，上传到HDFS中 vi blog.json 123&#123;"id":5,"title":"JavaScript高级程序设计","language":"javascript","author":"NicholasC.Zakas","price":66.4,"publish_time":"2012-03-02","desc":"JavaScript技术经典名著。"&#125;&#123;"id":3,"title":"Python科学计算","language":"python","author":"张若愚","price":81.4,"publish_time":"2014-01-02","desc":"零基础学python，光盘中坐着度假开发winPython运行环境，涵盖了Python各个扩展库。"&#125;&#123;"id":4,"title":"Python基础教程","language":"python","author":"Helant","price":54.5,"publish_time":"2014-03-02","desc":"经典的python入门教程，层次鲜明，结构严谨，内容详实。"&#125; [root@localhost hadoop]# hadoop fs -put blog.json /work 编写代码，从 hdfs 读取数据，写入到 Elasticsearch： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.learn.eshdoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.elasticsearch.hadoop.mr.EsOutputFormat;import java.io.IOException;/** * 读取 HDFS 上的内容然后写入 Elasticsearch */public class HdfsToEs &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); conf.setBoolean("mapred.map.tasks.speculative.execution", false); conf.setBoolean("mapred.reduce.tasks.speculative.execution", false); conf.set("es.nodes", "10.10.139.42"); conf.set("es.port", "9200"); conf.set("es.nodes.wan.only", "true"); conf.set("es.resource", "blog/_doc"); conf.set("es.mapping.id", "id"); conf.set("es.input.json", "yes"); Job job = Job.getInstance(conf, "EmrToES"); job.setJarByClass(HdfsToEs.class); job.setMapperClass(MyMapper.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(EsOutputFormat.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(otherArgs[0])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125; public static class MyMapper extends Mapper&lt;Object, Text, NullWritable, Text&gt; &#123; private Text line = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; if (value.getLength() &gt; 0) &#123; line.set(value); context.write(NullWritable.get(), line); &#125; &#125; &#125;&#125; pom中指定主类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.learn.eshdoop.HdfsToEs&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt;&lt;goal&gt;shade&lt;/goal&gt;&lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;com.learn.eshdoop.HdfsToEs&lt;/mainClass&gt; &lt;/transformer&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer"&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 运行：mvn clean package 生成对应的jar 包 [root@localhost hadoop]# mv es-hadoop-learn-1.0-SNAPSHOT-jar-with-dependencies.jar HdfsToEs.jar [root@localhost hadoop]# hadoop jar HdfsToEs.jar /work/blog.json 在Kibana 中 查看 GET blog/_mapping GET blog/_search 11.4、从Elasticsearch 到 HDFS1、将索引 blog 保存到 hdfs添加Java类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.eshdoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.Writable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.elasticsearch.hadoop.mr.EsInputFormat;import java.io.IOException;/** * 查询 Elasticsearch 索引然后将结果写入 HDFS */public class EsToHdfs &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); conf.set("es.nodes", "10.10.139.42"); conf.set("es.port", "9200"); conf.set("es.resource", "blog/_doc"); conf.set("es.output.json", "true"); Job job = Job.getInstance(conf, "hadoop es write test"); job.setMapperClass(MyMapper.class); job.setNumReduceTasks(1); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(EsInputFormat.class); FileOutputFormat.setOutputPath(job, new Path(otherArgs[0])); job.waitForCompletion(true); &#125; public static class MyMapper extends Mapper&lt;Writable, Writable, NullWritable, Text&gt; &#123; @Override protected void map(Writable key, Writable value, Context context) throws IOException, InterruptedException &#123; Text text = new Text(); text.set(value.toString()); context.write(NullWritable.get(), text); &#125; &#125;&#125; 修改pom.xml中的主类为：com.learn.eshdoop.EsToHdfs。 mvn clean package 重新打包 上传以后执行： [root@localhost hadoop]# mv es-hadoop-learn-1.0-SNAPSHOT-jar-with-dependencies.jar EsIndexToHdfs.jar [root@localhost hadoop]# hadoop jar EsIndexToHdfs.jar /work/blog_mapping [root@localhost hadoop]# hadoop fs -cat /work/blog_mapping/part-r-00000 2、将带条件查询 blog 的 数据 保存到 hdfs1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.eshdoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.Writable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.elasticsearch.hadoop.mr.EsInputFormat;import java.io.IOException;/** * 查询 Elasticsearch 数据 然后将结果写入 HDFS */public class EsQueryToHdfs &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); conf.set("es.nodes", "10.10.139.42"); conf.set("es.port", "9200"); conf.set("es.resource", "blog/_doc"); conf.set("es.output.json", "true"); conf.set("es.query", "?q=title:python"); Job job = Job.getInstance(conf, "query es to HDFS"); job.setMapperClass(EsToHdfs.MyMapper.class); job.setNumReduceTasks(1); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); job.setInputFormatClass(EsInputFormat.class); FileOutputFormat.setOutputPath(job, new Path(otherArgs[0])); job.waitForCompletion(true); &#125; public static class MyMapper extends Mapper&lt;Writable, Writable, NullWritable, Text&gt; &#123; @Override protected void map(Writable key, Writable value, Context context) throws IOException, InterruptedException &#123; Text text = new Text(); text.set(value.toString()); context.write(NullWritable.get(), text); &#125; &#125;&#125; 在pom.xml中修改主类名称为：com.learn.eshdoop.EsQueryToHdfs； mvn clean package 重新打包，然后上传； [root@localhost hadoop]# mv es-hadoop-learn-1.0-SNAPSHOT-jar-with-dependencies.jar EsQueryToHdfs.jar [root@localhost hadoop]# hadoop jar EsQueryToHdfs.jar /work/EsQueryToHdfs [root@localhost hadoop]# hadoop fs -cat /work/EsQueryToHdfs/part-r-00000]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用微服务组件]]></title>
    <url>%2Fmicro-service%2Fspring-cloud%2Fregister-center%2F</url>
    <content type="text"><![CDATA[注册中心nacos、eureka、zookeeper、redis、consul、etcd3 配置中心]]></content>
  </entry>
  <entry>
    <title><![CDATA[shardingSphere 之分布式事务]]></title>
    <url>%2Fjava%2Fsharding-sphere%2Fdisributed-transaction%2F</url>
    <content type="text"><![CDATA[强一致性事务XA 2PC 3PC 缺点：协调者或参与者 故障以后，会出现数据不一致的问题、3PC虽然比2PC有所改善，但并没有从根本上解决问题 市面上有关于2PC模型的实现，没有3PC模型的实现 基于2PC的模型：TCC、Seata sharding-shere 通过SPI实现的XA事务的组件： bitronix narayana atomickos 柔性事务BASE -&gt; Seata(AT模型) 其他事务通过SPI自己实现 TCC XA 和 TCC 的去别：XA 是基于资源的，代码不用改；TCC是基于应用的，比较复杂 TCC：try() ==&gt; 预留资源 ByteTCC TinyTCC transactionTCC Saga应用场景：长事务（整个事务运行时间比较长） 实现方式：MQ，ZK 拆分成，多个子事务 事件编排命令协调 SeataAT模式 工作流程 使用示例 Seata XA Atomikos TCC 业务实现TCC过程，由TM去调度 =&gt; 本地状态表 Saga 正向和反向， 编排+命令 =&gt; MQ/ZK AT 本地事务表 MQ JTA（Java Transaction API）轻量级事务：XA 重量级事务：容器事务Weblogic JBoss 数据编排治理配置中心：如果修改了配置，不需要重启应用 注册中心：运行时让DB动态上下线（熔断） 元数据中心： 数据库结构 zk做配置中心和注册中心 元数据中心 工程结构]]></content>
  </entry>
  <entry>
    <title><![CDATA[gradle编译springboot的jar包中，so库被修改]]></title>
    <url>%2Fdevops%2Fgradle%2Fgradle-bug-so-md5%2F</url>
    <content type="text"><![CDATA[最近将maven工程修改为gradle工程 打包后jni调用直接crash。通过和maven打出的jar包对比，发现.so库发生了签名发生了变更 解决方案： 将so库放在resources之外的文件夹中 修改 build.gradle ，添加如下指令 12345bootJar &#123; from(&quot;src/main/assets&quot;) &#123; into &quot;BOOT-INF/classes&quot; &#125;&#125;]]></content>
      <categories>
        <category>gradle</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch性能优化专题]]></title>
    <url>%2Felasticsearch%2Felasticsearch-performance-optimizing%2F</url>
    <content type="text"><![CDATA[写入性能优化1、用bulk批量写入你如果要往es里面灌入数据的话，那么根据你的业务场景来，如果你的业务场景可以支持让你将一批数据聚合起来，一次性写入es，那么就尽量采用bulk的方式，每次批量写个几百条这样子。 bulk批量写入的性能比你一条一条写入大量的document的性能要好很多。但是如果要知道一个bulk请求最佳的大小，需要对单个es node的单个shard做压测。先bulk写入100个document，然后200个，400个，以此类推，每次都将bulk size加倍一次。如果bulk写入性能开始变平缓的时候，那么这个就是最佳的bulk大小。并不是bulk size越大越好，而是根据你的集群等环境具体要测试出来的，因为越大的bulk size会导致内存压力过大，因此最好一个请求不要发送超过10mb的数据量。 先确定一个是bulk size，此时就尽量是单线程，一个es node，一个shard，进行测试。看看单线程最多一次性写多少条数据，性能是比较好的。 2、使用多线程将数据写入es单线程发送bulk请求是无法最大化es集群写入的吞吐量的。如果要利用集群的所有资源，就需要使用多线程并发将数据bulk写入集群中。为了更好的利用集群的资源，这样多线程并发写入，可以减少每次底层磁盘fsync的次数和开销。首先对单个es节点的单个shard做压测，比如说，先是2个线程，然后是4个线程，然后是8个线程，16个，每次线程数量倍增。一旦发现es返回了TOO_MANY_REQUESTS的错误，JavaClient也就是EsRejectedExecutionException。此时那么就说明es是说已经到了一个并发写入的最大瓶颈了，此时我们就知道最多只能支撑这么高的并发写入了。 3、增加refresh间隔默认的refresh间隔是1s，用index.refresh_interval参数可以设置，这样会其强迫es每秒中都将内存中的数据写入磁盘中，创建一个新的segment file。正是这个间隔，让我们每次写入数据后，1s以后才能看到。但是如果我们将这个间隔调大，比如30s，可以接受写入的数据30s后才看到，那么我们就可以获取更大的写入吞吐量，因为30s内都是写内存的，每隔30s才会创建一个segment file。 4、禁止refresh和replia如果我们要一次性加载大批量的数据进es，可以先禁止refresh和replia复制，将index.refresh_interval设置为-1，将index.number_of_replicas设置为0即可。这可能会导致我们的数据丢失，因为没有refresh和replica机制了。但是不需要创建segment file，也不需要将数据replica复制到其他的replica shasrd上面去。此时写入的速度会非常快，一旦写完之后，可以将refresh和replica修改回正常的状态。 5、禁止swapping交换内存如果要将es jvm内存交换到磁盘，再交换回内存，大量磁盘IO，性能很差 6、给filesystem cache更多的内存filesystem cache被用来执行更多的IO操作，如果我们能给filesystemcache更多的内存资源，那么es的写入性能会好很多。 7、使用自动生成的id如果我们要手动给es document设置一个id，那么es需要每次都去确认一下那个id是否存在，这个过程是比较耗费时间的。如果我们使用自动生成的id，那么es就可以跳过这个步骤，写入性能会更好。对于你的业务中的表id，可以作为es document的一个field。 8、用性能更好的硬件我们可以给filesystem cache更多的内存，也可以使用SSD替代机械硬盘，避免使用NAS等网络存储，考虑使用RAID 0来条带化存储提升磁盘并行读写效率，等等。 9、index buffer如果我们要进行非常重的高并发写入操作，那么最好将index buffer调大一些，indices.memory.index_buffer_size，这个可以调节大一些，设置的这个index buffer大小，是所有的shard公用的，但是如果除以shard数量以后，算出来平均每个shard可以使用的内存大小，一般建议，但是对于每个shard来说，最多给512mb，因为再大性能就没什么提升了。es会将这个设置作为每个shard共享的index buffer，那些特别活跃的shard会更多的使用这个buffer。默认这个参数的值是10%，也就是jvm heap的10%，如果我们给jvmheap分配10gb内存，那么这个index buffer就有1gb，对于两个shard共享来说，是足够的了。 查询性能优化]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ELK单节点集群搭建]]></title>
    <url>%2Felasticsearch%2Felk-single-node-cluster%2F</url>
    <content type="text"><![CDATA[安装 jdk（jdk7+）tar -zxf jdk-8u65-linux-x64.tar.gz -C /data/carloz/tools/设置java环境变量vim /etc/profile 1234export JAVA_HOME=/carloz/tools/jdk1.8.0_65export JRE_HOME=$JAVA_HOME/jreexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport CLASSPATH=$CLASSPATH:.:/JAVA_HOME/lib:$JAVA_HOME/jre/lib source /etc/profilejava -version 安装 Elasticsearchtar -zxf elasticsearch-7.0.0-linux-x86_64.tar.gz -C /data/carloz/tools/cd elasticsearch-7.0.0/ ./bin/elasticsearch 这是出于系统安全考虑设置的条件。由于ElasticSearch可以接收用户输入的脚本并且执行，为了系统安全考虑。创建elsearch用户组及elsearch用户groupadd elsearchuseradd elsearch -g elsearch -p elasticsearchcd /data/carloz/tools/chown -R elsearch:elsearch elasticsearch-7.0.0/su elsearchcd elasticsearch-7.0.0/./bin/elasticsearch -d 一个警告，操作系统内核版本太低，忽略即可 netstat -nltp | grep 9200 ps aux | grep elastic* curl ‘http://127.0.0.1:9200&#39;如下代表启动成功： 配置Elasticsearchsu elsearchcd /data/carloz/tools/elasticsearch-7.0.0vi config/elasticsearch.ymlgrep -v ^# config/elasticsearch.yml 集群监控curl -XGET ‘http://localhost:9200/_cluster/health?pretty=true&#39; 安装 Logstashsu rootcd /carloz/downloadtar -zxf logstash-7.0.0.tar.gz -C /data/carloz/tools/cd /data/carloz/tools/logstash-7.0.0/ 定义 一个叫 stdin 的数据源，和一个叫stdout的输出目标，无论我们输入什么，都会输出到标准命令行：./bin/logstash -e ‘input { stdin { } } output { stdout {} }’等待启动成功之后，输入 hello logastsh 1、Logstash 输入源2、Logstash 输出目标3、Logstash 过滤器 定义输入源，输出目标： mkdir -p myconfvi myconf/jyweb-nginx.conf 1234567891011input&#123; file&#123; path =&gt; ["/opt/software/nginx/logs/*.log"] &#125;&#125;output&#123; elasticsearch&#123; hosts =&gt; ["localhost:9200"] index =&gt; "jyweb_nginx_log" &#125;&#125; 启动logstash./bin/logstash -f myconf/jyweb-nginx.conf &amp; 安装 Kibanatar -zxf /carloz/download/kibana-7.0.0-linux-x86_64.tar.gz -C /data/carloz/tools/cd /data/carloz/tools/kibana-7.0.0-linux-x86_64 修改配置文件，指向Elasticsearch服务器vi config/kibana.yml 1234server.port: 5601server.host: "0.0.0.0"server.name: "logs-server"elasticsearch.hosts: ["http://localhost:9200"] 启动./bin/kibana 访问：http://10.10.139.42:5601]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7 下 Portainer安装配置]]></title>
    <url>%2Fdevops%2Fdocker%2Fportainer-setup%2F</url>
    <content type="text"><![CDATA[Portainer 安装 Portainer 是一款轻量级的应用，它提供了图形化界面，用于方便的管理Docker环境，包括单机环境和集群环境，下面我们将用Portainer来管理Docker容器中的应用。 官网地址：https://github.com/portainer/portainer 获取Docker镜像文件： 1docker pull portainer/portainer 使用docker容器运行Portainer： 12345docker run -p 9000:9000 -p 8000:8000 --name portainer \--restart=always \-v /var/run/docker.sock:/var/run/docker.sock \-v /data/docker/portainer/data:/data \-d portainer/portainer 开放端口 123firewall-cmd --permanent --add-port=9000/tcpfirewall-cmd --permanent --add-port=8000/tcpfirewall-cmd --reload 修改客户端host 1234C:\Windows\System32\drivers\etc\hosts# 文件改为读写192.168.145.137 czportainer.com# 文件改为只读 查看Portainer的DashBoard信息： http://czportainer.com:9000 admin/admin123 Endpoints 配置local连接到本机 docker API通过 docker 的 2375 端口，控制其他docker主机 配置docker远程api 1234567$ vi /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375systemctl daemon-reloadsystemctl restart dockerfirewall-cmd --permanent --add-port=2375/tcpfirewall-cmd --reload 在portainer上配置 在home中查看]]></content>
      <categories>
        <category>devops</category>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker仓库Harbor安装配置]]></title>
    <url>%2Fdevops%2Fdocker%2Fharbor-setup%2F</url>
    <content type="text"><![CDATA[技术选型开源Docker Registry的不足同很多团队进行容器云构建的初始阶段一样，一开始我们并不太注重安全，容器仓库一般都是配置成insecure-registry直接使用。这次，我们决定加强安全方面的控制。我们发现了它的一些不足如下： 用户与鉴权：Docker Registry V2可以基于htpasswd文件进行简单的用户管理，但是维护不便，也没有对外的API可供集成。 缺少日志与审记：Docker Registry没有日志收集能力，也缺少审记。 复制与同步：在不同的Registry实例之间复制镜像是十分普遍的需求，但Docker Registry只支持将公有Registry的镜像同步至私有仓库 。 镜像删除不便：Docker客户端没有提供删除仓库镜像的命令，删除仓库中的镜像，只能通过其它工具调用rest api 。 缺少图形化的管理界面：要想补上这些缺点，需要在自已的容器云中做一些加强，需要工作量。除了Docker Registry，还有没有其它的选择，已经补齐了这些能力，或部分补齐了呢？于是，我们做了一点小了解。 业界可选的Registry Docker Hub 提供了直观的界面、自动化构建、私有仓库以及众多官方镜像。这是官方的公有库，下载镜像必须联接外网。 Docker Registry 最流行的开源registry。你可以在自己的设施上运行或者使用Docker Hub。 Quay.io 最初由一个两人工作室开发的产品，专注于Docker私有库。目前已被Coreos收购。 CoreOS Enterprise Registry Coreos收购Quay.io之后推出的企业级Containger Registry，提供细化权限和审计跟踪。 Nexus 3.0 nexus原来只是一个maven的仓库服务器，升级到3.0之后，也可以使用它对docker的镜像进行管理。 Harbor vmware开源的企业级容器registry，基于开源的Docker Registry进行增强。 Docker Hub相信大家都不陌生，用来做私库的源库是可以的，可以将一些基础镜像从这个云库拉到私库中。它并不适合在容器云中直接使用，下载镜像时需要连外网，速度也是个大问题。 Docker Registry就不说了，接下来的Quay.io，CoreOS Enterprise Registry是付费的，或者是商业付费的，不考虑。Nexus 3有图形化的管理页面可以操作镜像，但其它的能力与Docker Registry差不多。最后看到的harbor，则令我们眼前一亮。 新的选择–HarborHarbor是VMware公司于2016年开源的企业级Docker Registry项目。它是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。 作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。 Harbor的特点 基于角色的访问控制：用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。 镜像复制：镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。 图形化用户界面：用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。 AD/LDAP 支持：Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。 审计管理：所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。 国际化：已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。 RESTful API：RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。 部署简单：提供在线和离线两种安装工具，也可以安装到vSphere平台(OVA方式)虚拟设备。 Harbor的整体架构 Harbor在架构上主要由五个组件构成： Proxy：Harbor的registry, UI, token等服务，通过一个前置的反向代理统一接收浏览器、Docker客户端的请求，并将请求转发给后端不同的服务。 Registry：负责储存Docker镜像，并处理dockerpush/pull 命令。由于我们要对用户进行访问控制，即不同用户对Docker image有不同的读写权限，Registry会指向一个token服务，强制用户的每次docker pull/push请求都要携带一个合法的token,Registry会通过公钥对token 进行解密验证。 Core services：这是Harbor的核心功能，主要提供以下服务：UI：提供图形化界面，帮助用户管理registry上的镜像（image）, 并对用户进行授权。 webhook：为了及时获取registry上image状态变化的情况， 在Registry上配置webhook，把状态变化传递给UI模块。token服务：负责根据用户权限给每个docker push/pull命令签发token.Docker客户端向Regiøstry服务发起的请求,如果不包含token，会被重定向到这里，获得token后再重新向Registry进行请求。 Database：为core-services提供数据库服务，负责储存用户权限、审计日志、Docker image分组信息等数据。 Log collector：为了帮助监控Harbor运行，负责收集其他组件的log，供日后进行分析。 harbor的核心概念 以上是Harbor的核心概念。一个项目可以看成一个用户的小私库，项目成员按照其角色，可以上传，下载，删除镜像，或添加删除成员等。系统管理员可以创建镜像库复制目标，为项目设置复制策略，可以从其它的镜像库复制镜像过来。复制策略会产生复制任务。项目下面可拥有多个镜像，而每个镜像则可以有多个镜像标签。项目包含日志。 Harbor安装前提docker 和 docker-compose 安装完成 创建 https 证书12345678910111213141516171819# 创建证书目录，并赋予权限mkdir -p /data/cert &amp;&amp; chmod -R 777 /data/cert &amp;&amp; cd /data/cert# 生成私钥，需要设置密码openssl genrsa -des3 -out harbor.key 2048# 生成CA证书，需要输入密码openssl req -sha512 -new \ -subj "/C=CN/ST=JS/L=WX/O=zwx/OU=jhmy/CN=hub.jhmy.com" \ -key harbor.key \ -out harbor.csr# 备份证书cp harbor.key harbor.key.org# 退掉私钥密码，以便docker访问（也可以参考官方进行双向认证）openssl rsa -in harbor.key.org -out harbor.key# 使用证书进行签名openssl x509 -req -days 365 -in harbor.csr -signkey harbor.key -out harbor.crt 安装文档 1https://goharbor.io/docs/1.10/install-config/ 下载解压1https://github.com/goharbor/harbor/releases 123cd /data/toolstar -zxf harbor-offline-installer-v1.10.1.tgzcd /data/tools/harbor 配置harbor.yml 1234567891011121314hostname: 192.168.145.130http: port: 80https: port: 443 certificate: /data/cert/harbor.crt private_key: /data/cert/harbor.keyharbor_admin_password: 123456database: password: 123456 安装 1./install.sh 卸载 1# docker-compose -f /data/tools/harbor/docker-compose.yml down 配置 12345vim /etc/docker/daemon.json&#123; "insecure-registries": ["192.168.145.130"]&#125;service docker restart 端口开放 12345firewall-cmd --permanent --add-port=80/tcpfirewall-cmd --permanent --add-port=443/tcpfirewall-cmd --permanent --add-port=5000/tcpfirewall-cmd --reloadfirewall-cmd --list-all 启动&amp;停止12docker-compose -f /data/tools/harbor/docker-compose.yml stopdocker-compose -f /data/tools/harbor/docker-compose.yml start 设置host解析linux123vi /etc/hosts192.168.145.130 czharbor.comservice network restart win71234C:\Windows\System32\drivers\etc\hosts# 文件改为读写192.168.145.130 czharbor.com# 文件改为只读 访问验证12https://czharbor.com/harbor/registriesadmin/123456 自定义仓库 上传测试镜像1234docker login -u admin -p 123456 czharbor.comdocker tag redis:3.2 czharbor.com/dev/redis:3.2docker push czharbor.com/dev/redis:3.2# docker logout czharbor.com]]></content>
      <categories>
        <category>devops</category>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jenkins+harbor+gitee]]></title>
    <url>%2Fdevops%2Fjenkins%2Fjenkins-deploy-springcloud-to-centos7%2F</url>
    <content type="text"><![CDATA[宿主机软件安装jdk12[root@centos7cz tools]# whereis javajava: /usr/bin/java /carloz/tools/jdk1.8.0_231/bin/java maven 下载解压 1234567cd /carloz/tools/wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gztar -zxf apache-maven-3.6.3-bin.tar.gzmv apache-maven-3.6.3 maven-3.6.3cd maven-3.6.3/pwd/carloz/tools/maven-3.6.3 配置环境变量 12345vi /etc/profileMAVEN_HOME=/carloz/tools/maven-3.6.3export PATH=$&#123;MAVEN_HOME&#125;/bin:$&#123;PATH&#125;source /etc/profile 查看结果 1mvn -v 替换阿里源 1vi /carloz/tools/maven-3.6.3/conf/settings.xml 找到&lt;mirrors&gt;&lt;/mirrors&gt;标签对，添加一下代码： 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt; 指定下载资源位置 123mkdir -p /data/maven-repovi /carloz/tools/maven-3.6.3/conf/settings.xml&lt;localRepository&gt;/data/maven-repo&lt;/localRepository&gt; 指定jdk版本 1234567891011121314vi /carloz/tools/maven-3.6.3/conf/settings.xml&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; git123456[root@centos7cz ~]# yum install -y git[root@centos7cz ~]# whereis gitgit: /usr/bin/git /usr/share/man/man1/git.1.gz[root@centos7cz ~]# git --versiongit version 1.8.3.1[root@centos7cz ~]# jenkins做常规配置点击系统管理-&gt;插件管理，进行一些自定义的插件安装： 角色管理权限插件Role-based Authorization Strategy ssh安装ssh插件 配置公钥凭证 在系统管理-&gt;系统配置中添加全局ssh的配置，这样Jenkins使用ssh就可以执行远程的linux脚本了： jdk路径： 系统管理 -&gt; 全局工具配置 -&gt; JDK maven通过系统管理-&gt;全局工具配置来进行全局工具的配置，比如maven的配置 git安装插件： Build With Parameters Git Parameter git可执行文件配置 gitee在线安装 前往 Manage Jenkins -&gt; Manage Plugins -&gt; Available 右侧 Filter 输入： Gitee 下方可选列表中勾选 Gitee（如列表中不存在 Gitee，则点击 Check now 更新插件列表） 点击 Download now and install after restart 添加码云链接配置 前往 Jenkins -&gt; Manage Jenkins -&gt; Configure System -&gt; Gitee Configuration -&gt; Gitee connections 在 Connection name 中输入 Gitee 或者你想要的名字 Gitee host URL 中输入码云完整 URL地址： https://gitee.com （码云私有化客户输入部署的域名） Credentials 中如还未配置码云 APIV5 私人令牌，点击Add- &gt; Jenkins Domain 选择 Global credentials Kind 选择 Gitee API Token Scope 选择你需要的范围 Gitee API Token 输入你的码云私人令牌，获取地址：https://gitee.com/profile/personal_access_tokens ID, Descripiton 中输入你想要的 ID 和描述即可。 Credentials 选择配置好的 Gitee APIV5 Token 点击 Advanced ，可配置是否忽略 SSL 错误（适您的Jenkins环境是否支持），并可设置链接测超时时间（适您的网络环境而定） 点击 Test Connection 测试链接是否成功，如失败请检查以上 3，5，6 步骤。 配置成功后如图所示： 执行脚本准备 首先我们先把需要远程执行的脚本准备好。 脚本文件都存放在了mall-swarm项目的/document/sh目录下 上传脚本前在IDEA中修改所有脚本文件的换行符格式为LF，否则脚本会无法执行； 将所有脚本文件上传到指定目录，这里我们上传到/data/carloz/mall-swarm目录下； 12345mkdir -p /data/carloz/mall-swarm-shcd /data/carloz/mall-swarm-sh# 将所有脚本文件都修改为可执行文件:chmod +x ./mall-* Jenkins中创建任务 接下来我们将通过在Jenkins中创建任务来实现自动化部署。由于我们的mall-swarm是个多模块的项目，部署上面和曾经的单模块项目还是有所区别的 创建gitee登录凭据 mall-admin 由于各个模块的执行任务的创建都大同小异，下面将详细讲解mall-admin模块任务的创建，其他模块将简略讲解。 首先我们选择构建一个自由风格的软件项目，然后输入任务名称为mall-admin，配置其Git仓库地址，这里我直接使用了Gitee上面的地址： 之后我们创建一个构建，构建mall-swarm项目中的依赖模块，否则当构建可运行的服务模块时会因为无法找到这些模块而构建失败； 12# 只install mall-common,mall-mbg,mall-security三个模块clean install -pl mall-common,mall-mbg,mall-security -am 依赖项目构建示意图： 再创建一个构建，单独构建并打包mall-admin模块： 添加一个构建来通过SSH去执行远程任务，用于执行mall-admin的运行脚本： 点击保存，完成mall-admin的执行任务创建 mall-registry mall-registry和其他模块与mall-admin创建任务方式基本一致，只需修改构建模块时的pom.xml文件位置和执行脚本位置即可。 我们可以直接从mall-admin模块的任务复制一个过来创建： 修改第二个构建中的pom.xml文件位置，改为：${WORKSPACE}/mall-registry/pom.xml 修改第三个构建中的SSH执行脚本文件位置，改为：/data/carloz/mall-swarm-sh/mall-registry.sh 点击保存，完成mall-registry的执行任务创建。 尝试构建 mall-registry 模块，如果构建成功，再创建其他模块的构建任务 其他模块其他模块的执行任务创建，参考mall-admin和mall-registry的创建即可。 模块启动顺序问题 关于各个模块的启动顺序问题，mall-registry模块必须第一个启动，mall-config模块必须第二个启动，其他模块没有启动顺序限制。 推荐启动顺序： mall-registry mall-config mall-monitor mall-gateway mall-admin mall-portal mall-search mall-demo 总结我们通过在Jenkins中创建任务，完成了微服务架构中服务的打包部署工作，这样当我们每次修改完代码后，只需点击启动任务，就可以实现一键打包部署，省去了频繁打包部署的麻烦。 项目地址https://github.com/macrozheng/mall-swarm]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>springcloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10、ES新闻搜索实战]]></title>
    <url>%2Felasticsearch%2Fdoc%2F10-Elasticsearch%E6%96%B0%E9%97%BB%E6%90%9C%E7%B4%A2%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[10.0、本文前提 10.1、从新浪采集新闻存储到mysql 10.2、将mysql数据 同步到 Elasticsearch 10.3、从Elasticsearch中搜索数据，返回给前端 10.4、ES新闻搜索实战 —— 查询界面 10.5、ES新闻搜索实战 —— 结果展示 -————————————————– 项目仓库：https://gitee.com/carloz/elastic-learn.git 具体地址：https://gitee.com/carloz/elastic-learn/tree/master/elasticsearch-news -————————————————– 本文前提已经部署好了ELK系统 10.1、从新浪采集新闻存储到mysqlhttp://finance.sina.com.cn/7x24/ 12345678910111213141516171819202122232425262728# 新浪新闻采集脚本### 1、使用python 2.7 解析器配置环境变量，path下添加：D:\python\tools\Python27;D:\python\tools\Python27\Scripts;安装依赖模块：pip install requestshttps://sourceforge.net/projects/mysql-python/下载64位版本的mysql: http://www.codegood.com/download/11/### 2、新建数据库CREATE DATABASE `sina_news`;CREATE TABLE `news` ( `id` int(11) NOT NULL, `news_type` enum('其他','央行','观点','市场','数据','公司','行业','宏观','A股') NOT NULL, `create_time` datetime DEFAULT NULL, `rich_text` text, PRIMARY KEY (`id`));### 3、运行，即可在数据库里看到数据 python数据采集脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/python# -*- coding: UTF-8 -*-import stringimport jsonimport timeimport requestsimport MySQLdbimport randomimport reconn = MySQLdb.connect("10.10.87.38", "crm", "crm@2015", "sina_news", charset='utf8')cursor = conn.cursor()template_url = string.Template( 'http://zhibo.sina.com.cn/api/zhibo/feed?callback=jQuery$jQueryId&amp;page=1&amp;page_size=$page_size&amp;zhibo_id=152&amp;tag_id=$tag_id&amp;dire=f&amp;dpc=1&amp;pagesize=$page_size&amp;_=$datetime')tag_ids = &#123;u'A股': 10, u'宏观': 1, u'行业': 2, u'公司': 3, u'数据': 4, u'市场': 5, u'观点': 6, u'央行': 7, u'其他': 8,&#125;headers = &#123; 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'zh-CN,zh;q=0.9', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36', 'Accept': '*/*', # 'Referer' : 'http://finance.sina.com.cn/7x24/?tag=10', 'Connection': 'keep-alive', 'Cookie': 'U_TRS1=000000d5.5a546170.5cb7de02.83b7c4e0; U_TRS2=000000d5.5a616170.5cb7de02.256bb0da; UOR=www.baidu.com,blog.sina.com.cn,; SINAGLOBAL=114.114.114.114_1555553794.454804; Apache=114.114.114.213_1555553794.454805; ULV=1555553794485:1:1:1:114.114.114.114_1555553794.454805:; SCF=AhOLahPmRlTviyZ4YQHaxRNdunCqZL3kO2SBnELkwjeVg8ZMdSXgud0IsBd4CaJIt5s-9YmaaRxgNVK4w6koPXE.; ULOGIN_IMG=gz-d89f6db983d2c25da42c59504991a4867f53; sso_info=v02m6alo5qztLSNk4S5jJOQs46TnKadlqWkj5OEuI6DnLCOg4y1jbOMwA==; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhM3uQ2UWBVDQgNwIoS4aG35NHD95Qp1hnNehn0SKM0Ws4Dqcjci--Xi-zRiKn7i--fiKysi-8Wi--fi-2Xi-2Ni--RiK.7iKyhi--fiKnfiK.Xi--fi-82iK.7; _s_upa=5; lxlrttp=1556243090; NEWSCENTER=522b9f1b6a2f61766931ac50242bed94; SUB=_2A25x5Y0UDeRhGedG41UR-C3JzD-IHXVSkvncrDV_PUNbm9BeLWitkW9NUOtWwD6pEwnHVFGGf0Y42aAcKr49dHwM; ALF=1589850308',&#125;for news_type, tag_id in tag_ids.items(): headers['Referer'] = 'http://finance.sina.com.cn/7x24/?tag=%s' % random.choice(tag_ids.values()) datetime = int(1000 * time.time()) crawlurl = template_url.substitute(datetime=datetime, jQueryId="111207214587420816325_%s" % datetime, tag_id=tag_id, page_size=20) try: text = requests.get(crawlurl, timeout=2, headers=headers).text news = json.loads(re.sub('^try[^\(]*\(|\);&#125;catch\(e\)&#123;&#125;;$', '', text))['result']['data']['feed']['list'] except Exception, e: print str(e) continue for data in news: unique_id = data['id'] rich_text = data['rich_text'] create_time = data['create_time'] try: mysql_command = u"insert into sina_news.news (id,news_type,create_time,rich_text) values ('%s','%s','%s','%s')" % (unique_id, news_type, create_time, rich_text) mysql_command += u" on duplicate key update news_type='%s', create_time='%s',rich_text='%s';" % (news_type, create_time, rich_text) cursor.execute(mysql_command) conn.commit() except Exception, e: print mysql_command print str(e) pass 采集到的数据： ​ 10.2、将mysql数据 同步到 Elasticsearch在Elasticsearch中新建索引： 1234567891011121314151617181920PUT sina_news&#123; "mappings": &#123; "properties": &#123; "id": &#123; "type": "integer" &#125;, "news_type": &#123; "type": "keyword" &#125;, "create_time": &#123; "type": "date" &#125;, "rich_text": &#123; "type": "text", "analyzer": "ik_smart" &#125; &#125; &#125;&#125; 同步可选技术： https://github.com/siddontang/go-mysql-elasticsearch —— 没有实际用于生产的例子，不太懂go； https://www.elastic.co/blog/logstash-jdbc-input-plugin —— 全量同步、增量同步，不支持删除；定时任务执行；文档《https://www.cnblogs.com/mignet/p/MySQL_ElasticSearch_sync_By_logstash_jdbc.html》； https://github.com/jprante/elasticsearch-jdbc/tree/master https://github.com/m358807551/mysqlsmom —— 从binlog同步，支持全量、增量同步，支持删除操作；文档《https://elasticsearch.cn/article/756》； https://github.com/mardambey/mypipe —— 写到kafka，需要自己从kafka消费，再处理成json以后写入Elasticsearch； 本次只是一个示例，而且需求里也没有实时同步删除数据，采用官方软件：logstash-input-jdbc； https://github.com/logstash-plugins/logstash-input-jdbc https://github.com/logstash-plugins/logstash-input-jdbc/releases 给Logstash安装jdbc插件： cd /data/carloz/tools/logstash-7.0.0/ ./bin/logstash-plugin install logstash-input-jdbc 上传jdbc包： [root@10-10-139-42 logstash-7.0.0]# mkdir -p mylib [root@10-10-139-42 mylib]# cd /data/carloz/tools/logstash-7.0.0/mylib 上传mysql-connector-java-8.0.15.jar到该目录： 设置配置文件： [root@10-10-139-42 myconf]# vi sina_news.conf 12345678910111213141516171819input &#123; jdbc &#123; jdbc_driver_library =&gt; "/data/carloz/tools/logstash-7.0.0/mylib/mysql-connector-java-8.0.15.jar" jdbc_driver_class =&gt; "com.mysql.jdbc.Driver" jdbc_connection_string =&gt; "jdbc:mysql://10.10.87.38:3306/sina_news" jdbc_user =&gt; "crm" jdbc_password =&gt; "crm@2015" schedule =&gt; "* * * * *" statement =&gt; "select * from news where id &gt; :sql_last_value" use_column_value =&gt; true tracking_column =&gt; "id" &#125;&#125;output &#123; elasticsearch &#123; index =&gt; "sina_news" hosts =&gt; "localhost:9200" &#125;&#125; 开始同步： [root@10-10-139-42 logstash-7.0.0]# ./bin/logstash -f myconf/sina_news.conf &amp; GET sina_news/_search 至此，数据上传成功，并且程序持续监听中。 查看数据： 123456789101112131415161718192021222324GET sina_news/_search&#123; "size": 0, "aggs": &#123; "news_stats": &#123; "stats": &#123; "field": "id" &#125; &#125; &#125;&#125;GET sina_news/_search&#123; "sort": [ &#123; "id": &#123; "order": "desc" &#125; &#125; ], "from": 0, "size": 20&#125; ​ 10.3、从Elasticsearch中搜索数据，返回给前端工程结构如图： 使用springboot开发，在pom.xml中键入依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.learn&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-news&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;elasticsearch-news&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 搜索入口 12345678910111213141516171819/** * @param keywords * @return 搜索结果页面 */@RequestMapping("/search")public ModelAndView searchFile(String keywords, @Nullable Integer from, @Nullable Integer size) &#123; ArrayList&lt;NewsModel&gt; hitsList = null; try &#123; hitsList = mySearchService.searchSinaNews(keywords, from, size); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; log.info(keywords + "：共搜到：" + hitsList.size() + " 条数据！"); ModelAndView mv = new ModelAndView("result.html"); mv.addObject("keywords", keywords); mv.addObject("resultList", hitsList); return mv;&#125; 搜索核心代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.learn.elasticsearchnews.service;import com.learn.elasticsearchnews.model.NewsModel;import com.learn.elasticsearchnews.utils.EsUtils;import org.elasticsearch.action.search.SearchRequest;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.common.unit.Fuzziness;import org.elasticsearch.common.unit.TimeValue;import org.elasticsearch.index.query.MultiMatchQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.rest.RestStatus;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHits;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;import org.elasticsearch.search.sort.FieldSortBuilder;import org.elasticsearch.search.sort.SortOrder;import org.springframework.stereotype.Service;import java.util.ArrayList;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;@Servicepublic class MySearchService &#123; public ArrayList&lt;NewsModel&gt; searchSinaNews(String keywords, Integer from, Integer size) throws Exception &#123; if (null == from) from = 0; if (null == size) size = 10; RestHighLevelClient client = EsUtils.getClient(); String index = "sina_news"; String field1 = "news_type"; String field2 = "rich_text"; MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(keywords, field1, field2) .fuzziness(Fuzziness.AUTO); HighlightBuilder highlightBuilder = new HighlightBuilder() .preTags("&lt;span style=\"color:red\"&gt;") .postTags("&lt;/span&gt;") .field(field1) .field(field2); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(multiMatchQueryBuilder); searchSourceBuilder.sort(new FieldSortBuilder("id").order(SortOrder.DESC)); searchSourceBuilder.from(from); searchSourceBuilder.size(size); searchSourceBuilder.timeout(new TimeValue(5, TimeUnit.SECONDS)); searchSourceBuilder.highlighter(highlightBuilder); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(index); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); ArrayList&lt;NewsModel&gt; resultList = new ArrayList&lt;&gt;(); if(searchResponse.status() == RestStatus.OK) &#123; SearchHits searchHits = searchResponse.getHits(); for (SearchHit hit : searchHits) &#123; Map&lt;String, Object&gt; resMap =hit.getSourceAsMap(); Map&lt;String, NewsModel&gt; newsMap = new HashMap&lt;&gt;(); NewsModel news = new NewsModel( Integer.valueOf(resMap.get("id").toString()), resMap.get("news_type").toString(), resMap.get("rich_text").toString(), resMap.get("create_time").toString()); resultList.add(news); &#125; &#125; return resultList; &#125;&#125; ​ 10.4、ES新闻搜索实战 —— 查询界面在浏览器中访问：http://localhost:18080/ ​ 10.5、ES新闻搜索实战 —— 结果展示在搜索框中搜索“美国”， 因为只是demo，就不做分页了，大家需要自己做即可]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.1、ES搜索详解 —— 搜索机制]]></title>
    <url>%2Felasticsearch%2Fdoc%2F6-1-Elasticsearch%E6%90%9C%E7%B4%A2%E8%AF%A6%E8%A7%A3-%E7%B4%A2%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本章参考文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html注意文档对应的Elasticsearch版本 索引过程： 第2象限 有 原始文档； Elasticsearch 保存 文档的原始内容 和 对应的倒排序索引文件； 搜索过程： 在 倒排索引文件 维护的 倒排记录表 找 关键词 对应的 文档集合，然后做评分、排序、高亮，将结果返回给用户； 过滤机制： 根据条件对文档进行过滤，不计算评分； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899DELETE booksPUT books&#123; "settings": &#123; "number_of_replicas": 1, "number_of_shards": 3 &#125;, "mappings": &#123; "properties": &#123; "id": &#123; "type": "long" &#125;, "title": &#123; "type": "text", "analyzer": "ik_max_word" &#125;, "language": &#123; "type": "keyword" &#125;, "author": &#123; "type": "keyword" &#125;, "price": &#123; "type": "double" &#125;, "publish_time": &#123; "type": "date", "format": "yyyy-MM-dd" &#125;, "desc": &#123; "type": "text", "analyzer": "ik_max_word" &#125; &#125; &#125;&#125;PUT books/_doc/1&#123; "id":1, "title":"Java编程思想", "language":"Java", "author": "Bruce Eckel", "price":70.20, "publish_time":"2017-01-02", "desc":"Java学习必读经典，殿堂级著作！赢得全球程序员的广泛赞誉。"&#125;PUT books/_doc/2&#123; "id":2, "title":"Java程序性能优化", "language":"Java", "author": "葛一鸣", "price":46.50, "publish_time":"2012-08-02", "desc":"Java程序更快，更稳定。深入剖析软件设计层面、代码层面、JVM层面的优化方法。"&#125;PUT books/_doc/3&#123; "id":3, "title":"Python科学计算", "language":"python", "author": "张若愚", "price":81.40, "publish_time":"2014-01-02", "desc":"零基础学python， 光盘中坐着度假开发winPython运行环境，涵盖了Python各个扩展库。"&#125;PUT books/_doc/4&#123; "id":4, "title":"Python基础教程", "language":"python", "author": "Helant", "price":54.50, "publish_time":"2014-03-02", "desc":"经典的python入门教程，层次鲜明，结构严谨，内容详实。"&#125;PUT books/_doc/5&#123; "id":5, "title":"JavaScript高级程序设计", "language":"javascript", "author": "Nicholas C. Zakas", "price":66.40, "publish_time":"2012-03-02", "desc":"JavaScript 技术经典名著。"&#125;GET books/_mappingmatch_all搜索,简化语法：GET books/_searchterm query 搜索：GET books/_search&#123; "query": &#123; "term": &#123; "title": "思想" &#125; &#125;&#125; 数据量很大的情况下，需要分页： from：开始位置 size：返回文档最大数量 12345678910GET books/_search&#123; "from": 1, "size": 2, "query": &#123; "term": &#123; "title": "java" &#125; &#125;&#125; 最小评分过滤机制： 相关文档很多的情况下，相关性比较低的文档可以过滤掉 123456789GET books/_search&#123; "min_score": 0.6, "query": &#123; "term": &#123; "title": "java" &#125; &#125;&#125; 高亮查询关键字： 12345678910111213GET books/_search&#123; "query": &#123; "term": &#123; "title": "java" &#125; &#125;, "highlight": &#123; "fields": &#123; "title": &#123;&#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式共识算法]]></title>
    <url>%2Fdistribution%2Fdistributed-consensus-algorithm%2F</url>
    <content type="text"><![CDATA[一、Paxos协议1、拜占庭将军问题 2、提出问题 3、故事背景 4、Paxos算法4.1、基本定义 4.2、算法详解Paxos算法有两个阶段 Prepare阶段（第一阶段） Accept阶段（第二阶段） 1)、第一阶段 Proposer希望议案V。首先发出Prepare请求至大多数Acceptor。Prepare请求内容为序列号K； Acceptor收到Prepare请求为编号K后，检查自己手里是否有处理过Prepare请求； 如果Acceptor没有接受过任何Prepare请求，那么用OK来回复Proposer，代表Acceptor必须接受收到的第一个议案； 否则，如果Acceptor之前接受过任何Prepare请求（如：MaxN），那么比较议案编号，如果K&lt;MaxN，则用reject或者error回复Proposer； 如果K&gt;=MaxN，那么检查之前是否有批准的议案，如果没有则用OK来回复Proposer，并记录K； 如果K&gt;=MaxN，那么检查之前是否有批准的议案，如果有则回复批准的议案编号和议案内容（如：&lt;AcceptN, AcceptV&gt;， AcceptN为批准的议案编号，AcceptV为批准的议案内容）。 2)、第二阶段 Proposer收到过半Acceptor发来的回复，回复都是OK，且没有附带任何批准过的议案编号和议案内容。那么Proposer继续提交批准请求，不过此时会连议案编号K和议案内容V一起提交（&lt;K, V&gt;这种数据形式） Proposer收到过半Acceptor发来的回复，回复都是OK，且附带批准过的议案编号和议案内容（&lt;pok，议案编号，议案内容&gt;）。那么Proposer找到所有回复中AcceptN最大的那个AccpetV（假设为&lt;pok，AcceptNx，AcceptVx&gt;）作为提交批准请求（请求为&lt;K，AcceptVx&gt;）发送给Acceptor。 Proposer没有收到过半Acceptor发来的回复，则修改议案编号K为Kx，并将编号重新发送给Acceptors（重复Prepare阶段的过程） Acceptor收到Proposer发来的Accept请求，如果编号K&lt;MaxN则不回应或者reject。 Acceptor收到Proposer发来的Accept请求，如果编号K&gt;=MaxN则批准该议案，并设置手里批准的议案为&lt;K，接受议案的编号，接受议案的内容&gt;，回复Proposer。 经过一段时间Proposer对比手里收到的Accept回复，如果超过半数，则结束流程（代表议案被批准），同时通知Leaner可以学习议案。 经过一段时间Proposer对比手里收到的Accept回复，如果未超过半数，则修改议案编号重新进入Prepare阶段。 5、Paxos流程图 二、ZAB协议zookeeper 的 leader 选举采用了 ZAB协议，消息广播的时候没有采用ZAB协议 ZooKeeper并没有完全采用Paxos算法，而是使用了一种称之为ZooKeeper Atomic Broadcast（ZAB，ZooKeeper原子广播协议）的协议作为其数据一致性的核心算法。 ZAB协议并不像Paxos算法那样，是一种通用的分布式一致性算法，它是一种特别为ZooKeeper设计的崩溃恢复的原子消息广播算法。ZooKeeper采用一个单一的主进程接受并处理客户端的所有事务请求，并将服务器数据的状态变更以事务Proposal的形式广播到所有的副本进程上去。 ZAB协议包含两种基本的模式： 崩溃恢复 消息广播 ZAB协议包含三个阶段： 阶段1：发现（Leader选举过程） 阶段2：同步（数据同步过程） 阶段3：广播（正式接受请求过程） 当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时， ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的Leader 服务器同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。 当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播 ， 那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 org.apache.zookeeper.server.quorum.QuorumPeer.ServerState 123public enum ServerState &#123; LOOKING, FOLLOWING, LEADING, OBSERVING;&#125; 1、崩溃恢复当整个服务器在启动过程中，或者当Leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并通过选举产生新的Leader服务器。 当选举产生了新的Leader服务器，同时集群中已经有过半机器与该Leader服务器完成状态同步之后，ZAB协议就会退出恢复模式。这里的状态同步指的就是数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据保持一致。 当集群中有过半的Follower服务器完成和Leader服务器的同步，那么整个服务器集群就可以进入消息广播模式。 当新的机器加入集群，由于集群已经存在一个Leader，那么新加入的机器会进入数据同步模式，即找到Leader服务器，并与其进行数据同步。 当Leader崩溃退出或者重启，或者及集群中不存在过半的服务器可以和Leader保持正常通信，那么在开始新一轮事务操作前所有机器会使用崩溃恢复协议来达到一个一致性的状态。 ZAB协议规定了如果一个事务Proposal在一台机器上被处理成功，那么应该在所有的机器上都被处理成功，哪怕机器出现故障崩溃。 每个Server会发出一个投票，第一次都是投自己。投票信息：（myid，ZXID） 收集来自各个服务器的投票 处理投票并重新投票，处理逻辑：优先比较ZXID，然后比较myid 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定Leader 改变服务器状态 123456789101112131415161718// 选票和服务器的当前选票进行对比protected boolean totalOrderPredicate(long newId, long newZxid, long newEpoch, long curId, long curZxid, long curEpoch) &#123; LOG.debug("id: " + newId + ", proposed id: " + curId + ", zxid: 0x" + Long.toHexString(newZxid) + ", proposed zxid: 0x" + Long.toHexString(curZxid)); if(self.getQuorumVerifier().getWeight(newId) == 0)&#123; return false; &#125; /* * 以下三种情况成立，则返回true: * 1- 新epoch更大 * 2- 新epoch与当前epoch相同，但新zxid更大 * 3- 新epoch与当前epoch相同，新zxid与当前zxid相同，但是服务器id更高。 */ return ((newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId)))));&#125; 2、消息广播消息广播类似于一个2PC提交过程。根据客户端的事务请求，Leader服务器会为其生成对应的事务投票（即Proposal）并将其发送给集群中其他服务器，然后在分表搜集各自的选票，最后进行事务提交。 与2PC不同的是，ZAB协议没有中断逻辑（所有Follower要么对Leader提成的事务Ack，要么就不回应），而且当过半的Follower服务器反馈Ack之后就开始提交事务，不用等待所有Follower都反馈。 整个消息广播协议是基于FIFO特性的TCP协议来进行网络通信，因此能够很容易地保证消息广播过程中消息接受与发送的顺序性。 Leader接收到消息请求后，将消息赋予一个全局唯一的64位自增id，叫做：Zxid，通过zxid的大小比较即可实现因果有序这一特性。 Leader通过先进先出队列(通过 TCP 协议来实现，以此实现了全局有序这一特性)将带有zxid的消息作为一个提案(Proposal)分发给所有Follower。 当Follower接收到Proposal，先将Proposal写到硬盘，写硬盘成功后再向Leader回一个ACK。 当Leader接收到合法数量的ACKs后，Leader就向所有Follower发送COMMIT命令，同时会在本地执行该消息。 当Follower收到消息的COMMIT命令时，就会执行该消息。 3、数据同步整个集群完成Leader选举后，Learner会向Leader进行注册，当Learner向Leader完成注册后，就进入数据同步环节，同步过程就是Leader将那些没有在Learner服务器上提交过的事务请求同步给Learner服务器。 3.1、直接差异化同步举例，某个时刻Leader服务器的事务队列对应的ZXID依次是： 123450x2000000010x2000000020x2000000030x2000000040x200000005 而需要数据同步的服务器最后处理的ZXID为： 10x200000003 这种场景就执行“直接差异化同步”，Leader会依次将0x200000004，0x200000005同步给服务器，同步过程中顺序如下： 3.2、先回滚再差异化同步假如在ZooKeeper集群中有A、B、C三台服务器，B当选为Leader服务器。 某个时刻，B正要处理一个ZXID=0x200000003的事务，并且已经将该事务写入到B服务器的本地的事务日志中，就在B要发送给其他Follower A、C机器进行同步的时候，B服务器挂了，Proposal并没有发送出去，而此时此时ZooKeeper会进行新一轮选举。假设A当选为新的Leader服务器对外进行工作，客户端又提交了 120x3000000010x300000002 而此时之前的奔溃的B服务器再次启动，并开始进行数据同步。 因为B之前为Leader，故它的本地日志中事务编号为： 1230x2000000010x2000000020x200000003 而A、C的本地日志中的事务编号为： 12340x2000000010x2000000020x3000000010x300000002 这时候就需要A服务器对数据进行回滚之后再同步，这个就称之为“先回滚再差异化同步” 3.3、仅回滚同步先回滚再差异化的特殊模式。 3.4、全量同步如：新加入的Follower服务器。 三、Raft协议 四、Gossip协议]]></content>
      <categories>
        <category>distributed</category>
      </categories>
      <tags>
        <tag>paxos</tag>
        <tag>zab</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.3、Elasticsearch集群入门 —— 映射详解]]></title>
    <url>%2Felasticsearch%2Fdoc%2F5-3-Elasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8-%E6%98%A0%E5%B0%84%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[5.3.1、映射分类 5.3.2、动态映射 5.3.3、日期检测 5.3.4、静态映射 5.3.5、字段类型 5.3.6、元字段 5.3.7、映射参数 5.3.8、映射模板 -————————————— ​ 映射 即 Mapping， 定义 一个文档及其所包含的字段 如何被存储和索引。 ​ 映射中 事先定义 数据类型、分词器 5.3.1、映射分类动态映射：ES 根据 字段类型 自动识别； —— 偷懒方式 静态映射：写入数据之前 对字段的属性 手工设置； 5.3.2、动态映射1234567PUT sinablog/user/1?routing=user123&#123; "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125;GET sinablog/_mappingGET blog/_mapping id 被推测为 long 类型 posttime被推测为 date 类型 title 和 content 被推测为 text 类型 如果 要把Elasticsearch当做 主要的数据存储 使用，动态Mapping 并不适用； 在 Mapping 中通过 dynamic 设置 是否自动新增字段，接收以下参数： true —— 自动新增； false —— 忽略新字段； strict —— 严格模式，发现新字段 抛出异常； 123456789101112PUT sinablog/user/1?routing=user123&#123; "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125;PUT sinablog/user/2?routing=user123&#123; "create_time": "2019-01-09", "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125;GET sinablog/_search?routing=user123 自定义Mapping： 1234567891011121314151617PUT tblog&#123; "mappings": &#123; "dynamic": "strict", "properties": &#123; "posttime": &#123; "type": "date" &#125;, "id": &#123; "type": "long" &#125;, "title": &#123; "type": "text" &#125; &#125; &#125;&#125; 写数据： Elasticsearch 6.0的时候默认一个 index 只能有一个type，7.0的时候已经有移除type的趋势，默认用_doc代替表示默认type； 参考文档：《Elasticsearch 移除 type 之后的新姿势》 123456POST tblog/_doc/1&#123; "id": 2, "title": "Git是一款免费、开源的分布式版本控制系统", "posttime": "2018-01-09"&#125; 查询 Mapping：GET tblog/_mapping 查询：GET tblog/_search 删除：DELETE tblog ​ 5.3.3、日期检测Elasticsearch碰到一个新的字符串类型的字段时，会检查是否是一个日期； 一旦检测为是，以后，如果写入的字段不是type，就会报错； 预先设置不自动检测日期，可避免： 12345678910111213PUT blog&#123; "mappings": &#123; "date_detection": false &#125;&#125;POST blog/_doc/2&#123; "id": 2, "title": "Git简介", "posttime": "2018-01-09", "content": "Git是一款免费、开源的分布式版本控制系统"&#125; GET blog/article/2 GET blog/_mapping GET blog/_search DELETE blog ​ 5.3.4、静态映射定义：创建索引时，手动指定索引映射； ​ 5.3.5、字段类型 文本： string：在 5.X 之后不再支持； text: 可以被全文检索的的，字段会被分拆，分词、生成倒排索引，不会用于 排序，基本不用于聚合； keyword: 只能用于精确搜索，用于排序、聚合 数字： 对于 float、half_float、scaled_float，-0.0 和 +0.0 是不同的值； es 底层会把 scaled_float 作为整形存储，节省空间，可以优先使用； 日期 date： 格式化的字符串，如 ‘2015-01-02’， ‘2015/01/02 12:10:31’； 代表epoch到今天的秒数的一个长整数，epoch 指 1970-01-01 00:00:00 UTC 二进制binary：接收base64编码的字符串，默认不存储（store=false），也不可搜索 数组**array**：es默认任何字段都可以包含一个或多个值，array的值 默认一个数组的值必须是统一类型； object： nested：object的一个特例，可以让数组独立索引和哈希 地理坐标 geo_point：存储 经纬度坐标点， 或 地理坐标的hash值； 可用来查找点周围的区域； 根据到点的距离排序； 地理图形 geo_shape：存储一块区域，矩形，三角形，或者 其他多边形，GeoJson； ip：存储 ipv4 或者 ipv6; range：常用于 时间选择表单 和 年龄范围选择表单： integer_range float_range long_range double_range： date_range：64bit整数，毫秒计时； 123456789101112131415161718192021222324252627PUT range_inde&#123; "mappings": &#123; "properties": &#123; "age_range": &#123; "type": "integer_range" &#125;, "time_range": &#123; "type": "date_range", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis" &#125; &#125; &#125;&#125;PUT range_inde/_doc/1&#123; "age_range": &#123; "gte": 18, "lte": 70 &#125;, "time_range": &#123; "gte": "2015-10-31 01:00:00", "lte": "2019-05-21" &#125;&#125;GET range_inde/_mappingGET range_inde/_search 令牌计数类型 token_count：用于统计字符串分词后的 词项个数。 12345678910111213141516171819202122232425262728PUT my_index&#123; "mappings": &#123; "properties": &#123; "name": &#123; "type": "text", "fields": &#123; "length": &#123; "type": "token_count", "analyzer": "standard" &#125; &#125; &#125; &#125; &#125;&#125;PUT my_index/_doc/1&#123; "name": "My name is xiaozhang"&#125;GET my_index/_search&#123; "query": &#123; "term": &#123; "name.length": 4 &#125; &#125;&#125; ​ 5.3.6、元字段定义：映射中，描述文档本身的字段 分类： 描述文档属性； 源文档的元字段； 索引的元字段； 路由的元字段； ​ 5.3.7、映射参数analyzer：指定文本字段的分词器，对索引和查询都有效； search_analyzer：索引 和 搜索 都应该指定相同的分词器；但有时候就 需要指定不一样的， ​ 默认 查询 会使用 analyzer指定的分词器，但也可以被 search_analyzer 指定 搜索分词器； 1234567891011121314151617181920212223242526272829303132333435PUT analyzer_index&#123; "settings": &#123; "analysis": &#123; "filter": &#123; "autocomplete_filter": &#123; "type": "edge_ngram", "min_gram": 1, "max_gram": 20 &#125; &#125;, "analyzer": &#123; "autocomplete": &#123; "type": "custom", "tokenizer": "standard", "filter": ["lowercase","autocomplete_filter"] &#125; &#125; &#125; &#125;, "mappings": &#123; "properties": &#123; "text": &#123; "type": "text", "analyzer": "autocomplete", "search_analyzer": "standard" &#125; &#125; &#125;&#125;PUT analyzer_index/_doc/1&#123; "text": "Quick Brown Fox"&#125;GET analyzer_index/_search normalizer：用于解析前的标准配置， 比如把所有字符串转化为小写 123456789101112131415161718192021222324252627282930313233343536373839404142434445DELETE my_indexPUT my_index&#123; "settings": &#123; "analysis": &#123; "normalizer": &#123; "my_normalizer": &#123; "type": "custom", "char_filter": [], "filter": ["lowercase", "asciifolding"] &#125; &#125; &#125; &#125;, "mappings": &#123; "properties": &#123; "foo": &#123; "type": "keyword", "normalizer": "my_normalizer" &#125; &#125; &#125;&#125;PUT my_index/_doc/1&#123; "foo": "BAR"&#125;PUT my_index/_doc/2&#123; "foo": "bar"&#125;PUT my_index/_doc/3&#123; "foo": "bazz"&#125;POST my_index/_refreshGET my_index/_search&#123; "query": &#123; "match": &#123; "foo": "BAR" &#125; &#125;&#125; boost：设置字段的权重，比如 设置关键字 在 title的权重是 content的2倍； DELETE my_index mapping指定(不重新索引文档，权重无法更改)： 1234567891011121314PUT my_index&#123; "mappings": &#123; "properties": &#123; "title": &#123; "type": "text", "boost": 2 &#125;, "content": &#123; "type": "text" &#125; &#125; &#125;&#125; 查询时指定（推荐）： 123456789101112131415PUT my_index/_doc/1&#123; "title": "Quick Brown Fox"&#125;POST my_index/_search&#123; "query": &#123; "match": &#123; "title": &#123; "query": "Quick", "boost": 2 &#125; &#125; &#125;&#125; corece：用于清除脏数据，默认值 true； copy_to：用于自定义 _all 字段，可以把 多个字段 复制合并成一个超级字段； doc_values：为了加快排序、聚合操作，建立 倒排索引的时候，额外增加一个列式存储映射，是一种空间换时间的做法 默认是开启的，如果确定不需要聚合或排序，可以关闭 以节省空间； 注：text 类型 不支持 doc_values; 123456789101112131415DELETE my_indexPUT my_index&#123; "mappings": &#123; "properties": &#123; "status_code": &#123; "type": "keyword" &#125;, "session_id": &#123; "type": "keyword", "doc_values": false &#125; &#125; &#125;&#125; enabled：es会默认索引搜有字段，而有些字段只需存储，不需要查询、聚合，设置mapping时，enabled设置为false的字段，es就会跳过字段内容，只能从_source中获取，不能被搜索；也能禁用映射； fielddata： 搜索解决的问题：包含查询关键词的文档哪些； 聚合要解决的：文档包含哪些词项； fielddata 默认是关闭的，开启非常耗内存； format：用于指定 date的格式； ignore_above：用于指定字段分词和索引的字符串最大长度；超多最大值会被忽略，只能用于keyword 类型； ignore_malformed：忽略不规则数据，设为true，异常会被忽略，出异常的字段不会被索引，其他字段正常索引； index_options：控制哪些信息存储到 倒排索引中； fields：让同一字段有多种不同的索引方式； position_increment_gap：支持近似 或者 短语查询； properties： similarty：用于指定 文档评分模型；参数：BM25，classic（TF/IDF评分），boolean 布尔评分模型； store：字段默认是被索引的，也可以搜索，但不存储。store可以指定 存储某个字段； term_vector：词向量，包含文本别解析以后的信息：词项集合、词项位置、词项起始字符映射到原始文档的位置； ​ 5.3.8、映射模板举例：如果字段已 long_ 开头，则将字符串转化为 long 类型； 1234567891011121314151617181920212223242526DELETE my_indexPUT my_index&#123; "mappings": &#123; "dynamic_templates": [ &#123; "longs_as_strings": &#123; "match_mapping_type": "string", "match": "long_*", "unmatch": "*_text", "mapping": &#123; "type": "long" &#125; &#125; &#125; ] &#125;&#125;PUT my_index/_doc/1&#123; "long_num": "5", "long_text": "foo"&#125;GET my_index/_mappingGET my_index/_search]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.2、Elasticsearch集群入门 —— 文档管理]]></title>
    <url>%2Felasticsearch%2Fdoc%2F5-2-Elasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8-%E6%96%87%E6%A1%A3%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[5.2.1、新建文档 5.2.2、获取文档 5.2.3、更新文档 5.2.4、查询更新 5.2.5、删除文档 5.2.6、查询删除 5.2.7、批量操作 5.2.8、版本控制 5.2.9、路由机制 -————————————— 5.2.1、新建文档格式： index/type/id，如果不设置id，es会自动生成它 1234567PUT blog/article/1&#123; "id": 1, "title": "Git简介", "posttime": "2018-01-09", "content": "Git是一款免费、开源的分布式版本控制系统"&#125; 1234567POST blog/article&#123; "id": 2, "title": "Git简介", "posttime": "2018-01-09", "content": "Git是一款免费、开源的分布式版本控制系统"&#125; ​ 5.2.2、获取文档GET blog/article/1 如果找不到 可以通过HEAD命令查看一个文档是否存在 HEAD blog/article/2 获取多个，根据index和 type： 12345678910111213141516171819202122232425262728GET blog/_mget&#123; "docs" : [ &#123; "_type": "article", "_id": "1" &#125;, &#123; "_type": "article", "_id": "2" &#125; ]&#125;GET _mget&#123; "docs" : [ &#123; "_index": "blog", "_type": "article", "_id": "1" &#125;, &#123; "_index": "blog", "_type": "article", "_id": "2" &#125; ]&#125; ​ 5.2.3、更新文档文档被索引以后要修改， 12345PUT index1/type1/1&#123; "counter": 1, "tags": ["red"]&#125; 使用脚本更新： 12345678910POST index1/type1/1/_update&#123; "script": &#123; "inline": "ctx._source.counter += params.count", "lang": "painless", "params": &#123; "count" : 4 &#125; &#125;&#125; ​ 5.2.4、条件查询更新如果满足条件则更新 123456789101112131415POST index1/_update_by_query&#123; "script": &#123; "inline": "ctx._source.category = params.category", "lang": "painless", "params": &#123; "category": "git" &#125; &#125;, "query": &#123; "term": &#123; "title": "git" &#125; &#125;&#125; 如果title中包含git关键字，则增加一个 category ​ 5.2.5、删除文档基于指定id从 索引库 中删除一个文档 DELETE blog/article/osGC1WoB4sRau0FmmUYE 再去查询： GET blog/article/osGC1WoB4sRau0FmmUYE HEAD blog/article/osGC1WoB4sRau0FmmUYE 如果 索引文档时指定了路由，那么也可以根据路由参数删除 DELETE blog/article/2?routing=user123 如果参数不正确，会删除失败 ​ 5.2.6、根据条件查询删除删除 title中包含关键字 hibernate的文档 123456POST blog/_delete_by_query&#123; "query": &#123; "title": "hibernate" &#125;&#125; 删除一个type（csdn）下所有的文档 123456POST blog/csdn/_delete_by_query&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; ​ 5.2.7、批量操作执行 Bulk API 可以 执行 批量索引、批量删除、批量更新等操作 一个 Bulk API 允许 单一请求 实现多个文档的 create、index、update、delete ​ 5.2.8、版本控制Elasticsearch API进行文档更新 过程： 读取源文档 -&gt; 对原文档更新 -&gt; 重新索引整个文档 使用同个线程同时修改一个文档， 会发生冲突； 1、 悲观锁控制同一时刻只有一个线程访问数据 2、 乐观锁控制Elasticsearch是分布式系统， 需要确保旧版本不会覆盖新版本 Elasticsearch 使用 _version 自增， 确保所有更新有序进行 内部版本控制： 每次版本号，相等 才能操作成功； 外部版本控制：外部文档 比 内部文档 版本高 时才能更新成功； 5.2.9、路由机制分片的路由机制： shard = hash（routing）% number_of_primary_shards routing 是一个任意字符串，取它的hash值，取模后 放在对应的 分片上； hash值相同的 文档 放在 同一个主分片中； 默认路由模式可以保障 文档id数据平均分布，ES无法根据 id 确定 文档的位置，需要广播到所有分片上查找； 自定义路由模式，可以使查询具有目的性，不用盲目广播 12345PUT sinablog/user/1?routing=user123&#123; "title": "hahaha ni mei", "text": "asd adsf fds vs rew"&#125; GET sinablog/_search?routing=user123]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.1、Elasticsearch集群入门 —— 索引管理]]></title>
    <url>%2Felasticsearch%2Fdoc%2F5-1-Elasticsearch%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8-%E7%B4%A2%E5%BC%95%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[5.1.1、新建索引 5.1.2、更新副本数 5.1.3、设置索引的读写权限 5.1.4、查看索引 5.1.5、删除索引 5.1.6、索引的打开和关闭 5.1.7、复制索引 5.1.8、收缩索引 5.1.9、索引别名 -—————————————- 5.1.1、新建索引 索引名称 不能有大写字母 使用Kibana的DevTools工具进行 创建索引： PUT test 响应： 12345&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "test"&#125; 并且不能重复创建 -———————————— 创建 5个分片，3个副本的索引： 1234567PUT test&#123; "settings": &#123;​ "number_of_shards": 5,​ "number_of_replicas": 2 &#125;&#125; ​ 5.1.2、更新副本数ES 支持修改已存在索引的副本数，如下： 1234PUT test/_settings&#123; "number_of_replicas": 3&#125; 响应： 123&#123; "acknowledged" : true&#125; ​ 5.1.3、设置索引的读写权限“blocks.read_only”: false ———— 当前索引只读，不允许写和更新 “blocks.read”: true ———— 禁止读 “blocks.write”: true ———— 禁止写 示例 设置禁止写 1234PUT test/_settings&#123; "blocks.write": true&#125; 尝试写入 1234PUT test/article/1&#123; "tile": "Java 虚拟机"&#125; 返回错误 重新设置允许写 1234PUT test/_settings&#123; "blocks.write": false&#125; 再试尝试写入， 写入成功： ​ 5.1.4、查看索引GET test/_settings 查看多个索引：GET test,girl/_settings 查看所有索引：GET _all/_settings ​ 5.1.5、删除索引DELETE test2 响应表示成功： 123&#123; "acknowledged" : true&#125; 删除不存在的索引会报错：404 ​ 5.1.6、索引的打开和关闭一个关闭了的索引，基本不占用系统资源； POST test/close ———— 关闭 test 索引 _POST test,girl/_close ———— 关闭 多个索引 POST _all/_close ———— 关闭所有索引 POST test*/_close ———— 关闭test开头的索引 ​ 5.1.7、复制索引_redinx操作， 把文档 从 源索引 复制到 目标索引，但目标索引的分片数、副本数需要单独设置。 全量复制： 12345POST _reindex&#123; "source": &#123;"index": "test"&#125;, "dest": &#123;"index": "girl"&#125;&#125; 把 test 索引下，type为article下，title中含有关键字 java的文档，复制到 girl 索引中： 123456789101112131415POST _reindex&#123; "source": &#123; "index": "test", "type": "article", "query": &#123; "term": &#123; "FIELD": &#123; "title": "java" &#125; &#125; &#125; &#125;, "dest": &#123;"index": "girl"&#125;&#125; ​ 5.1.8、收缩索引一个索引分片初始化以后无法再做修改，但可以使用shrink index AP提供的分片数机制，把一个索引变成更少的索引。 收缩以后的分片数 是 原始分片数的因子；比如 8个 可以收缩成 4,2,1；7,11只能收缩成1个； ​ 5.1.9、索引别名给一个索引 起另一个名字 1234567891011POST /_aliases&#123; "actions": [ &#123; "add": &#123; "index": "test", "alias": "alias_test" &#125; &#125; ]&#125; 移除别名 1234567891011POST /_aliases&#123; "actions": [ &#123; "remove": &#123; "index": "test", "alias": "alias_test" &#125; &#125; ]&#125; 查看索引的别名 GET /test/_alias 12345GET /test/_analyze&#123; "analyzer":"ik_smart", "text":"洪荒之力"&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.5、Elasticsearch-Head集群操作管理工具]]></title>
    <url>%2Felasticsearch%2Fdoc%2F4-5-Elasticsearch-Head%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[4.5.1、Head插件的安装 1、配置Node环境 2、下载Head插件源码 3、修改 Elasticsearch 配置文件 4、修改 Head 插件配置文件 5、启动 Head 插件 4.5.2、Head插件的使用 1、概览界面 2、索引查看界面 3、数据浏览界面 4、基本查询选项卡 5、复合查询 -——————————————– 4.5.1、Head插件的安装1、配置Node环境官网：https://nodejs.org/en/download/ wget https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz xz -d node-v10.15.3-linux-x64.tar.xz tar -xf node-v10.15.3-linux-x64.tar 设置环境变量 vi /etc/profile 添加： 12export NODE_HOME=/data/carloz/tools/node-v10.15.3-linux-x64/export PATH=$NODE_HOME/bin:$PATH source /etc/profile 使用npm安装 Grunt： npm install -g grunt-cli ​ 2、下载Head插件源码https://github.com/mobz/elasticsearch-head/releases wget https://github.com/mobz/elasticsearch-head/archive/v5.0.0.tar.gz mv v5.0.0.tar.gz elasticsearch-head-v5.0.0.tar.gz tar -zxf elasticsearch-head-v5.0.0.tar.gz cd elasticsearch-head-5.0.0/ npm install -g cnpm –registry=https://registry.npm.taobao.org ​ 3、修改 Elasticsearch 配置文件su elsearch vi elasticsearch-7.0.0/config/elasticsearch.yml 添加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 重启 elasticsearch exit —— 切换到 root ​ 4、修改 Head 插件配置文件vi elasticsearch-head-5.0.0/Gruntfile.js 修改connect， 指定 Elasticsearch-head 的访问ip： 不指定的话，所有ip都能访问他 ​ 5、启动 Head 插件cd elasticsearch-head-5.0.0/ grunt server ​ 4.5.2、Head插件的使用1、概览界面 试着去连接 Elasticsearch服务器，但是，我们的 Elasticsearch 服务器只允许本机连，要修改配置 su elsearch vi /data/carloz/tools/elasticsearch-7.0.0/config/elasticsearch.yml 错误参考：https://blog.csdn.net/zhou_p/article/details/80311972 “the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured” -———————————————- vi /data/carloz/tools/elasticsearch-7.0.0/config/elasticsearch.yml 修改： bootstrap.memory_lock: false 添加： bootstrap.system_call_filter: false cluster.initial_master_nodes: [“node-1”] -——————————————— vi /etc/sysctl.conf vm.max_map_count=655360 sysctl -p -——————————————— vi /etc/security/limits.d/90-nproc.conf -———————————————– http://10.10.139.42:9200/ 点击连接 2、索引查看界面 3、数据浏览界面 4、基本查询选项卡 5、复合查询]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.4、Elasticsearch中文分词器配置]]></title>
    <url>%2Felasticsearch%2Fdoc%2F4-4-Elasticsearch%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[4.4.1、IK分词器安装 4.4.2、扩展本地词库 4.4.3、配置远程词库 -——————————————– 4.4.1、IK分词器安装安装方式一： 1、打开 https://github.com/medcl/elasticsearch-analysis-ik/releases，因为我安装的 Elasticsearch 版本为 elasticsearch-7.0.0，所以下载 相同版本的 ik分词器 su elsearch wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.0.0/elasticsearch-analysis-ik-7.0.0.zip mkdir -p elasticsearch-7.0.0/plugins/ik unzip -d elasticsearch-7.0.0/plugins/ik elasticsearch-analysis-ik-7.0.0.zip 重启Elasticsearch ps aux | grep elasticsearch* kill 31670 ./bin/elasticsearch -d 安装方式二： 下载IK源码文件：https://github.com/medcl/elasticsearch-analysis-ik.git； 进入目录，执行mvn package，打包完成以后，生成target； target/releases 目录下 即为 IK安装文件； ​ 4.4.2、扩展本地词库未扩展之前 使用Kibana的DevTools 测试分词器 1234567PUT testGET /test/_analyze&#123; "analyzer":"ik_smart", "text":"洪荒之力"&#125; cd /data/carloz/tools/elasticsearch-7.0.0 vi plugins/ik/custom/hotwords.dic 写入 “洪荒之力” vi plugins/ik/config/IKAnalyzer.cfg.xml 重启 Elasticsearch tail -f logs/elasticsearch.log -n 100 扩展之后，再用Kibana测试相同的词汇 ​ 4.4.3、配置远程词库配置为相应的网址即可]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.2、安装Elasticsearch]]></title>
    <url>%2Felasticsearch%2Fdoc%2F4-2-%E5%AE%89%E8%A3%85Elasticsearch%2F</url>
    <content type="text"><![CDATA[-——————————————– 参考：《ELK — 单节点系统构建》 -——————————————– 4.2.1、安装Java 4.2.2、下载Elasticsearch 4.2.3、启动Elasticsearch curl http://127.0.0.1:9200 4.2.4、后台运行Elasticsearch 4.2.5、关闭Elasticsearch 4.2.6、基本配置]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.1、Elasticsearch概述]]></title>
    <url>%2Felasticsearch%2Fdoc%2F4-1-Elasticsearch%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[特点：分布式、基于JSON和HTTP接口 ​ 4.1.1、Elastic Stack家族基于 Elasticsearch 衍生出一系列开源软件，统称 Elastic Stack 分布式搜索引擎：Elasticsearch 可视化分析平台：Kibana 日志采集、解析工具：Logstash 数据采集工具：Beats 家族 Filebeat：收集文件数据，轻量级日志采集工具； Metricbeat：搜集 系统、进程、文件系统 级别的 CPU和内存使用情况； Packetbeat：收集网络流数据；实时监控 系统应用和服务，将延迟时间、错误、响应、SLA性能等发送到 Logstash或Elasticsearch Winlogbeat：收集 windows 事件日志数据； Heartbeat：监控服务器运行状态； Elastic家族配合使用时，版本必须一致； ​ 4.1.3、架构解读 Gateway 存储索引的文件系统： Local FileSystem —— 存储在本地文件系统； Shared FileSystem —— 共享存储； Hadoop HDFS —— 使用 hdfs 分布式存储； Amazon S3 —— 存储Amazon S3云服务上； 分布式 Lucene 层，Elasticsearch 的底层 API由 Lucene 提供，每一个ES节点 都有 一个Lucene引擎支持： 索引模块 搜索模块 映射解析模块 River模块 —— 用来导入第三方数据源，2.X之后已经废弃 Discovery模块 —— Elasticsearch 节点发现，集群内master选举，节点间通信 Scripting模块 —— 支持 JavaScript、Python 等多种语言 第三方插件模块 —— 支持 多种第三方插件； Transport —— ES传输模块，支持 Thrift，Memcached、HTTP，默认使用 HTTP； JMX——Java管理框架，用来管理 Elasticsearch 应用； RESTFull API —— ES提供给用户的接口 ​ 4.1.4、优点 分布式 —— 灵活的横向扩展，自动识别新节点 并 重新平衡分配数据； 全文检索 —— 多语言支持、强大的查询语言、地理位置支持、上下文感知的建议、自动完成和搜索片段； 近实时搜索和分析 —— 近实时搜索，也可以 聚合分析； 高可用 —— 容错机制，自动发现新节点 和失败节点，重新分配数据，确保集群可用； 模式自由 —— 动态 mapping 机制，自动检测 数据结构和类型，创建索引，使数据可搜索； RESTFul API —— 任何语言都能访问； ​ 4.1.5、应用场景1、站内搜索 —— 全文检索功能； 2、NoSQL数据库 —— 读写性能优于 MongoDB； 3、日志分析 —— 经典的ELK日志分析平台； ​ 4.1.6、核心概念集群 —— 多个节点组成的集群； 节点 —— 一个服务器 索引 —— 拥有 几分相似特征 的 文档的集合； 类型 —— 索引在一个逻辑上的分类 或 分区； 文档 —— JSON格式，被索引的基础信息单元； 分片 —— 一个索引大小 可以超出 单节点硬件限制，一个索引 可以分成多个分片； ​ 分片 水平分割和扩容 你的内存容量； ​ 可以在 主副分片 上 并行查询，提高性能和吞吐量； 副本 —— 分片的 一份或多份拷贝，分为主分片 和 副分片；]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 下 redis-sentinel 主从集群搭建]]></title>
    <url>%2Fredis%2Fredis-sentinel-setup%2F</url>
    <content type="text"><![CDATA[宏观上Sentinel功能的完整列表 监控。Sentinel会不断检查您的主实例和副本实例是否按预期工作。 通知。Sentinel可以通过API通知系统管理员或其他计算机程序，其中一个受监视的Redis实例出了问题。 自动故障转移。如果主服务器未按预期工作，则Sentinel可以启动故障转移过程，在该过程中将副本升级为主服务器，将其他附加副本重新配置为使用新的主服务器，并通知使用Redis服务器的应用程序要使用的新地址。连接时。 配置提供程序。Sentinel充当客户端服务发现的授权来源：客户端连接到Sentinels，以询问负责给定服务的当前Redis主服务器的地址。如果发生故障转移，Sentinels将报告新地址。 Sentinel的分布式性质Redis Sentinel是一个分布式系统： Sentinel本身设计为在有多个Sentinel进程协同合作的配置中运行。具有多个Sentinel进程进行协作的优点如下： 当多个哨兵就给定的主机不再可用这一事实达成共识时，将执行故障检测。这降低了误报的可能性。 即使不是所有的Sentinel进程都在工作，Sentinel仍能正常工作，从而使系统能够应对故障。毕竟，拥有故障转移系统本身就是一个单点故障 部署前有关Sentinel的基本知识 一个健壮的部署至少需要三个Sentinel实例。 应将三个Sentinel实例放置到被认为以独立方式发生故障的计算机或虚拟机中。因此，例如在不同的可用区域上执行的不同物理服务器或虚拟机。 Sentinel + Redis分布式系统不保证在故障期间保留已确认的写入，因为Redis使用异步复制。但是，有一些部署Sentinel的方法使窗口丢失写入仅限于某些时刻，而还有其他一些不太安全的方法来部署它。 您的客户需要Sentinel支持。流行的客户端库具有Sentinel支持，但不是全部。 如果您不在开发环境中不时进行测试，则没有安全的HA设置，如果可以，则在生产环境中甚至可以更好地进行测试。您可能有一个错误的配置，只有在为时已晚时（主服务器停止工作的凌晨3点），该错误才会变得明显。 Sentinel，Docker或其他形式的网络地址转换或端口映射应格外小心：Docker执行端口重新映射，破坏Sentinel对其他Sentinel进程的自动发现以及主副本的列表。有关更多信息，请参阅本文档后面有关Sentinel和Docker的部分。 1、安装redis（3台同步）官网文档：https://redis.io/topics/sentinel Github：https://github.com/antirez/redis/releases 1234567891011121314151617181920212223242526272829303132333435363738# 1、安装gcc依赖yum install -y gccyum install -y wget# 2、下载并解压安装包cd /data/soft/new/wget https://github.com/antirez/redis/archive/3.2.13.tar.gz -O redis-3.2.13.tar.gztar -zxf redis-3.2.13.tar.gz# 3、cd切换到redis解压目录下，执行编译cd redis-3.2.13make MALLOC=libc# 4、安装并指定安装目录make install PREFIX=/usr/local/rediscp redis.conf /usr/local/redis/bin/# 5、后台启动/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf# 6、开机启动cat &gt;&gt; /etc/systemd/system/redis.service &lt;&lt; EOF[Unit]Description=redis-serverAfter=network.target[Service]Type=forkingExecStart=/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.confPrivateTmp=true[Install]WantedBy=multi-user.targetEOF# 7、创建软连接ln -s /usr/local/redis/bin/redis-cli /usr/bin/redissystemctl daemon-reloadsystemctl status redis.service #查看服务当前状态systemctl enable redis.service #设置开机自启动systemctl disable redis.service #停止开机自启动systemctl start redis.service #启动redis服务systemctl stop redis.service #停止redis服务systemctl restart redis.service #重新启动服务 make test 报错解决方案： 1234567# You need tcl 8.5 or newer in order to run the Redis test.wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gzsudo tar xzvf tcl8.6.1-src.tar.gz -C /usr/local/cd /usr/local/tcl8.6.1/unix/sudo ./configuresudo makesudo make install 2、配置redis.confvi /usr/local/redis/bin/redis.conf scp root@dn1:/usr/local/redis/bin/redis.conf /usr/local/redis/bin/redis.conf 1234567891011121314151617181920212223242526272829303132333435# 使用 yes 启用守护进程daemonize yes# 设置数据库的数量，默认数据库为0，可以使用SELECT 命令在连接上指定数据库iddatabases 16pidfile /var/run/redis.pidport 6379# 绑定的主机地址bind 127.0.0.1# 设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH &lt;password&gt; 命令提供密码，默认关闭requirepass 123456# 当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能timeout 300# 设置同一时间最大客户端连接数，默认无限制，Redis 可以同时打开的客户端连接数为 Redis 进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息maxclients 128# 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启glueoutputbuf yes# 指定 Redis 最大内存限制，Redis 在启动时会把数据加载到内存中，达到最大内存后，Redis 会先尝试清除已到期或即将到期的 Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis 新的 vm 机制，会把 Key 存放内存，Value 会存放在 swap 区# maxmemory &lt;bytes&gt;# 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法hash-max-zipmap-entries 64hash-max-zipmap-value 512# 指定是否激活重置哈希，默认为开启（后面在介绍 Redis 的哈希算法时具体介绍）activerehashing yes# 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件# include /path/to/local.conf# 指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默认为 noticeloglevel notice# 日志记录方式，默认为标准输出，如果配置 Redis 为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 /dev/null# logfile stdoutlogfile /root/redis.log 123456789101112131415161718192021222324# rdb: 指定在多长时间内，有多少次更新操作，就将数据同步到rdb数据文件，可以多个条件配合# save &lt;seconds&gt; &lt;changes&gt;#当有一条Keys数据被改变时，900秒刷新到Disk一次save 900 1# 当有10条Keys数据被改变时，300秒刷新到Disk一次save 300 10# 当有10000条Keys数据被改变时，60秒刷新到Disk一次save 60 10000# rdb: 文件是否压缩数据，默认为 yes，Redis 采用 LZF 压缩，如果为了节省 CPU 时间，可以关闭该选项，但会导致数据库文件变的巨大rdbcompression yes# rdb: 指定本地RDB数据库文件名，默认值为 dump.rdbdbfilename dump.rdb# rdb: 指定本地数据库存放目录dir ./# aof: 如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis 本身同步数据文件是按上面 save 条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为 noappendonly no# aof: 指定更新日志文件名，默认为 appendonly.aofappendfilename "appendonly.aof"# aof: 指定更新日志条件，共有 3 个可选值：# no：表示等操作系统进行数据缓存同步到磁盘（快）# always：表示每次更新操作后手动调用 fsync() 将数据写到磁盘（慢，安全）# everysec：表示每秒同步一次（折中，默认值）appendfsync everysec 123456789101112# 指定是否启用虚拟内存机制，默认值为 no，简单的介绍一下，VM 机制将数据分页存放，由 Redis 将访问量较少的页即冷数据 swap 到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析 Redis 的 VM 机制）vm-enabled no# 虚拟内存文件路径，默认值为 /tmp/redis.swap，不可多个 Redis 实例共享vm-swap-file /tmp/redis.swap# 将所有大于 vm-max-memory 的数据存入虚拟内存，无论 vm-max-memory 设置多小，所有索引数据都是内存存储的(Redis 的索引数据 就是 keys)，也就是说，当 vm-max-memory 设置为 0 的时候，其实是所有 value 都存在于磁盘。默认值为 0vm-max-memory 0# Redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上面，但一个 page 上不能被多个对象共享，vm-page-size 是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page 大小最好设置为 32 或者 64bytes；如果存储很大大对象，则可以使用更大的 page，如果不确定，就使用默认值vm-page-size 32# 设置 swap 文件中的 page 数量，由于页表（一种表示页面空闲或使用的 bitmap）是在放在内存中的，，在磁盘上每 8 个 pages 将消耗 1byte 的内存vm-pages 134217728# 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4vm-max-threads 4 1234# 设置当本机为 slav 服务时，设置 master 服务的 IP 地址及端口，在 Redis 启动时，它会自动从 master 进行数据同步slaveof &lt;masterip&gt; &lt;masterport&gt;# 当 master 服务设置了密码保护时，slav 服务连接 master 的密码masterauth &lt;master-password&gt; 3、配置redis-sentinel集群 参考链接：https://www.jianshu.com/p/06ab9daf921d 配置3个哨兵 和 2个redis集群 服务类型 是否是主服务器 IP地址 端口 Redis 是 dn1 6379 Redis 否（dn1的slave） dn2 6379 Redis 是 dn3 6379 Sentinel - dn1 26379 Sentinel - dn2 26379 Sentinel - dn3 26379 3.1、修改 redis.confvi /usr/local/redis/bin/redis.conf scp root@dn1:/usr/local/redis/bin/redis.conf /usr/local/redis/bin/redis.conf dn1 和 dn3 配置123456# 使得Redis服务器可以跨网络访问bind 0.0.0.0# 设置密码requirepass 123456# 主服务器密码，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置masterauth 123456 dn2 配置123456789# 使得Redis服务器可以跨网络访问bind 0.0.0.0# 设置密码requirepass 123456# 主服务器密码，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置masterauth 123456# 指定主服务器，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置slaveof dn1 6379 3.2、修改sentinel.conf12345678910\cp /data/soft/new/redis-3.2.13/sentinel.conf /usr/local/redis/bin/vi /usr/local/redis/bin/sentinel.conf# vi编辑器如何把开头是#的行全都删掉:g/^#/d# 删除空行:g/^$/d# 删除空行以及只有空格的行:g/^\s*$/d# 删除以 # 开头或 空格# 或 tab#开头的行:g/^\s*#/d 123456789101112131415port 26379daemonize yesdir "/root/"logfile "redis-sentinel.log"# 禁止保护模式protected-mode no# 配置监听的主服务器，这里sentinel monitor代表监控，master-a代表服务器的名称，可以自定义，dn1 代表监控的主服务器，6379代表端口，2代表只有两个或两个以上的哨兵认为主服务器不可用的时候，才会进行failover操作。sentinel monitor master-a dn1 6379 2# sentinel author-pass定义服务的密码，mymaster是服务名称，123456是Redis服务器密码# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass master-a 123456sentinel monitor master-b dn3 6379 2sentinel auth-pass master-b 123456 1scp -r root@dn1:/usr/local/redis/bin/sentinel.conf /usr/local/redis/bin/sentinel.conf 开机启动 12echo '/usr/local/redis/bin/redis-sentinel /usr/local/redis/bin/sentinel.conf --sentinel' &gt;&gt; /etc/rc.localchmod +x /etc/rc.d/rc.local 开机启动 123456789101112131415161718cat &gt;&gt; /etc/systemd/system/redis-sentinel.service &lt;&lt; EOF[Unit]Description=redis-sentinelAfter=network.target[Service]Type=forkingExecStart=/usr/local/redis/bin/redis-sentinel /usr/local/redis/bin/sentinel.conf --sentinelExecStop=/usr/local/redis/bin/redis-cli -p 26379 shutdownPrivateTmp=true[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl status redis-sentinel.service #查看服务当前状态systemctl enable redis-sentinel.service #设置开机自启动systemctl disable redis-sentinel.service #停止开机自启动systemctl start redis-sentinel.service 配置环境变量 12echo 'export PATH=$PATH:/usr/local/redis/bin' &gt;&gt; /etc/profilesource /etc/profile 3.3、启动 redis 和 sentinel123systemctl start redis.service #启动redis服务systemctl stop redis.service #停止redis服务systemctl restart redis.service #重新启动服务 1234567891011$ /usr/local/redis/bin/redis-sentinel /usr/local/redis/bin/sentinel.conf --sentinel$ /usr/local/redis/bin/redis-cli -p 26379 shutdown$ /usr/local/redis/bin/redis-cli -p 26379127.0.0.1:26379&gt; sentinel master master-a127.0.0.1:26379&gt; sentinel slaves master-a127.0.0.1:26379&gt; sentinel sentinels master-a127.0.0.1:26379&gt; sentinel get-master-addr-by-name master-a127.0.0.1:26379&gt; info sentinel# 相对于常规的故障切换，其无需进行Sentinel节点的领导者选举。直接由当前Sentinel节点进行后续的故障切换。127.0.0.1:26379&gt; sentinel failover master-a]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3.8、Lucene文件检索实战 —— 结果展示]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-8-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[1、引入 Thymeleaf 模板引擎 2、定义搜索结果 —— result.html 3、调试搜索结果页面 4、点击文件名下载文件 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 工程结构 ​ 1、引入 Thymeleaf 模板引擎1、pom.xml 中引入依赖： 1234&lt;dependency&gt;​ &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;​ &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 2、application.properties 添加 Thymeleaf 配置 12345spring.thymeleaf.mode=HTML5spring.thymeleaf.encoding=UTF-8spring.thymeleaf.content-type=text/htmlspring.thymeleaf.cache=falsespring.thymeleaf.prefix=classpath:/templates/ ​ 2、定义搜索结果 —— result.html123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475&lt;!DOCTYPE html&gt;&lt;html xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html; charset=UTF-8"/&gt; &lt;title&gt;搜索结果&lt;/title&gt; &lt;link type="text/css" rel="stylesheet" href="css/base.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="css/page_search_result.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="css/moudle_g_search_bar.css"/&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="wrap"&gt; &lt;div class="main clearfix"&gt; &lt;div class="header"&gt;&lt;br/&gt;&lt;br/&gt;&lt;/div&gt; &lt;div id="search_result_panel"&gt; &lt;div class="content_top"&gt; &lt;div class="g_search_bar"&gt; &lt;div style="display: inline-block"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img alt="文件检索" src="images/logo.png" width="40px" height="35px"/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/div&gt; &lt;div style="display: inline-block" &gt; &lt;form class="search_form" action="/search"&gt; &lt;input class="search_text" name="keywords" type="text" th:value="$&#123;keywords&#125;" /&gt; &lt;input class="btn_submit" type="submit" value="搜索" /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="clear"&gt;&lt;/div&gt; &lt;div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="content_left"&gt; &lt;div class="result_item_article" style="display: none;"&gt; &lt;h3&gt;&lt;a href="#"&gt;coding的最新相关信息&lt;/a&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;最新更新：2015-4-3 2:25&lt;/li&gt; &lt;li&gt;122人赞同&lt;/li&gt; &lt;li&gt;44人反对&lt;/li&gt; &lt;/ul&gt; &lt;div class="article_body"&gt; &lt;a href="#"&gt; &lt;img alt="" src="http://p.blog.csdn.net/images/p_blog_csdn_net/sealyao/594039/o_clip_image001_thumb.jpg" /&gt; &lt;/a&gt; &lt;div&gt;哈哈哈哈哈哈嘎嘎嘎嘎嘎嘎哈哈哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈嘎嘎嘎嘎嘎哈哈哈哈&lt;/div&gt; &lt;/div&gt; &lt;div class="article_label"&gt; &lt;span&gt;标签：&lt;/span&gt; &lt;a href="#"&gt;ide&lt;/a&gt; &lt;a href="#"&gt;coding&lt;/a&gt; &lt;a href="#"&gt;ide&lt;/a&gt; &lt;a href="#"&gt;coding&lt;/a&gt; &lt;a href="#"&gt;ide&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="result_item_article" th:each="resultItem:$&#123;resultList&#125;"&gt; &lt;h3&gt; &lt;a th:href="@&#123;'/download?fileName='+$&#123;resultItem.filename&#125;&#125;" th:utext="$&#123;resultItem.title&#125;"&gt;&lt;/a&gt; &lt;/h3&gt; &lt;div class="article_body" th:utext="$&#123;resultItem.content&#125;"&gt;&lt;/div&gt; &lt;div class="article_label"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div th:if="$&#123;null == resultList || 0 == resultList.size()&#125;"&gt; &lt;h1&gt;没有搜索到相关数据 (;-_-)ᴇᴍᴍᴍ&lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="content_right"&gt;&lt;/div&gt; &lt;div class="clear"&gt;&lt;/div&gt; &lt;div class="refer_link"&gt;&lt;/div&gt; &lt;div class="page_index"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div th:include="footer"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; ​ 3、调试搜索结果页面定义url：search 12345678910111213/** * @param keywords * @return 搜索结果页面 */@RequestMapping("/search")public String searchFile(String keywords, Model model) &#123; String indexPathStr = "indexdir"; ArrayList&lt;FileModel&gt; hitsList = getTopDoc(keywords, indexPathStr, 100); log.info(keywords + "：共搜到：" + hitsList.size() + " 条数据！"); model.addAttribute("keywords", keywords); model.addAttribute("resultList", hitsList); return "result";&#125; 访问它： http://localhost:18080/search?keywords=%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6 http://localhost:18080/search?keywords=%E8%B4%9F%E8%B4%A3 ​ 4、点击文件名下载文件12345678910111213141516171819202122232425262728293031323334353637383940package com.learn.lucenefilesearch.controller;import lombok.extern.slf4j.Slf4j;import org.springframework.core.io.FileSystemResource;import org.springframework.http.*;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import java.io.File;import java.nio.charset.Charset;/** * 文件下载 * 访问：http://localhost:18080/download?fileName=Mycat-config.xml */@Slf4j@Controllerpublic class FileDownloadServletController &#123; @RequestMapping("/download") public ResponseEntity&lt;FileSystemResource&gt; downloadFile(String fileName) throws Exception &#123; String filePath = "files/" + fileName; File file = new File(filePath); log.info("download path:" + file.getPath()); FileSystemResource fileSystemResource = new FileSystemResource(file); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_OCTET_STREAM); headers.setContentLength(file.length()); headers.setCacheControl(CacheControl.noStore()); headers.setContentDisposition( ContentDisposition.builder("attachment") .filename(fileName, Charset.forName("UTF-8")) .size(file.length()) .build() ); return ResponseEntity.ok() .headers(headers) .body(fileSystemResource); &#125;&#125; 访问：http://localhost:18080/download?fileName=Mycat-config.xml http://localhost:18080/download?fileName=session共享版上线（第二版）.pptx 控制台输出：]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.7、Lucene文件检索实战 —— 文件检索]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-7-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[1、根据索引查找文件 2、搜索结果调试 3、整个文件如下 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 1、根据索引查找文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * @param keywords 用户输入的 关键词 * @param indexPathStr 索引路径 * @param N 结果条数 * @return 在索引中搜索 关键词，返回前N条结果 */ public static ArrayList&lt;FileModel&gt; getTopDoc(String keywords, String indexPathStr, int N) &#123; ArrayList&lt;FileModel&gt; hitsList = new ArrayList&lt;FileModel&gt;(); // 检索域 String[] fields = &#123;"title", "content"&#125;; Path indexPath = Paths.get(indexPathStr); try &#123; Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(true); MultiFieldQueryParser parser = new MultiFieldQueryParser(fields, analyzer); // 查询字符串 Query query = parser.parse(keywords); TopDocs topDocs = searcher.search(query, N); // 定制高亮标签 SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter("&lt;span style=\"color:red;\"&gt;", "&lt;/span&gt;"); QueryScorer scoreTitle = new QueryScorer(query, fields[0]); Highlighter hlTitle = new Highlighter(htmlFormatter, scoreTitle); QueryScorer scoreContent = new QueryScorer(query, fields[1]); Highlighter hlContent = new Highlighter(htmlFormatter, scoreContent); TopDocs hits = searcher.search(query, 100); for (ScoreDoc sd : topDocs.scoreDocs) &#123; Document doc = searcher.doc(sd.doc); String title = doc.get("title"); String content = doc.get("content"); TokenStream tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[0], new IKAnalyzer8x(true)); Fragmenter fragmenter = new SimpleSpanFragmenter(scoreTitle); hlTitle.setTextFragmenter(fragmenter); String hlTitleStr = hlTitle.getBestFragment(tokenStream, title); // 获取高亮的片段，可以对其数量进行限制 tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[1], new IKAnalyzer8x(true)); fragmenter = new SimpleSpanFragmenter(scoreContent); String hlContentStr = hlContent.getBestFragment(tokenStream, content); // 获取高亮的片段，可以对其数量进行限制 FileModel fileModel = new FileModel( RegexHtml.delHtmlTag(title), hlTitleStr != null ? hlTitleStr : title, hlContentStr != null ? hlContentStr : content); hitsList.add(fileModel); &#125; reader.close(); directory.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return hitsList; &#125; ​ 2、搜索结果调试定义接口 search-list 用于调试数据 123456789101112/** * @param keywords * @return 搜索数据调试 */@RequestMapping("/search-list")@ResponseBodypublic ArrayList&lt;FileModel&gt; searchFileList(String keywords) &#123; String indexPathStr = "indexdir"; ArrayList&lt;FileModel&gt; hitsList = getTopDoc(keywords, indexPathStr, 100); log.info("共搜到：" + hitsList.size() + " 条数据！"); return hitsList;&#125; http://localhost:18080/search-list?keywords=session ​ 3、整个文件如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.learn.lucenefilesearch.controller;import com.learn.lucenefilesearch.model.FileModel;import com.learn.lucenefilesearch.service.IKAnalyzer8x;import com.learn.lucenefilesearch.service.RegexHtml;import lombok.extern.slf4j.Slf4j;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import java.nio.file.Path;import java.nio.file.Paths;import java.util.ArrayList;@Slf4j@Controllerpublic class SearchFileController &#123; @RequestMapping("/") public String index() &#123; return "index"; &#125; /** * @param keywords * @return 搜索数据调试 */ @RequestMapping("/search-list") @ResponseBody public ArrayList&lt;FileModel&gt; searchFileList(String keywords) &#123; String indexPathStr = "indexdir"; ArrayList&lt;FileModel&gt; hitsList = getTopDoc(keywords, indexPathStr, 100); log.info("共搜到：" + hitsList.size() + " 条数据！"); return hitsList; &#125; /** * @param keywords 用户输入的 关键词 * @param indexPathStr 索引路径 * @param N 结果条数 * @return 在索引中搜索 关键词，返回前N条结果 */ public static ArrayList&lt;FileModel&gt; getTopDoc(String keywords, String indexPathStr, int N) &#123; ArrayList&lt;FileModel&gt; hitsList = new ArrayList&lt;FileModel&gt;(); // 检索域 String[] fields = &#123;"title", "content"&#125;; Path indexPath = Paths.get(indexPathStr); try &#123; Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(true); MultiFieldQueryParser parser = new MultiFieldQueryParser(fields, analyzer); // 查询字符串 Query query = parser.parse(keywords); TopDocs topDocs = searcher.search(query, N); // 定制高亮标签 SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter("&lt;span style=\"color:red;\"&gt;", "&lt;/span&gt;"); QueryScorer scoreTitle = new QueryScorer(query, fields[0]); Highlighter hlTitle = new Highlighter(htmlFormatter, scoreTitle); QueryScorer scoreContent = new QueryScorer(query, fields[1]); Highlighter hlContent = new Highlighter(htmlFormatter, scoreContent); TopDocs hits = searcher.search(query, 100); for (ScoreDoc sd : topDocs.scoreDocs) &#123; Document doc = searcher.doc(sd.doc); String title = doc.get("title"); String content = doc.get("content"); TokenStream tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[0], new IKAnalyzer8x(true)); Fragmenter fragmenter = new SimpleSpanFragmenter(scoreTitle); hlTitle.setTextFragmenter(fragmenter); String hlTitleStr = hlTitle.getBestFragment(tokenStream, title); // 获取高亮的片段，可以对其数量进行限制 tokenStream = TokenSources.getAnyTokenStream( searcher.getIndexReader(), sd.doc, fields[1], new IKAnalyzer8x(true)); fragmenter = new SimpleSpanFragmenter(scoreContent); String hlContentStr = hlContent.getBestFragment(tokenStream, content); // 获取高亮的片段，可以对其数量进行限制 FileModel fileModel = new FileModel( RegexHtml.delHtmlTag(title), hlTitleStr != null ? hlTitleStr : title, hlContentStr != null ? hlContentStr : content); hitsList.add(fileModel); &#125; reader.close(); directory.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return hitsList; &#125;&#125; ​]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.6、Lucene文件检索实战 —— 查询界面]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-6-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%9F%A5%E8%AF%A2%E7%95%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[1、工程结构 2、index.html 文件 3、效果图 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 索引构建完成以后，编写index.jsp页面接收用户 关键词。 1、工程结构 ​ 2、index.html 文件对应的css文件，从我的代码仓库中获取 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html; ; charset=UTF-8"/&gt; &lt;title&gt;文件检索&lt;/title&gt; &lt;link type="text/css" rel="stylesheet" href="../css/base.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="../css/index.css"/&gt; &lt;link type="text/css" rel="stylesheet" href="../css/moudle_g_search_bar.css"/&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="wrap"&gt; &lt;div class="main" class="clearfix"&gt; &lt;div class="header"&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/div&gt; &lt;div class="g_search_bar"&gt; &lt;div class="f-large tc"&gt; &lt;h1&gt;文件&amp;nbsp;&amp;nbsp; &lt;img alt="文件检索" src="images/logo.png" width="70px" height="60px"/&gt; &amp;nbsp;&amp;nbsp;搜索 &lt;/h1&gt; &lt;/div&gt; &lt;br/&gt; &lt;div class="tc"&gt; &lt;form class="search_form" style="display: inline-block" action="/search"&gt; &lt;input class="search_text" name="keywords" type="text" /&gt; &lt;input class="btn_submit" type="submit" value="搜索" /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="clear"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="footer"&gt; &lt;div class="copyright"&gt; &lt;div&gt;基于Lucene的文件检索系统&lt;/div&gt; &lt;br/&gt; &lt;div&gt;Copyright ©&lt;strong&gt;CarloZ&lt;/strong&gt;&amp;nbsp; All Rights Reversed.&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617package com.learn.lucenefilesearch.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;import java.util.ArrayList;import java.util.List;@Controllerpublic class SearchFileController &#123; @RequestMapping("/") public String index() &#123; return "index"; &#125;&#125; 3、效果图]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.5、Lucene文件检索实战 —— 索引文档]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-4-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E5%B7%A5%E7%A8%8B%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[先建立model对象 创建索引 -———————————- 代码仓库：https://gitee.com/carloz/lucene-learn.git https://gitee.com/carloz/lucene-learn/tree/master/lucene-filesearch -———————————- 工程搭建完成以后，首先构建索引； 检索的对象：文件； 为了简单：只索引 文档名 和 文档内容； 1、先建立model对象123456789101112131415package com.learn.lucenefilesearch.model;import lombok.*;/** * 文件对象 */@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class FileModel &#123; private String title; // 文件标题 private String content; // 文件内容&#125; 2、创建索引将 IKTokenizer8x 和 IKAnalyzer8x 从第2章的工程里复制过来； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package com.learn.lucenefilesearch.service;import com.learn.lucenefilesearch.model.FileModel;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.tika.exception.TikaException;import org.apache.tika.metadata.Metadata;import org.apache.tika.parser.AutoDetectParser;import org.apache.tika.parser.ParseContext;import org.apache.tika.parser.Parser;import org.apache.tika.sax.BodyContentHandler;import org.xml.sax.SAXException;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.ArrayList;import java.util.Date;import java.util.List;/** * 对 webapp/files 下的文档生成索引，保存在webapp/indexdir中 */public class CreateIndex &#123; public static void main(String[] args) throws IOException &#123; Analyzer analyzer = new IKAnalyzer8x(true); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE); Directory directory = null; IndexWriter indexWriter = null; Path indexPath = Paths.get("indexdir"); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); fieldType.setStored(true); fieldType.setTokenized(true); fieldType.setStoreTermVectors(true); fieldType.setStoreTermVectorPositions(true); fieldType.setStoreTermVectorOffsets(true); Date start = new Date(); if (!Files.isReadable(indexPath)) &#123; System.out.println(indexPath.toAbsolutePath() + "不存在或不可读，请检查"); System.exit(1); &#125; directory = FSDirectory.open(indexPath); indexWriter = new IndexWriter(directory, indexWriterConfig); ArrayList&lt;FileModel&gt; fileModelList = (ArrayList&lt;FileModel&gt;) extractFile(); for (FileModel f : fileModelList) &#123; Document doc = new Document(); doc.add(new Field("title", f.getTitle(), fieldType)); doc.add(new Field("content", f.getContent(), fieldType)); indexWriter.addDocument(doc); &#125; indexWriter.commit(); indexWriter.close(); directory.close(); Date end = new Date(); System.out.println("索引文档完成，共耗时：" + (end.getTime() - start.getTime()) + " 毫秒。"); &#125; public static List&lt;FileModel&gt; extractFile() throws IOException &#123; ArrayList&lt;FileModel&gt; list = new ArrayList&lt;&gt;(); File fileDir = new File("files"); File[] allFiles = fileDir.listFiles(); for (File f : allFiles) &#123; FileModel fm = new FileModel(f.getName(), ParserExtraction(f)); list.add(fm); &#125; return list; &#125; public static String ParserExtraction(File file) &#123; String fileContent = ""; BodyContentHandler handler = new BodyContentHandler(); Parser parser = new AutoDetectParser(); // 自动解析器接口 Metadata metadata = new Metadata(); FileInputStream inputStream; try &#123; inputStream = new FileInputStream(file); ParseContext context = new ParseContext(); parser.parse(inputStream, handler, metadata, context); fileContent = handler.toString(); inputStream.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (SAXException e) &#123; e.printStackTrace(); &#125; catch (TikaException e) &#123; e.printStackTrace(); &#125; return fileContent; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.3、Lucene文件检索实战 —— 文本内容抽取]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-3-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%96%87%E6%9C%AC%E5%86%85%E5%AE%B9%E6%8A%BD%E5%8F%96%2F</url>
    <content type="text"><![CDATA[3.3.1 Tika 简介 3.3.2、Tika下载 3.3.3、搭建工程 3.3.4、内容抽取 3.3.5、自动解析 -—————————————– 代码仓库：https://gitee.com/carloz/lucene-learn/tree/master/tika-demo -—————————————– ​ 3.3.1 Tika 简介Apache Tika 用于 文件类型检测 和 文件内容提取； 使用目标群体： 搜索引擎、 内容索引和分析工具； 编程语言：Java； Tika 可以检测超过1000种不同类型的文档，比如 DOC、DOCX、PPT、PPTX、TXT、PDF； 所有的文本类型 都可以通过一个简单的接口被解析； 广泛应用于 搜索引擎、内容分析、文本翻译、数字资产管理 等领域； 通过一个通用的API提取 多种文件格式 的内容； Tika特点： 统一解析器接口 低内存占用 快速处理 灵活元数据 解析器集成 MIME类型检测 语言检测 ​ 3.3.2、Tika下载官网：https://tika.apache.org/download.html 下载地址：http://mirror.bit.edu.cn/apache/tika/tika-app-1.20.jar 以 GUI 的方式打开：java -jar tika-app-1.20.jar -g 使用 File -&gt; open 代开一个本地文件，查看提取完成以后的数据 ​ 3.3.3、搭建工程——Java中使用Tika1、创建Maven工程 TikaDemo 2、添加pom引用 12345&lt;dependency&gt;​ &lt;groupId&gt;org.apache.tika&lt;/groupId&gt;​ &lt;artifactId&gt;tika-app&lt;/artifactId&gt;​ &lt;version&gt;1.20&lt;/version&gt;&lt;/dependency&gt; 3、工程根目录下 新建 files文件夹， 存放 待提取的测试文件 4、新建名为 TikaParsePdfDemo.java ​ 3.3.4、内容抽取——提取PDF文件的内容123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.learn.tikademo;import org.apache.tika.exception.TikaException;import org.apache.tika.io.TikaInputStream;import org.apache.tika.metadata.Metadata;import org.apache.tika.parser.ParseContext;import org.apache.tika.parser.pdf.PDFParser;import org.apache.tika.sax.BodyContentHandler;import org.xml.sax.SAXException;import java.io.*;/** * 3.3.4 Tika提供PDF文件内容 */public class TikaParsePdfDemo &#123; public static void main(String[] args) throws FileNotFoundException, IOException, TikaException, SAXException &#123; // 文件路径 String filepath = "files/IKAnalyzer中文分词器V2012使用手册.pdf"; // 新建File对象 File pdfFile = new File(filepath); // 创建 内容处理器对象 BodyContentHandler handler = new BodyContentHandler(); // 创建 元数据对象 Metadata metadata = new Metadata(); // 读入文件// FileInputStream inputStream = new FileInputStream(pdfFile); InputStream inputStream = TikaInputStream.get(pdfFile); // 创建解析器对象 ParseContext parseContext = new ParseContext(); // 实例化 PDFParser 对象 PDFParser pdfParser = new PDFParser(); // 调用 parse() 方法解析文件 pdfParser.parse(inputStream, handler, metadata, parseContext); // 遍历元数据内容 System.out.println("文件属性信息"); for (String name : metadata.names()) &#123; System.out.println(name + ":" + metadata.get(name)); &#125; // 打印 pdf 文件中的内容 System.out.println("pdf 文件中的内容："); System.out.println(handler.toString()); &#125;&#125; PDFParser —— 用于解析 PDF 文件； PDFParser pdfParser = new PDFParser(); OOXMLParser —— 用于解析 MS Office 文件；OOXMLParser ooxmlParser = new OOXMLParser(); TXTParser —— 用于解析 文本文件； TXTParser txtParser = new TXTParser(); HtmlParser —— 用于解析 HTML 文件；HtmlParser htmlParser = new HtmlParser(); XMLParser —— 用于解析 XML 文件；XMLParser xmlParser = new XMLParser(); ClassParser —— 用于解析 class 文件；ClassParser classParser = new ClassParser(); Tika还可以解析 图像、音频（如 MP3）、视频（如MP4）等多种类型的文件 ​ 3.3.5、自动解析上述解析pdf的例子如下：确定PDF文件 -&gt; 实例化PDFParser -&gt; 提取内容； 强大之处：Tika可以 先判断文件类型， 再根据文档类型实例化解析接口； 自动解析文档过程： 传任意类型文件到 Tika ==&gt; Tika 自身检测文件类型； Tika解析器库，根据 文件类型，选择合适的解析器； 解析完成后，对文档内容 进行提取，元数据提取； 两种解析方案： 使用Tika对象解析 使用Parse接口解析 使用Tika对象解析：123456789101112131415161718192021222324252627282930313233package com.learn.tikademo;import org.apache.tika.Tika;import org.apache.tika.exception.TikaException;import java.io.File;import java.io.IOException;/** * 3.3.5、自动解析 —— 使用Tika对象解析 * 1、传任意类型文件到 Tika ==&gt; Tika 自身检测文件类型； * 2、Tika解析器库，根据 文件类型，选择合适的解析器； * 3、解析完成后，对文档内容 进行提取，元数据提取； */public class TikaExtractionDemo &#123; public static void main(String[] args) throws IOException, TikaException &#123; Tika tika = new Tika(); // 新建存放各种文件的文件里 files File fileDir = new File("files"); // 如果文件夹路径错误，退出程序 if (!fileDir.exists()) &#123; System.out.println("文件夹不存在，请检查"); System.exit(0); &#125; // 获取文件夹下的所有文件，存放在File数组中 File[] fileArr = fileDir.listFiles(); String fileContent; for (File f : fileArr) &#123; fileContent = tika.parseToString(f); // 自动解析 System.out.println("Extracted Content: " + fileContent); &#125; &#125;&#125; 使用Parse接口解析12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.learn.tikademo;import org.apache.tika.exception.TikaException;import org.apache.tika.metadata.Metadata;import org.apache.tika.parser.AutoDetectParser;import org.apache.tika.parser.ParseContext;import org.apache.tika.parser.Parser;import org.apache.tika.sax.BodyContentHandler;import org.xml.sax.SAXException;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;/** * 3.3.5、自动解析 —— 使用Parse接口解析 * 1、传任意类型文件到 Tika ==&gt; Tika 自身检测文件类型； * 2、Tika解析器库，根据 文件类型，选择合适的解析器； * 3、解析完成后，对文档内容 进行提取，元数据提取； */public class ParserExtractionDemo &#123; public static void main(String[] args) throws FileNotFoundException, IOException, SAXException, TikaException &#123; // 新建存放各种文件的文件里 files File fileDir = new File("files"); // 如果文件夹路径错误，退出程序 if (!fileDir.exists()) &#123; System.out.println("文件夹不存在，请检查"); System.exit(0); &#125; Metadata metadata = new Metadata(); // 创建元数据对象 Parser parser = new AutoDetectParser(); ParseContext parseContext = new ParseContext(); // 自动检测分词器 // 获取文件夹下的所有文件，存放在File数组中 FileInputStream inputStream = null; File[] fileArr = fileDir.listFiles(); for (File f : fileArr) &#123; inputStream = new FileInputStream(f); BodyContentHandler handler = new BodyContentHandler(); // 创建内容处理器对象 parser.parse(inputStream, handler, metadata, parseContext); System.out.println("ParserExtractionDemo-" + f.getName() + ":\n" + handler.toString()); &#125; &#125;&#125; 运行结果查询：]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.2、Lucene文件检索实战 —— 架构设计]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-2-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[文档存储系统 —— 存储了不同类型的文件； 后台 —— 提取出 文件名 和 文档内容； Lucene —— 使用Lucune对 文件名 和 文档内容 进行索引； 前端 —— 对用户提供查询接口； 检索过程 —— 用户提交关键词，检索索引库， 返回匹配文档至前端页面 能够下载检索到的文件； 能够实现关键字的高亮； 工具准备： 使用 Tika 完成信息抽取； 使用 Lucene 构建索引； 使用 JSP页面 给用户提供查询接口； 使用 Servlet 完成搜索； 构建类似百度文库的小型文件检索系统]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.1、Lucene文件检索实战 —— 需求分析]]></title>
    <url>%2Felasticsearch%2Fdoc%2F3-1-Lucene%E6%96%87%E4%BB%B6%E6%A3%80%E7%B4%A2%E5%AE%9E%E6%88%98-%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[假设有一批文档，文档格式有 DOC、DOCX、PPT、PPTX、TXT、PDF 要实现类似于百度文库的文件检索系统。 需求如下： 能够对文件名进行检索； 能够对文件内容进行检索； 能够下载检索到的文件； 能够实现关键字的高亮；]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.7、Lucene新闻高频词提取]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-7-Lucene%E6%96%B0%E9%97%BB%E9%AB%98%E9%A2%91%E8%AF%8D%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[2.7.1、问题提出 2.7.2、需求分析 2.7.3、编程实现 -———————————————- 2.7.1、问题提出统计 一篇新闻文档，统计出现频率最高的哪些词语 2.7.2、需求分析文本关键词提取算法、开源工具很多 本文：《从Lucene索引中 提取 词项频率Top N》 词条化：从文本中 去除 标点、停用词等； 索引过程的本质：词条化 生成 倒排索引的过程； 代码思路：IndexReader的getTermVector获取文档的某一个字段 Terms，从 terms 中获取 tf（term frequency），拿到词项的 tf 以后，放到map中 降序排序，取出 Top-N. 2.7.3、编程实现网上找到新闻稿《李开复：无人驾驶进入黄金时代 AI有巨大投资机会》，放在 testfile/news.txt 文件中。 对 testfile/news.txt 生成索引： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.learn.lucene.chapter2.highfrequency;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.*;import java.nio.file.Paths;/** * 2.7.1、问题提出 * 统计 一篇新闻文档，统计出现频率最高的哪些词语 * 2.7.2、需求分析 * 文本关键词提取算法、开源工具很多 * 本文：《从Lucene索引中 提取 词项频率Top N》 * 词条化：从文本中 去除 标点、停用词等； * 索引过程的本质：词条化 生成 倒排索引的过程； * 代码思路：IndexReader的getTermVector获取文档的某一个字段 Terms，从 terms 中获取 tf（term frequency），拿到词项的 tf 以后，放到map中 降序排序，取出 Top-N. * 2.7.3、编程实现 * 网上找到新闻稿《李开复：无人驾驶进入黄金时代 AI有巨大投资机会》，放在 testfile/news.txt 文件中。 * * 索引文档 */public class IndexDocs &#123; public static void main(String[] args) throws IOException &#123; File newfile = new File("testfile/news.txt"); String text1 = textToString(newfile); Analyzer smcAnalyzer = new IKAnalyzer8x(true); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(smcAnalyzer); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE); // 索引的存储路径 Directory directory = FSDirectory.open(Paths.get("indexdir")); // 索引的增删改由 IndexWriter 创建 IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 新建 FieldType，用于指定字段索引时的信息 FieldType type = new FieldType(); type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); type.setStored(true); // 原始字符串全部被保存在索引中 type.setStoreTermVectors(true); // 存储词项 量 type.setTokenized(true); // 词条化 Document doc1 = new Document(); Field field1 = new Field("content", text1, type); doc1.add(field1); indexWriter.addDocument(doc1); indexWriter.close(); directory.close(); &#125; public static String textToString(File file) &#123; StringBuilder result = new StringBuilder(); try &#123; // 构造一个 BufferedReader 类来读取文件 BufferedReader bufferedReader = new BufferedReader(new FileReader(file)); String str = null; // 使用 readline 方法，一次读一行 while (null != (str = bufferedReader.readLine())) &#123; result.append(System.lineSeparator() + str); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return result.toString(); &#125;&#125; 运行。 提取高频词： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.learn.lucene.chapter2.highfrequency;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Terms;import org.apache.lucene.index.TermsEnum;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.BytesRef;import java.io.IOException;import java.nio.file.Paths;import java.util.*;/** * 提取高频词 */public class GetTopTerms &#123; public static void main(String[] args) throws IOException &#123; Directory directory = FSDirectory.open(Paths.get("indexdir")); IndexReader reader = DirectoryReader.open(directory); // 因为之索引了一个文档，所以DocID为0 // 通过 getTermVector 获取 content 字段的词项 Terms terms = reader.getTermVector(0, "content"); // 遍历词项 TermsEnum termsEnum = terms.iterator(); Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); BytesRef thisTerm; while (null != (thisTerm = termsEnum.next())) &#123; String termText = thisTerm.utf8ToString(); // 词项 // 通过 totalTermFreq() 方法获取词项频率 map.put(termText, (int) termsEnum.totalTermFreq()); &#125; // 按 value 排序 List&lt;Map.Entry&lt;String, Integer&gt;&gt; sortedMap = new ArrayList&lt;&gt;(map.entrySet()); Collections.sort(sortedMap, new Comparator&lt;Map.Entry&lt;String, Integer&gt;&gt;() &#123; @Override public int compare(Map.Entry&lt;String, Integer&gt; o1, Map.Entry&lt;String, Integer&gt; o2) &#123; return (o2.getValue() - o1.getValue()); &#125; &#125;); getTopN(sortedMap, 10); &#125; public static void getTopN(List&lt;Map.Entry&lt;String, Integer&gt;&gt; sortedMap, int N) &#123; for (int i = 0; i &lt; N; i++) &#123; System.out.println(sortedMap.get(i).getKey() + ":" + sortedMap.get(i).getValue()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.6、Lucene查询高亮]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-6-Lucene%E6%9F%A5%E8%AF%A2%E9%AB%98%E4%BA%AE%2F</url>
    <content type="text"><![CDATA[找到需要高亮的片段 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.learn.lucene.chapter2.hignlight;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 找到需要高亮的片段 */public class HighlighterTest &#123; public static void main(String[] args) throws IOException, ParseException, InvalidTokenOffsetsException &#123; String field = "title"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(); QueryParser parser = new QueryParser(field, analyzer); Query query = parser.parse("北大"); System.out.println("Query: " + query); QueryScorer scorer = new QueryScorer(query, field); SimpleHTMLFormatter htmlFormatter = new SimpleHTMLFormatter("&lt;span style=\"color:red;\"&gt;", "&lt;/span&gt;"); Highlighter highlighter = new Highlighter(htmlFormatter, scorer); // 高亮分词器 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); TokenStream tokenStream = TokenSources.getAnyTokenStream(searcher.getIndexReader(), scoreDoc.doc, field, analyzer); Fragmenter fragmenter = new SimpleSpanFragmenter(scorer); highlighter.setTextFragmenter(fragmenter); String str = highlighter.getBestFragment(tokenStream, doc.get(field)); System.out.println("高亮的片段：" + str); &#125; directory.close(); reader.close(); &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.5、Lucene查询详解]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-5-Lucene%E6%9F%A5%E8%AF%A2%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录2.5.1、搜索入门 2.5.2、多域搜索（MultiFieldQueryParse） 2.5.3、词项搜索（TermQuery） 2.5.4、布尔搜索（BooleanQuery） 2.5.5、范围搜索（RangeQuery） 2.5.6、前缀搜索（PrefixQuery） 2.5.7、多关键字搜索（PhraseQuery） 2.5.8、模糊搜索（FuzzyQuery） 2.5.9、通配符搜索（WildcardQuery） -————————————————— 文档索引完成以后就能对其进行搜索； 当用户输入一个关键字， ​ –&gt; 首先 对这个关键字 进行 分析和处理， 转化成后台可以理解的形式 ​ –&gt; 进行检索 2.5.1、搜索入门处理关键词 &lt;==&gt; 构建Query对象的过程； 搜索文档 &lt;==&gt; 实例化 IndexSearcher 对象，使用search()方法完成； ​ 参数：Query对象 ​ 结果：保存在 TopDocs 类型的文档集合中； 删除indexdir下的索引文件后，重新使用CreateIndex.java 生成索引 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.learn.lucene.chapter2.queries;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.1、查询 */public class QueryParseTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String field = "title"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(); QueryParser parser = new QueryParser(field, analyzer); parser.setDefaultOperator(QueryParser.Operator.AND); Query query = parser.parse("农村学生"); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： 修改后再运行： ​ 2.5.2、多域搜索（MultiFieldQueryParse）根据多个字段搜索 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.learn.lucene.chapter2.queries;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.2、多域搜索 */public class MultiFieldQueryParseTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String[] field = &#123;"title", "content"&#125;; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(true); MultiFieldQueryParser parser = new MultiFieldQueryParser(field, analyzer); parser.setDefaultOperator(QueryParser.Operator.AND); Query query = parser.parse("美国"); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.3、词项搜索（TermQuery）TermQuery 是 最常用的 Query TermQuery 是 Lucene中搜索的最基本单位 本质上：一个词条就是一个 key/value 对 使用TermQuery： 首先构造一个 Term对象；Term term = new Term(“title”, “美国”); 然后使用Term对象为参数，构造一个TermQuery对象；TermQuery query = new TermQuery(term); 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.3 词项搜索 */public class TermQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Term term = new Term("title", "美国"); TermQuery query = new TermQuery(term); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.4、布尔搜索（BooleanQuery）BooleanQuery 可以 组合 其他 Query，并标明他们的逻辑关系； 例如：查询 content 中包含美国，并且 title 不包含美国的文档； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.4、布尔搜索（BooleanQuery） * 查询 content 中包含美国，并且 title 不包含美国的文档； */public class BooleanQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); TermQuery query1 = new TermQuery(new Term("title", "美国")); TermQuery query2 = new TermQuery(new Term("content", "美国")); BooleanClause booleanClause1 = new BooleanClause(query1, BooleanClause.Occur.MUST_NOT); BooleanClause booleanClause2 = new BooleanClause(query2, BooleanClause.Occur.MUST); BooleanQuery query = new BooleanQuery.Builder().add(booleanClause1).add(booleanClause2).build(); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.5、范围搜索（RangeQuery）举例：查询新闻回复条数在 500~1000 之间的文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.document.IntPoint;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.5、范围搜索（RangeQuery） * 举例：查询新闻回复条数在 500~1000 之间的文档 */public class RangeQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = IntPoint.newRangeQuery("reply", 500, 1000); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; ​ 2.5.6、前缀搜索（PrefixQuery）举例：搜索 包含以“学”开头的词项 的文档 123456789101112131415161718192021222324252627282930313233343536373839404142package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.6、前缀搜索（PrefixQuery） * 举例：搜索 包含以“学”开头的词项 的文档 */public class PrefixQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Term term = new Term("title", "学"); Query query = new PrefixQuery(term); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果： ​ 2.5.7、多关键字搜索（PhraseQuery） PhraseQuery 可以 通过add方法添加多个关键字 还可以通过 setSlop() 设定“坡度”，允许关键字之间 无关词汇存在量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.PhraseQuery;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.7、多关键字搜索（PhraseQuery） * PhraseQuery 可以 通过add方法添加多个关键字 * 还可以通过 setSlop() 设定“坡度”，允许关键字之间 无关词汇存在量 */public class PhraseQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String str = "习近平会见奥巴马，学习国外经验"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); PhraseQuery.Builder builder = new PhraseQuery.Builder(); builder.add(new Term("title", "奥巴马"), str.indexOf("奥巴马")); builder.add(new Term("title", "学习国外经验"), str.indexOf("学习国外经验")); PhraseQuery query = builder.build(); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 运行结果(感觉没有成功)： ​ 2.5.8、模糊搜索（FuzzyQuery）它可以简单的识别两个相近的词语。 举例：“Trump”，写成“Trmp”，拼写错误，仍然可以搜索得到正确的结果 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.8、模糊搜索（FuzzyQuery） * 它可以简单的识别两个相近的词语。 * 举例：“Trump”，写成“Trmp”，拼写错误，仍然可以搜索得到正确的结果 */public class FuzzyQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = new FuzzyQuery(new Term("title", "Trmp")); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; ​ 2.5.9、通配符搜索（WildcardQuery）1234567891011121314151617181920212223242526272829303132333435363738394041package com.learn.lucene.chapter2.queries;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 2.5.9、通配符搜索（WildcardQuery） */public class WildcardQueryTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = new WildcardQuery(new Term("title", "习?平")); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("content: " + doc.get("content")); System.out.println("reply: " + doc.get("reply_display")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125;]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[宋维钢38000词汇速记]]></title>
    <url>%2Fenglish%2Fword-38000%2F</url>
    <content type="text"><![CDATA[人群一：学生【中高考，大学生，考研生】1、中高考【应对考试】时间规划：每周2小时词汇速记【每次上课30分钟即可】 学习策略：1~300词根是中高考的核心 学习顺序： ① 词根词缀完全默写：汉语意思完全掌握【结合词元理解词根】 ② 总结：根据每个词根下历次进行重复的词缀统计 ③ 熟记核心词：模仿老师的教学笔记，尝试通过词根词缀拆分核心词，之后根据核心词的含义和词性 ④ 根据教学内容：准确划分思维导图【不需要增加汉语意思】 ⑤ 读单词必须依据音标：学音标没有为什么，只有是什么 2、大学生【口语和应用为主】学习强度：每周至少3小时词汇速记【3节课】 学习策略： ① 思维导图务必采用软件制作 ② 单词的发音务必及时掌握 ③ 要在固定的英语文章当中刻意寻找刻意拆分的单词 ④ 针对考研学生：所有的考研历年真题都是拆分单词的核心试题，最好通过词源网站的英文解释来加深考研词汇的印象。abbreviate 查词元 -&gt; 拆单词：ab-brev-i-ate 考研单词很多词频都在10000~20000之间 = 400 词根 人群二：职场人士 —— 学习顺序同上（中高考）时间规划：一周零碎时间学习 模式一（特别忙）： 每天中午20分钟 —— 一周保证2节课 每天速记笔记，完成作业的时间 —— 睡前30分钟 模式二： 选择周六日的时间，一天一节课，课后1小时完成作业 人群三：完全出于兴趣，自我提升，没有压力① 通过枚举和BBC类型的纪录片 学习词根和词缀。凡是出现再片中的词根都是高频词根 ② 每周的学习频率至少保持在2~4节课 人群四：出国人群学习策略 ① 强度： 如果在国外的学生，每天务必学习单词1小时 如果实在国内备战的学生：雅思托福的准备时间：3~6个月（不能再长） 单词速记要求一周之内： 5节课 + 作业（每个词根化思维导图）+ 真题答案单词的拆分 替换高级词汇： difficult = complicated ② 上课听讲 - 记笔记， 下课回放 看三轮 每节作业保质量，课后作业要自觉； 分布考查要自信，分级练习要坚持； 思维导图是核心]]></content>
  </entry>
  <entry>
    <title><![CDATA[2.4、Lucene索引详解]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-4-Lucene%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录2.4.1、Lucene 字段类型 2.4.2、索引文档示例 2.4.3、在Luke中查看索引 2.4.4、索引的删除 2.4.5 索引的更新 -———————————————– 上一节 介绍完了 Lucene 分词器，这节介绍 Lucene 是如何索引文档的。 ​ 2.4.1、Lucene 字段类型文档：文档是 Lucene索引的基本单位； 字段：比文档更小的单位，字段是文档的一部分； ​ 每个字段 由 3部分组成：名称（name），类型（type），取值（value）； ​ 字段的取值（value）一般为：文本、二进制、数值 Lucene的主要字段类型： TextField：字段内容 -&gt; 索引并词条化 -&gt; 不保存 词向量 -&gt; 整篇文档的body字段，常用TextField进行索引； StringField：只 索引 -&gt; 不词条化 -&gt; 不保存 词向量； IntPoint：适合 int类型 的索引； LongPoint：适合 long类型的 索引； FloatPoint： DoublePoint： SortedDocValuesField：存储值为 文本内容的 DocValue字段，且需要 按值排序； SortedSetDocValuesField：多值域为 DocValue字段，值为文本内容，且 需要 按值分组、聚合； NumbericDocValuesField： DocValue为（int，long，float，double） SortedNumbericDocValuesField：需要排序的 （int，long，float，double） SortedField：索引 保存字段值，不进行其他操作 Lucene 使用 倒排索引 快速搜索 &lt;===&gt; 建立 词项和文档id的关系映射； 搜索过程： ​ 通过 类似hash算法 -&gt; 定位到 搜索关键词 -&gt; 读取文档id集合 上述过程的缺陷： ​ 当我们需要对数据做 聚合操作，排序、分组时，Lucene会提取所有出现在文档集合中的排序字段，再次构建一个排好序的文件集合；这个过程全部在内存中进行，如果数据量巨大，会造成 内存溢出 和 性能缓慢； Lucene 4.X 之后出现了 DocValues，DocValues是 Lucene 构建索引时，额外建立的一个有序的基于 document =&gt; field/value 的映射列表。 ​ 2.4.2、索引文档示例代表新闻的实例类 News.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.learn.lucene.chapter2.index;/** * 代表新闻的实例类 News.java */public class News &#123; private int id; private String title; private String content; private int reply; public News() &#123; &#125; public News(int id, String title, String content, int reply) &#123; super(); this.id = id; this.title = title; this.content = content; this.reply = reply; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getTitle() &#123; return title; &#125; public void setTitle(String title) &#123; this.title = title; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; public int getReply() &#123; return reply; &#125; public void setReply(int reply) &#123; this.reply = reply; &#125;&#125; 创建索引文件的CreateIndex.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package com.learn.lucene.chapter2.index;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.Date;/** * 创建索引 */public class CreateIndex &#123; public static void main(String[] args) &#123; // 创建 3 个News对象 News news1 = new News(); news1.setId(1); news1.setTitle("习近平会见美国总统奥巴马，学习国外经验"); news1.setContent("国家主席习近平9月3日在杭州西湖宾馆会见前来出席二十国集团领导人杭州峰会的美国总统奥巴马"); news1.setReply(672); News news2 = new News(); news2.setId(2); news2.setTitle("北大迎4380名新生，农村学生700多人今年最多"); news2.setContent("昨天，北京大学迎来4380名来自全国各地及数十个国家的本科新生。其中，农村学生工700余名，为今年最多..."); news2.setReply(995); News news3 = new News(); news3.setId(3); news3.setTitle("特朗普宣誓（Donald Trump）就任美国第45任总统"); news3.setContent("当地时机1月20日，唐纳德·特朗普在美国国会宣誓就职，正式成为美国第45任总统。"); news3.setReply(1872); // 创建IK分词器 Analyzer analyzer = new IKAnalyzer8x(); IndexWriterConfig iwc = new IndexWriterConfig(analyzer); iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE); Directory directory = null; IndexWriter indexWriter = null; // 索引目录 Path indexPath = Paths.get("indexdir"); // 开始时间 Date start = new Date(); try &#123; if (!Files.isReadable(indexPath)) &#123; System.out.println("Document directory '" + indexPath.toAbsolutePath() + "' is not readable! please check"); System.exit(1); &#125; directory = FSDirectory.open(indexPath); indexWriter = new IndexWriter(directory, iwc); // 设置新闻ID 索引 并存储 FieldType idType = new FieldType(); idType.setIndexOptions(IndexOptions.DOCS); idType.setStored(true); // 设置新闻标题索引文档，词项频率，位移信息，和偏移量，存储并词条化 FieldType titleType = new FieldType(); titleType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); titleType.setStored(true); titleType.setTokenized(true); FieldType contentType = new FieldType(); contentType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); contentType.setStored(true); contentType.setTokenized(true); contentType.setStoreTermVectors(true); contentType.setStoreTermVectorPositions(true); contentType.setStoreTermVectorOffsets(true); contentType.setStoreTermVectorPayloads(true); Document doc1 = new Document(); doc1.add(new Field("id", String.valueOf(news1.getId()), idType)); doc1.add(new Field("title", news1.getTitle(), titleType)); doc1.add(new Field("content", news1.getContent(), contentType)); doc1.add(new IntPoint("reply", news1.getReply())); doc1.add(new StoredField("reply_display", news1.getReply())); Document doc2 = new Document(); doc2.add(new Field("id", String.valueOf(news2.getId()), idType)); doc2.add(new Field("title", news2.getTitle(), titleType)); doc2.add(new Field("content", news2.getContent(), contentType)); doc2.add(new IntPoint("reply", news2.getReply())); doc2.add(new StoredField("reply_display", news2.getReply())); Document doc3 = new Document(); doc3.add(new Field("id", String.valueOf(news3.getId()), idType)); doc3.add(new Field("title", news3.getTitle(), titleType)); doc3.add(new Field("content", news3.getContent(), contentType)); doc3.add(new IntPoint("reply", news3.getReply())); doc3.add(new StoredField("reply_display", news3.getReply())); indexWriter.addDocument(doc1); indexWriter.addDocument(doc2); indexWriter.addDocument(doc3); indexWriter.commit(); indexWriter.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; Date end = new Date(); System.out.println("索引文档用时：" + (end.getTime() - start.getTime()) + " milliseconds."); &#125;&#125; 运行结果： 生成对应的索引文件 ​ 2.4.3、在Luke中查看索引 打开索引文件目录：D:\java\oschina\lucene-learn\lucene-chapter2\indexdir ​ 2.4.4、索引的删除索引同样存在 CRUD 操作 本节演示 根据 Term 来删除点单个或多个Document，删除 title 中 包含关键词“美国”的文档。 1234567891011121314151617181920212223242526272829303132333435363738394041package com.learn.lucene.chapter2.index;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 删除索引 */public class DeleteIndex &#123; public static void main(String[] args) &#123; // 删除 title 中含有关键字“美国”的文档 deleteDoc("title", "美国"); &#125; public static void deleteDoc(String field, String key) &#123; Analyzer analyzer = new IKAnalyzer8x(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); Path indexPath = Paths.get("indexdir"); Directory directory; try &#123; directory = FSDirectory.open(indexPath); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); indexWriter.deleteDocuments(new Term(field, key)); indexWriter.commit(); indexWriter.close(); System.out.println("删除完成"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果： 除此之外，IndexWriter还提供了以下方法： DeleteDocuments(Query query)：根据Query条件来删除单个或多个Document。 DeleteDocuments(Query[] queries)：根据Query条件来删除单个或多个Document。 DeleteDocuments(Term term)：根据Term条件来删除单个或多个Document。 DeleteDocuments(Term[] terms)：根据Term条件来删除单个或多个Document。 DeleteAll()：删除所有的Document。 使用IndexWriter进行Document删除操作时，文档并不会立即被删除，而是把这个删除动作缓存起来，当IndexWriter.Commit() 或 IndexWriter.Close()时，删除操作才会真正执行。 ​ 使用Luke 重新打开 索引之后，只剩下了一个索引文档： ​ 2.4.5 索引的更新本质：先删除索引，再建立新的文档。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.learn.lucene.chapter2.index;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/** * 更新索引 */public class UpdateIndex &#123; public static void main(String[] args) &#123; Analyzer analyzer = new IKAnalyzer8x(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); Path indexPath = Paths.get("indexdir"); Directory directory; try &#123; directory = FSDirectory.open(indexPath); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); Document doc = new Document(); // 设置新闻ID 索引 并存储 FieldType idType = new FieldType(); idType.setIndexOptions(IndexOptions.DOCS); idType.setStored(true); // 设置新闻标题索引文档，词项频率，位移信息，和偏移量，存储并词条化 FieldType titleType = new FieldType(); titleType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); titleType.setStored(true); titleType.setTokenized(true); FieldType contentType = new FieldType(); contentType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); contentType.setStored(true); contentType.setTokenized(true); contentType.setStoreTermVectors(true); contentType.setStoreTermVectorPositions(true); contentType.setStoreTermVectorOffsets(true); contentType.setStoreTermVectorPayloads(true); doc.add(new Field("id", "2", idType)); doc.add(new Field("title", "北大迎4380名新生", titleType)); doc.add(new Field("content", "昨天，北京大学迎来4380名来自全国各地及数十个国家的本科新生。其中，农村学生工700余名，为今年最多...", contentType)); indexWriter.updateDocument(new Term("title", "北大"), doc); indexWriter.commit(); indexWriter.close(); System.out.println("更新完成"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果： 修改前： 修改后：]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.3、Lucene分词详解]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-3-Lucene%E5%88%86%E8%AF%8D%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录2.3.1、Lucene分词系统 2.3.2、分词测试 2.3.3、IK分词器配置 2.3.4、中文分词器对比 2.3.5、扩展停用词词典 2.3.6、扩展自定义词典 -—————————————————————— ​ 2.3.1、Lucene分词系统索引和查询 都是以 词项 为基本单位 Lucene中，分词 主要依靠 Analyzer类 解析实现 Analyzer是抽象类，内部调用 TokenStream 实现 ​ 2.3.2、分词测试StandardAnalyzer 分词器测试： 1234567891011121314151617181920212223242526272829303132333435363738package com.learn.lucene.chapter2.analyzer;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import java.io.IOException;import java.io.StringReader;/** * StandardAnalyzer demo */public class StdAnalyzerDemo &#123; private static String strCh = "中华人名共和国简称中国，是一个有13亿人口的国家"; private static String strEn = "Dogs can not achieve a place, eyes can reach;"; public static void main(String[] args) throws IOException &#123; System.out.println("StandardAnalyzer 对中文分词："); stdAnalyzer(strCh); System.out.println("StandardAnalyzer 对英文分词："); stdAnalyzer(strEn); &#125; public static void stdAnalyzer(String str) throws IOException &#123; Analyzer analyzer = null; analyzer = new StandardAnalyzer(); StringReader reader = new StringReader(str); TokenStream tokenStream = analyzer.tokenStream(str, reader); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class); System.out.println("分词结果："); while (tokenStream.incrementToken()) &#123; System.out.print(charTermAttribute.toString() + "|"); &#125; System.out.println("\n"); analyzer.close(); &#125;&#125; 运行结果： 测试多种Analyzer，注意要特意指定 jdk8： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.learn.lucene.chapter2.analyzer;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.cjk.CJKAnalyzer;import org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer;import org.apache.lucene.analysis.core.KeywordAnalyzer;import org.apache.lucene.analysis.core.SimpleAnalyzer;import org.apache.lucene.analysis.core.StopAnalyzer;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import java.io.IOException;import java.io.StringReader;import java.util.StringJoiner;/** * 测试多种分词器 */public class VariousAnalyzersDemo &#123; private static String strCh = "中华人民共和国简称中国，是一个有13亿人口的国家"; private static String strEn = "Dogs can not achieve a place, eyes can reach;"; public static void main(String[] args) throws IOException &#123; System.out.println("标准分词：" + printAnalyzer(new StandardAnalyzer(), strCh)); System.out.println("空格分词：" + printAnalyzer(new WhitespaceAnalyzer(), strCh)); System.out.println("简单分词：" + printAnalyzer(new SimpleAnalyzer(), strCh)); System.out.println("二分法分词：" + printAnalyzer(new CJKAnalyzer(), strCh)); System.out.println("关键字分词：" + printAnalyzer(new KeywordAnalyzer(), strCh)); System.out.println("停用词分词：" + printAnalyzer(new StopAnalyzer(new StringReader(strCh)), strCh)); System.out.println("中文智能分词：" + printAnalyzer(new SmartChineseAnalyzer(), strCh)); &#125; public static String printAnalyzer(Analyzer analyzer, String str) throws IOException &#123; StringReader reader = new StringReader(str); TokenStream tokenStream = analyzer.tokenStream(str, reader); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class); StringJoiner stringJoiner = new StringJoiner("|"); while (tokenStream.incrementToken()) &#123; stringJoiner.add(charTermAttribute.toString()); &#125; analyzer.close(); return stringJoiner.toString(); &#125;&#125; 运行结果： ​ 2.3.3、IK分词器配置Lucene 8.0 实用 IK分词器需要修改 IKTokenizer 和 IKAnalyzer 在 com.learn.lucene.chapter2.ik 下 新建 IKTokenizer8x.java 和 IKAnalyzer8x.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.learn.lucene.chapter2.ik;import org.apache.lucene.analysis.Tokenizer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;import org.apache.lucene.analysis.tokenattributes.TypeAttribute;import org.wltea.analyzer.core.IKSegmenter;import org.wltea.analyzer.core.Lexeme;import java.io.IOException;/** * 修改 IKTokenizer —— incrementToken() */public class IKTokenizer8x extends Tokenizer &#123; // IK分词器实现 private IKSegmenter _IKIkSegmenter; // 词元文本属性 private final CharTermAttribute termAttribute; // 词元位移属性 private final OffsetAttribute offsetAttribute; // 词元分类属性 // (该属性分类参考 org.wltea.analyzer.core.Lexeme 中的分类常量) private final TypeAttribute typeAttribute; // 记录最后一个词元的结束位置 private int endPosttion; // Lucene 8.x Tokenizer适配器类构造函数，实现最新的 Tokenizer 接口 public IKTokenizer8x(boolean useSmart) &#123; super(); offsetAttribute = addAttribute(OffsetAttribute.class); termAttribute = addAttribute(CharTermAttribute.class); typeAttribute = addAttribute(TypeAttribute.class); _IKIkSegmenter = new IKSegmenter(input, useSmart); &#125; @Override public boolean incrementToken() throws IOException &#123; clearAttributes(); // 清除所有的词元属性 Lexeme nextLexeme = _IKIkSegmenter.next(); if (nextLexeme != null) &#123; // 将 Lexeme 转化成 Attributes termAttribute.append(nextLexeme.getLexemeText()); // 设置词元文本 termAttribute.setLength(nextLexeme.getLength()); // 设置词元长度 offsetAttribute.setOffset(nextLexeme.getBeginPosition(), nextLexeme.getEndPosition()); // 设置词元位移 endPosttion = nextLexeme.getEndPosition(); // 记录 分词的最后位置 typeAttribute.setType(nextLexeme.getLexemeText()); // 记录词元分分类 return true; // 返回true 告知 还有下个词元 &#125; return false; &#125; @Override public void reset() throws IOException &#123; super.reset(); _IKIkSegmenter.reset(input); &#125; @Override public final void end() throws IOException &#123; int finalOffset = correctOffset(this.endPosttion); offsetAttribute.setOffset(finalOffset, finalOffset); &#125;&#125; ​ 1234567891011121314151617181920212223242526272829303132333435package com.learn.lucene.chapter2.ik;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.Tokenizer;/** * 修改IKAnalyzer —— createComponents(String fieldName) */public class IKAnalyzer8x extends Analyzer &#123; private boolean useSmart; public IKAnalyzer8x() &#123; this(false); &#125; public IKAnalyzer8x(boolean useSmart) &#123; super(); this.useSmart = useSmart; &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; Tokenizer _IKTokenizer = new IKTokenizer8x(this.isUseSmart()); return new TokenStreamComponents(_IKTokenizer); &#125; public boolean isUseSmart() &#123; return useSmart; &#125; public void setUseSmart(boolean useSmart) &#123; this.useSmart = useSmart; &#125;&#125; 实例化 IKAnalyzer8x 就能实用IK分词器了 1、 默认使用细粒度切分算法： ​ Analyzer analyzer = new IKAnalyzer8x(); 2、创建智能切分算法的 IKAnalyzer： ​ Analyzer analyzer = new IKAnalyzer8x(true); ​ 2.3.4、中文分词器对比​ 分词效果会直接影响文档搜索的准确性 ​ 我们对比一下 Lucene自带的 SmartChineseAnalyzer 和 IK Analyzer的效率。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.learn.lucene.chapter2.analyzer;import com.learn.lucene.chapter2.ik.IKAnalyzer8x;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import java.io.IOException;import java.io.StringReader;import java.util.StringJoiner;/** * 中文分词器效果对比 * 1、Lucene 自带的 SmartChineseAnalyzer 分词器 * 2、IKAnalyzer8x分词器 */public class IkVSSmartchDemo &#123; private static String str1 = "公路局正在治理解放大道路面积水问题。"; private static String str2 = "IKAnalyzer 是一个开源的，基于java语言开发的轻量级的中文分词工具包。"; public static void main(String[] args) throws IOException &#123; System.out.println("句子一：" + str1); System.out.println("SmartChineseAnalyzer分词结果：" + printAnalyzer(new SmartChineseAnalyzer(), str1)); System.out.println("IKAnalyzer8x分词结果：" + printAnalyzer(new IKAnalyzer8x(true), str1));// System.out.println("IKAnalyzer分词结果(bug)：" + printAnalyzer(new IKAnalyzer(), str1)); System.out.println("----------------------------------------"); System.out.println("句子二：" + str2); System.out.println("SmartChineseAnalyzer分词结果：" + printAnalyzer(new SmartChineseAnalyzer(), str2)); System.out.println("IKAnalyzer8x分词结果：" + printAnalyzer(new IKAnalyzer8x(true), str2));// System.out.println("IKAnalyzer分词结果(bug)：" + printAnalyzer(new IKAnalyzer(true), str2)); &#125; public static String printAnalyzer(Analyzer analyzer, String str) throws IOException &#123; StringReader reader = new StringReader(str); TokenStream tokenStream = analyzer.tokenStream(str, reader); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class); StringJoiner stringJoiner = new StringJoiner("|"); while (tokenStream.incrementToken()) &#123; stringJoiner.add(charTermAttribute.toString()); &#125; analyzer.close(); return stringJoiner.toString(); &#125;&#125; ​ 2.3.5、扩展停用词词典IK Analyzer 默认的停用词词典为 IKAnalyzer2012_u6/stopword.dic 这个词典只有30多个英文停用词，并不完整 推荐使用扩展额停用词词表：https://github.com/cseryp/stopwords 在工程中新建 ext_stopword.dic，放在IKAnalyzer.cfg.xml同一目录； 编辑IKAnalyzer.cfg.xml， ​]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.2、Lucene开发准备]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-2-Lucene%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[目录 官网 下载Lucune文件库 下载Luke——查看索引的GUI工具 下载 IK 分词工具 —— 轻量级中文分词工具包 工程搭建 -————————————————— ​ 官网http://lucene.apache.org/ 下载Lucune文件库 https://mirrors.tuna.tsinghua.edu.cn/apache/lucene/java/8.0.0/ 或者在工程中添加maven依赖 https://mvnrepository.com/artifact/org.apache.lucene/lucene-core/ 12345&lt;dependency&gt;​ &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;​ &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;​ &lt;version&gt;8.0.0&lt;/version&gt;&lt;/dependency&gt; ​ 下载Luke——查看索引的GUI工具主要功能： 查看文档和分析字段内容； 搜索索引； 执行索引维护； 从HDFS读取索引； 将全部或部分索引转换为XML格式导出； 测试自定义的Lucene分词器； 注意：Luke的版本要跟Lucene保持一致 项目地址：https://github.com/DmitryKey/luke/releases ​ 下载 IK 分词工具 —— 轻量级中文分词工具包独立于Lucene，面向Java的公用分词组件 同时提供了 对Lucene的默认优化实现 IK Analyzer2012的特性： 采用“正向迭代最细力度切分算法”，支持细粒度和智能分词两种切分模式； 具有 160万字/s 的高速处理能力，i73.4G双核、4G内存，win7 64位，jdk1.6； 支持简单的分词排歧义处理、数量词合并输出； 多子处理器分析模式，支持英文字母、数字、中文词汇等分词处理、兼容韩文、日文字符； 优化词典存储，更小的内存占用。 下载地址（需要翻墙）：https://code.google.com/p/ik-analyzer/downloads/list 也可有上我的 oschina上下载： 安装包文件列表： drwxr-xr-x 1 doc/ —— API 文档说明 -rw-r–r– 1 IKAnalyzer.cfg.xml —— 分词器扩展配置文件 -rw-r–r– 1 IKAnalyzer2012_u6.jar —— 主jar包 -rw-r–r– 1 IKAnalyzer中文分词器V2012_U5使用手册.pdf -rw-r–r– 1 IKAnalyzer中文分词器V2012使用手册.pdf -rw-r–r– 1 LICENSE.txt —— Apache版权申明 -rw-r–r– 1 NOTICE.txt —— Apache版权申明 -rw-r–r– 1 stopword.dic —— 停止词典 部署步骤： 把 IKAnalyzer2012_u6.jar 放在部署项目的lib目录下 把 stopword.dic 和 IKAnalyzer.cfg.xml 放在 class根目录，对于web工程时 WEB-INFO/classes 目录下 ​ 工程搭建1、maven 安装 IKAnalyzer2012_u6.jar mvn install:install-file -Dfile=D:\java\oschina\lucene-learn\lucene-chapter2\lib\IKAnalyzer2012_u6.jar -DgroupId=org.wltea.analyzer -DartifactId=IKAnalyzer -Dversion=2012_u6 -Dpackaging=jar -DgeneratePom=true -DcreateChecksum=true 2、在pom.xml中添加 12345&lt;dependency&gt;​ &lt;groupId&gt;org.wltea.analyzer&lt;/groupId&gt;​ &lt;artifactId&gt;IKAnalyzer&lt;/artifactId&gt;​ &lt;version&gt;2012_u6&lt;/version&gt;&lt;/dependency&gt; 3、 stopword.dic 和 IKAnalyzer.cfg.xml 放在 src\main\resources 目录下 这样编译完成后，它就会放在 classes 目录下]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.1、Lucene概述]]></title>
    <url>%2Felasticsearch%2Fdoc%2F2-1-Lucene%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Lucene 3个优点1、稳定、索引性能高 2、高效、准确、高性能的搜索算法 3、跨平台解决方案 ​ Lucene架构1、信息采集 —— 文件、数据库、万维网、及手工输入，都能作为采集对象 2、索引文档 —— 即倒排序索引的构建过程 3、搜索文档 —— 用户发起查询 到 拿到结果的过程]]></content>
      <categories>
        <category>elastic</category>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>lucene</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 事务消息]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F8-rocketmq-transaction-msg%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 主从同步机制（HA）]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F7-rocketmq-ha-synchronize%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 消息过滤 FilterServer]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F6-rocketmq-filterserver%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 消息消费 Consumer]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F5-rocketmq-consumer%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 消息存储 Broker]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F4-rocketmq-broker%2F</url>
    <content type="text"><![CDATA[重点内容 RocketMQ 存储概要设计 消息发送存储流程 存储文件组织与内存映射机制 RocketMQ 存储文件 消息消费队列、索引文件构建机制 RocketMQ 文件恢复机制 RocketMQ 刷盘机制 RocketMQ 文件删除机制 一、存储概要设计1、设计的核心 Commitlog：将所有topic 的 消息存储在同一个文件（Commitlog）中，确保消息发送时顺序写文件，尽最大努力确保消息发送的高性能 和 吞吐量； ConsumeQueue：由于消息中间件 一般是基于 topic 的订阅机制，这样给 topic 检索带来了极大的不便。为了提高消费的效率，引入 ConsumeQueue 文件，每个 topic 对应 多个 MessageQueue，每个 MessageQueue 对应一个 ConsumeQueue文件； IndexFile索引文件：为了加速消息的检索性能，根据消息属性 快速从 Commitlog文件中 检索消息。 2、数据流向 1、Commitlog：消息存储文件，所有消息主题的消息都存储在 Commitlog 文件中； 2、ConsumeQueue：消息消费队列，消息到达 Commitlog 文件后，将异步转发到 ConsumeQueue，供消费者消费； 3、IndexFile：消息索引文件，主要存储 消息Key 与 Offset 的对应关系 4、事务状态服务：存储每条消息的事务状态； 5、定时消息任务：每一个延迟级别对应一个消息消费队列，存储延迟队列的消息拉取进度； 3、消息存储架构 RocketMQ Broker单个实例下所有的队列都使用同一个日志数据文件(CommitLog)来存储(即单个实例消息整体有序)，这点与kafka不同(kafka采用每个分区一个日志文件存储)。 CommitLog：日志文件，存储Producer发送的消息内容，单个文件大小默认1G (MessageStoreConfig类的mapedFileSizeCommitLog属性)，文件文件名是起始偏移量，总共20位，起始偏移量是0。比如第一个文件的文件名为00000000000000000000,假设文件按照默认大小1G来算，当第一个文件被写满之后，开始写入第二个文件，第二个文件的文件名为 00000000001073741824(1073741824=1024*1024*1024)，第三个文件的名是00000000002147483648(文件名相差1073741824=1024*1024*1024) ConsumeQueue：消息的消费的逻辑队列，RocketMQ的队列不存储实际的消息数据，只存储CommitLog中的【起始物理位置偏移量，消息的内容大小，消息Tag的哈希值】，对于物理存储来说，ConsumeQueue 对应每个 Topic 和 QueueId 下面的文件，文件路径是consumequeue/${topicName}/${queueId}/${fileName}，每个文件默认由 30W 条数据组成(MessageStoreConfig 类的 mapedFileSizeConsumeQueue 属性)，每条数据由20个字节组成，即每个文件为 600w 字节，单个消费队列的文件大小约为5.722M(600w/(1024*1024)) IndexFile：索引文件，物理存储上，文件名为创建时间的时间戳命名，固定的单个 IndexFile 文件大小约为400M，一个 IndexFile 可以保存 2000W 个索引 MapedFileQueue：对连续物理存储的抽象(存储目录的抽象)，MapedFileQueue 可以看作是${ROCKET_HOME}/store/commitlog 文件夹，此文件夹下有多个 MappedFile MappedFile：消息字节写入Page Cache 缓存区（commit 方法），或者原子性地将消息持久化的刷盘（flush方法） s 二、初识消息存储消息存储实现类：org.apache.rocketmq.store.DefaultMessageStore ​ 它是存储模块里最重要的一个类，包含了很多对存储文件操作的 API，其他模块对消息实体的操作都是通过 DefaultMessageStore 进行操作。 核心属性如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 消息存储实现类 */public class DefaultMessageStore implements MessageStore &#123; private static final InternalLogger log = InternalLoggerFactory.getLogger(LoggerName.STORE_LOGGER_NAME); // 消息存储配置属性 private final MessageStoreConfig messageStoreConfig; // CommitLog 消息存储文件 private final CommitLog commitLog; // 消息队列 存储缓存表，按消息topic分组 private final ConcurrentMap&lt;String/* topic */, ConcurrentMap&lt;Integer/* queueId */, ConsumeQueue&gt;&gt; consumeQueueTable; // 消息队列文件 ConsumeQueue 刷盘线程 private final FlushConsumeQueueService flushConsumeQueueService; // 清除 Commitlog 文件服务 private final CleanCommitLogService cleanCommitLogService; // 清除 ConsumeQueue 文件服务 private final CleanConsumeQueueService cleanConsumeQueueService; // 索引文件实现类 private final IndexService indexService; // MappedFile 分配服务 private final AllocateMappedFileService allocateMappedFileService; // Commitlog 消息分发，根据 Commitlog 文件构建 ConsumeQueue、IndexFile 文件 private final ReputMessageService reputMessageService; // 存储 HA 机制 private final HAService haService; private final ScheduleMessageService scheduleMessageService; private final StoreStatsService storeStatsService; // 消息内存缓存池 private final TransientStorePool transientStorePool; private final RunningFlags runningFlags = new RunningFlags(); private final SystemClock systemClock = new SystemClock(); private final ScheduledExecutorService scheduledExecutorService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryImpl("StoreScheduledThread")); private final BrokerStatsManager brokerStatsManager; // 消息拉取，长轮询模式消息到达监听器 private final MessageArrivingListener messageArrivingListener; // Broker 配置属性 private final BrokerConfig brokerConfig; private volatile boolean shutdown = true; // 文件刷盘监测点 private StoreCheckpoint storeCheckpoint; private AtomicLong printTimes = new AtomicLong(0); // Commitlog 文件转发请求 private final LinkedList&lt;CommitLogDispatcher&gt; dispatcherList; private RandomAccessFile lockFile; private FileLock lock; boolean shutDownNormal = false;&#125; 三、消息发送存储流程消息存储入口：org.apache.rocketmq.store.DefaultMessageStore#putMessage 方法： 123456789101112/** * 将 producer 发来的消息 存在 broker 的 commitlog 中 * @param msg Message instance to store * @return */public PutMessageResult putMessage(MessageExtBrokerInner msg) &#123;...&#125;/** * 将 producer 发来的消息 存在 broker 的 commitlog 中 * @param messageExtBatch Message batch. * @return */public PutMessageResult putMessages(MessageExtBatch messageExtBatch) &#123;...&#125; 1、这3中情况下拒绝消息写入，抛出异常① 当前 Broker停止工作，角色是 SLAVE，或当前Broker不制止写入； ② 消息topic长度超过 256字符，消息属性长度超过 65536字符； ③ 系统PageCache 繁忙； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 将 producer 发来的消息 存在 broker 的 commitlog 中 * @param msg Message instance to store * @return */public PutMessageResult putMessage(MessageExtBrokerInner msg) &#123; if (this.shutdown) &#123; // 如果已经shutdown 抛出异常 log.warn("message store has shutdown, so putMessage is forbidden"); return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null); &#125; if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) &#123; // 如果 当前broker是 slave，则抛出异常 long value = this.printTimes.getAndIncrement(); if ((value % 50000) == 0) &#123; log.warn("message store is slave mode, so putMessage is forbidden "); &#125; return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null); &#125; if (!this.runningFlags.isWriteable()) &#123; // 如果 当前flag中 有不可写 flag，则返回异常 long value = this.printTimes.getAndIncrement(); if ((value % 50000) == 0) &#123; log.warn("message store is not writeable, so putMessage is forbidden " + this.runningFlags.getFlagBits()); &#125; return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null); &#125; else &#123; this.printTimes.set(0); &#125; if (msg.getTopic().length() &gt; Byte.MAX_VALUE) &#123; // 如果msg的topic 太长，则抛出消息异常 log.warn("putMessage message topic length too long " + msg.getTopic().length()); return new PutMessageResult(PutMessageStatus.MESSAGE_ILLEGAL, null); &#125; if (msg.getPropertiesString() != null &amp;&amp; msg.getPropertiesString().length() &gt; Short.MAX_VALUE) &#123; // 如果 消息的 propertiesString 扩展属性字符串太长，则抛出异常 log.warn("putMessage message properties length too long " + msg.getPropertiesString().length()); return new PutMessageResult(PutMessageStatus.PROPERTIES_SIZE_EXCEEDED, null); &#125; if (this.isOSPageCacheBusy()) &#123; // 如果 系统PageCache 繁忙， 则 返回异常 系统繁忙 return new PutMessageResult(PutMessageStatus.OS_PAGECACHE_BUSY, null); &#125; long beginTime = this.getSystemClock().now(); // 调用 commitlog 实际保存 msg PutMessageResult result = this.commitLog.putMessage(msg); long eclipseTime = this.getSystemClock().now() - beginTime; if (eclipseTime &gt; 500) &#123; log.warn("putMessage not in lock eclipse time(ms)=&#123;&#125;, bodyLength=&#123;&#125;", eclipseTime, msg.getBody().length); &#125; // 统计存储1次消息，耗时 大概处于哪个区间，方便分析性能 this.storeStatsService.setPutMessageEntireTimeMax(eclipseTime); if (null == result || !result.isOk()) &#123; // 如果 失败，统计失败次数 this.storeStatsService.getPutMessageFailedTimes().incrementAndGet(); &#125; return result;&#125; 接下来调用 commitLog.putMessage(msg); 实际保存消息到 commitlog 文件 2、commitLog.putMessage(msg) 四、存储文件组织与内存映射机制 1、MappedFileQueueMappedFileQueue 是 MappedFile 的管理容器，是对 存储目录的封装。 例如：${rocketmq_home}/store/commitlog 下会存在多个 MappedFile 1)、关键属性12345678910111213141516171819public class MappedFileQueue &#123; // 批量删除的最大文件数 private static final int DELETE_FILES_BATCH_MAX = 10; // 存储目录 private final String storePath; // 单个文件的存储大小 private final int mappedFileSize; // MappedFile 文件列表 private final CopyOnWriteArrayList&lt;MappedFile&gt; mappedFiles = new CopyOnWriteArrayList&lt;MappedFile&gt;(); // 创建 MappedFile 服务类 private final AllocateMappedFileService allocateMappedFileService; // 当前刷盘指针，表示该指针之前的数据已经 全部持久化 到了磁盘 private long flushedWhere = 0; // 当前提交指针，内存中 ByteBuffer 当前的写指针，该值大于等于 flushedWhere private long committedWhere = 0; private volatile long storeTimestamp = 0;&#125; 2)、关键方法 根据时间戳 查找MappedFile，找到第一个lastModifiedTimestamp &gt;= timestamp 的文件 1234567891011121314151617181920/** * 根据时间戳 查找MappedFile，找到第一个lastModifiedTimestamp &gt;= timestamp 的文件 * @param timestamp 时间戳 long * @return 满足条件的MappedFile */public MappedFile getMappedFileByTime(final long timestamp) &#123; Object[] mfs = this.copyMappedFiles(0); if (null == mfs) return null; for (int i = 0; i &lt; mfs.length; i++) &#123; MappedFile mappedFile = (MappedFile) mfs[i]; if (mappedFile.getLastModifiedTimestamp() &gt;= timestamp) &#123; return mappedFile; &#125; &#125; return (MappedFile) mfs[mfs.length - 1];&#125; 查找哪个 MappedFile 包含 offset，则返回对应的 MappedFile 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Finds a mapped file by offset. * 查找哪个 MappedFile 包含 offset，则返回对应的 MappedFile * @param offset Offset. * @param returnFirstOnNotFound If the mapped file is not found, then return the first one. * @return Mapped file or null (when not found and returnFirstOnNotFound is &lt;code&gt;false&lt;/code&gt;). */public MappedFile findMappedFileByOffset(final long offset, final boolean returnFirstOnNotFound) &#123; try &#123; MappedFile firstMappedFile = this.getFirstMappedFile(); MappedFile lastMappedFile = this.getLastMappedFile(); if (firstMappedFile != null &amp;&amp; lastMappedFile != null) &#123; if (offset &lt; firstMappedFile.getFileFromOffset() || offset &gt;= lastMappedFile.getFileFromOffset() + this.mappedFileSize) &#123; // 如果参数 offset 不在 当前MappedFileQueue 范围内，返回 returnFirstOnNotFound LOG_ERROR.warn("Offset not matched. Request offset: &#123;&#125;, firstOffset: &#123;&#125;, lastOffset: &#123;&#125;, mappedFileSize: &#123;&#125;, mappedFiles count: &#123;&#125;", offset, firstMappedFile.getFileFromOffset(), lastMappedFile.getFileFromOffset() + this.mappedFileSize, this.mappedFileSize, this.mappedFiles.size()); &#125; else &#123; // 如果参数 offset 在 当前MappedFileQueue 范围内 // 拿到 offset 对应的 index int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize)); MappedFile targetFile = null; try &#123; targetFile = this.mappedFiles.get(index); &#125; catch (Exception ignored) &#123; &#125; if (targetFile != null &amp;&amp; offset &gt;= targetFile.getFileFromOffset() &amp;&amp; offset &lt; targetFile.getFileFromOffset() + this.mappedFileSize) &#123; // 如果 offset 在 index 文件的 偏移量范围内，返回 targetFile return targetFile; &#125; for (MappedFile tmpMappedFile : this.mappedFiles) &#123; // 遍历 所有 mappedFiles，查找满足条件的 mappedFile。 // 一般上一步可以找到，这里属于托底 if (offset &gt;= tmpMappedFile.getFileFromOffset() &amp;&amp; offset &lt; tmpMappedFile.getFileFromOffset() + this.mappedFileSize) &#123; return tmpMappedFile; &#125; &#125; &#125; if (returnFirstOnNotFound) &#123; return firstMappedFile; &#125; &#125; &#125; catch (Exception e) &#123; log.error("findMappedFileByOffset Exception", e); &#125; return null;&#125; 2、MappedFile org.apache.rocketmq.store.MappedFile 属性 1234567891011121314151617181920212223242526272829&gt; public static final int OS_PAGE_SIZE = 1024 * 4; // 操作系统每页大小&gt; protected static final InternalLogger log = InternalLoggerFactory.getLogger(LoggerName.STORE_LOGGER_NAME);&gt; &gt; // 当前 JVM 实例中 MappedFile 虚拟内存&gt; private static final AtomicLong TOTAL_MAPPED_VIRTUAL_MEMORY = new AtomicLong(0);&gt; // 当前 JVM 实例中 MappedFile 对象个数&gt; private static final AtomicInteger TOTAL_MAPPED_FILES = new AtomicInteger(0);&gt; &gt; // 当前该文件的写指针，从 0 开始（内存映射文件中的写指针）&gt; protected final AtomicInteger wrotePosition = new AtomicInteger(0);&gt; // 当前该文件的提交指针，如果开启transientStorePoolEnable，则数据会存在transientStorePool，然后提交到mappedByteBuffer，再刷到磁盘&gt; protected final AtomicInteger committedPosition = new AtomicInteger(0);&gt; // 刷盘指针，该指针之前的数据 已经持久化到次盘中&gt; private final AtomicInteger flushedPosition = new AtomicInteger(0);&gt; protected int fileSize; // 文件大小&gt; protected FileChannel fileChannel; // 写文件channel&gt; &gt; // 堆内存ByteBuffer。如果不为空（transientStorePoolEnable=true），数据首先写入writeBuffer中，&gt; // 然后提交到MappedFile对应的 mappedByteBuffer&gt; protected ByteBuffer writeBuffer = null;&gt; protected TransientStorePool transientStorePool = null; // 临时内存池，transientStorePoolEnable=true时启用&gt; &gt; private String fileName; // 文件名&gt; private long fileFromOffset; // 文件中 第一个msg 偏移量&gt; private File file; // 物理文件&gt; private MappedByteBuffer mappedByteBuffer; // 物理文件对应的 内存映射buffer&gt; private volatile long storeTimestamp = 0; // 文件最后一次内容写入时间戳&gt; private boolean firstCreateInQueue = false; // 是否是 MappedFileQueue 队列中第一个文件&gt; 1)、MappedFile 初始化1234org.apache.rocketmq.store.MappedFile#init(String fileName, int fileSize, TransientStorePool transientStorePool);org.apache.rocketmq.store.MappedFile#init(String fileName, int fileSize); 分为 是否启用 TransientStorePool TransientStorePoolEnable=true：内容先存在堆外内存 –&gt; 通过commit线程提交到 mappedByteBuffer –&gt; flush线程 将 mappedByteBuffer 中到数据持久化到磁盘； 123456789101112131415/** * 启用 TransientStorePool * ①、初始化MappedFile, 将 物理文件file 跟 内存关联依赖，用 mappedByteBuffer 来操作 * ②、使用 writeBuffer 和 transientStorePool * @param fileName 物理文件名 * @param fileSize 文件大小 * @param transientStorePool 堆外内存池 * @throws IOException */public void init(final String fileName, final int fileSize, final TransientStorePool transientStorePool) throws IOException &#123; init(fileName, fileSize); this.writeBuffer = transientStorePool.borrowBuffer(); // 返回 transientStorePool 中的第1块 可用buffer this.transientStorePool = transientStorePool;&#125; 关键点： transientStorePool.borrowBuffer()：从transientStorePool中借一块buffer使用 TransientStorePoolEnable=false 1234567891011121314151617181920212223242526272829303132333435363738/** * 不启用 TransientStorePool * 初始化MappedFile, 将 物理文件file 跟 内存关联依赖，用 mappedByteBuffer 来操作 * @param fileName fileName 物理文件名 * @param fileSize fileSize 文件大小 * @throws IOException */private void init(final String fileName, final int fileSize) throws IOException &#123; this.fileName = fileName; this.fileSize = fileSize; this.file = new File(fileName); // 根据文件名，拿到文件第1个msg的偏移量 this.fileFromOffset = Long.parseLong(this.file.getName()); boolean ok = false; ensureDirOK(this.file.getParent()); // 确保目录存在 try &#123; // 把 file 使用NIO的内存映射机制，把文件映射到内存中，引用为 mappedByteBuffer this.fileChannel = new RandomAccessFile(this.file, "rw").getChannel(); this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize); // JVM 持有的虚拟内存大小 +fileSize TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize); TOTAL_MAPPED_FILES.incrementAndGet(); // JVM 持有的文件数+1 ok = true; &#125; catch (FileNotFoundException e) &#123; log.error("create file channel " + this.fileName + " Failed. ", e); throw e; &#125; catch (IOException e) &#123; log.error("map file " + this.fileName + " Failed. ", e); throw e; &#125; finally &#123; if (!ok &amp;&amp; this.fileChannel != null) &#123; this.fileChannel.close(); &#125; &#125;&#125; 关键点： 把 file 使用NIO的内存映射机制，把文件映射到内存中，引用为 mappedByteBuffer 2)、MappedFile 提交（commit）123org.apache.rocketmq.store.MappedFile#commit(final int commitLeastPages);org.apache.rocketmq.store.MappedFile#commit0(final int commitLeastPages); 关键点： this.transientStorePool.returnBuffer(writeBuffer);：将 writeBuffer 返还给 transientStorePool； byteBuffer = writeBuffer.slice()：声明一套新指针byteBuffer，指向writeBuffer[position, limit]。内容相同，指针不同； this.fileChannel.write(byteBuffer);：通过 fileChannel 把 byteBuffer 中的内容写入 mappedByteBuffer； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * writeBuffer 中的 内容提交到 fileChannel * @param commitLeastPages 至少提交的页数 * @return 返回 已提交位置 committedPosition */public int commit(final int commitLeastPages) &#123; if (writeBuffer == null) &#123; // 不启用 TransientStorePool，就不需要内存拷贝。仅需要 committedPosition = wrotePosition 就行了 //no need to commit data to file channel, so just regard wrotePosition as committedPosition. return this.wrotePosition.get(); &#125; if (this.isAbleToCommit(commitLeastPages)) &#123; // 当前的脏页 大于commitLeastPages 才能提交 if (this.hold()) &#123; // 当前 MappedFile 可用 commit0(commitLeastPages); this.release(); &#125; else &#123; log.warn("in commit, hold failed, commit offset = " + this.committedPosition.get()); &#125; &#125; // All dirty data has been committed to FileChannel. if (writeBuffer != null &amp;&amp; this.transientStorePool != null &amp;&amp; this.fileSize == this.committedPosition.get()) &#123; // 如果启用了 transientStorePool，并且 已提交位置 已经写满整个物理文件，则 将 writeBuffer 归还 transientStorePool this.transientStorePool.returnBuffer(writeBuffer); this.writeBuffer = null; &#125; return this.committedPosition.get();&#125;/** * 提交 writeBuffer 中的 未提交内容 到 fileChannel * @param commitLeastPages 至少提交页数 */protected void commit0(final int commitLeastPages) &#123; int writePos = this.wrotePosition.get(); // 当前写指针位置 int lastCommittedPosition = this.committedPosition.get(); // 上次 提交指针 位置 if (writePos - this.committedPosition.get() &gt; 0) &#123; try &#123; // 声明一套新指针byteBuffer，指向 writeBuffer[position, limit]。内容相同，指针不同 ByteBuffer byteBuffer = writeBuffer.slice(); byteBuffer.position(lastCommittedPosition); byteBuffer.limit(writePos); // 将 writeBuffer[lastCommittedPosition, writePos] 之间的数据 通过fileChannel 写入 物理文件 this.fileChannel.position(lastCommittedPosition); this.fileChannel.write(byteBuffer); // committedPosition 执向当前 写指针位置（writePos） this.committedPosition.set(writePos); &#125; catch (Throwable e) &#123; log.error("Error occurred when commit data to FileChannel.", e); &#125; &#125;&#125; 3)、MappedFile 刷盘（flush）刷盘是指 将内存中的数据 写到磁盘，永久存储在磁盘，具体由 MappedFile 的 flush 方法实现. 关键点： 启用了 transientStorePool：this.fileChannel.force(false)，然后将 flushedPosition 设置为 commitPosition； 没启用 transientStorePool：this.mappedByteBuffer.force()，然后 将 flushedPosition 指向 wrotePosition，因为没有 commitPosition； 代码如下所示。 12345678910111213141516171819202122232425262728293031323334/** * 刷盘 * @param flushLeastPages 刷盘的最小页数 * @return The current flushed position, 已刷盘的位置 */public int flush(final int flushLeastPages) &#123; if (this.isAbleToFlush(flushLeastPages)) &#123; if (this.hold()) &#123; // 当前 MappedFile 可用，记录正在使用 int value = getReadPosition(); try &#123; //We only append data to fileChannel or mappedByteBuffer, never both. if (writeBuffer != null || this.fileChannel.position() != 0) &#123; // writeBuffer不为空，flushedPosition 相当于 上一次 commit指针 this.fileChannel.force(false); &#125; else &#123; // writeBuffer为空, 数据直接进入mappedByteBuffer，wrotePosition代表mappedByteBuffer的指针 this.mappedByteBuffer.force(); &#125; &#125; catch (Throwable e) &#123; log.error("Error occurred when force data to disk.", e); &#125; this.flushedPosition.set(value); this.release(); // 记录本次 MappedFile 使用完成 &#125; else &#123; log.warn("in flush, hold failed, flush offset = " + this.flushedPosition.get()); this.flushedPosition.set(getReadPosition()); &#125; &#125; return this.getFlushedPosition();&#125; 4)、获取MappedFile 最大读指针（getReadPostion）1、获取当前最大可读指针 writeBuffer == null，说明未启用 transientStorePool，没有提交过程，所以 mappedByteBuffer的写指针wrotePosition 就是当前最大可读地址； writeBuffer ！= null，则启用了 transientStorePool，最大可读指针 就是committedPosition，其他部分还没有提交 123456789/** * writeBuffer == null，说明未启用 transientStorePool，没有提交过程，所以 mappedByteBuffer的 * 写指针wrotePosition 就是当前最大可读地址 * writeBuffer ！= null，则启用了 transientStorePool，最大可读指针 就是committedPosition，其他部分还没有提交 * @return The max position which have valid data */public int getReadPosition() &#123; return this.writeBuffer == null ? this.wrotePosition.get() : this.committedPosition.get();&#125; 2、查找 pos 到 getReadPosition() 之前的数据 123456789101112131415161718192021/** * 从 MappedFile 中 选择一块区域 * @param pos MappedFile 中 相对于 fileFromOffset 的 位置 * @return 一套指针，指向 MappedFile中 的 指定位置 */public SelectMappedBufferResult selectMappedBuffer(int pos) &#123; int readPosition = getReadPosition(); if (pos &lt; readPosition &amp;&amp; pos &gt;= 0) &#123; if (this.hold()) &#123; ByteBuffer byteBuffer = this.mappedByteBuffer.slice(); byteBuffer.position(pos); int size = readPosition - pos; ByteBuffer byteBufferNew = byteBuffer.slice(); byteBufferNew.limit(size); return new SelectMappedBufferResult(this.fileFromOffset + pos, byteBufferNew, size, this); &#125; &#125; return null;&#125; ​ 由于在整个写入期间都未曾改变 mappedByteBuffer 的指针，所以 mappedByteBuffer.slice() 方法返回的 共享缓存区空间 为 整个 MappedFile，然后通过设置 byteBuffer的 postion 为 待查找的值，读取字节为当前可读字节长度，最终返回的 ByteBuffer 的 limit（可读最大长度） 为 size。整个共享缓存区的容量为 MappedFile#fileSize-pos，故在操作 SelectMappedBufferResult 不能对里面的byteBuffer 调用 flip 方法。 5)、MappedFile 销毁（destory）MappedFile文件销毁方法：org.apache.rocketmq.store.MappedFile#destroy 12345678910111213141516171819202122232425262728293031323334/** * MappedFile 销毁 * @param intervalForcibly 拒绝被销毁的最大存活时间 * @return boolean 是否销毁成功 */public boolean destroy(final long intervalForcibly) &#123; // 释放资源 this.shutdown(intervalForcibly); if (this.isCleanupOver()) &#123; // 如果已经清理完成 try &#123; this.fileChannel.close(); // 关闭channel log.info("close file channel " + this.fileName + " OK"); long beginTime = System.currentTimeMillis(); boolean result = this.file.delete(); // 删除文件 // 记录删除时信息 log.info("delete file[REF:" + this.getRefCount() + "] " + this.fileName + (result ? " OK, " : " Failed, ") + "W:" + this.getWrotePosition() + " M:" + this.getFlushedPosition() + ", " + UtilAll.computeEclipseTimeMilliseconds(beginTime)); &#125; catch (Exception e) &#123; log.warn("close file channel " + this.fileName + " Failed. ", e); &#125; return true; &#125; else &#123; log.warn("destroy mapped file[REF:" + this.getRefCount() + "] " + this.fileName + " Failed. cleanupOver: " + this.cleanupOver); &#125; return false;&#125; 1、关闭 MappedFile。 如果初次调用时 this.available 为 true，设置 this.available 为 false； 设置初次关闭的时间戳（firstShutdownTimestamp）为当前时间戳； 调用 this.release(); 释放资源 如果this.available 为 false，但 引用次数this.getRefCount() &gt; 0，并且 当前时间戳已经超过了 最大被销毁存活时间，则强制把 引用数 this.refCount 设为 负值，然后通过 release 释放资源。 12345678910111213141516/** * 将 对象置为 不可用，引用数降低 * @param intervalForcibly */public void shutdown(final long intervalForcibly) &#123; if (this.available) &#123; this.available = false; this.firstShutdownTimestamp = System.currentTimeMillis(); this.release(); // 减少refCount，如果时机合适，调用 cleanup() 释放资源 &#125; else if (this.getRefCount() &gt; 0) &#123; if ((System.currentTimeMillis() - this.firstShutdownTimestamp) &gt;= intervalForcibly) &#123; this.refCount.set(-1000 - this.getRefCount()); this.release(); &#125; &#125;&#125; 2、cleanup 释放资源 如果 available 为 true，表示MappedFile 当前可用，无须清理，返回false； 如果资源已经被清除，返回true； 如果是堆外内存，调用堆外内存的cleanup方法清除； 维护 TOTAL_MAPPED_VIRTUAL_MEMORY，TOTAL_MAPPED_FILES 返回清理完成 12345678910111213141516171819202122232425262728/** * 释放资源 * @param currentRef 当前引用数，用于记日志 不做判断 * @return 是否释放成功 */@Overridepublic boolean cleanup(final long currentRef) &#123; if (this.isAvailable()) &#123; // 当前MappedFile是否可用 log.error("this file[REF:" + currentRef + "] " + this.fileName + " have not shutdown, stop unmapping."); return false; &#125; if (this.isCleanupOver()) &#123; // 是否清理完成 log.error("this file[REF:" + currentRef + "] " + this.fileName + " have cleanup, do not do it again."); return true; &#125; // 释放 mappedByteBuffer clean(this.mappedByteBuffer); // JVM 虚拟内存数减少 TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(this.fileSize * (-1)); // JVM 引用文件数减少 TOTAL_MAPPED_FILES.decrementAndGet(); log.info("unmap file[REF:" + currentRef + "] " + this.fileName + " OK"); return true;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 通过反射清理 MappedByteBuffer * @param buffer MappedByteBuffer */public static void clean(final ByteBuffer buffer) &#123; if (buffer == null || !buffer.isDirect() || buffer.capacity() == 0) return; // 嵌套递归获取directByteBuffer的最内部的attachment或者viewedBuffer方法 // 获取directByteBuffer的Cleaner对象，然后调用cleaner.clean方法，进行释放资源 invoke(invoke(viewed(buffer), "cleaner"), "clean");&#125;/** * 递归获取directByteBuffer的最内部的attachment或者viewedBuffer方法，调用 * @param buffer MappedByteBuffer * @return MappedByteBuffer */private static ByteBuffer viewed(ByteBuffer buffer) &#123; String methodName = "viewedBuffer"; Method[] methods = buffer.getClass().getMethods(); for (int i = 0; i &lt; methods.length; i++) &#123; if (methods[i].getName().equals("attachment")) &#123; methodName = "attachment"; break; &#125; &#125; // 如果 有attachment，则调用attachment；如果没有，则调用 viewedBuffer ByteBuffer viewedBuffer = (ByteBuffer) invoke(buffer, methodName); if (viewedBuffer == null) return buffer; else return viewed(viewedBuffer);&#125;/** * 通过反射去调用 target 的 方法 methodName * @param target 目标对象 * @param methodName 目标对象的方法 * @param args 目标对象的方法 的参数 * @return */private static Object invoke(final Object target, final String methodName, final Class&lt;?&gt;... args) &#123; return AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; try &#123; Method method = method(target, methodName, args); method.setAccessible(true); return method.invoke(target); &#125; catch (Exception e) &#123; throw new IllegalStateException(e); &#125; &#125; &#125;);&#125; 3、TransientStorePool TransientStorePool：短暂的存储池。RocketMQ 单独创建 一个 MappedByteBuffer 内存缓存池，用来临时存储数据，数据先写入该内存映射中，然后由 commit 线程定时将数据从 该内存 复制到 与目的物理文件对应的内存映射 中。 引入该机制的原因： 提供一种内存锁定，将当前堆外内存一直锁定在内存中，避免被进程将内存交换到磁盘。 核心属性 12345678910111213141516/** * TransientStorePool 是一个队列，包含多个可用的 buffer */public class TransientStorePool &#123; private static final InternalLogger log = InternalLoggerFactory.getLogger(LoggerName.STORE_LOGGER_NAME); // avaliableBuffers 个数，可通过在 broker中配置文件中设置 transientStorePoolSize大小，默认为5 // 创建 poolSize 个堆外内存，并利用 com.sum.jna.Library 类库将该批内存锁定，避免被置换到 交换区，提高存储性能 private final int poolSize; // 每个 ByteBuffer 大小，默认为 mapedFileSizeCommitLog，表明TransientStorePool为 commitLog 文件服务 private final int fileSize; // 存放 avaliableBuffer 的 双端队列 private final Deque&lt;ByteBuffer&gt; availableBuffers; // 存储相关的配置 private final MessageStoreConfig storeConfig;&#125; 五、RocketMQ 存储文件RocketMQ 存储路径为 ${ROCKET_MQ}/store，主要存储文件夹 commitlog：消息存储目录 config：运行期间的一些配置信息consumerFilter.json：主题消息过滤信息；consumerOffset.json：集群消费模式消息消费进度；delayOffset.json：延时消息队列拉取进度；subscriptionGroup.json：消息消费组配置信息；topics.json：topic配置属性； consumequeue：消息消费队列存储目录 index：消息索引文件存储目录 abort：如果存在abort文件说明 broker 非正常关闭，该文件默认启动时创建，正常退出之前删除 checkpoint：文件监测点，存储 commitlog 文件最后一次刷盘时间戳、consumequeue 最后一次刷盘时间、index索引文件最后一次刷盘时间戳 1、Commitlog 文件 2、ConsumeQueue 文件 3、Index 索引文件 4、checkpoint 文件 六、实时更新消息消费队列、索引文件1、根据消息更新 ConsumeQueue 2、根据消息更新 Index 索引文件 七、消息队列和索引文件恢复1、Broker正常停止文件恢复 2、Broker异常停止文件恢复 八、文件刷盘机制1、Broker同步刷盘 2、Broker异步刷盘 九、过期文件删除机制]]></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux awk]]></title>
    <url>%2Flinux%2Flinux-shell-awk%2F</url>
    <content type="text"><![CDATA[awk 实现类似 group by功能文件（filename.txt）内容 1234567891011121314151617499499499499499499499408499499499499400499499499... 命令： 1234$ awk '&#123;arr[$1]+=1&#125;END&#123;for(i in arr)print i,arr[i]&#125;' filename.txt408 6400 6499 172 awk 使用 if 条件过滤1$ grep '2020:09' nginx-access.log* | grep -v 'DNSPod' | awk '&#123; if($9&gt;=400&amp;&amp;$9&lt;500) print $9 &#125;']]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM基础]]></title>
    <url>%2Fjava%2Fjvm%2Fjvm-yunxi%2F</url>
    <content type="text"><![CDATA[class类文件结构 类加载机制 jvm运行时数据区 一、Class文件结构二、类加载机制class的声明周期 1、加载1、根据全限定名获取二进制字节流 123Class.forName("com.mysql.jdbc.Driver");Applet : 通过网络读取 class文件 2、把字节流代表的静态存储结构（class文件） 转化为 方法区的运行时数据结构 3、Java堆上生成一个代表该类的对象 ​ Class 类 ==&gt; 元数据区（方法区）==&gt; 蓝图 ​ Object对象 ==&gt; 根据蓝图做成的 成品 Kclass 2、验证1、魔数：cafebabe 2、主版本号、次版本号 3、常量池：常量池里的常量是否有不被支持的类型 4、验证常量池中的值是否存在 3、准备准备阶段是准备内存的过程，初始化为0 4、解析5、初始化jdk编译默认添加构造函数 clinit class init: 类的初始化 有static 变量 才有 clinit clinit 父类 静态变量， 父类静态块，子类静态变量，子类静态块 init():V 父类的变量初始化，父类的构造方法， 6、使用7、卸载三、类加载器8、ClassLoader类加载器的默认机制：双亲委派机制 12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; C:/Program Files/Java/jdk1.8.0_60/jre/lib/rt.jar!/sun/misc/Launcher.class BootstrapClassLoader 加载 System.getProperty(“sun.boot.class.path”); 路径下的class ExtClassLoader 加载 System.getProperty(“java.ext.dirs”); 路径下的class AppClassLoader 加载 System.getProperty(“java.class.path”); 路径下的class 使用双亲委派的原因： 保护jdk的基类 9、Tomcat打破了双亲委派机制自定义类加载器，重写 loadClass，就能打破双亲委派 判断两个Class是否相等， false，false，true，false 即使是同一个class文件，使用不同的ClassLoader加载进来，那么它就是不同的 class 9、Class.forName四、JVM运行时数据区JVM运行时数据结构 1、方法区（元空间）各个线程的共享存储区域，用于存储： 已被虚拟机加载的类信息 常量 final 静态变量 static 即时编译器编译后的代码 热点代码：代码段被反复调用次数超过阈值（10000）。基于采样 和 计数 会出现的异常：OutOfMemory。原因：常量池撑爆 CPU 操作栈 操作寄存器 2、栈和栈帧 栈帧局部变量表(本地变量表) 原子性协议：32位的机器（一个slot可以存放一个32位以内的数据类型），long是在并发更新的时候不是线程安全的 long、double 分为两个连续的slot空间，高低位存储 在32位机器上会跳出循环，在64位机器上不会 操作数栈 动态连接 类似于 “多态，反射”，只有在运行时才能 把 符号引用 --&gt; 直接引用 其他大多数情况，在解释时 就确定了 符号引用 --&gt; 直接引用 方法返回连接 帧数据区 false，true 栈默认大小：-Xss108K； 栈的调用深度 由 栈帧的大小 和 栈的大小 共同决定；栈帧大小不是确定的，所以栈的深度也是动态的 3、程序计数器每条机器指令都有自己的地址 CPU从指令寄存器中获取要执行的指令，获取完后，清空指令寄存器； 程序计数器中存放下一条要执行的指令地址，指令寄存器为空后，将自己指向的指令放入指令寄存器中，同时，自己指向下一条指令； 五、字节码执行引擎函数如何调用 符号引用 –&gt; 直接引用 重写，重载 方法调用： 1、静态分派函数调用，对应的字节码指令： 0xb6 invokevirtual 调用实例方法 0xb7 invokespecial 调用超类构建方法, 实例初始化方法, 私有方法 0xb8 invokestatic 调用静态方法 0xb9 invokeinterface 调用接口方法 0xba invokedynamic 调用动态方法 在编译期确定，并且是根据静态类型来确定函数调用版本。可以反编译class文件，看到实际调用方法。 函数overload 2、动态分派不是根据静态类型来确定，而是在运行时根据实际类型来确定函数的版本 函数override 方法表 虚拟机动态分配机制 虚方法表（vtable）存在方法区上 单分派与多分派静态多分派（方法重载），动态单分派（方法重写） 3、字节码 操作码iload istore iadd isub imul idiv 操作数六、垃圾回收垃圾回收：方法区 和 堆区 空间不够用，OutOfMemoryError 加载类：空间不够用（类信息） 堆区：空间不够用（分配实例） 垃圾回收的要点知识引用计数法给对象添加一个引用计数器。每当有一个地方引用这个对象，这个计数器就加1；每当引用失效，这个计数器就减1；当计数器为0的时候就代表该对象不能再被使用； 可达性分析算法 GC Root对象 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中 类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈JNI（即一般说的Native方法）引用的对象； GC关心的东西：这块数据是不是一个指针 对象的生命周期 在Java中，对象的生命周期包括以下几个阶段： 创建阶段(Created) 应用阶段(In Use) 不可见阶段(Invisible) 不可达阶段(Unreachable) 收集阶段(Collected) 终结阶段(Finalized) 对象空间重分配阶段(De-allocated) 四种引用1)、强引用 类似 Object a = new Object();只要强引用存在，垃圾收集器永远不会回收它 jvm 在内存不够时，会抛出OutOfMemory 使用完以后a = null;才会被回收 强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfM moryError错误，使程序异常终止，也不会靠随意回收具有强引用 对象来解决内存不足的问题 2)、软引用SoftReference 软引用是用来描述一些还有用但并非必须的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常 1234Object obj = new Object();SoftReference softRef = new SoftReference(obj); // 将软引用指向 Object实例obj = null; // 结束 obj 对 Object实例的强引用Object anotherRef = (Object) softRef.get(); // 重新将 anotherRef 强引用指向 Object实例 3)、弱引用 WeekReference 弱引用也是用来描述非必须对象的，他的强度比软引用更弱一些，被弱引用关联的对象，在垃圾回收时，如果这个对象只被弱引用关联（没有任何强引用关联他），那么这个对象就会被回收。 4)、虚引用 PhanReference 一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来获取一个对象的实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。虚引用和弱引用对关联对象的回收都不会产生影响，如果只有虚引用活着弱引用关联着对象，那么这个对象就会被回收。它们的不同之处在于弱引用的get方法，虚引用的get方法始终返回null,弱引用可以使用ReferenceQueue,虚引用必须配合ReferenceQueue使用。 jdk中直接内存的回收就用到虚引用，由于jvm自动内存管理的范围是堆内存，而直接内存是在堆内存之外（其实是内存映射文件，自行去理解虚拟内存空间的相关概念），所以直接内存的分配和回收都是有Unsafe类去操作，java在申请一块直接内存之后，会在堆内存分配一个对象保存这个堆外内存的引用，这个对象被垃圾收集器管理，一旦这个对象被回收，相应的用户线程会收到通知并对直接内存进行清理工作。 GC最重要的几件事 1、哪些内存要回收？ GCRoot 里面没有引用的对象 ​ 代码不断变化，如果让GC Root不变 =&gt; stop the world 2、什么时候回收 安全点 3、如何回收 oopMap 存 对象位置，类型 垃圾回收算法System.gc(); 手动执行垃圾回收。程序告诉GC收集器执行回收动作，不过到底什么时候执行GC 属于FullGC 有哪些缺陷？ STW（stop the world） 12 标记清除算法 分为两个阶段 “标记” 和 “清除”，首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象； 两个不足： 一个是效率问题，标记和清除效率都不高 另一个是空间问题，标记清除之后产生大量不连续的内存碎片 标记整理算法标记阶段仍然和标记清除算法一样， 复制算法效率最高 MinorGC：Eden-&gt; S0/S1 MajorGC：Old区 内部收集算法 FullGC：Eden + S0/S1 -&gt; Old，永久代回收，都是FullGC 现在商业虚拟机的垃圾收集都采用“分代收集算法”。不同分区使用不同的收集算法，达到最高的使用几率 安全点Safepoint safeRegion oopMap JDK垃圾回收器-XX:+Use[Name]]GC Serial收集器开启参数： -XX:+UseSerialGC 复制算法 单线程的收集器，它的“单线程”的意义并不仅仅说明它会使用一个CPU或者一条收集线程去完成垃圾收集工作，最重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束 SerialOld收集器复制算法 ParNew收集器开启参数：-XX:UseParNewGC 复制算法 Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其他行为包括Serial收集器可用的所有控制参数，收集算法，STW，对象分配规则，回收策略 与 Serial收集器完全一样 Parallel Scavenge收集器开启参数：-XX:UseParallelGC 3个参数选一个设置即可 Parallel Old收集器 Concurrent Mark Sweep收集器开启参数：-XX:ConcMarkSweepGC 标记清除算法 优点：减小停顿时间，提高用户体验 缺陷： 对CPU特别敏感，cpu越少对程序影响越大。CMS默认的回收线程数 = (cpu核心数量 + 3)/4，进一法 CMS 和 浮动垃圾，标记和清理阶段，用户线程仍在运行-XX:CMSInitatingOccupancyFraction=68，当老年代达到68%时，判断（100-68）的空间是否够用户线程使用：如果够则触发CMS垃圾回收；如果剩余内存不够用户线程运行，则会抛出Concurrent Mode Failure错误，触发SerialOld回收（停顿时间过长） 碎片整理 — FullGC-XX:UseCMSCompactAtFullCollection整理碎片整理过程没法并发，通过SerialOld进行 G1收集器 Humongous：超大对象 G1最大可使用32G 整体上看是 标记整理算法，局部是复制算法 MaxGCPauseMillis 最大GC停顿时间 RememberSet 开启参数：-XX:+UseG1GC 垃圾回收机制空间担保 FullGC的触发条件七、jvm性能调优 jps1jps -mlv jstat 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748jstat -class pid x yjstat -compiler pid x yjstat -gc pid x yS0C S0 Capacity S0容量大小S1C S1 Capacity S1容量大小S0U S0 Used S0已使用容量大小S1U S1 Used S1已使用容量大小EC Eden Capacity Eden容量大小EU Eden Used Eden已使用容量大小OC Old Capacity Old容量大小OU Old Used Old已使用容量大小MC Metaspace Capacity Metaspace容量大小MU Metaspace Capacity Metaspace已使用空间大小CCSC 压缩类 容量大小CCSU 压缩类 已使用容量大小YGC YoungGC 次数YGCT YoungGC 累计时间FGC FullGC 次数FGCT FullGC 累计时间GCT GC 时间jstat -gccapacity pid x yNGCMN （New Generation Capacity min）新生代最小容量NGCMXNGC （New Generation Capacity）当前新生代容量OGCMNOGCMXOGC （Old Generation Capacity）当前老年代容量OC （Old Generation Capacity）老年代大小MCMN （Metaspace Generation Capacity）当前元空间最小MCMXjstat -gcnew pid x yTT TenuringMTT DC jstat -gcold pid x yPC Perm CapacityPU Perm Usedjstat -gcoldcapacity pid x yjstat -gcmetacapacity pid x yjstat -gcutil pid x yjstat -gccause pid x y 打印GC失败原因 12jstat -gccause pid x yLGCC Last Gc Cause 最后一次GC原因 jinfo 123jinfo -flag pidjinfo -flag +PrintGCDetails pid 如果程序运行时没有打印gc日志，可以指定打印jinfo -flag -PrintGCDetails pid 如果程序运行时已经打印了gc日志，可以禁止打印 jvm标准参数 java -help, 不会随着jdk版本变化 jvm非标准参数 java -X java -XX jmap jmap -histo:live pid jmap pid &gt; pid.hprof jstackjstack pid 线程状态 基础线程 Finalizer线程 finaliner() 放在一个FQueue里，有一个守护线程负责清理FQueue中的 Monitor Ctrl-Break 监听Ctrl+C 中断任务 Attach Listener 接收外部命令，执行该命令 Signal Dispatcher 分发外部命令 Reference Handler 处理引用线程（软引用/弱引用/虚引用）的垃圾回收问题 实现调优调优的原则 合理编写代码文件句柄网络 合理利用硬件资源 合理的进行调优 终极规则：降低FGC的频次，减少GC的时间 大对象导出 百万级 7~8W记录 result for() =&gt; 本地没有测试 慢sql tomcat 每隔三个月必然爆一次，tomcat有非常多的hprof， -XX:HeapDumpOnOutOfMemoryError 32G 重启工程师 数据库设计有问题 一张表：上千万，更多， -&gt; 分库分表 冷热数据 很多冗余（空间换时间） 日志：亿]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 消息发送 Producer]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F3-rocketmq-producer%2F</url>
    <content type="text"><![CDATA[1、漫谈消息发送消息发送方式：同步（sync）、异步（async）、单向（oneway） 同步（sync）：发送消息时，同步等待 broker返回发送结果； 异步（async）：发送消息后，通过回调函数 获取发送结果，发送过程不阻塞； 单向（oneway）：只管发，不在乎是否发送成功； 3个疑问？ 消息队列（broker）如何进行负载？ 主动从NameServer拉取，更新缓存 消息发送如何实现高可用？ 重试 + 规避 批量消息发送如何实现一致性？ 压缩+校验码 重点骨架1、消息生产者启动流程 MQClientInstance、消息生产者之间的关系 2、消息队列负载机制 生产者在发送消息时，如果本地路由表中未缓存topic的路由信息，向NameServer发送获取路由信息请求，更新本地路由表。并且消息生产者每隔 30s 从NameServer 更新路由表。 3、消息发送异常机制 消息发送高可用两个手段：重试 + Broker规避。 Borker规避：在一次消息发送出错时，在某一段时间内，消息生产者不会选择该Broker上的消息队列，提高消息发送成功率 4、批量消息发送 RocketMQ 支持将同一主题下的多条消息一次性发送到Broker 2、RocketMQ消息结构消息：org.apache.rocketmq.common.message.Message 类设计如下： 1234567891011package org.apache.rocketmq.common.message;public class Message implements Serializable &#123; private static final long serialVersionUID = 8445773977080406428L; private String topic; // 消息所属主题topic private int flag; // 消息flag private Map&lt;String, String&gt; properties; // 扩展属性 private byte[] body; // 消息体 private String transactionId; // 事务ID&#125; 消息flagorg.apache.rocketmq.common.sysflag.MessageSysFlag 12345678910package org.apache.rocketmq.common.sysflag;public class MessageSysFlag &#123; public final static int COMPRESSED_FLAG = 0x1; public final static int MULTI_TAGS_FLAG = 0x1 &lt;&lt; 1; public final static int TRANSACTION_NOT_TYPE = 0; public final static int TRANSACTION_PREPARED_TYPE = 0x1 &lt;&lt; 2; public final static int TRANSACTION_COMMIT_TYPE = 0x2 &lt;&lt; 2; public final static int TRANSACTION_ROLLBACK_TYPE = 0x3 &lt;&lt; 2;&#125; 扩展属性propertiestag：消息TAG，用于消息过滤 keys：Message 索引键，多个用空格隔开。RocketMQ可以根据这些key快速检索到消息。 waitStoreMsgOK：消息发送时是否等消息存储完成再返回。 delayTimeLevel：消息延迟级别，用于定时消息 或 消息重试。 3、Producer启动流程消息生产者代码都在 client 模块中，对于 RocketMQ 来说 它就是客户端。 1、初识DefaultMQProducerDefaultMQProducer —&gt; 默认 消息生产者实现类，实现 MQAdmin 接口。 1234public interface MQProducer extends MQAdmin &#123;&#125;package org.apache.rocketmq.client.producer;public class DefaultMQProducer extends ClientConfig implements MQProducer &#123;&#125; 主要方法1）创建topic void createTopic(String key, String newTopic, int queueNum, int topicSysFlag) key： accesskey 目前没有实际作用，可以与 newTopic 相同； newTopic： topic name 主题名称； queueNum： topic’s queue number 队列数量； topicSysFlag： topic system flag 主题系统标签，默认为0； 2）根据时间戳从队列中查找其偏移量 long searchOffset(MessageQueue mq, long timestamp) 3）查找该消息队列中的 最大/最小 物理偏移量 long maxOffset(MessageQueue mq) long minOffset(MessageQueue mq) 4）根据偏移量查找消息 MessageExt viewMessage(String offsetMsgId) 5）根据条件查找消息 QueryResult queryMessage(String topic, String key, int maxNum, long begin, long end) topic：主题 key：消息索引字段 maxNum：最大取出条数； begin：开始时间错 end：结束时间戳 6）同步发送消息。若不指定messageQueue，由负载算法决定发到哪个队列 SendResult send(Message msg[, MessageQueue mq[, long timeout]]) 同步批量发送 SendResult send(Collection&lt;Message&gt; msgs[, MessageQueue messageQueue[, long timeout]]) 7）异步发送消息 void send(Message msg, MessageQueue mq, SendCallback sendCallback[, long timeout]) 8）单向发送消息 void sendOneway(Message msg, MessageQueue mq) void sendOneway(Message msg, MessageQueueSelector selector, Object arg) 生产者核心属性1234567891011public class DefaultMQProducer extends ClientConfig implements MQProducer &#123; private String producerGroup; private String createTopicKey = MixAll.AUTO_CREATE_TOPIC_KEY_TOPIC; private volatile int defaultTopicQueueNums = 4; // 一个topic默认队列数量 private int sendMsgTimeout = 3000; // 发送消息超时ms private int compressMsgBodyOverHowmuch = 1024 * 4; // 消息体超过多大时 压缩 private int retryTimesWhenSendFailed = 2; // 同步发送失败重试次数 private int retryTimesWhenSendAsyncFailed = 2; // 异步发送失败重试次数 private boolean retryAnotherBrokerWhenNotStoreOK = false; // 存储失败时尝试其他Broker private int maxMessageSize = 1024 * 1024 * 4; // 4M，最大消息大小&#125; producerGroup：rocketmq聚合角色相同的Producer为一组，在broker回查事务消息的状态时，随机选择该组中的任何一个生产者发起事务回查请求。对事务消息特别重要，对非事务消息，只要同一进程唯一就可以了； createTopicKey：默认Topic Key； 2、Producer启动流程DefaultMQProducer#start() --&gt; DefaultMQProducerImpl#start(final boolean startFactory) 12345678910111213141516171819202122232425262728293031323334353637383940414243public void start(final boolean startFactory) throws MQClientException &#123; switch (this.serviceState) &#123; case CREATE_JUST: this.serviceState = ServiceState.START_FAILED; // step1：检查 producerGroup 是否合法，并改变生产者的instanceName为pid this.checkConfig(); if (!this.defaultMQProducer.getProducerGroup().equals( MixAll.CLIENT_INNER_PRODUCER_GROUP)) &#123; this.defaultMQProducer.changeInstanceNameToPID(); &#125; // step2: 创建 MQClientInstance 实例。 // 整个JVM中只存在一个 MQClientManager实例，维护一个 MQClientInstance 缓存表 factoryTable。也就是同一个 clientId 只会创建一个 MQClientInstance // clientId 格式：客户端IP@instanceName（@unitName） this.mQClientFactory = MQClientManager.getInstance() .getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook); // step3：向 MQClientInstance 注册，将当前生产者 加入到 MQClientInstance 的管理中。方便后续调用网络请求、进行心跳检测 boolean registerOK = mQClientFactory.registerProducer( this.defaultMQProducer.getProducerGroup(), this); if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; throw new MQClientException("The producer group[" + this.defaultMQProducer.getProducerGroup() + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null); &#125; this.topicPublishInfoTable.put(this.defaultMQProducer.getCreateTopicKey(), new TopicPublishInfo()); // step4；启动 MQClientInstance if (startFactory) &#123; mQClientFactory.start(); &#125; log.info("the producer [&#123;&#125;] start OK. sendMessageWithVIPChannel=&#123;&#125;", this.defaultMQProducer.getProducerGroup(), this.defaultMQProducer.isSendMessageWithVIPChannel()); this.serviceState = ServiceState.RUNNING; break; case RUNNING: case START_FAILED: case SHUTDOWN_ALREADY: throw new MQClientException("The producer service state not OK, maybe started once, " + this.serviceState + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK), null); default: break; &#125; this.mQClientFactory.sendHeartbeatToAllBrokerWithLock();&#125; step1：检查 producerGroup 是否合法，并改变生产者的instanceName为pid； step2: 创建 MQClientInstance 实例。 整个JVM中只存在一个 MQClientManager实例，维护一个 MQClientInstance 缓存表 factoryTable。也就是同一个 clientId 只会创建一个 MQClientInstance clientId 格式：客户端IP@instanceName（@unitName） step3：向 MQClientInstance 注册，将当前生产者 加入到 MQClientInstance 的管理中。方便后续调用网络请求、进行心跳检测； step4；启动 MQClientInstance； MQClientInstance：封装了 网络处理API，是Producer、Consumer 与 NameServer、Broker 打交道的网络通道。 4、消息发送流程 消息发送步骤： 验证消息； 查找路由； 消息发送（包括异常处理）； 同步发送消息方法 调用链如下： org.apache.rocketmq.client.producer.DefaultMQProducer 123public SendResult send(Message msg) &#123; return this.defaultMQProducerImpl.send(msg);&#125; org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public SendResult send(Message msg) &#123; return send(msg, this.defaultMQProducer.getSendMsgTimeout());&#125;public SendResult send(Message msg, long timeout) &#123; return this.sendDefaultImpl(msg, CommunicationMode.SYNC, null, timeout);&#125;/** * 发送消息方法 的 默认实现 * @param msg 消息 * @param communicationMode 同步、异步、单向 * @param sendCallback 异步发送的回调接口 * @param timeout 超时时间 * @return * @throws MQClientException * @throws RemotingException * @throws MQBrokerException * @throws InterruptedException */private SendResult sendDefaultImpl( Message msg, final CommunicationMode communicationMode, final SendCallback sendCallback, final long timeout) &#123; ... // 查找 topic 对应的 路由信息 TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) &#123; // 重试机制，最多重试 timesTotal 次 for (; times &lt; timesTotal; times++) &#123; String lastBrokerName = null == mq ? null : mq.getBrokerName(); // 选择一个消息队列 MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName); if (mqSelected != null) &#123; mq = mqSelected; brokersSent[times] = mq.getBrokerName(); try &#123; // 实际发送消息 sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime); endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false); switch (communicationMode) &#123; case ASYNC: case ONEWAY: return null; case SYNC: if (sendResult.getSendStatus() != SendStatus.SEND_OK) &#123; if (this.defaultMQProducer.isRetryAnotherBrokerWhenNotStoreOK()) &#123; continue; &#125; &#125; return sendResult; default: break; &#125; &#125; catch (...) &#123; ... &#125; &#125; // end-if &#125; // end-for &#125; ...&#125; 下面主要分析 sendDefaultImpl(...) 方法的实现 1、消息长度验证发送之前校验： 生产者处于运行状态 消息是否符合相应的规范（topic、消息体 不能为空，消息长度≠0，不超过允许的最大长度4M(1024*1024*4)） 2、查找topic路由信息消息发送之前，首先需要获取 topic的路由信息，然后才能知道要发往哪个 Broker 节点。 12345678910111213141516171819202122232425/** * 查找 topic 对应的路由信息 * @param topic 主题 * @return */private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) &#123; TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic); // 如果 producer 的 topicPublishInfoTable 中没有缓存 topic，则向 NameServer 查询topic的路由信息 if (null == topicPublishInfo || !topicPublishInfo.ok()) &#123; this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo()); // 从 NameServer 拉取 topic 的路由信息，并更新 topicRouteTable 缓存 this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic); topicPublishInfo = this.topicPublishInfoTable.get(topic); &#125; // 如果producer中缓存了 topic 路由信息，则直接返回 if (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) &#123; return topicPublishInfo; &#125; else &#123; this.mQClientFactory.updateTopicRouteInfoFromNameServer( topic, true, this.defaultMQProducer); topicPublishInfo = this.topicPublishInfoTable.get(topic); return topicPublishInfo; &#125;&#125; 关键类【Topic路由信息】： TopicPublishInfo: 123456789public class TopicPublishInfo &#123; private boolean orderTopic = false; // 消息是否顺序消息 private boolean haveTopicRouterInfo = false; // 是否包含 topic 路由信息 // 该 topic 下的消息队列 private List&lt;MessageQueue&gt; messageQueueList = new ArrayList&lt;MessageQueue&gt;(); // 发送到哪个queue，[1, Integer.MAX_VALUE] private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex(); private TopicRouteData topicRouteData; // topic 的 路由信息&#125; 1234567public class TopicRouteData extends RemotingSerializable &#123; private String orderTopicConf; // private List&lt;QueueData&gt; queueDatas; // topic 队列元数据 private List&lt;BrokerData&gt; brokerDatas; // topic 分布的 broker 元数据 // broker 上 过滤服务器地址列表 private HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable;&#125; 3、选择消息队列 ​ 返回一个 MessageQueue 用于发送消息，同时引入规避机制【规避上一次发送失败的Broker，提高发送效率】 两种选择方式： 1、默认机制，不启用故障延迟机制 sendLatencyFaultEnable = false 2、启用故障延时机制 sendLatencyFaultEnable = true API 调用链 DefaultMQProducerImpl#selectOneMessageQueue --&gt; MQFaultStrategy#selectOneMessageQueue 123456789101112131415161718192021/** * 从路由信息中选择一个消息队列去发送消息 * @param tpInfo 路由信息 * @param lastBrokerName 上次发送消息出错的 broker，优先排除 * @return */public MessageQueue selectOneMessageQueue( final TopicPublishInfo tpInfo, final String lastBrokerName) &#123; if (this.sendLatencyFaultEnable) &#123; // 如果启用了故障延迟机制 (sendLatencyFaultEnable = true) // 1、【在健康的Broker中】遍历 messageQueueList，找一个 lastBrokerName 之外的 消息队列 tpInfo.getMessageQueueList() // 2、尝试从 【被 规避的Broker中（被关进小黑屋的Broker）】选择一个可用的Broker。如果没找到，返回 null final String notBestBroker = latencyFaultTolerance.pickOneAtLeast(); // 3、如果前两步都没找到，找一个托底的 MessageQueue return tpInfo.selectOneMessageQueue(); &#125; // 如果没有启用故障延迟机制(sendLatencyFaultEnable = false) return tpInfo.selectOneMessageQueue(lastBrokerName);&#125; 1、默认机制 sendLatencyFaultEnable = false 不启用故障延迟机制 选择队列核心API： TopicPublishInfo#selectOneMessageQueue 12345678910111213141516171819202122232425262728293031/** * 不启用故障延迟机制，获取消息队列 * @param lastBrokerName 上一次消息发送失败的 brokerName * @return MessageQueue */public MessageQueue selectOneMessageQueue(final String lastBrokerName) &#123; if (lastBrokerName == null) &#123; return selectOneMessageQueue(); &#125; else &#123; int index = this.sendWhichQueue.getAndIncrement(); for (int i = 0; i &lt; this.messageQueueList.size(); i++) &#123; int pos = Math.abs(index++) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; MessageQueue mq = this.messageQueueList.get(pos); // 规避上次 发送失败的 MessageQueue 所在 Broker if (!mq.getBrokerName().equals(lastBrokerName)) &#123; return mq; &#125; &#125; // 上述机制找不到可用的MessageQueue时，找一个托底的 MessageQueue返回 return selectOneMessageQueue(); &#125;&#125;public MessageQueue selectOneMessageQueue() &#123; int index = this.sendWhichQueue.getAndIncrement(); int pos = Math.abs(index) % this.messageQueueList.size(); if (pos &lt; 0) pos = 0; return this.messageQueueList.get(pos);&#125; 该机制的缺陷：虽然可以规避上一次发送失败的Broker，但每次查询都要走这个流程，太耗费性能； 为什么Broker不可用之后，路由信息中还包含该Broker的路由信息呢？ 首先， NameServer 检测 Broker 是有延迟的，最短的一次心跳为 10s； 其次，NameServer 检测到 Broker 故障后，不会马上推给Producer，而是 Producer 每隔 30s 拉取一次路由信息； 所以， Producer 最快感知道Broker故障信息也需要 30s； ​ 下面引入一种机制，如果一次消息发送失败之后，可以将该 Broker 暂时排除在消息队列的选择范围外，过一段时间再加入 可选择范围内。 2、Broker故障延迟机制 1）sendLatencyFaultEnable = true 启用故障延迟机制 2）【故障Broker存储】实现类 LatencyFaultTolerance&lt;String&gt; latencyFaultTolerance = new LatencyFaultToleranceImpl(); 选择队列核心API：MQFaultStrategy#selectOneMessageQueue 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 从路由信息中选择一个消息队列去发送消息 * @param tpInfo 路由信息 * @param lastBrokerName 上次发送消息出错的 broker，优先排除 * @return */public MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) &#123; if (this.sendLatencyFaultEnable) &#123; // 如果启用了故障延迟机制 (sendLatencyFaultEnable = true) try &#123; // 1、【在健康的Broker中】遍历 messageQueueList，找一个 lastBrokerName 之外的 消息队列 int index = tpInfo.getSendWhichQueue().getAndIncrement(); for (int i = 0; i &lt; tpInfo.getMessageQueueList().size(); i++) &#123; int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size(); if (pos &lt; 0) pos = 0; MessageQueue mq = tpInfo.getMessageQueueList().get(pos); // 根据 入狱时间（startTimestamp） 判断是否小黑屋期满，期满 则 重新可用 if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) &#123; if (null == lastBrokerName || !mq.getBrokerName().equals(lastBrokerName)) // bug return mq; &#125; &#125; // 2、尝试从 【被 规避的Broker中（被关进小黑屋的Broker）】选择一个可用的Broker final String notBestBroker = latencyFaultTolerance.pickOneAtLeast(); int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker); if (writeQueueNums &gt; 0) &#123; final MessageQueue mq = tpInfo.selectOneMessageQueue(); if (notBestBroker != null) &#123; mq.setBrokerName(notBestBroker); mq.setQueueId(tpInfo.getSendWhichQueue() .getAndIncrement() % writeQueueNums); &#125; return mq; &#125; else &#123; latencyFaultTolerance.remove(notBestBroker); &#125; &#125; catch (Exception e) &#123; log.error("Error occurred when selecting message queue", e); &#125; // 3、如果前两步都没找到，找一个托底的 MessageQueue return tpInfo.selectOneMessageQueue(); &#125; // 如果没有启用故障延迟机制(sendLatencyFaultEnable = false) return tpInfo.selectOneMessageQueue(lastBrokerName);&#125; 重点【latencyFaultTolerance】故障延迟机制 存储、更新、判断：LatencyFaultToleranceImpl 4、消息发送消息发送核心API入口：DefaultMQProducerImpl#sendKernelImpl 123456789101112131415161718192021/** * 消息发送API 核心入口 * @param msg 待发送消息 * @param mq MessageQueue【topic，brokerName，queueId】 * @param communicationMode 发送模式， 同步，异步，Oneway * @param sendCallback 异步发送 回调函数 * @param topicPublishInfo 路由信息 * @param timeout 消息发送超时时间 * @return SendResult 发送结果 * @throws MQClientException * @throws RemotingException * @throws MQBrokerException * @throws InterruptedException */private SendResult sendKernelImpl( final Message msg, final MessageQueue mq, final CommunicationMode communicationMode, final SendCallback sendCallback, final TopicPublishInfo topicPublishInfo, final long timeout) 发送步骤： 1、根据MessageQueue获取Broker网络地址。 如果mQClientFactory.brokerAddrTable还没有缓存该地址，则从NameServer主动更新一下topic路由信息。 如果还是找不到Broker信息，则抛出异常 MQClientException。 2、为消息分配全局唯一ID。 如果 消息提超过4K，会对 消息体采用 zip 压缩，并将消息标记为 MessageSysFlag.COMPRESSED_FLAG。 如果消息是事务Prepare消息，则设置消息的系统标记为MessageSysFlag.TRANSACTION_PREPARED_TYPE。 3、如果注册了消息发送钩子函数，则执行消息发送之前的增强逻辑。通过DefaultMQProducerImpl#registerSendMessageHook 注册钩子处理类 4、构建消息发送请求包。主要包含如下重要信息： 生产者组、主题名称、默认创建主题Key、该主题在单个Broker默认队列数、队列ID、 消息系统标记（MessageSysFlag）、消息发送时间、消息标记flag、消息扩展性、重试次数、是否批量消息 5、根据消息发送方式：同步、异步、单向 方式进行网络传输 发送消息核心函数 MQClientAPIImpl#sendMessage ： 真正网络发送函数： 123456// 单向发送this.remotingClient.invokeOneway(addr, request, timeoutMillis);// 同步发送RemotingCommand response = this.remotingClient.invokeSync(addr, request, timeoutMillis);// 异步发送this.remotingClient.invokeAsync(addr, request, timeoutMillis, new InvokeCallback()&#123;...&#125;); 5、批量消息发送DefaultMQProducer.java#send(Collection&lt;Message&gt; msgs) 在 Producer，调用 batch方法，将一批消息封装成 MessageBatch 对象。 MessageBatch 继承自Message 对象，MessageBatch 内部持有 List&lt;Message&gt; messages。 批量消息封装格式批量封装：MessageDecoder#encodeMessages(List&lt;Message&gt; messages) ##### 单条封装MessageDecoder#encodeMessage(Message message)]]></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell解析nginx日志]]></title>
    <url>%2Fnginx%2Flog%2Fnginxlog-shell%2F</url>
    <content type="text"><![CDATA[12345678910grep '2019:14' nginx-access.log* | grep -v 'DNSPod' | awk '&#123;print $1, $4, $9, $(NF-4), $NF, $(NF-2) &#125;' &gt; ~/trade20190210.loggrep '2019:09' nginx-access.log* | grep -v 'DNSPod' | awk '&#123; if($NF &gt; 1) print $1, $4, $9, $(NF-4), $NF, $(NF-2) &#125;' &gt; ~/trade20190210.loggrep '2019:09' nginx-access.log* | grep -v 'DNSPod' | awk '&#123; if($NF &gt; 1) print $0 &#125;' &gt; ~/trade20190211.loggrep '2019:14' nginx-access.log* | grep -v 'DNSPod' | awk '&#123; if($NF-$(NF-2)&gt;0.1) print $1, $4, $9, $(NF-4), $NF, $(NF-2) &#125;' &gt; ~/trade20190210.loggrep '2019:14' nginx-access.log* | grep -v 'DNSPod' | awk '&#123; if($NF-$(NF-2)&gt;0.1) print $0 &#125;' &gt; ~/trade20190210.loggrep '2019:14' nginx-access.log* | grep -v 'DNSPod' | awk '&#123;if($NF&gt;5) print $0 &#125;' &gt; ~/trade20190210-14.logcat nginx-access.log- | grep -v 'DNSPod' | awk '&#123;if($NF&gt;0&amp;&amp;$NF&lt;1) print $0 &#125;' &gt; ~/trade20190210.loggrep '2019:09' nginx-access.log* | grep -v 'DNSPod' | awk '&#123; if($9&gt;=400&amp;&amp;$9&lt;500) print $9 &#125;' &gt; ~/trade20190211.logawk '&#123;arr[$1]+=1&#125;END&#123;for(i in arr)print i,arr[i]&#125;' ~/trade20190211.log]]></content>
      <categories>
        <category>nginx</category>
        <category>log</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2、RocketMQ 路由中心 NameServer]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F2-rocketmq-nameserver%2F</url>
    <content type="text"><![CDATA[NameServer 是 整个 RocketMQ 的 ”大脑“； 路由管理、服务注册、服务发现 机制； 本章重点内容： NameServer 整体架构设计； NameServer 动态路由 发现与剔除机制； 1、NameServer 架构设计RocketMQ 逻辑部署图： Broker 启动时，向 所有NameServer 注册； Producer在 发送消息 之前，先从 NameServer 获取 Broker 服务器地址列表，然后 根据负载均衡 算法 从 列表中选择一台服务器进行消息发送； NameServer 与 每台 Broker 保持 长链接，并且间隔 30s 检测 Broker是否存活。如果Broker宕机，则从 路由注册表中将其移除； NameServer 本身的 高可用 通过 部署多台NameServer 服务器来实现；但彼此之间不通信； 2、NameServer 启动流程NamaServer 启动类：org.apache.rocketmq.namesrv.NamesrvStartup 1234567891011121314public static NamesrvController main0(String[] args) &#123; try &#123; NamesrvController controller = createNamesrvController(args); start(controller); String tip = "The Name Server boot success. serializeType=" + RemotingCommand.getSerializeTypeConfigInThisServer(); log.info(tip); System.out.printf("%s%n", tip); return controller; &#125; catch (Throwable e) &#123; e.printStackTrace(); System.exit(-1); &#125; return null;&#125; 2.1、初始化 NamesrvController NamesrvController controller = createNamesrvController(args); 创建： org.apache.rocketmq.namesrv.NamesrvStartup.createNamesrvController 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public static NamesrvController createNamesrvController(String[] args) throws IOException, JoranException &#123; System.setProperty(RemotingCommand.REMOTING_VERSION_KEY, Integer.toString(MQVersion.CURRENT_VERSION)); //PackageConflictDetect.detectFastjson(); // 将 args 转换成 CommandLine 对象 Options options = ServerUtil.buildCommandlineOptions(new Options()); commandLine = ServerUtil.parseCmdLine("mqnamesrv", args, buildCommandlineOptions(options), new PosixParser()); if (null == commandLine) &#123; System.exit(-1); return null; &#125; // 读取 CommandLine 中 -c namesrv.conf 中的配置, 加载 到 NamesrvConfig 和 NettyServerConfig 中 final NamesrvConfig namesrvConfig = new NamesrvConfig(); final NettyServerConfig nettyServerConfig = new NettyServerConfig(); nettyServerConfig.setListenPort(9876); if (commandLine.hasOption('c')) &#123; String file = commandLine.getOptionValue('c'); if (file != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(file)); properties = new Properties(); properties.load(in); MixAll.properties2Object(properties, namesrvConfig); MixAll.properties2Object(properties, nettyServerConfig); namesrvConfig.setConfigStorePath(file); System.out.printf("load config properties file OK, %s%n", file); in.close(); &#125; &#125; // 如果有 -p 参数，则 向console打印 namesrvConfig 和 nettyServerConfig 所有属性，退出 if (commandLine.hasOption('p')) &#123; InternalLogger console = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_CONSOLE_NAME); MixAll.printObjectProperties(console, namesrvConfig); MixAll.printObjectProperties(console, nettyServerConfig); System.exit(0); &#125; // 将 commandLine 中的其他参数 加载到 namesrvConfig 中 MixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig); // 如果没有 设置 rocketmqHome，则 退出 if (null == namesrvConfig.getRocketmqHome()) &#123; System.out.printf("Please set the %s variable in your environment to match the location of the RocketMQ installation%n", MixAll.ROCKETMQ_HOME_ENV); System.exit(-2); &#125; // 从 logback_namesrv.xml 中 加载 日志配置信息 LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory(); JoranConfigurator configurator = new JoranConfigurator(); configurator.setContext(lc); lc.reset(); configurator.doConfigure(namesrvConfig.getRocketmqHome() + "/conf/logback_namesrv.xml"); log = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_LOGGER_NAME); // 向 日志文件.log 打印 namesrvConfig 和 nettyServerConfig MixAll.printObjectProperties(log, namesrvConfig); MixAll.printObjectProperties(log, nettyServerConfig); // 使用 namesrvConfig 和 nettyServerConfig 初始化 NamesrvController // 同时初始化 Configuration final NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig); // remember all configs to prevent discard // 将 properties 全部记录到 Configuration 中 controller.getConfiguration().registerConfig(properties); return controller;&#125; 代码功能： 需要先加载 命令行中-c configFile 指定的配置文件 和 命令行中的其他参数 初始化 NamesrvConfig 和 NettyServerConfig； 使用 NamesrvConfig 和 NettyServerConfig 初始化 NamesrvController； 然后将 所有参数 持久化在 configFile 中； org.apache.rocketmq.namesrv.NamesrvController 12345678910111213public NamesrvController( NamesrvConfig namesrvConfig, NettyServerConfig nettyServerConfig) &#123; this.namesrvConfig = namesrvConfig; this.nettyServerConfig = nettyServerConfig; this.kvConfigManager = new KVConfigManager(this); this.routeInfoManager = new RouteInfoManager(); this.brokerHousekeepingService = new BrokerHousekeepingService(this); this.configuration = new Configuration( log, this.namesrvConfig, this.nettyServerConfig ); this.configuration.setStorePathFromConfig(this.namesrvConfig, "configStorePath");&#125; 初始化： kvConfigManager：使用 configTable（HashMap）存储 所有配置； routeInfoManager：存储路由信息； brokerHousekeepingService： 2.2、启动 NamesrvController start(controller); 12345678910111213141516171819202122232425org.apache.rocketmq.namesrv.NamesrvController;public static NamesrvController start(final NamesrvController controller) throws Exception &#123; if (null == controller) &#123; throw new IllegalArgumentException("NamesrvController is null"); &#125; // 初始化 boolean initResult = controller.initialize(); if (!initResult) &#123; controller.shutdown(); System.exit(-3); &#125; // 注册钩子，JVM 关闭时，关闭controller Runtime.getRuntime().addShutdownHook( new ShutdownHookThread(log, new Callable&lt;Void&gt;() &#123; @Override public Void call() throws Exception &#123; controller.shutdown(); return null; &#125; &#125;)); // 启动 controller controller.start(); return controller;&#125; 初始化 controller boolean initResult = controller.initialize(); 从 configFile 中 加在配置 放在 kvConfigManager 的 configTable（HashMap） 中； 初始化 NettyRemotingServer； 创建 业务线程池 remotingExecutor； 把 remotingExecutor 作为 remotingServer 的 默认处理线程池； 每 10s 扫描一次不活跃的 Broker，从 brokerLiveTable 中移除； 每 10s 打印一次 KV配置； 创建一个文件监控服务，当 sslContext 变更时，重新加载 sslContext； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970org.apache.rocketmq.namesrv.NamesrvController;public boolean initialize() &#123; // 从 configFile 中 加在配置 放在 kvConfigManager 的 configTable（HashMap） 中 this.kvConfigManager.load(); // 初始化 NettyRemotingServer this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService); // 创建 业务线程池 this.remotingExecutor = Executors.newFixedThreadPool( nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl("RemotingExecutorThread_")); // 把 remotingExecutor 作为 remotingServer 的 默认处理线程池 this.registerProcessor(); // 每 10s 扫描一次不活跃的 Broker，从 brokerLiveTable 中移除 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); // 每 10s 打印一次 KV配置 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.kvConfigManager.printAllPeriodically(); &#125; &#125;, 1, 10, TimeUnit.MINUTES); // 创建一个文件监控服务，当 sslContext 变更时，重新加载 sslContext if (TlsSystemConfig.tlsMode != TlsMode.DISABLED) &#123; // Register a listener to reload SslContext try &#123; fileWatchService = new FileWatchService( new String[] &#123; TlsSystemConfig.tlsServerCertPath, TlsSystemConfig.tlsServerKeyPath, TlsSystemConfig.tlsServerTrustCertPath &#125;, new FileWatchService.Listener() &#123; boolean certChanged, keyChanged = false; @Override public void onChanged(String path) &#123; if (path.equals(TlsSystemConfig.tlsServerTrustCertPath)) &#123; log.info("The trust certificate changed, reload the ssl context"); reloadServerSslContext(); &#125; if (path.equals(TlsSystemConfig.tlsServerCertPath)) &#123; certChanged = true; &#125; if (path.equals(TlsSystemConfig.tlsServerKeyPath)) &#123; keyChanged = true; &#125; if (certChanged &amp;&amp; keyChanged) &#123; log.info("The certificate and private key changed, reload the ssl context"); certChanged = keyChanged = false; reloadServerSslContext(); &#125; &#125; private void reloadServerSslContext() &#123; ((NettyRemotingServer) remotingServer).loadSslContext(); &#125; &#125;); &#125; catch (Exception e) &#123; log.warn("FileWatchService created error, can't load the certificate dynamically"); &#125; &#125; return true;&#125; controller 的 关闭和启动 controller.shutdown();controller.start(); 1234567891011121314151617public void start() throws Exception &#123; this.remotingServer.start(); // 启动 Netty服务器 if (this.fileWatchService != null) &#123; this.fileWatchService.start(); // 启动 sslContext 文件监控服务 &#125;&#125;public void shutdown() &#123; this.remotingServer.shutdown(); // 关闭 Netty服务器 this.remotingExecutor.shutdown(); // 关闭 线程池 this.scheduledExecutorService.shutdown(); // 关闭 sslContext 文件监控服务 if (this.fileWatchService != null) &#123; this.fileWatchService.shutdown(); &#125;&#125; 2.3、NameServer 路由注册、故障剔除NameServer的主要作用： 为Producer 和 Consumer 提供 Topic的 路由信息； NameServer功能：存储路由信息 、管理Broker节点； 1）路由元信息12345HashMap&lt;String/* topic */, List&lt;QueueData&gt;&gt; topicQueueTable;HashMap&lt;String/* brokerName */, BrokerData&gt; brokerAddrTable;HashMap&lt;String/* clusterName */, Set&lt;String/* brokerName */&gt;&gt; clusterAddrTable;HashMap&lt;String/* brokerAddr */, BrokerLiveInfo&gt; brokerLiveTable;HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable; topicQueueTable：Topic消息队列 路由信息，Producer 根据 路由表 进行负载均衡； brokerAddrTable：Broker基础信息，包含 brokerName、所属集群名称、主备 broker地址； clusterAddrTable：Broker集群信息，存储集群中 所有 Broker 名称； brokerLiveTable：Broker状态信息。NameServer每次收到心跳包时会替换该信息； filterServerTable：Broker上的 FilterServer 列表，用于类模式消息过滤； 12345678910111213141516171819202122232425262728293031323334package org.apache.rocketmq.common.protocol.route;public class QueueData implements Comparable&lt;QueueData&gt; &#123; private String brokerName; private int readQueueNums; private int writeQueueNums; private int perm; // 读写权限，具体含义参考 PermName private int topicSynFlag; // topic同步标记，具体含义参考 TopicSynFlag&#125;package org.apache.rocketmq.common.protocol.route;public class BrokerData implements Comparable&lt;BrokerData&gt; &#123; private String cluster; private String brokerName; // brokerId = 0, 表示Master, brokerId &gt; 0 表示Slave private HashMap&lt;Long/* brokerId */, String/* broker address */&gt; brokerAddrs; public String selectBrokerAddr() &#123; String addr = this.brokerAddrs.get(MixAll.MASTER_ID); // 先取 Master，取不到Master则随机返回一个Slave if (addr == null) &#123; List&lt;String&gt; addrs = new ArrayList&lt;String&gt;(brokerAddrs.values()); return addrs.get(random.nextInt(addrs.size())); &#125; return addr; &#125;&#125;package org.apache.rocketmq.namesrv.routeinfo;class BrokerLiveInfo &#123; private long lastUpdateTimestamp; private DataVersion dataVersion; private Channel channel; private String haServerAddr;&#125; 2）路由注册机制：通过Broker 与 NameServer 心跳功能实现的。 Broker 启动时 向集群中 所有NameServer 发送心跳语句; Broker 每隔 30s 向集群中 所有NameServer 发送心跳语句; NameServer 收到 Broker 心跳包时，会更新 brokerLiveTable 缓存中 BrokerLiveInfo 的 lastUpdateTimestamp； NameServer 每隔 10s 扫描 brokerLiveTable，如果连续 120s 没有收到 心跳包，则 NameServer将移除 该Broker 的路由信息，同时关闭 socket 连接； ①、Broker发送心跳包核心代码： org.apache.rocketmq.broker.BrokerController.start() 1234567891011121314151617181920public void start() throws Exception &#123; ...... // Broker 启动时 向集群中 所有NameServer 发送心跳语句; this.registerBrokerAll(true, false, true); // Broker 每隔 30s 向集群中 所有NameServer 发送心跳语句; this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; BrokerController.this.registerBrokerAll( true, false, brokerConfig.isForceRegister()); &#125; catch (Throwable e) &#123; log.error("registerBrokerAll Exception", e); &#125; &#125; &#125;, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS); ......&#125; 遍历 NameServer 列表，Broker 消息服务器 依次向 NameServer 发送心跳包： org.apache.rocketmq.broker.out.BrokerOuterAPI 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * 向所有 NameServer 注册 Broker * @param clusterName String 集群名称 * @param brokerAddr String broker地址 * @param brokerName String broker名称 * @param brokerId long brokerId，0-Master，大于0-Salve * @param haServerAddr String Master地址，初次请求时，该值为空，salve 向 NameServer注册后返回 * @param topicConfigWrapper TopicConfigSerializeWrapper 主题（topic）配置，存储在$&#123;&#125;rocket_home&#125;/store/config/topic.json中 * @param filterServerList List&lt;String&gt; 消息过滤服务器列表 * @param oneway boolean 是否 单向请求，不关心返回结果 * @param timeoutMills int * @param compressed boolean * @return */public List&lt;RegisterBrokerResult&gt; registerBrokerAll( final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final String haServerAddr, final TopicConfigSerializeWrapper topicConfigWrapper, final List&lt;String&gt; filterServerList, final boolean oneway, final int timeoutMills, final boolean compressed) &#123; final List&lt;RegisterBrokerResult&gt; registerBrokerResultList = Lists.newArrayList(); List&lt;String&gt; nameServerAddressList = this.remotingClient.getNameServerAddressList(); if (nameServerAddressList != null &amp;&amp; nameServerAddressList.size() &gt; 0) &#123; // 构造 requestHeader final RegisterBrokerRequestHeader requestHeader = new RegisterBrokerRequestHeader(); requestHeader.setBrokerAddr(brokerAddr); requestHeader.setBrokerId(brokerId); requestHeader.setBrokerName(brokerName); requestHeader.setClusterName(clusterName); requestHeader.setHaServerAddr(haServerAddr); requestHeader.setCompressed(compressed); // 构造 requestBody RegisterBrokerBody requestBody = new RegisterBrokerBody(); requestBody.setTopicConfigSerializeWrapper(topicConfigWrapper); requestBody.setFilterServerList(filterServerList); final byte[] body = requestBody.encode(compressed); final int bodyCrc32 = UtilAll.crc32(body); requestHeader.setBodyCrc32(bodyCrc32); // 使用 crc32 作为 校验码 final CountDownLatch countDownLatch = new CountDownLatch(nameServerAddressList.size()); // 遍历 NameServer 列表，Broker 消息服务器 依次向 NameServer 发送心跳包 for (final String namesrvAddr : nameServerAddressList) &#123; brokerOuterExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; // 向单个 NameServer 注册, // 从 responseHeader 里 拿到Master地址 和 HaServer地址 // 从 response.getBody() 中解析，放入 KVTable RegisterBrokerResult result = registerBroker( namesrvAddr,oneway, timeoutMills,requestHeader,body); if (result != null) &#123; registerBrokerResultList.add(result); &#125; log.info("register broker to name server &#123;&#125; OK", namesrvAddr); &#125; catch (Exception e) &#123; log.warn("registerBroker Exception, &#123;&#125;", namesrvAddr, e); &#125; finally &#123; countDownLatch.countDown(); &#125; &#125; &#125;); &#125; // 使用 countDownLatch 等待 固定时间timeoutMills try &#123; countDownLatch.await(timeoutMills, TimeUnit.MILLISECONDS); &#125; catch (InterruptedException e) &#123; &#125; &#125; // 返回 注册结果 return registerBrokerResultList;&#125; ②、NameServer处理心跳包 处理机制：org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor.processRequest() 网络处理 解析请求类型，如果请求类型为 RequestCode.REGISTER_BROKER，则请求最终转发到 RouteInfoManager.registerBroker()； org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor; 123456789101112131415161718public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException &#123; switch (request.getCode()) &#123; ....... case RequestCode.REGISTER_BROKER: // 如果 请求类型 为 Broker 心跳 Version brokerVersion = MQVersion.value2Version(request.getVersion()); if (brokerVersion.ordinal() &gt;= MQVersion.Version.V3_0_11.ordinal()) &#123; return this.registerBrokerWithFilterServer(ctx, request); &#125; else &#123; return this.registerBroker(ctx, request); &#125; ...... default: break; &#125; return null;&#125; org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager.registerBroker() 路由注册需要加写锁（ReentrantReadWriteLock.writeLock()）, 防止 并发 修改 RouteInfoManager 的路由表；首先判断 Broker 集群 是否存在，如果不存在，则 创建。然后将 broker名 加入到对应集群中； 维护 BrokerData信息； 如果Broker是Master，并且 broker中的topic 配置信息发生变化 或 首次注册，则创建或更新 Topic的路由元数据，填充 topicQueueTable； 更新 BrokerLiveInfo，存活的 Broker信息表； 注册Broker的过滤器 Server地址列表，一个Broker 上会关联 多个FilterSever消息过滤器； 如果 brokerName 不是Master，则 获取对应的 masterAddr 和 masterAddr 对应的 haServerAddr 作为返回值； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/** * 处理 broker来的 心跳包，更新NameServer中的元数据信息； * 更新 clusterAddrTable，brokerAddrTable，topicConfigWrapper * @param clusterName String * @param brokerAddr String * @param brokerName String * @param brokerId long * @param haServerAddr String * @param topicConfigWrapper TopicConfigSerializeWrapper * @param filterServerList List&lt;String&gt; * @param channel Channel * @return */public RegisterBrokerResult registerBroker( final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final String haServerAddr, final TopicConfigSerializeWrapper topicConfigWrapper, final List&lt;String&gt; filterServerList, final Channel channel) &#123; RegisterBrokerResult result = new RegisterBrokerResult(); try &#123; try &#123; this.lock.writeLock().lockInterruptibly(); // 将 brokerName 放入所属的 集群地址表clusterAddrTable（HashMap&lt;String, Set&lt;String&gt;&gt;） Set&lt;String&gt; brokerNames = this.clusterAddrTable.get(clusterName); if (null == brokerNames) &#123; brokerNames = new HashSet&lt;String&gt;(); this.clusterAddrTable.put(clusterName, brokerNames); &#125; brokerNames.add(brokerName); // 判断是否第一次注册，写入broker地址表 brokerAddrTable（HashMap&lt;String, BrokerData&gt;） boolean registerFirst = false; BrokerData brokerData = this.brokerAddrTable.get(brokerName); if (null == brokerData) &#123; registerFirst = true; brokerData = new BrokerData(clusterName, brokerName, new HashMap&lt;Long, String&gt;()); this.brokerAddrTable.put(brokerName, brokerData); &#125; String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr); registerFirst = registerFirst || (null == oldAddr); // 如果 brokerName 是 Master节点，并且topicConfig有变更，则 更新 topicConfigWrapper 中的元数据信息 if (null != topicConfigWrapper &amp;&amp; MixAll.MASTER_ID == brokerId) &#123; if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion()) || registerFirst) &#123; ConcurrentMap&lt;String, TopicConfig&gt; tcTable = topicConfigWrapper.getTopicConfigTable(); if (tcTable != null) &#123; for (Map.Entry&lt;String, TopicConfig&gt; entry : tcTable.entrySet()) &#123; // 添加或更新 brokerName 的 topic 配置 this.createAndUpdateQueueData(brokerName, entry.getValue()); &#125; &#125; &#125; &#125; // 更新 brokerLiveTable 中的 对应 broker 的 最近一次心跳时间 BrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr, new BrokerLiveInfo( System.currentTimeMillis(), topicConfigWrapper.getDataVersion(), channel, haServerAddr)); if (null == prevBrokerLiveInfo) &#123; log.info("new broker registered, &#123;&#125; HAServer: &#123;&#125;", brokerAddr, haServerAddr); &#125; // 更新 brokerAddr 对应的 过滤服务器列表 if (filterServerList != null) &#123; if (filterServerList.isEmpty()) &#123; this.filterServerTable.remove(brokerAddr); &#125; else &#123; this.filterServerTable.put(brokerAddr, filterServerList); &#125; &#125; // 如果 brokerName 不是Master，则 获取对应的 masterAddr 和 masterAddr 对应的 haServerAddr if (MixAll.MASTER_ID != brokerId) &#123; String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID); if (masterAddr != null) &#123; BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr); if (brokerLiveInfo != null) &#123; result.setHaServerAddr(brokerLiveInfo.getHaServerAddr()); result.setMasterAddr(masterAddr); &#125; &#125; &#125; &#125; finally &#123; this.lock.writeLock().unlock(); &#125; &#125; catch (Exception e) &#123; log.error("registerBroker Exception", e); &#125; return result;&#125; 3） 路由删除心跳包：{BrokerId，Broker地址，Broker名称，Broker所属集群名称}； 剔除失效Broker机制：NameServer 每隔 10s 扫描 brokerLiveTable 状态表，如果 BrokerLive的 lastUpdateTimestamp 的时间戳 距 当前时间超过 120s，则 认为 Broker失效；移除 该 Broker，关闭与Broker连接，并同时更新 topicQueueTable、brokerAddrTable、brokerLiveTable、filterServerTable； RocketMQ 有 两个触发点来 触发 路由删除： Namaserver 定时扫描brokerLiveTable，检测上次心跳包与当前系统时间差，如果时间间隔 &gt; 120s，则移除该Broker信息； Broker 在正常被关闭的情况下，会执行 unregisterBroker 指令； 两种方法的触发方式的公共代码： 12345678910111213org.apache.rocketmq.namesrv.NamesrvController;public boolean initialize() &#123; ...... // 每 10s 扫描一次不活跃的 Broker，从 brokerLiveTable 中移除 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); ......&#125; 1org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager; 4）路由发现]]></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA 调试 rocketmq 源码]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2F1-rocketmq-srcode-debug%2F</url>
    <content type="text"><![CDATA[1、启动 NameServerStep_1：源码路径，使用Idea导入工程 在命令行执行 1mvn clean install Step_2：展开 namesrv 模块 右键 NamasrvStartup.java，点击 NamaStart.main() 运行，报错 Step_3：在 Debug Configuration 界面设置 环境变量 1ROCKETMQ_HOME=D:\java\rocketmq Step_4：在 D:\java\rocketmq 下创建 conf、logs、store 三个文件夹 Step_5：将 D:\java\srcode\rocketmq-master\distribution\conf 下的 broker.conf、logback_broker.xml、logback_namesrv.xml、logback_tools.xml 复制到 D:\java\rocketmq\conf 目录下 修改 broker.conf 123456789101112131415brokerClusterName = DefaultClusterbrokerName = broker-abrokerId = 0deleteWhen = 04fileReservedTime = 48brokerRole = ASYNC_MASTERflushDiskType = ASYNC_FLUSHnamesrvAddr = 127.0.0.1:9876storePathRoot = D:\\java\\rocketmq\\storestorePathCommitLog = D:\\java\\rocketmq\\store\\commitlogstorePathConsumeQueue = D:\\java\\rocketmq\\store\\consumequeuestorePathIndex = D:\\java\\rocketmq\\store\\indexstoreCheckpoint = D:\\java\\rocketmq\\store\\checkpointabortFile = D:\\java\\rocketmq\\store\\abort 可以修改 logback_namesrv.xml 和 logback_tools.xml 中的log 路径； Step_6：运行 控制台出现 2、启动 Broker 3、发送消息示例 修改 Producer.java ，指定 Namesrv地址 producer.setNamesrvAddr(“127.0.0.1:9876”)： 123456789101112131415161718192021222324252627282930313233import org.apache.rocketmq.client.exception.MQClientException;import org.apache.rocketmq.client.producer.DefaultMQProducer;import org.apache.rocketmq.client.producer.SendResult;import org.apache.rocketmq.common.message.Message;import org.apache.rocketmq.remoting.common.RemotingHelper;public class Producer &#123; public static void main(String[] args) throws MQClientException, InterruptedException &#123; DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); producer.setNamesrvAddr("127.0.0.1:9876"); producer.start(); for (int i = 0; i &lt; 1000; i++) &#123; try &#123; Message msg = new Message("TopicTest", "TagA", ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) ); SendResult sendResult = producer.send(msg); System.out.printf("%s%n", sendResult); &#125; catch (Exception e) &#123; e.printStackTrace(); Thread.sleep(1000); &#125; &#125; producer.shutdown(); &#125;&#125; 运行结果： 4、测试消费示例修改 Consumer.java ，指定 Namesrv地址 producer.setNamesrvAddr(“127.0.0.1:9876”)： 12345678910111213141516171819202122232425262728293031323334353637package org.apache.rocketmq.example.quickstart;import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyContext;import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;import org.apache.rocketmq.client.exception.MQClientException;import org.apache.rocketmq.common.consumer.ConsumeFromWhere;import org.apache.rocketmq.common.message.MessageExt;import java.util.List;public class Consumer &#123; public static void main(String[] args) throws MQClientException &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4"); consumer.setNamesrvAddr("127.0.0.1:9876"); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.subscribe("TopicTest", "*"); consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage( List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); consumer.start(); System.out.printf("Consumer Started.%n"); &#125;&#125; 运行结果： 5、RocketMQ 源代码目录结构工程结构如下图： RoctketMQ 核心目录说明如下： broker：broker 模块（broker 启动进程）； client：消息客户端，包含 消息生产者、消息消费者 相关类； common：公共包； dev：开发者信息（非源代码）； distribution：部署实例文件夹（非源码）； example：RocketMQ 示例代码； filter：消息过滤相关基础类； filtersrv：消息过滤服务器 实现相关类（Filter启动进程）； logappender：日志实现相关类； namesrv：NameServer 实现相关类（NameServer 启动进程）； openmessaging：消息开放标准，正在指定中； remoting：远程通信模块，基于Netty； srvutil：服务器工具类； store：消息存储实现 相关类； style：checkstyle 相关实现； test：测试 相关类； tools：工具类，监控命令 相关实现类； 6、RocketMQ 的设计理念和目标6.1、设计理念1、基于 主题（Topic） 发布/订阅 模式 2、核心功能：消息发送，消息存储（Broker），消息消费 3、NameServer： 做元数据管理，舍弃 ZooKeeper，NameServer集群之间互不通信，降低实现复杂度，对网络要求也降低，但性能却 极大提升； 4、IO存储机制： 文件组（N个大小固定的文件），引入内存映射机制； 所有消息顺序写 —— 极大提升 写性能； 引入 消费队列文件 和 索引文件 —— 兼顾 消息消费 和 查找 5、容忍设计缺陷：保证消息至少被消费一次，不保证只消费一次，由 消费者自己保证； 6.2、设计目标1、架构模式：发布订阅模式，消息发送者，消息Broker，消息消费，路由发现； 2、顺序消息：保证消息 严格有序； 3、消息过滤：Broker过滤， 消费者过滤； 4、消息存储：消息堆积能力 和 消息存储性能； 内存映射机制，所有主题消息顺序写入同一个文件中； 引入文件过期机制和 文件存储空间报警机制； 5、消息高可用：同步刷盘、异步刷盘，异步复制，双写机制； 6、消息到达低延迟：长轮询模式 实现 准实时的消息推送； 7、确保消息必须被消费一次：无法做到 只被消费一次，会重复消费； 8、回溯消息：消费过的消息，支持按时间 重新消费（可精确到毫秒）向前或向后回溯； 9、消息堆积：文件组无限循环使用。提供了过期机制； 10、定时消息：特定延迟级别的 延迟队列； 11、消息重试机制：通过ack机制，未确认的消息 可 重新消费]]></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 tcp内核参数优化]]></title>
    <url>%2Flinux%2Flinux-tcp-optimizing%2F</url>
    <content type="text"><![CDATA[查看系统tcp参数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113[root@centos7cz ~]# sysctl -a|grep ipv4|grep -i --color tcpnet.ipv4.tcp_abort_on_overflow = 0net.ipv4.tcp_adv_win_scale = 1net.ipv4.tcp_allowed_congestion_control = cubic renonet.ipv4.tcp_app_win = 31net.ipv4.tcp_autocorking = 1net.ipv4.tcp_available_congestion_control = cubic renonet.ipv4.tcp_base_mss = 512net.ipv4.tcp_challenge_ack_limit = 1000net.ipv4.tcp_congestion_control = cubicnet.ipv4.tcp_dsack = 1net.ipv4.tcp_early_retrans = 3net.ipv4.tcp_ecn = 2net.ipv4.tcp_fack = 1net.ipv4.tcp_fastopen = 0net.ipv4.tcp_fastopen_key = 00000000-00000000-00000000-00000000net.ipv4.tcp_fin_timeout = 60net.ipv4.tcp_frto = 2net.ipv4.tcp_invalid_ratelimit = 500net.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9net.ipv4.tcp_keepalive_time = 7200net.ipv4.tcp_limit_output_bytes = 262144net.ipv4.tcp_low_latency = 0net.ipv4.tcp_max_orphans = 32768net.ipv4.tcp_max_ssthresh = 0net.ipv4.tcp_max_syn_backlog = 256net.ipv4.tcp_max_tw_buckets = 32768net.ipv4.tcp_mem = 184824 246434 369648net.ipv4.tcp_min_tso_segs = 2net.ipv4.tcp_moderate_rcvbuf = 1net.ipv4.tcp_mtu_probing = 0net.ipv4.tcp_no_metrics_save = 0net.ipv4.tcp_notsent_lowat = -1net.ipv4.tcp_orphan_retries = 0net.ipv4.tcp_reordering = 3net.ipv4.tcp_retrans_collapse = 1net.ipv4.tcp_retries1 = 3net.ipv4.tcp_retries2 = 15net.ipv4.tcp_rfc1337 = 0net.ipv4.tcp_rmem = 4096 87380 6291456net.ipv4.tcp_sack = 1net.ipv4.tcp_slow_start_after_idle = 1net.ipv4.tcp_stdurg = 0net.ipv4.tcp_syn_retries = 6net.ipv4.tcp_synack_retries = 5net.ipv4.tcp_syncookies = 1net.ipv4.tcp_thin_dupack = 0net.ipv4.tcp_thin_linear_timeouts = 0net.ipv4.tcp_timestamps = 1net.ipv4.tcp_tso_win_divisor = 3net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_tw_reuse = 0net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_wmem = 4096 16384 4194304net.ipv4.tcp_workaround_signed_windows = 0[root@centos7cz ~]# sysctl -a|grep ipv4|grep -i --color tcpnet.ipv4.tcp_abort_on_overflow = 0net.ipv4.tcp_adv_win_scale = 1net.ipv4.tcp_allowed_congestion_control = cubic renonet.ipv4.tcp_app_win = 31net.ipv4.tcp_autocorking = 1net.ipv4.tcp_available_congestion_control = cubic renonet.ipv4.tcp_base_mss = 512net.ipv4.tcp_challenge_ack_limit = 1000net.ipv4.tcp_congestion_control = cubicnet.ipv4.tcp_dsack = 1net.ipv4.tcp_early_retrans = 3net.ipv4.tcp_ecn = 2net.ipv4.tcp_fack = 1net.ipv4.tcp_fastopen = 0net.ipv4.tcp_fastopen_key = 00000000-00000000-00000000-00000000net.ipv4.tcp_fin_timeout = 60net.ipv4.tcp_frto = 2net.ipv4.tcp_invalid_ratelimit = 500net.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9net.ipv4.tcp_keepalive_time = 7200net.ipv4.tcp_limit_output_bytes = 262144net.ipv4.tcp_low_latency = 0net.ipv4.tcp_max_orphans = 32768net.ipv4.tcp_max_ssthresh = 0net.ipv4.tcp_max_syn_backlog = 256net.ipv4.tcp_max_tw_buckets = 32768net.ipv4.tcp_mem = 184824 246434 369648net.ipv4.tcp_min_tso_segs = 2net.ipv4.tcp_moderate_rcvbuf = 1net.ipv4.tcp_mtu_probing = 0net.ipv4.tcp_no_metrics_save = 0net.ipv4.tcp_notsent_lowat = -1net.ipv4.tcp_orphan_retries = 0net.ipv4.tcp_reordering = 3net.ipv4.tcp_retrans_collapse = 1net.ipv4.tcp_retries1 = 3net.ipv4.tcp_retries2 = 15net.ipv4.tcp_rfc1337 = 0net.ipv4.tcp_rmem = 4096 87380 6291456net.ipv4.tcp_sack = 1net.ipv4.tcp_slow_start_after_idle = 1net.ipv4.tcp_stdurg = 0net.ipv4.tcp_syn_retries = 6net.ipv4.tcp_synack_retries = 5net.ipv4.tcp_syncookies = 1net.ipv4.tcp_thin_dupack = 0net.ipv4.tcp_thin_linear_timeouts = 0net.ipv4.tcp_timestamps = 1net.ipv4.tcp_tso_win_divisor = 3net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_tw_reuse = 0net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_wmem = 4096 16384 4194304net.ipv4.tcp_workaround_signed_windows = 0[root@centos7cz ~]#]]></content>
      <categories>
        <category>linux</category>
        <category>centos7</category>
      </categories>
      <tags>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式唯一ID —— 雪花算法（snowflake）]]></title>
    <url>%2Fdistribution%2Fsnowflake%2F</url>
    <content type="text"><![CDATA[雪花算法（snowflake） 算法描述： 最高位是符号位，始终为0，不可用。 41位的时间序列，精确到毫秒级，41位的长度可以使用69年。时间位还有一个很重要的作用是可以根据时间进行排序。 10位的机器标识，10位的长度最多支持部署1024个节点。 12位的计数序列号，序列号即一系列的自增id，可以支持同一节点同一毫秒生成多个ID序号，12位的计数序列号支持每个节点每毫秒产生4096个ID序号。 缺点： 时钟回拨问题； 趋势递增，而不是绝对递增； 不能在一台服务器上部署多个分布式ID服务； Twitter实现GitHub：https://github.com/twitter-archive/snowflake 官方文档：https://twitter.github.io/twitter-server/ Mybatis采用默认的twitter-snowflake，没有解决时钟回拨问题； Baidu实现GitHub：https://github.com/baidu/uid-generator Snowflake算法描述：指定机器 &amp; 同一时刻 &amp; 某一并发序列，是唯一的。据此可生成一个64 bits的唯一ID（long）。默认采用上图字节分配方式： sign(1bit)固定1bit符号标识，即生成的UID为正数。 delta seconds (28 bits)当前时间，相对于时间基点”2016-05-20”的增量值，单位：秒，最多可支持约8.7年（2的28次方/86400=3106.89天） worker id (22 bits)机器id，最多可支持约420w次机器启动（2的22次方是4194304）。内置实现为在启动时由数据库分配，默认分配策略为用后即弃，后续可提供复用策略。 sequence (13 bits)每秒下的并发序列，13 bits可支持每秒8192个并发。 CachedUidGeneratorRingBuffer环形数组，数组每个元素成为一个slot。RingBuffer容量，默认为Snowflake算法中sequence最大值，且为2^N。可通过boostPower配置进行扩容，以提高RingBuffer 读写吞吐量。 Tail指针、Cursor指针用于环形数组上读写slot： Tail指针表示Producer生产的最大序号(此序号从0开始，持续递增)。Tail不能超过Cursor，即生产者不能覆盖未消费的slot。当Tail已赶上curosr，此时可通过rejectedPutBufferHandler指定PutRejectPolicy Cursor指针表示Consumer消费到的最小序号(序号序列与Producer序列相同)。Cursor不能超过Tail，即不能消费未生产的slot。当Cursor已赶上tail，此时可通过rejectedTakeBufferHandler指定TakeRejectPolicy CachedUidGenerator采用了双RingBuffer，Uid-RingBuffer用于存储Uid、Flag-RingBuffer用于存储Uid状态(是否可填充、是否可消费) 由于数组元素在内存中是连续分配的，可最大程度利用CPU cache以提升性能。但同时会带来「伪共享」FalseSharing问题，为此在Tail、Cursor指针、Flag-RingBuffer中采用了CacheLine 补齐方式。 RingBuffer填充时机 初始化预填充RingBuffer初始化时，预先填充满整个RingBuffer. 即时填充Take消费时，即时检查剩余可用slot量(tail - cursor)，如小于设定阈值，则补全空闲slots。阈值可通过paddingFactor来进行配置，请参考Quick Start中CachedUidGenerator配置 周期填充通过Schedule线程，定时补全空闲slots。可通过scheduleInterval配置，以应用定时填充功能，并指定Schedule时间间隔 CachedUidGenerator在初始化时除了给workerId赋值，还会初始化RingBuffer。这个过程主要工作有： 根据boostPower的值确定RingBuffer的size； 构造RingBuffer，默认paddingFactor为50。这个值的意思是当RingBuffer中剩余可用ID数量少于50%的时候，就会触发一个异步线程往RingBuffer中填充新的唯一ID（调用BufferPaddingExecutor中的paddingBuffer()方法，这个线程中会有一个标志位running控制并发问题），直到填满为止； 判断是否配置了属性scheduleInterval，这是另外一种RingBuffer填充机制, 在Schedule线程中, 周期性检查填充。默认:不配置, 即不使用Schedule线程. 如需使用, 请指定Schedule线程时间间隔, 单位:秒； 初始化Put操作拒绝策略，对应属性rejectedPutBufferHandler。即当RingBuffer已满, 无法继续填充时的操作策略。默认无需指定, 将丢弃Put操作, 仅日志记录. 如有特殊需求, 请实现RejectedPutBufferHandler接口(支持Lambda表达式)； 初始化Take操作拒绝策略，对应属性rejectedTakeBufferHandler。即当环已空, 无法继续获取时的操作策略。默认无需指定, 将记录日志, 并抛出UidGenerateException异常. 如有特殊需求, 请实现RejectedTakeBufferHandler接口； 初始化填满RingBuffer中所有slot（即塞满唯一ID，这一步和第2步骤一样都是调用BufferPaddingExecutor中的paddingBuffer()方法）； 开启buffer补丁线程（前提是配置了属性scheduleInterval），原理就是利用ScheduledExecutorService的scheduleWithFixedDelay()方法。 解决时钟回拨第二步的异步线程实现非常重要，也是UidGenerator解决时钟回拨的关键：在满足填充新的唯一ID条件时，通过时间值递增得到新的时间值（lastSecond.incrementAndGet()），而不是System.currentTimeMillis()这种方式，而lastSecond是AtomicLong类型，所以能保证线程安全问题。 ​ 传统的雪花算法实现都是通过System.currentTimeMillis()来获取时间并与上一次时间进行比较，这样的实现严重依赖服务器的时间。而UidGenerator的时间类型是AtomicLong，且通过incrementAndGet()方法获取下一次的时间，从而脱离了对服务器时间的依赖，也就不会有时钟回拨的问题（这种做法也有一个小问题，即分布式ID中的时间信息可能并不是这个ID真正产生的时间点，例如：获取的某分布式ID的值为3200169789968523265，它的反解析结果为{“timestamp”:”2019-05-02 23:26:39”,”workerId”:”21”,”sequence”:”1”}，但是这个ID可能并不是在”2019-05-02 23:26:39”这个时间产生的）。]]></content>
      <categories>
        <category>distribution</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>唯一ID</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Falgorithm%2Fdp-analysis%2F</url>
    <content type="text"><![CDATA[动态规划的本质 —— 递推公式 常见问题1、找前一个 2、找前面最大]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql lock、transaction 和 mvcc]]></title>
    <url>%2Fmysql%2Fmysql-innodb-lock-transaction-mvcc%2F</url>
    <content type="text"><![CDATA[一、Locking 锁1、共享锁和独占锁InnoDB实现了两种 行级锁：共享锁（S） 和 独占锁（X） 共享锁：允许 事务 持有 read 一行 的锁； 独占锁：允许 事务 持有 update 或 delete 一行的锁； 2、Intention Locks 意向锁​ InnoDB 支持 多颗粒度加锁，允许 行锁 和 表锁 共存。例如， LOCK TABLES ... WRITE 就是在指定表上加独占锁。为了支持多颗粒度加锁，InnoDB设计了 意向锁。意向锁 是 一种 表级锁，用来 指示 一个事务对表中的一行数据需要加 哪种类型（shared 或 exclusive）的锁。 意向锁有两种： intention shared lock (IS)意向共享锁：表示 一个事务 倾向于 对一行数据 使用共享锁；在 transaction 获取一个 共享锁（S） 之前 必须先获得一个 IS 锁； intention exclusive lock (IX)意向独占锁：表示 一个事务 倾向于 对一行数据 使用独占锁；在 transaction 获取一个 独占锁（X） 之前 必须先获得一个 IX 锁； 例如：SELECT ... LOCK IN SHARE MODE 使用的就是 IS 锁， SELECT ... FOR UPDATE 使用的是 IX 锁； 表级锁的兼容性： X IX S IS X Conflict Conflict Conflict Conflict IX Conflict Compatible Conflict Compatible S Conflict Conflict Compatible Compatible IS Conflict Compatible Compatible Compatible 如果请求事务与现有锁兼容，则授予它锁，但如果与现有锁冲突，则不授予它锁。事务将一直等待，直到释放冲突的现有锁。如果锁请求与现有锁发生冲突，并且由于会导致死锁而无法被授予，则会发生错误。 3、Record Locks 记录锁record lock 是 对 index record 加锁的一种锁。例如 SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; 阻止其他事务 插入、更新、删除 t.c1=10 的行； record lock 总是对 index record 加锁，即使没有对表定义任何索引。在这种情况下 InnoDB 会添加一个隐藏主键； 4、Gap Locks 间隙锁​ 一个 gap lock 是 加在 索引记录之间的 锁。例如 SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE可以防止其他事务把 t.c1=15的记录插入表中； ​ 一个 gap 可能是 一行记录，多行记录，或者 为空； ​ gap lock 是 性能 和 并发 之间的一种折中方案，只在某些隔离级别下生效； SELECT * FROM child WHERE id = 100; 如果id本身是唯一索引，那么 gap lock 是不生效的； 如果 id 和 其他的列 组成的联合索引是 位移索引， 那么 gap lock 是生效的，锁定了 child[100][x] ~ child[100][y] 之间的记录； 如果 id 上 没有加索引 或者 不是唯一索引，gap lock 生效； 简单来说就是 gap lock 锁定的是一个范围（0~N条记录），而不是单条记录； ​ InnoDB中的Gap锁是“纯粹的抑制性锁”，这意味着它们的唯一目的是防止其他事务插入到Gap中。间隙锁可以共存。一个事务所采取的间隙锁并不会阻止另一个事务对同一间隙采取间隙锁。共享锁和独占锁之间没有区别。它们彼此不冲突，并且执行相同的功能。 禁用gap lock： 将 隔离级别 设为 READ COMMITTED ； 启用 innodb_locks_unsafe_for_binlog 5、Next-Key Locksnext-key lock 是 record lock 和 gap lock 的组合 当 InnoDB 搜索索引时，它会在 index 上加一个行级锁；因此 行级锁 实际上 是 index-record lock。 假设 一个 index 包含 10，11，13，20。那么可能的 next-key lock 如下： 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) 在 REPEATABLE READ 隔离级别下，并且禁用了 innodb_locks_unsafe_for_binlog, InnoDB 使用 next-key lock 扫描索引的时候 可以防止幻读； 6、Insert Intention Lockschild表包含90，102两条数据，事务A获取一个 gap lock （100, max_value） 12345678910mysql&gt; CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;mysql&gt; INSERT INTO child (id) values (90),(102);mysql&gt; START TRANSACTION;mysql&gt; SELECT * FROM child WHERE id &gt; 100 FOR UPDATE;+-----+| id |+-----+| 102 |+-----+ 如果事务B 想插入 101，则需要等待事务A提交 12mysql&gt; START TRANSACTION;mysql&gt; INSERT INTO child (id) VALUES (101); 7、AUTO-INC Locksauto-inc lock 是一种特殊的表级锁。如果一个事务正在向表中插入值，那么任何其他事务都必须等待，以便由第一个事务插入的行接收连续的主键值。 innodb_autoinc_lock_mode 用于 二、不同sql语句设置的锁 三、Deadlocks 死锁​ 死锁是指由于每个事务都持有对方需要的锁而无法进行其他事务的情况。因为这两个事务都在等待资源变得可用，所以都不会释放它持有的锁。 1、死锁示例该示例涉及两个客户端A和B。 首先，客户端A创建一个包含一行的表，然后开始事务。在事务中，A通过S在共享模式下选择该行来获得对该行的 锁定： 123456789101112131415mysql&gt; CREATE TABLE t (i INT) ENGINE = InnoDB;Query OK, 0 rows affected (1.07 sec)mysql&gt; INSERT INTO t (i) VALUES(1);Query OK, 1 row affected (0.09 sec)mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM t WHERE i = 1 LOCK IN SHARE MODE;+------+| i |+------+| 1 |+------+ 接下来，客户端B开始事务并尝试从表中删除该行： 1234mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; DELETE FROM t WHERE i = 1; 删除操作需要一个X锁。无法授予该S锁，因为它与客户端A持有的锁不兼容 ，因此该请求进入针对行和客户端B块的锁请求队列中。 最后，客户端A还尝试从表中删除该行： 123mysql&gt; DELETE FROM t WHERE i = 1;ERROR 1213 (40001): Deadlock found when trying to get lock;try restarting transaction 此处发生死锁是因为客户端A需要 X锁才能删除该行。但是，不能授予该锁定请求，因为客户端B已经有一个X锁定请求，并且正在等待客户端A释放其S锁定。由于B事先要求锁，所以SA持有的锁也不能 升级 X为X锁。结果， InnoDB为其中一个客户端生成错误并释放其锁。客户端返回此错误： 12ERROR 1213 (40001): Deadlock found when trying to get lock;try restarting transaction 届时，可以授予对另一个客户端的锁定请求，并从表中删除该行。 2、死锁检测和回滚​ InnoDB自动检测事务 死锁并回滚一个或多个事务以打破死锁。 InnoDB尝试选择要回滚的小事务，其中事务的大小由插入，更新或删除的行数确定。 ​ 如果死锁无法检测，通过设置 innodb_lock_wait_timeout 来解决； ​ 如果 一个事务被完整的会滚，那么它所持有的所有锁都会被释放；但如果由于出错仅仅一条sql被会滚，那么某些锁可能不会被释放；这是因为 InnoDB存储行锁的格式 无法确定后续的锁会被哪些sql持有； 要查看InnoDB用户事务中的最后一个死锁，请使用SHOW ENGINE INNODB STATUS命令。 如果频繁出现死锁，则说明事务结构或应用程序错误处理存在问题，请在innodb_print_all_deadlocks 启用该设置的情况下运行，以 将有关所有死锁的信息打印到 mysqld错误日志中 3、如何最小化并处理死锁InnoDB使用自动行级锁定。即使在仅插入或 删除 单行的事务中，也可能会遇到死锁。这是因为这些操作并不是真正的“ 原子 ”操作。它们会自动对插入或删除的行的（可能是多个）索引记录设置锁定。 您可以使用以下技术来处理死锁并减少发生死锁的可能性： 在任何时候，发出 SHOW ENGINE INNODB STATUS命令以确定最近死锁的原因。这可以帮助您调整应用程序以避免死锁。 如果频繁出现死锁警告引起关注，请通过启用innodb_print_all_deadlocks 配置选项来收集更广泛的调试信息 。有关每个死锁的信息，而不仅仅是最新的死锁，都记录在MySQL 错误日志中。完成调试后，请禁用此选项。 如果由于死锁而失败，请始终准备重新发出事务。死锁并不危险。请再试一次。 保持交易小巧且持续时间短，以使交易不易发生冲突。 进行一系列相关更改后立即提交事务，以减少冲突的发生。特别是，不要长时间未提交事务而使交互式 mysql会话保持打开状态。 如果您使用锁定读取（SELECT ... FOR UPDATE或 SELECT ... LOCK IN SHARE MODE），请尝试使用较低的隔离级别，例如 READ COMMITTED。 修改事务中的多个表或同一表中的不同行集时，每次都要以一致的顺序执行这些操作。然后，事务形成定义明确的队列，并且不会死锁。例如，组织数据库操作到功能在应用程序中，或调用存储程序，而不是编码的多个相似序列 INSERT，UPDATE以及 DELETE在不同的地方语句。 将选择好的索引添加到表中。然后，您的查询需要扫描更少的索引记录，并因此设置更少的锁。使用EXPLAIN SELECT以确定哪些索引MySQL认为最适合您的查询。 使用较少的锁定。如果你能负担得起，以允许 SELECT从一个旧的快照返回数据，不要添加条款FOR UPDATE或LOCK IN SHARE MODE给它。在READ COMMITTED这里使用隔离级别是件好事，因为同一事务中的每个一致性读取均从其自己的新快照读取。 如果没有其他帮助，请使用表级锁序列化事务。LOCK TABLES与事务表（例如InnoDB 表）一起使用的正确方法 是，以SET autocommit = 0（not START TRANSACTION）后跟来开始事务，直到明确提交事务后才LOCK TABLES调用 UNLOCK TABLES。例如，如果您需要写表 t1和从表中读取数据 t2，则可以执行以下操作： 1234SET autocommit=0;LOCK TABLES t1 WRITE, t2 READ, ...;... do something with tables t1 and t2 here ...COMMIT;UNLOCK TABLES; 表级锁可防止对表的并发更新，从而避免死锁，但代价是对繁忙系统的响应速度较慢。 序列化事务的另一种方法是创建一个仅包含一行的辅助“ 信号量 ”表。在访问其他表之前，让每个事务更新该行。这样，所有事务都以串行方式发生。请注意，InnoDB 在这种情况下，即时死锁检测算法也适用，因为序列化锁是行级锁。对于MySQL表级锁，必须使用超时方法来解决死锁。 四、Transaction Model 事务模型​ InnoDB 的事务模型 的目标是 将 multi-versioning 和two-phase locking 的最佳属性结合起来。默认情况下 InnoDB 的查询以 “非锁定一致性读” 和 行级锁 的方式运行。 1、autocommit, commit, rollback​ 在 InnoDB 中 所有的 用户操作都包裹在事务中；如果 启用了 autocommit，那么每条语句都是一个事务；默认情况下，每创建一个session，autocommit 都是启用的；如果SQL语句正确执行，那么就会自动条，否则会滚之前的操作，然后报错； 在 autocommit 情况下 如果 想把多条语句包裹在一个事务里，则需要如下格式： 12345START TRANSACTION;SELECT @A:=SUM(salary) FROM table1 WHERE type=1;UPDATE table2 SET summary=@A WHERE type=1;COMMIT;# 不想提交，则执行 ROLLBACK; 禁用 autocommit： 1SET autocommit=0; 2、存在的问题​ 平常开发过程中免不了对数据库的操作，并且还会有多个线程同时开启事务后对数据库进行访问，那此时不可避免就会出现多个线程之间交叉访问而导致数据的不一致，通过对数据库的隔离级别进行设置可以保证各线程数据获取的准确性。 在介绍隔离级别之前先要弄清楚数据库在并发事务下会出现的一些状态： 1)、脏读脏读就是一个事务读取了另外一个事务未提交的数据。 事务2读取了事务1未提交的数据。 2)、不可重复读在同一事务中，两次读取同一数据，得到内容不同 例如事务1读取了某个数据，然后事务2更新了这个数据并提交，然后事务1又来读取了一次，那这两次读取的结果就会不一样。 3)、幻读在一个事务的两次查询中数据记录数不一致，例如有一个事务1查询了几列数据，而事务2在此时插入了新的几列数据，事务1在接下来的查询中，就会发现有几列数据是它先前所没有的。 不可重复读是针对于多次读取同一条数据出现不同结果，幻读是多次读取而产生的记录数不一样 3、隔离级别（Isolation）​ Isolation 是 ACID 中的 I；Isolation 是在多个事务同时进行更改和执行查询时，对性能与可靠性、一致性和结果再现性之间的平衡进行微调的设定。 ​ InnoDB 提供了 4 种隔离级别：READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, 和 SERIALIZABLE。默认的 隔离级别是 REPEATABLE READ。 ​ InnoDB 使用不同的锁策略 来实现隔离级别。REPEATABLE READ 用来操作重要的数据，保证 ACID；如果使用 READ UNCOMMITTED 或READ COMMITTED 可以降低锁的开销；SERIALIZABLE是一种比 SERIALIZABLE更严格的规则，一般用在专门的场景，比如 XA事务 或者 解决并发问题和死锁。 1)、read uncommitted可以看到未提交的数据（脏读），举个例子：别人说的话你都相信了，但是可能他只是说说，并不实际做。 2)、read committed读取提交的数据。但是，可能多次读取的数据结果不一致（不可重复读，幻读）。用读写的观点就是：读取的行数据，可以写。 3)、repeatable read(默认)可以重复读取，但有幻读。读写观点：读取的数据行不可写，但是可以往表中新增数据。在MySQL中，其他事务新增的数据，看不到，不会产生幻读。采用多版本并发控制（MVCC）机制解决幻读问题。 4)、serializable可读，不可写。像java中的锁，写数据必须等待另一个事务结束。 4、查看隔离级别1234-- 1.查看当前会话隔离级别select @@tx_isolation;-- 查看系统当前隔离级别select @@global.tx_isolation; 设置隔离级别 12345678-- 设置当前会话隔离级别set session transaction isolatin level repeatable read;-- 设置系统当前隔离级别set global transaction isolation level repeatable read;set tx_isolation='read-uncommitted';set tx_isolation='read-committed';set tx_isolation='repeatable-read';set tx_isolation='serializable'; 五、隔离级别实现原理1、MVCC参考： https://www.cnblogs.com/cjsblog/p/8365921.html MVCC的全称是“多版本并发控制”。这项技术使得InnoDB的事务隔离级别下执行一致性读操作有了保证，换言之，就是为了查询一些正在被另一个事务更新的行，并且可以看到它们被更新之前的值。这是一个可以用来增强并发性的强大的技术，因为这样的一来的话查询就不用等待另一个事务释放锁。这项技术在数据库领域并不是普遍使用的。一些其它的数据库产品，以及mysql其它的存储引擎并不支持它。 1)、说明​ 网上看到大量的文章讲到MVCC都是说给每一行增加两个隐藏的字段分别表示行的创建时间以及过期时间，它们存储的并不是时间，而是事务版本号。 ​ 事实上，这种说法并不准确，严格的来讲，InnoDB会给数据库中的每一行增加三个字段，它们分别是DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID。 ​ 但是，为了理解的方便，我们可以这样去理解，索引接下来的讲解中也还是用这两个字段的方式去理解。 2)、增删查改​ 在InnoDB中，给每行增加两个隐藏字段来实现MVCC，一个用来记录数据行的创建时间，另一个用来记录行的过期时间（删除时间）。在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 于是乎，默认的隔离级别（REPEATABLE READ）下，增删查改变成了这样： SELECT：读取创建版本小于或等于当前事务版本号，并且删除版本为空或大于当前事务版本号的记录。这样可以保证在读取之前记录是存在的。 INSERT：将当前事务的版本号保存至行的创建版本号 UPDATE：新插入一行，并以当前事务的版本号作为新行的创建版本号，同时将原记录行的删除版本号设置为当前事务版本号 DELETE：将当前事务的版本号保存至行的删除版本号 3)、快照读和当前读 快照读：读取的是快照版本，也就是历史版本 当前读：读取的是最新版本 ​ 普通的SELECT就是快照读，而UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE是当前读。 2、一致性非锁定读和锁定读1)、锁定读 在一个事务中，标准的SELECT语句是不会加锁，但是有两种情况例外。SELECT … LOCK IN SHARE MODE 和 SELECT … FOR UPDATE。 SELECT … LOCK IN SHARE MODE：给记录假设共享锁，这样一来的话，其它事务只能读不能修改，直到当前事务提交； SELECT … FOR UPDATE：给索引记录加锁，这种情况下跟UPDATE的加锁情况是一样的； 2)、一致性非锁定读一致性读（consistent read）意味着 InnoDB 对一个 query 展示的数据是 多版本中一个时间点的 snapshot。 如果隔离级别是REPEATABLE READ，那么在同一个事务中的所有一致性读都读的是事务中第一个这样的读读到的快照； 如果是READ COMMITTED，那么一个事务中的每一个一致性读都会读到它自己刷新的快照版本。 Consistent read（一致性读）是READ COMMITTED和REPEATABLE READ隔离级别下普通SELECT语句默认的模式。一致性读不会给它所访问的表加任何形式的锁，因此其它事务可以同时并发的修改它们。 3、悲观锁和乐观锁 悲观锁，正如它的名字那样，数据库总是认为别人会去修改它所要操作的数据，因此在数据库处理过程中将数据加锁。其实现依靠数据库底层。 乐观锁，如它的名字那样，总是认为别人不会去修改，只有在提交更新的时候去检查数据的状态。通常是给数据增加一个字段来标识数据的版本。 4、锁有这样三种锁我们需要了解 Record Locks（记录锁）：在索引记录上加锁。 Gap Locks（间隙锁）：在索引记录之间加锁，或者在第一个索引记录之前加锁，或者在最后一个索引记录之后加锁。 Next-Key Locks：在索引记录上加锁，并且在索引记录之前的间隙加锁。它相当于是Record Locks与Gap Locks的一个结合。 假设一个索引包含以下几个值：10,11,13,20。那么这个索引的next-key锁将会覆盖以下区间： (negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) 了解了以上概念之后，接下来具体就简单分析下REPEATABLE READ隔离级别是如何实现的 5、理论分析​ 之所以说是理论分析，是因为要是实际操作证明的话我也不知道怎么去证明，毕竟作者水平实在有限。但是，这并不意味着我在此胡说八道，有官方文档为证。 ​ 这段话的大致意思是，在默认的隔离级别中，普通的SELECT用的是一致性读不加锁。而对于锁定读、UPDATE和DELETE，则需要加锁，至于加什么锁视情况而定。如果你对一个唯一索引使用了唯一的检索条件，那么只需锁定索引记录即可；如果你没有使用唯一索引作为检索条件，或者用到了索引范围扫描，那么将会使用间隙锁或者next-key锁以此来阻塞其它会话向这个范围内的间隙插入数据。 ​ 作者曾经有一个误区，认为按照前面说MVCC下的增删查改的行为就不会出现任何问题，也不会出现不可重复读和幻读。但其实是大错特错。 ​ 举个很简单的例子，假设事务A更新表中id=1的记录，而事务B也更新这条记录，并且B先提交，如果按照前面MVVC说的，事务A读取id=1的快照版本，那么它看不到B所提交的修改，此时如果直接更新的话就会覆盖B之前的修改，这就不对了，可能B和A修改的不是一个字段，但是这样一来，B的修改就丢失了，这是不允许的。 ​ 所以，在修改的时候一定不是快照读，而是当前读。 ​ 而且，前面也讲过只有普通的SELECT才是快照读，其它诸如UPDATE、删除都是当前读。修改的时候加锁这是必然的，同时为了防止幻读的出现还需要加间隙锁。 一致性读保证了可用重复读 间隙锁防止了幻读 回想一下 1、利用MVCC实现一致性非锁定读，这就有保证在同一个事务中多次读取相同的数据返回的结果是一样的，解决了不可重复读的问题 2、利用Gap Locks和Next-Key可以阻止其它事务在锁定区间内插入数据，因此解决了幻读问题 综上所述，默认隔离级别的实现依赖于MVCC和锁，再具体一点是一致性读和锁。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql 锁和事务模型]]></title>
    <url>%2Fmysql%2Fmysql-innodb-locking-and-transaction-model%2F</url>
    <content type="text"><![CDATA[一、Locking 锁1、锁分类1.1、共享锁和独占锁InnoDB实现了两种标准行级锁，一种是共享锁 (shared locks，S锁)，另一种是独占锁，或者叫排它锁 (exclusive locks，X锁)。 共享锁：允许 事务 持有 read 一行 的锁； 独占锁：允许 事务 持有 update 或 delete 一行的锁； S锁 如果事务T1持有了行r上的S锁，则其他事务可以同时持有行r的S锁，但是不能对行r加X锁。 X锁 如果事务T1持有了行r上的X锁，则其他任何事务不能持有行r的X锁，必须等待T1在行r上的X锁释放。 如果事务T1在行r上保持S锁，则另一个事务T2对行r的锁的请求按如下方式处理： T2可以同时持有S锁 T2如果想在行r上获取X锁，必须等待其他事务对该行添加的S锁或X锁的释放。 1.2、Intention Locks 意向锁​ InnoDB 支持 多颗粒度加锁，允许 行锁 和 表锁 共存。例如， LOCK TABLES ... WRITE 就是在指定表上加独占锁。为了支持多颗粒度加锁，InnoDB设计了 意向锁。意向锁 是 一种 表级锁，用来 指示 一个事务对表中的一行数据需要加 哪种类型（shared 或 exclusive）的锁。 意向锁有两种： intention shared lock (IS)意向共享锁：表示 一个事务 倾向于 对一行数据 使用共享锁；在 transaction 获取一个 共享锁（S） 之前 必须先获得一个 IS 锁； intention exclusive lock (IX)意向独占锁：表示 一个事务 倾向于 对一行数据 使用独占锁；在 transaction 获取一个 独占锁（X） 之前 必须先获得一个 IX 锁； 例如：SELECT ... LOCK IN SHARE MODE 使用的就是 IS 锁， SELECT ... FOR UPDATE 使用的是 IX 锁； 1.3、锁的兼容性锁的兼容矩阵如下： — 排它锁(X) 意向排它锁(IX) 共享锁(S) 意向共享锁(IS) 排它锁(X) N N N N 意向排它锁(IX) N OK N OK 共享锁(S) N N OK OK 意向共享锁(IS) N OK OK OK 按照上面的兼容性，如果不同事务之间的锁兼容，则当前加锁事务可以持有锁，如果有冲突则会等待其他事务的锁释放。 如果一个事务请求锁时，请求的锁与已经持有的锁冲突而无法获取时，互相等待就可能会产生死锁。 意向锁不会阻止除了全表锁定请求之外的任何锁请求。意向锁的主要目的是显示事务正在锁定某行或者正意图锁定某行。 2、InnoDB 中的锁2.1、准备工作示例的基础是一个只有两列的数据库表。 1234567891011mysql&gt; create database test;mysql&gt; use test;mysql&gt; CREATE TABLE test (id int(11) NOT NULL,code int(11) NOT NULL, PRIMARY KEY(id), KEY (code)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; mysql&gt; INSERT INTO test(id,code) values(1,1),(10,10); 数据表test只有两列，id是主键索引，code是普通的索引(注意，一定不要是唯一索引)，并初始化了两条记录，分别是(1,1),(10,10)。这样，我们验证唯一键索引就可以使用id列，验证普通索引(非唯一键二级索引)时就使用code列。 2.2、查看锁状态的方法要看到锁的情况，必须手动开启多个事务，其中一些锁的状态的查看则必须使锁处于waiting状态，这样才能在mysql的引擎状态日志中看到。 命令： 1mysql&gt; show engine innodb status; 这条命令能显示最近几个事务的状态、查询和写入情况等信息。当出现死锁时，命令能给出最近的死锁明细。 2.3、Record Locks 记录锁record lock 是 对 index record 加锁的一种锁。例如 SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; 阻止其他事务 插入、更新、删除 t.c1=10 的行； record lock 总是对 index 加锁，即使表没有定义任何索引。在这种情况下 InnoDB 会添加一个隐藏主键； 查看记录锁开启第一个事务，不提交，测试完之后回滚。 1234567&gt; mysql&gt; start transaction;&gt; Query OK, 0 rows affected (0.00 sec)&gt; &gt; mysql&gt; update test set id=2 where id=1;&gt; Query OK, 1 row affected (0.00 sec)&gt; Rows matched: 1 Changed: 1 Warnings: 0&gt; 事务加锁情况 12345678910&gt; mysql&gt; show engine innodb status\G;&gt; ... &gt; ------------&gt; TRANSACTIONS&gt; ------------&gt; ---TRANSACTION 366811, ACTIVE 690 sec&gt; 2 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 2&gt; MySQL thread id 785, OS thread handle 123145432457216, query id 729076 localhost 127.0.0.1 root&gt; ...&gt; 可以看到有一行被加了锁。由之前对锁的描述可以推测出，update语句给id=1这一行上加了一个X锁。 注意：X锁广义上是一种抽象意义的排它锁，即锁一般分为X模式和S模式，狭义上指row或者index上的锁，而Record锁是索引上的锁。为了不修改数据，可以用select ... for update语句，加锁行为和update、delete是一样的，insert加锁机制较为复杂，后面的章节会提到。 第一个事务保持原状，不要提交或者回滚，现在开启第二个事务。 12345&gt; mysql&gt; start transaction;&gt; Query OK, 0 rows affected (0.00 sec)&gt; &gt; mysql&gt; update test set id=3 where id=1;&gt; 执行update时，sql语句的执行被阻塞了。查看下事务状态: 123456789101112&gt; mysql&gt; show engine innodb status\G;&gt; ...&gt; ------- TRX HAS BEEN WAITING 4 SEC FOR THIS LOCK TO BE GRANTED:&gt; RECORD LOCKS space id 62 page no 3 n bits 72 index PRIMARY of table `test`.`test` trx id 366820 lock_mode X locks rec but not gap waiting&gt; Record lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 32&gt; 0: len 8; hex 0000000000000001; asc ;;&gt; 1: len 6; hex 0000000598e3; asc ;;&gt; 2: len 7; hex 7e000001a80896; asc ~ ;;&gt; &gt; ------------------&gt; ...&gt; 喜闻乐见，我们看到了这个锁的状态。状态标题是’事务正在等待获取锁’，描述中的lock_mode X locks rec but not gap就是本章节中的record记录锁，直译一下’X锁模式锁住了记录’。后面还有一句but not gap意思是只对record本身加锁，并不对间隙加锁，间隙锁的叙述见下一个章节。 2.4、Gap Locks 间隙锁​ gap lock 作用在索引记录之间的间隔，又或者作用在第一个索引之前，最后一个索引之后的间隙。不包括索引本身。例如，SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE;这条语句阻止其他事务插入10和20之间的数字，无论这个数字是否存在。gap lock 是性能和并发权衡的产物，只存在于部分事务隔离级别。 SELECT * FROM child WHERE id = 100; 如果id本身是唯一索引，那么 gap lock 是不生效的； 如果 id 和 其他的列 组成的联合索引是 唯一索引， 那么 gap lock 是生效的，锁定了 child[100][x] ~ child[100][y] 之间的记录； 如果 id 上 没有加索引 或者 不是唯一索引，gap lock 生效； 简单来说就是 gap lock 锁定的是一个范围（0~N条记录），而不是单条记录； ​ InnoDB中的Gap锁是“纯粹的抑制性锁”，这意味着它们的唯一目的是防止其他事务插入到Gap中。间隙锁可以共存。一个事务所采取的间隙锁并不会阻止另一个事务对同一间隙采取间隙锁。共享锁和独占锁之间没有区别。它们彼此不冲突，并且执行相同的功能。 禁用gap lock： 将 隔离级别 设为 READ COMMITTED ； 启用 innodb_locks_unsafe_for_binlog 查看间隙锁按照官方文档，where子句查询条件是唯一键且指定了值时，只有record锁，没有gap锁。如果where语句指定了范围，gap锁是存在的。这里只测试验证一下当指定非唯一键索引的时候，gap锁的位置，按照文档的说法，会锁定当前索引及索引之前的间隙。(指定了非唯一键索引,例如code=10，间隙锁仍然存在\) 开启第一个事务，锁定一条非唯一的普通索引记录 1234567891011&gt;mysql&gt; start transaction;&gt;Query OK, 0 rows affected (0.00 sec)&gt;&gt;mysql&gt; select * from test where code = 10 for update;&gt;+----+------+&gt;| id | code |&gt;+----+------+&gt;| 10 | 10 |&gt;+----+------+&gt;1 row in set (0.00 sec)&gt; 由于预存了两条数据，row(1,1)和row(10,10)，此时这个间隙应该是`1。我们先插入row(2,2)来验证下gap锁的存在，再插入row(0,0)来验证gap的边界。 按照间隙锁的官方文档定义，select * from test where code = 10 for update;会锁定code=10这个索引，并且会锁定code&lt;10的间隙。 开启第二个事务，在code=10之前的间隙中插入一条数据，看下这条数据是否能够插入。 12345&gt;mysql&gt; start transaction;&gt;Query OK, 0 rows affected (0.00 sec)&gt;&gt;mysql&gt; insert into test values(2,2);&gt; 插入的时候，执行被阻塞，查看引擎状态： 12345678910111213141516&gt;mysql&gt; show engine innodb status\G;&gt;...&gt;---TRANSACTION 366864, ACTIVE 5 sec inserting&gt;mysql tables in use 1, locked 1&gt;LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 1&gt;MySQL thread id 793, OS thread handle 123145434963968, query id 730065 localhost 127.0.0.1 root update&gt;insert into test values(2,2)&gt;------- TRX HAS BEEN WAITING 5 SEC FOR THIS LOCK TO BE GRANTED:&gt;RECORD LOCKS space id 63 page no 4 n bits 72 index code of table `test`.`test` trx id 366864 lock_mode X locks gap before rec insert intention waiting&gt;Record lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 0&gt; 0: len 8; hex 800000000000000a; asc ;;&gt; 1: len 8; hex 000000000000000a; asc ;;&gt;&gt;------------------&gt;...&gt; 插入语句被阻塞了，lock_mode X locks gap before rec，由于第一个事务锁住了1到10之间的gap，需要等待获取锁之后才能插入。 如果再开启一个事务，插入(0,0) 1234&gt;mysql&gt; start transaction;&gt;mysql&gt; insert into test values(0,0);&gt;Query OK, 1 row affected (0.00 sec)&gt; 可以看到：指定的非唯一建索引的gap锁的边界是当前索引到上一个索引之间的gap\。 最后给出锁定区间的示例,首先插入一条记录(5,5) 123&gt;mysql&gt; insert into test values(5,5);&gt;Query OK, 1 row affected (0.00 sec)&gt; 开启第一个事务: 12345678910111213&gt;mysql&gt; start transaction;&gt;Query OK, 0 rows affected (0.00 sec)&gt;&gt;mysql&gt; select * from test where code between 1 and 10 for update;&gt;+----+------+&gt;| id | code |&gt;+----+------+&gt;| 1 | 1 |&gt;| 5 | 5 |&gt;| 10 | 10 |&gt;+----+------+&gt;3 rows in set (0.00 sec)&gt; 第二个事务，试图去更新code=5的行： 12345&gt;mysql&gt; begin;&gt;Query OK, 0 rows affected (0.00 sec)&gt;&gt;mysql&gt; update test set code=4 where code=5;&gt; 执行到这里，如果第一个事务不提交或者回滚的话，第二个事务一直等待直至mysql中设定的超时时间。 2.5、Next-Key LocksNext-key 锁实际上是Record锁和gap锁的组合。Next-key锁是在 下一个索引记录本身 和 索引之前的gap 加上S锁或是X锁(如果是读就加上S锁，如果是写就加X锁)。 ​ 默认情况下，InnoDB的事务隔离级别为RR，系统参数 innodb_locks_unsafe_for_binlog 的值为false。InnoDB使用next-key锁对索引进行扫描和搜索，这样就读取不到幻象行，避免了幻读的发生。 幻读是指在同一事务下，连续执行两次同样的SQL语句，第二次的SQL语句可能会返回之前不存在的行。 ​ 当查询的索引是唯一索引时，Next-key lock会进行优化，降级为Record Lock，此时Next-key lock仅仅作用在索引本身，而不会作用于gap和下一个索引上。 查看Next-key锁Next-key锁的作用范围如上述例子，数据表test初始化了row(1,1),row(10,10)，然后插入了row(5,5)。数据表如下： 12345678910&gt; mysql&gt; select * from test;&gt; +----+------+&gt; | id | code |&gt; +----+------+&gt; | 1 | 1 |&gt; | 5 | 5 |&gt; | 10 | 10 |&gt; +----+------+&gt; 3 rows in set (0.00 sec)&gt; 由于id是主键、唯一索引，mysql会做优化，因此使用code这个非唯一键的二级索引来举例说明。 对于code，可能的next-key锁的范围是： 12345&gt; (-∞,1]&gt; (1,5]&gt; (5,10]&gt; (10,+∞)&gt; 开启第一个事务，在code=5的索引上请求更新: 1234567891011&gt; mysql&gt; start transaction;&gt; Query OK, 0 rows affected (0.00 sec)&gt; &gt; mysql&gt; select * from test where code=5 for update;&gt; +----+------+&gt; | id | code |&gt; +----+------+&gt; | 5 | 5 |&gt; +----+------+&gt; 1 row in set (8.81 sec)&gt; 之前在gap锁的章节中介绍了，code=5 for update会在code=5的索引上加一个record锁，还会在1&lt;gap&lt;5的间隙上加gap锁。现在不再验证，直接插入一条(8,8)： 1234&gt; mysql&gt; start transaction;&gt; Query OK, 0 rows affected (0.00 sec)&gt; mysql&gt; insert into test values(8,8);&gt; insert处于等待执行的状态，这就是next-key锁生效而导致的结果。第一个事务，锁定了区间(1,5]，由于RR的隔离级别下next-key锁处于开启生效状态，又锁定了(5,10]区间。所以插入SQL语句的执行被阻塞。 解释：在这种情况下，被锁定的区域是code=5前一个索引到它的间隙，以及next-key的区域。code=5 for update对索引的锁定用区间表示，gap锁锁定了(1,5)，record锁锁定了{5}索引记录，next-key锁锁住了(5,10]，也就是说整个(1,10]的区间被锁定了。由于是for update，所以这里的锁都是X锁，因此阻止了其他事务中带有冲突锁定的操作执行。 如果我们在第一个事务中，执行了code &gt; 8 for update，在扫描过程中，找到了code=10，此时就会锁住10之前的间隙(5到10之间的gap)，10本身(record)，和10之后的间隙(next-key)。此时另一个事务插入(6,6),(9,9)和(11,11)都是不被允许的，只有在前一个索引5及5之前的索引和间隙才能执行插入(更新和删除也会被阻塞)。 2.6、Insert Intention Locks插入意向锁在行插入之前由INSERT设置一种间隙锁，是意向排它锁的一种。在多事务同时写入不同数据至同一索引间隙的时，不会发生锁等待，事务之间互相不影响其他事务的完成，这和间隙锁的定义是一致的。 假设一个记录索引包含4和7，其他不同的事务分别插入5和6，此时只要行不冲突，插入意向锁不会互相等待，可以直接获取。参照锁兼容/冲突矩阵。插入意向锁的例子不再列举，可以查看gap锁的第一个例子。 2.7、AUTO-INC Locks​ auto-inc lock 是一种特殊的表级锁。如果一个事务正在向表中插入值，那么任何其他事务都必须等待，以便由第一个事务插入的行接收连续的主键值。 ​ 我们一般把主键设置为AUTO_INCREMENT的列，默认情况下这个字段的值为0，InnoDB会在AUTO_INCREMENT修饰下的数据列所关联的索引末尾设置独占锁。在访问自增计数器时，InnoDB使用自增锁，但是锁定仅仅持续到当前SQL语句的末尾，而不是整个事务的结束，毕竟自增锁是表级别的锁，如果长期锁定会大大降低数据库的性能。由于是表锁，在使用期间，其他会话无法插入表中。 二、不同sql语句设置的锁​ 如果一个SQL语句要对二级索引(非主键索引)设置X模式的Record锁，InnoDB还会检索出相应的聚簇索引(主键索引)并对它们设置锁定。 1、 SELECT … FROM…不加锁SELECT ... FROM是快照读取，除了SERIALIZABLE的事务隔离级别，该SQL语句执行时不会加任何锁。 SERIALIZABLE级别下，SELECT语句的执行会在遇到的索引记录上设置S模式的next-key锁。但是对于唯一索引，只锁定索引记录，而不会锁定gap。 2、 UPDATE系列S锁读取(SELECT ... LOCK IN SHARE MODE)，X锁读取(SELECT ... FOR UPDATE)、更新UPDATE和删除DELETE这四类语句，采用的锁取决于搜索条件中使用的索引类型。 如果使用唯一索引，InnoDB仅锁定索引记录本身，不锁定间隙。 如果使用非唯一索引，或者未命中索引，InnoDB使用间隙锁或者next-key锁来锁定索引范围，这样就可以阻止其他事务插入锁定范围。 2.1、 UPDATE语句UPDATE ... WHERE ...在搜索遇到的每条记录上设置一个独占的next-key锁，如果是唯一索引只锁定记录。当UPDATE修改聚簇索引时，将对受影响的二级索引采用隐式锁，隐式锁是在索引中对二级索引的记录逻辑加锁，实际上不产生锁对象，不占用内存空间。 ​ 例如 update test set code=100 where id=10; 执行的时候 code=10 的索引(code是二级索引，见文中给出的建表语句)会被加隐式锁，只有隐式锁产生冲突时才会变成显式锁(如S锁、X锁)。即此时另一个事务也去更新 id=10 这条记录，隐式锁就会升级为显示锁。​ 这样做的好处是降低了锁的开销。 UPDATE可能会导致新的普通索引的插入。当新的索引插入之前，会首先执行一次重复索引检查。在重复检查和插入时，更新操作会对受影响的二级索引记录采用共享锁定(S锁)。 2.2、 DELETE语句DELETE FROM ... WHERE ...在搜索遇到的每条记录上设置一个独占的next-key锁,如果是唯一索引只锁定记录。 3、 INSERTINSERT区别于UPDATE系列单独列出，是因为它的处理方式较为特别。 插入行之前，会设置一种插入意向锁，插入意向锁表示插入的意图。如果其它事务在 要插入的位置 上设置了X锁，则无法获取插入意向锁，插入操作也因此阻塞。 INSERT在插入的行上设置X锁。该锁是一个Record锁，并不是next-key锁，即只锁定记录本身，不锁定间隙，因此不会阻止其他 session 在这行记录前的间隙中插入新的记录。 三、Deadlocks 死锁​ 死锁是指由于每个事务都持有对方需要的锁而无法进行其他事务的情况。因为这两个事务都在等待资源变得可用，所以都不会释放它持有的锁。 1、死锁示例该示例涉及两个客户端A和B。 首先，客户端A创建一个包含一行的表，然后开始事务。在事务中，A通过S在共享模式下选择该行来获得对该行的 锁定： 123456789101112131415mysql&gt; CREATE TABLE t (i INT) ENGINE = InnoDB;Query OK, 0 rows affected (1.07 sec)mysql&gt; INSERT INTO t (i) VALUES(1);Query OK, 1 row affected (0.09 sec)mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM t WHERE i = 1 LOCK IN SHARE MODE;+------+| i |+------+| 1 |+------+ 接下来，客户端B开始事务并尝试从表中删除该行： 1234mysql&gt; START TRANSACTION;Query OK, 0 rows affected (0.00 sec)mysql&gt; DELETE FROM t WHERE i = 1; 删除操作需要一个X锁。无法授予该S锁，因为它与客户端A持有的锁不兼容 ，因此该请求进入针对行和客户端B块的锁请求队列中。 最后，客户端A还尝试从表中删除该行： 123mysql&gt; DELETE FROM t WHERE i = 1;ERROR 1213 (40001): Deadlock found when trying to get lock;try restarting transaction 此处发生死锁是因为客户端A需要 X锁才能删除该行。但是，不能授予该锁定请求，因为客户端B已经有一个X锁定请求，并且正在等待客户端A释放其S锁定。由于B事先要求锁，所以SA持有的锁也不能 升级 X为X锁。结果， InnoDB为其中一个客户端生成错误并释放其锁。客户端返回此错误： 12ERROR 1213 (40001): Deadlock found when trying to get lock;try restarting transaction 届时，可以授予对另一个客户端的锁定请求，并从表中删除该行。 2、死锁检测和回滚​ InnoDB自动检测事务 死锁并回滚一个或多个事务以打破死锁。 InnoDB尝试选择要回滚的小事务，其中事务的大小由插入，更新或删除的行数确定。 ​ 如果死锁无法检测，通过设置 innodb_lock_wait_timeout 来解决； ​ 如果 一个事务被完整的会滚，那么它所持有的所有锁都会被释放；但如果由于出错仅仅一条sql被会滚，那么某些锁可能不会被释放；这是因为 InnoDB存储行锁的格式 无法确定后续的锁会被哪些sql持有； 要查看InnoDB用户事务中的最后一个死锁，请使用SHOW ENGINE INNODB STATUS命令。 如果频繁出现死锁，则说明事务结构或应用程序错误处理存在问题，请在innodb_print_all_deadlocks 启用该设置的情况下运行，以 将有关所有死锁的信息打印到 mysqld错误日志中 3、如何最小化并处理死锁InnoDB使用自动行级锁定。即使在仅插入或 删除 单行的事务中，也可能会遇到死锁。这是因为这些操作并不是真正的“ 原子 ”操作。它们会自动对插入或删除的行的（可能是多个）索引记录设置锁定。 您可以使用以下技术来处理死锁并减少发生死锁的可能性： 在任何时候，发出 SHOW ENGINE INNODB STATUS命令以确定最近死锁的原因。这可以帮助您调整应用程序以避免死锁。 如果频繁出现死锁警告引起关注，请通过启用innodb_print_all_deadlocks 配置选项来收集更广泛的调试信息 。有关每个死锁的信息，而不仅仅是最新的死锁，都记录在MySQL 错误日志中。完成调试后，请禁用此选项。 如果由于死锁而失败，请始终准备重新发出事务。死锁并不危险。请再试一次。 保持交易小巧且持续时间短，以使交易不易发生冲突。 进行一系列相关更改后立即提交事务，以减少冲突的发生。特别是，不要长时间未提交事务而使交互式 mysql会话保持打开状态。 如果您使用锁定读取（SELECT ... FOR UPDATE或 SELECT ... LOCK IN SHARE MODE），请尝试使用较低的隔离级别，例如 READ COMMITTED。 修改事务中的多个表或同一表中的不同行集时，每次都要以一致的顺序执行这些操作。然后，事务形成定义明确的队列，并且不会死锁。例如，组织数据库操作到功能在应用程序中，或调用存储程序，而不是编码的多个相似序列 INSERT，UPDATE以及 DELETE在不同的地方语句。 将选择好的索引添加到表中。然后，您的查询需要扫描更少的索引记录，并因此设置更少的锁。使用EXPLAIN SELECT以确定哪些索引MySQL认为最适合您的查询。 使用较少的锁定。如果你能负担得起，以允许 SELECT从一个旧的快照返回数据，不要添加条款FOR UPDATE或LOCK IN SHARE MODE给它。在READ COMMITTED这里使用隔离级别是件好事，因为同一事务中的每个一致性读取均从其自己的新快照读取。 如果没有其他帮助，请使用表级锁序列化事务。LOCK TABLES与事务表（例如InnoDB 表）一起使用的正确方法 是，以SET autocommit = 0（not START TRANSACTION）后跟来开始事务，直到明确提交事务后才LOCK TABLES调用 UNLOCK TABLES。例如，如果您需要写表 t1和从表中读取数据 t2，则可以执行以下操作： 1234SET autocommit=0;LOCK TABLES t1 WRITE, t2 READ, ...;... do something with tables t1 and t2 here ...COMMIT;UNLOCK TABLES; 表级锁可防止对表的并发更新，从而避免死锁，但代价是对繁忙系统的响应速度较慢。 序列化事务的另一种方法是创建一个仅包含一行的辅助“ 信号量 ”表。在访问其他表之前，让每个事务更新该行。这样，所有事务都以串行方式发生。请注意，InnoDB 在这种情况下，即时死锁检测算法也适用，因为序列化锁是行级锁。对于MySQL表级锁，必须使用超时方法来解决死锁。 四、Transaction Model 事务模型​ InnoDB 的事务模型 的目标是 将 multi-versioning 和two-phase locking 的最佳属性结合起来。默认情况下 InnoDB 的查询以 “非锁定一致性读” 和 行级锁 的方式运行。 1、autocommit, commit, rollback​ 在 InnoDB 中 所有的 用户操作都包裹在事务中；如果 启用了 autocommit，那么每条语句都是一个事务；默认情况下，每创建一个session，autocommit 都是启用的；如果SQL语句正确执行，那么就会自动条，否则会滚之前的操作，然后报错； 在 autocommit 情况下 如果 想把多条语句包裹在一个事务里，则需要如下格式： 12345START TRANSACTION;SELECT @A:=SUM(salary) FROM table1 WHERE type=1;UPDATE table2 SET summary=@A WHERE type=1;COMMIT;# 不想提交，则执行 ROLLBACK; 禁用 autocommit： 1SET autocommit=0; 2、存在的问题​ 平常开发过程中免不了对数据库的操作，并且还会有多个线程同时开启事务后对数据库进行访问，那此时不可避免就会出现多个线程之间交叉访问而导致数据的不一致，通过对数据库的隔离级别进行设置可以保证各线程数据获取的准确性。 在介绍隔离级别之前先要弄清楚数据库在并发事务下会出现的一些状态： 1)、脏读脏读就是一个事务读取了另外一个事务未提交的数据。 事务2读取了事务1未提交的数据。 2)、不可重复读在同一事务中，两次读取同一数据，得到内容不同 例如事务1读取了某个数据，然后事务2更新了这个数据并提交，然后事务1又来读取了一次，那这两次读取的结果就会不一样。 3)、幻读在一个事务的两次查询中数据记录数不一致，例如有一个事务1查询了几列数据，而事务2在此时插入了新的几列数据，事务1在接下来的查询中，就会发现有几列数据是它先前所没有的。 不可重复读是针对于多次读取同一条数据出现不同结果，幻读是多次读取而产生的记录数不一样 3、隔离级别（Isolation）​ Isolation 是 ACID 中的 I；Isolation 是在多个事务同时进行更改和执行查询时，对性能与可靠性、一致性和结果再现性之间的平衡进行微调的设定。 ​ InnoDB 提供了 4 种隔离级别：READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, 和 SERIALIZABLE。默认的 隔离级别是 REPEATABLE READ。 ​ InnoDB 使用不同的锁策略 来实现隔离级别。REPEATABLE READ 用来操作重要的数据，保证 ACID；如果使用 READ UNCOMMITTED 或READ COMMITTED 可以降低锁的开销；SERIALIZABLE是一种比 SERIALIZABLE更严格的规则，一般用在专门的场景，比如 XA事务 或者 解决并发问题和死锁。 1)、read uncommitted可以看到未提交的数据（脏读），举个例子：别人说的话你都相信了，但是可能他只是说说，并不实际做。 2)、read committed读取提交的数据。但是，可能多次读取的数据结果不一致（不可重复读，幻读）。用读写的观点就是：读取的行数据，可以写。 3)、repeatable read(默认)可以重复读取，但有幻读。读写观点：读取的数据行不可写，但是可以往表中新增数据。在MySQL中，其他事务新增的数据，看不到，不会产生幻读。采用多版本并发控制（MVCC）机制解决幻读问题。 4)、serializable可读，不可写。像java中的锁，写数据必须等待另一个事务结束。 4、查看隔离级别1234-- 1.查看当前会话隔离级别select @@tx_isolation;-- 查看系统当前隔离级别select @@global.tx_isolation; 设置隔离级别 12345678-- 设置当前会话隔离级别set session transaction isolatin level repeatable read;-- 设置系统当前隔离级别set global transaction isolation level repeatable read;set tx_isolation='read-uncommitted';set tx_isolation='read-committed';set tx_isolation='repeatable-read';set tx_isolation='serializable'; 五、隔离级别实现原理1、MVCC参考： https://www.cnblogs.com/cjsblog/p/8365921.html MVCC的全称是“多版本并发控制”。这项技术使得InnoDB的事务隔离级别下执行一致性读操作有了保证，换言之，就是为了查询一些正在被另一个事务更新的行，并且可以看到它们被更新之前的值。这是一个可以用来增强并发性的强大的技术，因为这样的一来的话查询就不用等待另一个事务释放锁。这项技术在数据库领域并不是普遍使用的。一些其它的数据库产品，以及mysql其它的存储引擎并不支持它。 1.1、说明​ 网上看到大量的文章讲到MVCC都是说给每一行增加两个隐藏的字段分别表示行的创建时间以及过期时间，它们存储的并不是时间，而是事务版本号。 ​ 事实上，这种说法并不准确，严格的来讲，InnoDB会给数据库中的每一行增加三个字段，它们分别是DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID。 ​ 但是，为了理解的方便，我们可以这样去理解，索引接下来的讲解中也还是用这两个字段的方式去理解。 1.2、增删查改​ 在InnoDB中，给每行增加两个隐藏字段来实现MVCC，一个用来记录数据行的创建时间，另一个用来记录行的过期时间（删除时间）。在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 于是乎，默认的隔离级别（REPEATABLE READ）下，增删查改变成了这样： SELECT：读取创建版本小于或等于当前事务版本号，并且删除版本为空或大于当前事务版本号的记录。这样可以保证在读取之前记录是存在的。 INSERT：将当前事务的版本号保存至行的创建版本号 UPDATE：新插入一行，并以当前事务的版本号作为新行的创建版本号，同时将原记录行的删除版本号设置为当前事务版本号 DELETE：将当前事务的版本号保存至行的删除版本号 1.3、快照读和当前读 快照读：读取的是快照版本，也就是历史版本 当前读：读取的是最新版本 ​ 普通的SELECT就是快照读，而UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE是当前读。 2、一致性非锁定读和锁定读2.1、锁定读 在一个事务中，标准的SELECT语句是不会加锁，但是有两种情况例外。SELECT … LOCK IN SHARE MODE 和 SELECT … FOR UPDATE。 SELECT … LOCK IN SHARE MODE：给记录假设共享锁，这样一来的话，其它事务只能读不能修改，直到当前事务提交； SELECT … FOR UPDATE：给索引记录加锁，这种情况下跟UPDATE的加锁情况是一样的； 2.2、一致性非锁定读一致性读（consistent read）意味着 InnoDB 对一个 query 展示的数据是 多版本中一个时间点的 snapshot。 如果隔离级别是REPEATABLE READ，那么在同一个事务中的所有一致性读都读的是事务中第一个这样的读读到的快照； 如果是READ COMMITTED，那么一个事务中的每一个一致性读都会读到它自己刷新的快照版本。 Consistent read（一致性读）是READ COMMITTED和REPEATABLE READ隔离级别下普通SELECT语句默认的模式。一致性读不会给它所访问的表加任何形式的锁，因此其它事务可以同时并发的修改它们。 3、悲观锁和乐观锁 悲观锁，正如它的名字那样，数据库总是认为别人会去修改它所要操作的数据，因此在数据库处理过程中将数据加锁。其实现依靠数据库底层。 乐观锁，如它的名字那样，总是认为别人不会去修改，只有在提交更新的时候去检查数据的状态。通常是给数据增加一个字段来标识数据的版本。 4、锁有这样三种锁我们需要了解 Record Locks（记录锁）：在索引记录上加锁。 Gap Locks（间隙锁）：在索引记录之间加锁，或者在第一个索引记录之前加锁，或者在最后一个索引记录之后加锁。 Next-Key Locks：在索引记录上加锁，并且在索引记录之前的间隙加锁。它相当于是Record Locks与Gap Locks的一个结合。 假设一个索引包含以下几个值：10,11,13,20。那么这个索引的next-key锁将会覆盖以下区间： (negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) 了解了以上概念之后，接下来具体就简单分析下REPEATABLE READ隔离级别是如何实现的 5、理论分析​ 之所以说是理论分析，是因为要是实际操作证明的话我也不知道怎么去证明，毕竟作者水平实在有限。但是，这并不意味着我在此胡说八道，有官方文档为证。 ​ 这段话的大致意思是，在默认的隔离级别中，普通的SELECT用的是一致性读不加锁。而对于锁定读、UPDATE和DELETE，则需要加锁，至于加什么锁视情况而定。如果你对一个唯一索引使用了唯一的检索条件，那么只需锁定索引记录即可；如果你没有使用唯一索引作为检索条件，或者用到了索引范围扫描，那么将会使用间隙锁或者next-key锁以此来阻塞其它会话向这个范围内的间隙插入数据。 ​ 作者曾经有一个误区，认为按照前面说MVCC下的增删查改的行为就不会出现任何问题，也不会出现不可重复读和幻读。但其实是大错特错。 ​ 举个很简单的例子，假设事务A更新表中id=1的记录，而事务B也更新这条记录，并且B先提交，如果按照前面MVVC说的，事务A读取id=1的快照版本，那么它看不到B所提交的修改，此时如果直接更新的话就会覆盖B之前的修改，这就不对了，可能B和A修改的不是一个字段，但是这样一来，B的修改就丢失了，这是不允许的。 ​ 所以，在修改的时候一定不是快照读，而是当前读。 ​ 而且，前面也讲过只有普通的SELECT才是快照读，其它诸如UPDATE、删除都是当前读。修改的时候加锁这是必然的，同时为了防止幻读的出现还需要加间隙锁。 一致性读保证了可用重复读 间隙锁防止了幻读 回想一下 1、利用MVCC实现一致性非锁定读，这就有保证在同一个事务中多次读取相同的数据返回的结果是一样的，解决了不可重复读的问题 2、利用Gap Locks和Next-Key可以阻止其它事务在锁定区间内插入数据，因此解决了幻读问题 综上所述，默认隔离级别的实现依赖于MVCC和锁，再具体一点是一致性读和锁。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Netty-4.1.3X NIO与平台相关性]]></title>
    <url>%2Fjava%2Fnetty%2Fnetty-nio%2F</url>
    <content type="text"><![CDATA[Netty NIO 底层实现Netty 实现了 Reactor 模型，核心是 Selector； Netty 的 Selector 底层是基于 JDK 的 Selector 实现的，依赖于 操作系统的 NIO实现； 1234567891011121314151617public static SelectorProvider provider() &#123; synchronized (lock) &#123; if (provider != null) return provider; return AccessController.doPrivileged( new PrivilegedAction&lt;SelectorProvider&gt;() &#123; public SelectorProvider run() &#123; if (loadProviderFromProperty()) return provider; if (loadProviderAsService()) return provider; provider = sun.nio.ch.DefaultSelectorProvider.create(); return provider; &#125; &#125;); &#125;&#125; JDK DefaultSelectorProvider.create()下载 openjdk-8 的源码 搜索 SelectorProvider 即可发现，不同的操作系统 有不同的 DefaultSelectorProvider 实现： 在mac下 DefaultSelectorProvider 定义 封装： 123456789101112public class DefaultSelectorProvider &#123; /** * Prevent instantiation. */ private DefaultSelectorProvider() &#123; &#125; /** * Returns the default SelectorProvider. */ public static SelectorProvider create() &#123; return new sun.nio.ch.KQueueSelectorProvider(); &#125;&#125; KQueueSelectorProvider 定义 kqueue 实现； KQueueSelectorImpl 定义 kqueue 的实现逻辑； KQueueArrayWrapper 真正通过 jni 调用 操作系统实现 kqueue； 在window下 可以看到NIO只有 poll 在 solaris 下1234567891011121314public class DefaultSelectorProvider &#123; /** * Returns the default SelectorProvider. */ public static SelectorProvider create() &#123; String osname = AccessController .doPrivileged(new GetPropertyAction("os.name")); if (osname.equals("SunOS")) return createProvider("sun.nio.ch.DevPollSelectorProvider"); if (osname.equals("Linux")) return createProvider("sun.nio.ch.EPollSelectorProvider"); return new sun.nio.ch.PollSelectorProvider(); &#125;&#125; 可以看到如果 操作系统是 SunOS 下，则调用 DevPollSelectorProvider，否则调用 EPollSelectorProvider， 如果不能确定，默认调用 PollSelectorProvider； 可以看到 EPoll 的实现也是 jni 调用系统实现]]></content>
      <categories>
        <category>java</category>
        <category>netty</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.6 innodb doublewrite buffer]]></title>
    <url>%2Fmysql%2Fmysql-innodb-disk-doublewrite-buffer%2F</url>
    <content type="text"><![CDATA[一、脏页刷盘风险关于IO的最小单位： 1、数据库IO的最小单位是16K（MySQL默认，oracle是8K） 2、文件系统IO的最小单位是4K（也有1K的） 3、磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险（写入了一部分）： 二、doublewrite：两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 1、doublewrite解决了什么问题 一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了2K突然掉电，也就是说前2K数据是新的，后14K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页。redo只能加上旧、校检完整的数据页恢复一个脏块，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。 2、使用情景 当数据库正在从内存向磁盘写一个数据页是，数据库宕机，从而导致这个页只写了部分数据，这就是部分写失效，它会导致数据丢失。这时是无法通过重做日志恢复的，因为重做日志记录的是对页的物理修改，如果页本身已经损坏，重做日志也无能为力。 3、doublewrite工作流程 doublewrite由两部分组成，一部分为内存中的 doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 1、当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 2、接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 3、待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 4、doublewrite 的崩溃恢复 如果操作系统在将页写入磁盘的过程中发生崩溃，在恢复过程中，innodb存储引擎可以从共享表空间的doublewrite中找到该页的一个最近的副本，将其复制到表空间文件，再应用redo log，就完成了恢复过程。 因为有副本所以也不担心表空间中数据页是否损坏。 Q：为什么log write不需要doublewrite的支持？ A：因为 redo log 写入的单位就是512字节，也就是磁盘IO的最小单位，所以无所谓数据损坏。 三、doublewrite的副作用1、doublewrite 带来的写负载 1、double write是一个buffer, 但其实它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而硬盘的fsync性能是很慢的, 所以它会降低mysql的整体性能。 2、但是，doublewrite buffer写入磁盘共享表空间这个过程是连续存储，是顺序写，性能非常高，(约占写的%10)，牺牲一点写性能来保证数据页的完整还是很有必要的。 2、监控 doublewrite 工作负载12345678mysql&gt; show global status like '%dblwr%';+----------------------------+-------+| Variable_name | Value |+----------------------------+-------+| Innodb_dblwr_pages_written | 92384 || Innodb_dblwr_writes | 18862 |+----------------------------+-------+2 rows in set (0.01 sec) 关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes 开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。 而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在3左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。 3、关闭double write适合的场景1、海量DML 2、不惧怕数据损坏和丢失 3、系统写负载成为主要负载 1234567mysql&gt; show variables like '%double%';+--------------------+-------+| Variable_name | Value |+--------------------+-------+| innodb_doublewrite | ON |+--------------------+-------+1 row in set (0.00 sec) 作为InnoDB的一个关键特性，doublewrite功能默认是开启的，但是在上述特殊的一些场景也可以视情况关闭，来提高数据库写性能。静态参数，配置文件修改，重启数据库。 4、为什么没有把double write里面的数据写到data page里面呢？ 1、double write里面的数据是连续的，如果直接写到data page里面，而data page的页又是离散的，写入会很慢。 2、double write里面的数据没有办法被及时的覆盖掉，导致double write的压力很大；短时间内可能会出现double write溢出的情况。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql5.6 innodb 索引组织表]]></title>
    <url>%2Fmysql%2Fmysql-innodb-disk-iot%2F</url>
    <content type="text"><![CDATA[B+树1、简介​ B+树是应文件系统所需而产生的一种B树的变形树（文件的目录一级一级索引，只有最底层的叶子节点（文件）保存数据）非叶子节点只保存索引，不保存实际的数据，数据都保存在叶子节点中，这不就是文件系统文件的查找吗? ​ 我们就举个文件查找的例子：有3个文件夹a、b、c， a包含b，b包含c，一个文件yang.c，a、b、c就是索引（存储在非叶子节点）， a、b、c只是要找到的yang.c的key，而实际的数据yang.c存储在叶子节点上。 ​ 所有的非叶子节点都可以看成索引部分！ 2、B+树的性质 (下面提到的都是和B树不相同的性质) 1、非叶子节点的子树指针与关键字个数相同；2、非叶子节点的子树指针p[i],指向关键字值属于[k[i],k[i+1]]的子树.(B树是开区间,也就是说B树不允许关键字重复, B+树允许重复)；3、为所有叶子节点增加一个链指针；4、所有关键字都在叶子节点出现(稠密索引). (且链表中的关键字恰好是有序的)；5、非叶子节点相当于是叶子节点的索引(稀疏索引),叶子节点相当于是存储(关键字)数据的数据层；6、更适合于文件系统； 索引组织表（IOT）​ 索引组织表 (index organized table, IOT) 就是 数据存储在一个索引结构中的表。存储在堆中的表是无组织的 (也就是说，只要有可用的空间，数据可以放在任何地方)，IOT中的数据则按主键存储和排序。对你的应用来说，IOT表和一个“常规”表并无二致。 ​ 索引组织表的数据按主键排序手段被存储在B-树索引中，除了存储主键列值外还存储非键列的值。普通索引只存储索引列，而索引组织表则存储表的所有列的值。 ` IOT`有什么意义呢？使用堆组织表时，我们必须为表和表主键上的索引分别留出空间。而IOT不存在主键的空间开销，因为索引就是数据，数据就是索引，二者已经合二为一。但是，IOT带来的好处并不止于节约了磁盘空间的占用，更重要的是大幅度降低了I/O，减少了访问缓冲区缓存(尽管从缓冲区缓存获取数据比从硬盘读要快得多，但缓冲区缓存并不免费，而且也绝对不是廉价的。每个缓冲区缓存获取都需要缓冲区缓存的多个闩，而闩是串行化设备，会限制应用的扩展能力)IOT适用的场合 1、完全由主键组成的表。这样的表如果采用堆组织表，则表本身完全是多余的开销，因为所有的数据全部同样也保存在索引里，此时，堆表是没用的。 2、代码查找表。如果你只会通过一个主键来访问一个表，这个表就非常适合实现为IOT. 3、如果你想保证数据存储在某个位置上，或者希望数据以某种特定的顺序物理存储，IOT就是一种合适的结构。 IOT提供如下的好处 提高缓冲区缓存效率，因为给定查询在缓存中需要的块更少。 减少缓冲区缓存访问，这会改善可扩缩性。 获取数据的工作总量更少，因为获取数据更快。 每个查询完成的物理I/O更少，因为对于任何给定的查询，需要的块更少，而且对地址记录的一个物理 I/O 很可能可以获取所有地址（而不只是其中一个地址，但堆表实现就只是获取一个地址）。如果经常在一个主键或惟一键上使用BETWEEN 查询也是如此，因为相近的记录存在一起，查询时引入的逻辑IO和物理IO都会更少。 从一个简单的表开始1234567create table user( id int primary key, age int, height int, weight int, name varchar(32))engine = innoDb; 相信只要入门数据库的同学都可以理解这个语句，我们也将从这个最简单的表开始，一步步地理解MySQL的索引结构。 首先，我们往这个表中插入一些数据。 12345INSERT INTO user(id,age,height,weight,name)VALUES(2,1,2,7,'小吉');INSERT INTO user(id,age,height,weight,name)VALUES(5,2,1,8,'小尼');INSERT INTO user(id,age,height,weight,name)VALUES(1,4,3,1,'小泰');INSERT INTO user(id,age,height,weight,name)VALUES(4,1,5,2,'小美');INSERT INTO user(id,age,height,weight,name)VALUES(3,5,6,7,'小蔡'); 我们来查一下，看看这些数据是否已经放入表中。 1234567891011mysql&gt; select * from user;+----+------+--------+--------+--------+| id | age | height | weight | name |+----+------+--------+--------+--------+| 1 | 4 | 3 | 1 | 小泰 || 2 | 1 | 2 | 7 | 小吉 || 3 | 5 | 6 | 7 | 小蔡 || 4 | 1 | 5 | 2 | 小美 || 5 | 2 | 1 | 8 | 小尼 |+----+------+--------+--------+--------+5 rows in set (0.00 sec) 可以看到，数据已经完整地放到了我们创建的user表中。 但是不知道大家发现了什么没有，好像发生了一件非常诡异的事情，我们插入的数据好像乱序了… MySQL好像悄悄的给我们按照id排了个序。 为什么会出现MySQL在我们没有显式排序的情况下，默默帮我们排了序呢？它是在什么时候进行排序的？ 页的引入​ 不知道大家毕业多长时间了，作为一个刚学完操作系统不久的学渣，页的概念依旧在脑中还没有变凉。其实MySQL中也有类似页的逻辑存储单位，听我慢慢道来。 ​ 在操作系统的概念中，当我们往磁盘中取数据，假设要取出的数据的大小是1KB，但是操作系统并不会只取出这1kb的数据，而是会取出4KB的数据，因为操作系统的一个页表项的大小是4KB。那为什么我们只需要1KB的数据，但是操作系统要取出4KB的数据呢？ ​ 这就涉及到一个程序局部性的概念，大概就是“一个程序在访问了一条数据之后，在之后会有极大的可能再次访问这条数据和访问这条数据的相邻数据”，所以索性直接加载4KB的数据到内存中，下次要访问这一页的数据时，直接从内存中找，可以减少磁盘IO次数，我们知道，磁盘IO是影响程序性能主要的因素，因为磁盘IO和内存IO的速度是不可同日而语的。 ​ 或许看完上面那一大段描述，还是有些抽象，所以我们索性回到数据库层面中，重新理解页的概念。 ​ 抛开所有东西不谈，假设还是我们刚才插入的那些数据，我们现在要找id = 5的数据，依照最原始的方式，我们一定会想到的就是——遍历，没错，这也是我们刚开始学计算机的时候最常用的寻找数据的方式。那么我们就来看看，以遍历的方式，我们找到id=5的数据，需要经历几次磁盘IO。 ​ 首先，我们得先从id=1的数据开始读起，然后判断是否是我们需要的数据，如果不是，就再取id=2的数据，再进行判断，循环往复。毋庸置疑，在MySQL帮我们排好序之后，我们需要经历五次磁盘IO，才能将5号数据找到并读出来。 那么我们再来看看引入页的概念之后，我们是如何读数据的。 ​ 在引入页的概念之后，MySQL会将多条数据存在一个叫“页”的数据结构中，当MySQL读取id=1的数据时，会将id=1数据所在的页整页读到内存中，然后在内存中进行遍历判断，由于内存的IO速度比磁盘高很多，所以相对于磁盘IO，几乎可以忽略不计，那么我们来看看这样读取数据我们需要经历几次磁盘IO（假设每一页可以存4条数据）。 ​ 那么我们第一次会读取id=1的数据，并且将id=1到id=4的数据全部读到内存中，这是第一次磁盘IO，第二次将读取id=5的数据到内存中，这是第二次磁盘IO。所以我们只需要经历2次磁盘IO就可以找到id=5的这条数据。 ​ 但其实，在MySQL的InnoDb引擎中，页的大小是16KB，是操作系统的4倍，而int类型的数据是4个字节，其它类型的数据的字节数通常也在4000字节以内，所以一页是可以存放很多很多条数据的，而MySQL的数据正是以页为基本单位组合而成的。 ​ 上图就是我们目前为止所理解的页的结构，他包含我们的多条数据，另外，MySQL的数据以页组成，那么它有指向下一页的指针和指向上一页的指针。 ​ 那么说到这里，其实可以回答第一个问题了，MySQL实际上就是在我们插入数据的时候，就帮我们在页中排好了序，至于为什么要排序，这里先卖个关子，接着往下看。 排序对性能的影响​ 上文中我们提了一个问题，为什么数据库在插入数据时要对其进行排序呢？我们按正常顺序插入数据不是也挺好的吗？ ​ 这就要涉及到一个数据库查询流程的问题了，无论如何，我们是绝对不会去平白无故地在插入数据时增加一个操作来让流程复杂化的，所以插入数据时排序一定有其目的，就是优化查询的效率。 ​ 而我们不难看出，页内部存放数据的模块，实质上就是一个链表的结构，链表的特点也就是增删快，查询慢，所以优化查询的效率是必须的。 基于单页模式存储的查询流程还是基于我们第一节中的那张页图来谈，我们插入了五条数据，id分别是从1-5，那么假设我要找一个表中不存在的id，假设id=-1，那么现在的查询流程就是： ​ 将id=1的这一整页数据取出，进行逐个比对，那么当我们找到id=1的这条数据时，发现这个id大于我们所需要找的哪个id，由于数据库在插入数据时，已经进行过排序了，那么在id=1的数据后面，都是id&gt;1的数据，所以我们就不需要再继续往下寻找了。 如果在插入时没有进行排序，那毋庸置疑，我们需要再继续往下进行寻找，逐条查找直到到结尾也没有找到这条数据，才能返回不存在这条数据。 当然，这只是排序优化的冰山一角，接着往下看。 上述页模式可能带来的问题说完了排序，下面就来分析一下我们在第一节中的那幅图，对于大数据量下有什么弊端，或者换一个说法，我们可以怎么对这个模式进行优化。 我们不难看出，在现阶段我们了解的页模式中，只有一个功能，就是在查询某条数据的时候直接将一整页的数据加载到内存中，以减少硬盘IO次数，从而提高性能。但是，我们也可以看到，现在的页模式内部，实际上是采用了链表的结构，前一条数据指向后一条数据，本质上还是通过数据的逐条比较来取出特定的数据。 那么假设，我们这一页中有一百万条数据，我们要查的数据正好在最后一个，那么我们是不是一定要从前往后找到这一条数据呢？如果是这样，我们需要查找的次数就达到了一百万次，即使是在内存中查找，这个效率也是不高的。那么有什么办法来优化这种情况下的查找效率呢？ 页目录的引入​ 我们可以打个比方，我们在看书的时候，如果要找到某一节，而这一节我们并不知道在哪一页，我们是不是就要从前往后，一节一节地去寻找我们需要的内容的页码呢？答案是否定的，因为在书的前面，存在目录，它会告诉你这一节在哪一页，例如，第一节在第1页、第二节在第13页。在数据库的页中，实际上也使用了这种目录的结构，这就是页目录。 ​ 那么引入页目录之后，我们所理解的页结构，就变成了这样： 分析一下这张图，实际上页目录就像是我们在看书的时候书本的目录一样，目录项1就相当于第一节，目录项2就相当于第二节，而每一条数据就相当于书本的每一页，这张图就可以解释成，第一节从第一页开始，第二节从第三页开始，而实际上，每个目录项会存放自己这个目录项当中最小的id，也就是说，目录项1中会存放1，而目录项2会存放3。 那么对比一下数据库在没有页目录时候的查找流程，假设要查找id=3的数据，在没有页目录的情况下，需要查找id=1、id=2、id=3，三次才能找到该数据，而如果有页目录之后，只需要先查看一下id=3存在于哪个目录项下，然后直接通过目录项进行数据的查找即可，如果在该目录项下没有找到这条数据，那么就可以直接确定这条数据不存在，这样就大大提升了数据库的查找效率，但是这种页目录的实现，首先就需要基于数据是在已经进行过排序的的场景下，才可以发挥其作用，所以看到这里，大家应该明白第二个问题了，为什么数据库在插入时会进行排序，这才是真正发挥排序的作用的地方。 页的扩展在上文中，我们基本上说明白了MySQL数据库中页的概念，以及它是如何基于页来减少磁盘IO次数的，以及排序是如何优化查询的效率的。 那么我们现在再来思考第三个问题：在开头说页的概念的时候，我们有说过，MySQL中每一页的大小只有16KB，不会随着数据的插入而自动扩容，所以这16KB不可能存下我们所有的数据，那么必定会有多个页来存储数据，那么在多页的情况下，MySQL中又是怎么组织这些页的呢？ 针对这个问题，我们继续来画出我们现在所了解的多页的结构图： 可以看到，在数据不断变多的情况下，MySQL会再去开辟新的页来存放新的数据，而每个页都有指向下一页的指针和指向上一页的指针，将所有页组织起来（这里修改了一下数据，将每一列的数据都放到了数据区中，其中第一个空格之前的代表id），第一页中存放id为1-5的数据，第二页存放id为6-10的数据，第三页存放id为11-15的数据，需要注意的是在开辟新页的时候，我们插入的数据不一定是放在新开辟的页上，而是要进行所有页的数据比较，来决定这条插入的数据放在哪一页上，而完成数据插入之后，最终的多页结构就会像上图中画的那样。 多页模式​ 在多页模式下，MySQL终于可以完成多数据的存储了，就是采用开辟新页的方式，将多条数据放在不同的页中，然后同样采用链表的数据结构，将每一页连接起来。那么可以思考第四个问题：多页情况下是否对查询效率有影响呢？ 多页模式对于查询效率的影响针对这个问题，既然问出来了，那么答案是肯定的，多页会对查询效率产生一定的影响，影响主要就体现在，多页其本质也是一个链表结构，只要是链表结构，查询效率一定不会高。 假设数据又非常多条，数据库就会开辟非常多的新页，而这些新页就会像链表一样连接在一起，当我们要在这么多页中查询某条数据时，它还是会从头节点遍历到存在我们要查找的那条数据所存在的页上，我们好不容易通过页目录优化了页中数据的查询效率，现在又出现了以页为单位的链表，这不是前功尽弃了吗？ 如何优化多页模式由于多页模式会影响查询的效率，那么肯定需要有一种方式来优化多页模式下的查询。相信有同学已经猜出来了，既然我们可以用页目录来优化页内的数据区，那么我们也可以采取类似的方式来优化这种多页的情况。 是的，页内数据区和多页模式本质上都是链表，那么的确可以采用相同的方式来对其进行优化，它就是目录页。 所以我们对比页内数据区，来分析如何优化多页结构。在单页时，我们采用了页目录的目录项来指向一行数据，这条数据就是存在于这个目录项中的最小数据，那么就可以通过页目录来查找所需数据。 所以对于多页结构也可以采用这种方式，使用一个目录项来指向某一页，而这个目录项存放的就是这一页中存放的最小数据的索引值。和页目录不同的地方在于，这种目录管理的级别是页，而页目录管理的级别是行。 那么分析到这里，我们多页模式的结构就会是下图所示的这样： 存在一个目录页来管理页目录，目录页中的数据存放的就是指向的那一页中最小的数据。 这里要注意的一点是：其实目录页的本质也是页，普通页中存的数据是项目数据，而目录页中存的数据是普通页的地址。 假设我们要查找id=19的数据，那么按照以前的查找方式，我们需要从第一页开始查找，发现不存在那么再到第二页查找，一直找到第四页才能找到id=19的数据，但是如果有了目录页，就可以使用id=19与目录页中存放的数据进行比较，发现19大于任何一条数据，于是进入id=16指向的页进行查找，直接然后再通过页内的页目录行级别的数据的查找，很快就可以找到id为19的数据了。随着数据越来越多，这种结构的效率相对于普通的多页模式，优势也就越来越明显。 回归正题，相信有对MySQL比较了解的同学已经发现了，我们画的最终的这幅图，就是MySQL中的一种索引结构——B+树。 B+树的引入B+树的特点我在《[从入门到入土]令人脱发的数据库底层设计》已经有详细叙述过了，在这里就不重复叙述了，如果有不了解的同学可以去看这篇博客。 我们接着往下聊，我们将我们画的存在目录页的多页模式图宏观化，可以形成下面的这张图： 这就是我们兜兜转转由简到繁形成的一颗B+树。和常规B+树有些许不同，这是一棵MySQL意义上的B+树，MySQL的一种索引结构，其中的每个节点就可以理解为是一个页，而叶子节点也就是数据页，除了叶子节点以外的节点就是目录页。 这一点在图中也可以看出来，非叶子节点只存放了索引，而只有叶子节点中存放了真实的数据，这也是符合B+树的特点的。 B+树的优势 由于叶子节点上存放了所有的数据，并且有指针相连，每个叶子节点在逻辑上是相连的，所以对于范围查找比较友好。 B+树的所有数据都在叶子节点上，所以B+树的查询效率稳定，一般都是查询3次。 B+树有利于数据库的扫描。 B+树有利于磁盘的IO，因为他的层高基本不会因为数据扩大而增高（三层树结构大概可以存放两千万数据量。 页的完整结构说完了页的概念和页是如何一步一步地组合称为B+树的结构之后，相信大家对于页都有了一个比较清楚的认知，所以这里就要开始说说官方概念了，基于我们上文所说的，给出一个完整的页结构，也算是对上文中自己理解页结构的一种补充。 上图为 Page 数据结构，File Header 字段用于记录 Page 的头信息，其中比较重要的是 FIL_PAGE_PREV 和 FIL_PAGE_NEXT 字段，通过这两个字段，我们可以找到该页的上一页和下一页，实际上所有页通过两个字段可以形成一条双向链表。 Page Header 字段用于记录 Page 的状态信息。接下来的 Infimum 和 Supremum 是两个伪行记录，Infimum（下确界）记录比该页中任何主键值都要小的值，Supremum （上确界）记录比该页中任何主键值都要大的值，这个伪记录分别构成了页中记录的边界。 User Records 中存放的是实际的数据行记录，具体的行记录结构将在本文的第二节中详细介绍。Free Space 中存放的是空闲空间，被删除的行记录会被记录成空闲空间。Page Directory 记录着与二叉查找相关的信息。File Trailer 存储用于检测数据完整性的校验和等数据。 引用来源：https://www.cnblogs.com/bdsir/p/8745553.html 基于B+树聊聊MySQL的其它知识点看到这里，我们已经了解了MySQL从单条数据开始，到通过页来减少磁盘IO次数，并且在页中实现了页目录来优化页中的查询效率，然后使用多页模式来存储大量的数据，最终使用目录页来实现多页模式的查询效率并形成我们口中的索引结构——B+树。既然说到这里了，那我们就来聊聊MySQL的其他知识点。 聚簇索引和非聚簇索引关于聚簇索引和非聚簇索引在[从入门到入土]令人脱发的数据库底层设计这篇文章中已经有了详细的介绍，这里简单地说说，所谓聚簇索引，就是将索引和数据放到一起，找到索引也就找到了数据，我们刚才看到的B+树索引就是一种聚簇索引，而非聚簇索引就是将数据和索引分开，查找时需要先查找到索引，然后通过索引回表找到相应的数据。InnoDB有且只有一个聚簇索引，而MyISAM中都是非聚簇索引。 联合索引的最左前缀匹配原则在MySQL数据库中不仅可以对某一列建立索引，还可以对多列建立一个联合索引，而联合索引存在一个最左前缀匹配原则的概念，如果基于B+树来理解这个最左前缀匹配原则，相对来说就会容易很很多了。 首先我们基于文首的这张表建立一个联合索引： 1create index idx_obj on user(age asc,height asc,weight asc) 我们已经了解了索引的数据结构是一颗B+树，也了解了B+树优化查询效率的其中一个因素就是对数据进行了排序，那么我们在创建idx_obj这个索引的时候，也就相当于创建了一颗B+树索引，而这个索引就是依据联合索引的成员来进行排序，这里是age,height,weight。 看过我之前那篇博客的同学知道，InnoDB中只要有主键被定义，那么主键列被作为一个聚簇索引，而其它索引都将被作为非聚簇索引，所以自然而然的，这个索引就会是一个非聚簇索引。 所以根据这些我们可以得出结论： idx_obj这个索引会根据age,height,weight进行排序 idx_obj这个索引是一个非聚簇索引，查询时需要回表 根据这两个结论，首先需要了解的就是，如何排序？ 单列排序很简单，比大小嘛，谁都会，但是多列排序是基于什么原则的呢（重点）？ 实际上在MySQL中，联合索引的排序有这么一个原则，从左往右依次比较大小，就拿刚才建立的索引举例子，他会先去比较age的大小，如果age的大小相同，那么比较height的大小，如果height也无法比较大小， 那么就比较weight的大小，最终对这个索引进行排序。 那么根据这个排序我们也可以画出一个B+树，这里就不像上文画的那么详细了，简化一下： 数据： B+树： 注意：此时由于是非聚簇索引，所以叶子节点不在有数据，而是存了一个主键索引，最终会通过主键索引来回表查询数据。 B+树的结构有了，就可以通过这个来理解最左前缀匹配原则了。 我们先写一个查询语句 1SELECT * FROM user WHERE age=1 and height = 2 and weight = 7 毋庸置疑，这条语句一定会走idx_obj这个索引。 那么我们再看一个语句： 1SELECT * FROM user WHERE height=2 and weight = 7 思考一下，这条SQL会走索引吗？ 答案是否定的，那么我们分析的方向就是，为什么这条语句不会走索引。 上文中我们提到了一个多列的排序原则，是从左到右进行比较然后排序的，而我们的idx_obj这个索引从左到右依次是age,height,weight，所以当我们使用height和weight来作为查询条件时，由于age的缺失，那么就无法从age来进行比较了。 看到这里可能有小伙伴会有疑问，那如果直接用height和weight来进行比较不可以吗？显然是不可以的，可以举个例子，我们把缺失的这一列写作一个问号，那么这条语句的查询条件就变成了?27，那么我们从这课B+树的根节点开始，根节点上有127和365，那么以height和weight来进行比较的话，走的一定是127这一边，但是如果缺失的列数字是大于3的呢？比如427，527，627，那么如果走索引来查询数据，将会丢失数据，错误查询。所以这种情况下是绝对不会走索引进行查询的。这就是最左前缀匹配原则的成因。 最左前缀匹配原则，MySQL会一直向右匹配直到遇到范围查询（&gt;、&lt;、between、like）就停止匹配，比如 a=3 and b=4 and c&gt;5 and d=6,如果建立(a,b,c,d)顺序的索引，d是无法使用索引的，如果建立(a,b,d,c)的索引则都可以使用到，a、b、d的顺序可以任意调整。 =和in可以乱序，比如 a=1 and b=2 and c=3 建立(a,b,c)索引可以任意顺序，MySQL的查询优化器会帮你优化成索引可以识别的形式。 根据我们了解的可以得出结论： 只要无法进行排序比较大小的，就无法走联合索引。 可以再看几个语句： 1SELECT * FROM user WHERE age=1 and height = 2 这条语句是可以走idx_obj索引的，因为它可以通过比较 (12?&lt;365)。 1SELECT * FROM user WHERE age=1 and weight=7 这条语句也是可以走ind_obj索引的，因为它也可以通过比较(1?7&lt;365)，走左子树，但是实际上weight并没有用到索引，因为根据最左匹配原则，如果有两页的age都等于1，那么会去比较height，但是height在这里并不作为查询条件，所以MySQL会将这两页全都加载到内存中进行最后的weight字段的比较，进行扫描查询。 1SELECT * FROM user where age&gt;1 这条语句不会走索引，但是可以走索引。这句话是什么意思呢？这条SQL很特殊，由于其存在可以比较的索引，所以它走索引也可以查询出结果，但是由于这种情况是范围查询并且是全字段查询，如果走索引，还需要进行回表，MySQL查询优化器就会认为走索引的效率比全表扫描还要低，所以MySQL会去优化它，让他直接进行全表扫描。 1SELECT * FROM user WEHRE age=1 and height&gt;2 and weight=7 这条语句是可以走索引的，因为它可以通过age进行比较，但是weight不会用到索引，因为height是范围查找，与第二条语句类似，如果有两页的height都大于2，那么MySQL会将两页的数据都加载进内存，然后再来通过weight匹配正确的数据。 为什么InnoDB只有一个聚簇索引，而不将所有索引都使用聚簇索引？因为聚簇索引是将索引和数据都存放在叶子节点中，如果所有的索引都用聚簇索引，则每一个索引都将保存一份数据，会造成数据的冗余，在数据量很大的情况下，这种数据冗余是很消耗资源的。 补充两个关于索引的点这两个点也是上次写关于索引的博客时漏下的，这里补上。 1.什么情况下会发生明明创建了索引，但是执行的时候并没有通过索引呢？ 科普时间：查询优化器 一条SQL语句的查询，可以有不同的执行方案，至于最终选择哪种方案，需要通过优化器进行选择，选择执行成本最低的方案。 在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案。这个成本最低的方案就是所谓的执行计划。 优化过程大致如下： 1、根据搜索条件，找出所有可能使用的索引2、计算全表扫描的代价3、计算使用不同索引执行查询的代价4、对比各种执行方案的代价，找出成本最低的那一个 。 参考：https://juejin.im/post/5d23ef4ce51d45572c0600bc 根据我们刚才的那张表的非聚簇索引，这条语句就是由于查询优化器的作用，造成没有走索引： 1SELECT * FROM user where age&gt;1 2.在稀疏索引情况下通常需要通过叶子节点的指针回表查询数据，什么情况下不需要回表？ 科普时间：覆盖索引 覆盖索引（covering index）指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。 当一条查询语句符合覆盖索引条件时，MySQL只需要通过索引就可以返回查询所需要的数据，这样避免了查到索引后再返回表操作，减少I/O提高效率。 如，表covering_index_sample中有一个普通索引 idx_key1_key2(key1,key2)。当我们通过SQL语句：select key2 from covering_index_sample where key1 = &#39;keytest&#39;;的时候，就可以通过覆盖索引查询，无需回表。 参考：https://juejin.im/post/5d23ef4ce51d45572c0600bc 例如: 1SELECT age FROM user where age = 1 这句话就不需要进行回表查询。 结语本篇文章着重聊了一下关于MySQL的索引结构，从零开始慢慢构建了一个B+树索引，并且根据这个过程谈了B+树是如何一步一步去优化查询效率的。 简单地归纳一下就是： 排序：优化查询的根本，插入时进行排序实际上就是为了优化查询的效率。页：用于减少IO次数，还可以利用程序局部性原理，来稍微提高查询效率。页目录：用于规避链表的软肋，避免在查询时进行链表的扫描。多页：数据量增加的情况下开辟新页来保存数据。目录页：“特殊的页目录”，其中保存的数据是页的地址。查询时可以通过目录页快速定位到页，避免多页的扫描。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3、kafka设计原理]]></title>
    <url>%2Fmessage-queue%2Fkafka%2F3-kafka-design-principle%2F</url>
    <content type="text"><![CDATA[1、broker 端设计架构1.1、消息设计 1.2、集群管理1.2.1、成员管理kafka 的自动化服务发现 和 成员管理 靠 zookeeper 实现； 每当一个 broker 启动时，它会将 自己 注册到 zookeeper 的一个节点（/brokers/ids/&lt;broker.id&gt;） 注册信息格式如下： 12345678910111213&#123; "listener_security_protocol_map":&#123; "PLAINTEXT":"PLAINTEXT" &#125;, "endpoints":[ "PLAINTEXT://172.18.1.51:9092" ], "jmx_port":-1, "host":"172.18.1.51", "timestamp":"1570677366545", "port":9092, "version":4&#125; listener_security_protocol_map： 该broker与外界通信使用的安全协议类型； endpoints：指定 broker 的 endpoint 列表；endpoint可以配置多个，每种协议一个，只是端口号不能冲突； rack：指定broker机架信息，若设置了机架信息，kafka 会把副本分配在多个机架上； jmx_port： broker 的 JMX 监控端口； host：broker 主机名或 IP 地址； port：broker 服务端口号； timestamp：broker 启动时间； version：broker 当前版本号；不是kafka版本号，每个 broker版本，信息格式不一样； 注意 ephemeralOwner值，该值不是0，意味着 这是一个 zookeeper的临时节点； kafka 管理集群及其成员的主要流程： 1、broker启动时，在 zk 上创建对应的临时节点，同时还会创建一个 监听器（listener）监听该临时节点的状态； 2、一旦 broker 启动后，监听器会自动同步整个集群信息到该 broker 上； 3、一旦 broker 崩溃，broker 与 zk 的会话断开，该临时节点会被自动清楚掉，监听器被触发，然后处理 broker 崩溃的后续事宜。 1.2.2、使用到的ZooKeeper 路径新版本的 producer 和 consumer 已经不再 需要连接 zookeeper 下图涵盖了 Kafka 用到的 ZooKeeper 节点： 1.3、副本与ISR设计副本：partition 在其他 broker 上的备份； ISR：与 leader partition 上 保持同步的副本集合； 问题： 如何判断 副本与leader partition是否同步？ 1.3.1、follower副本同步 一个partition的格式如上图，其中比较重要的位置信息如下： 起始位移（base offset）：表示该副本当前所含第一条消息的offset； 高水印值（high watermark，HW）：副本高水印值。保存了 最新一条”已提交“消息的位移。超过 HW 值的所有消息都被视为 ”未提交成功的“，因此 consumer 是看不到的。注意：不只有 leader partition 有 HW 值，每个 follower 都有 HW 值，只不过只有 leader 的 HW 值 可以决定 consumer 能看到的消息数； 日志末端位移（log end offset，LEO）：副本日志 下一条待写入消息的 offset，所有的 副本都需要维护自己的 LEO 信息。follower 向 leader 请求到数据后 会增加自己的 LEO。事实上 只有所有ISR副本都更新了对应的 LEO 之后 leader 才会 向右移动对应的 HW 值，来表明 消息写入成功。如下图 1.3.2、ISR 设计如何判定 ISR？ 1、0.9.0.0 版本之前 参数：replica.lag.max.messages=n，follower落后leader的消息数； 参数：replica.lag.time.max.ms=m， follower 在 m 毫秒内 无法 向 leader 请求数据； follower 落后 leader 的可能原因： 请求速度追不上：follower 所在的broker 的 网络 I/O开销过大，导致 获取消息速度过慢； 进程卡住：follower 在一段时间内 无法 向 leader 请求数据，比如频繁GC和进程bug等； 新创建的副本：如果用户新增了 副本数，新副本追赶 leader 进度期间 通常都是不同步的； 参数：replica.lag.max.messages 的缺陷：无法动态适配场景，容易导致follower 频繁的 踢出加入 ISR； 2、0.9.0.0 版本之后 参数：replica.lag.time.max.ms 默认值10s。用于检测由于慢和进程卡壳的滞后，即 follower 落后 leader 的 时间间隔。 删除了 replica.lag.max.messages 参数，这样，只要 follower 不是持续落后，就不会被反复踢出 ISR； 1.4、水印（watermark）和 leader epoch摘要： watermark机制的缺陷； leader epoch 解决 watermark 的缺陷； 1.4.1、LEO更新机制分类：leader 的 LEO 更新机制、follower 的 LEO 更新机制； follower 的 LEO 保存位置：【follower 所在 broker 的缓存上】，【leader 所在broker 的缓存上】；换句话说 leader 所在broker 上保存了 该 partition 下所有 follower 的 LEO； follower 的 LEO 更新机制： 更新【follower 所在 broker 的缓存】的 LEO，follower 发送 FETCH 请求从leader 拿到数据后，follower 开始向底层log 写数据，从而自动更新 LEO值； 更新【leader 所在 broker 的缓存】的 LEO，leader 收到 follower的 FETCH请求后，它首先 从自己的log 读取响应的数据，但在 给 follower 返回数据之前，它会 先去更新自己 broker上的对应follower 的 LEO值； leader 的 LEO 更新机制：和 follower 更新自己 的LEO相同，leader 在写入log 时 会自动更新自己的 LEO值； 1.4.2、HW更新机制follower 的 HW更新机制： follower.HW = min（follower.LEO，leader.HW）； follower 在写完log数据以后，就会尝试更新HW值； leader 的 HW更新机制：leader的 HW 会影响 consumer的 可见性，所以比较重要；在以下 4种情况下，会做更新HW的尝试： 副本称为 leader 时：有必要检查副本的状态； 集群中有broker崩溃导致 它的 follower被踢出 ISR时：拖后腿的 ISR没有了，有必要检查一下； producer 向 leader 写入消息时： leader 处理 follower FETCH 请求时： leader.HW = min（foreach（ISR.partition.LEO），（落后时间&lt; replica.lag.time.max.ms）partition.LEO）；选择【 ISR】 和 【落后于leader的时长不大于 replica.lag.time.max.ms 的所有副本】的最小 LEO值 作为 HW; 1.4.3、watermark备份机制的缺陷leader 的 HW更新 依赖于 下一轮的 follower FETCH才能完成，这可能引起如下问题： 备份数据丢失； 备份数据不一致； 问题1：备份数据丢失 上图中有两个副本：A和B。开始状态是A是leader。我们假设producer端min.insync.replicas设置为1，那么当producer发送两条消息给A后，A写入到底层log，此时Kafka会通知producer说这两条消息写入成功。 但是在broker端，leader和follower底层的log虽都写入了2条消息且分区HW已经被更新到2，但follower HW尚未被更新（也就是上面紫色颜色标记的第二步尚未执行）。倘若此时副本B所在的broker宕机，那么重启回来后B会自动把LEO调整到之前的HW值，故副本B会做日志截断(log truncation)，将offset = 1的那条消息从log中删除，并调整LEO = 1，此时follower副本底层log中就只有一条消息，即offset = 0的消息。 B重启之后需要给A发FETCH请求，但若A所在broker机器在此时宕机，那么Kafka会令B成为新的leader，而当A重启回来后也会执行日志截断，将HW调整回1。这样，位移=1的消息就从两个副本的log中被删除，即永远地丢失了。 这个场景丢失数据的前提是在min.insync.replicas=1时，一旦消息被写入leader端log即被认为是“已提交”，而延迟一轮FETCH RPC更新HW值的设计使得follower HW值是异步延迟更新的，倘若在这个过程中leader发生变更，那么成为新leader的follower的HW值就有可能是过期的，使得clients端认为是成功提交的消息被删除 问题2：备份数据不一致 这种情况的初始状态与情况1有一些不同的：A依然是leader，A的log写入了2条消息，但B的log只写入了1条消息。分区HW更新到2，但B的HW还是1，同时producer端的min.insync.replicas = 1。 这次我们让A和B所在机器同时挂掉，然后假设B先重启回来，因此成为leader，分区HW = 1。假设此时producer发送了第3条消息(绿色框表示)给B，于是B的log中offset = 1的消息变成了绿色框表示的消息，同时分区HW更新到2（A还没有回来，就B一个副本，故可以直接更新HW而不用理会A）之后A重启回来，需要执行日志截断，但发现此时分区HW=2而A之前的HW值也是2，故不做任何调整。此后A和B将以这种状态继续正常工作。 显然，这种场景下，A和B底层log中保存在offset = 1的消息是不同的记录，从而引发不一致的情形出现。 1.4.4、leader epoch 如何解决造成上述两个问题的根本原因在于HW值被用于衡量副本备份的成功与否以及在出现failture时作为日志截断的依据，但HW值的更新是异步延迟的，特别是需要额外的FETCH请求处理流程才能更新，故这中间发生的任何崩溃都可能导致HW值的过期。鉴于这些原因，Kafka 0.11引入了leader epoch来取代HW值。Leader端多开辟一段内存区域专门保存leader的epoch信息，这样即使出现上面的两个场景也能很好地规避这些问题。 所谓leader epoch实际上是一对值：（epoch，offset）。epoch表示leader的版本号，从0开始，当leader变更过1次时epoch就会+1，而offset则对应于该epoch版本的leader写入第一条消息的位移。因此假设有两对值： (0, 0) (1, 120) 则表示第一个leader从位移0开始写入消息；共写了120条[0, 119]；而第二个leader版本号是1，从位移120处开始写入消息。 leader broker中会保存这样的一个缓存，并定期地写入到一个checkpoint文件中。 当leader写底层log时它会尝试更新整个缓存——如果这个leader首次写消息，则会在缓存中增加一个条目；否则就不做更新。而每次副本重新成为leader时会查询这部分缓存，获取出对应leader版本的位移，这就不会发生数据不一致和丢失的情况。 下面我们依然使用图的方式来说明下利用leader epoch如何规避上述两种情况 一、规避数据丢失 上图左半边已经给出了简要的流程描述，这里不详细展开具体的leader epoch实现细节（比如OffsetsForLeaderEpochRequest的实现），我们只需要知道每个副本都引入了新的状态来保存自己当leader时开始写入的第一条消息的offset以及leader版本。这样在恢复的时候完全使用这些信息而非水位来判断是否需要截断日志。 二、规避数据不一致 同样的道理，依靠leader epoch的信息可以有效地规避数据不一致的问题。 总结 0.11.0.0版本的Kafka通过引入leader epoch解决了原先依赖水位表示副本进度可能造成的数据丢失/数据不一致问题。有兴趣的读者可以阅读源代码进一步地了解其中的工作原理。 源代码位置：kafka.server.epoch.LeaderEpochCache.scala （leader epoch数据结构）、kafka.server.checkpoints.LeaderEpochCheckpointFile（checkpoint检查点文件操作类）还有分布在Log中的CRUD操作。 1.5、日志存储设计1、kafka 日志 kafka的日志：不同于 请求日志，kafka日志 是 一种 专门为程序访问的 日志；更像是mysql中的binlog； kafka 把 消息体 和 元数据信息 打包成 一个record，以追加的方式 写入日志； kafka的日志设计：topic -&gt; 多个 partition（分区）-&gt; 每个partition 又可以细分为 段日志（segment log），包含 日志段文件 和 索引文件； 下图中 .log 结尾的 就是 segment log，.index 和 .timeindex 结尾的就是 对应的索引文件； 2、日志段文件 parition 分区名字：topic名-分区号； 日志段文件：00000000000000000196.log，每个 日志段文件大小都有上限，默认大小 1GB，通过 broker参数 log.segment.bytes 控制；日志段文件被填满以后，会自动创建一组新的 日志段文件和索引文件； 3、索引文件 除了 .log 文件外，kafka分区日志还包括 两个特殊的文件 .index 和 .timeindex； .index 是位移索引文件；kafka可以用二分查找 将整体时间复杂度 降到 O(logN); .timeindex 是 时间戳索引文件；可以根据时间戳找到一定范围的记录； 4、日志留存 kafka 会定期清除日志，而且清除的单位是 日志段文件；即清除 .log 日志 和 对应的 两个索引文件； 留存策略： 基于时间的留存策略：kafka会morn清除7天前的日志段数据；可以通过 broker参数 log.retention.{hours|minutes|ms}用于配置清除日志的时间间隔； 基于大小的留存策略：kafka 会 保存 log.retention.bytes 参数值大小 字节数 的 日志。默认值 是 -1，表示不限制大小； 注意：日志清除对于当前日志段是不生效的； 5、日志压实 compaction kafka可以对key相同的 日志进行和合并，只保留最新的 value； 1.6、通信协议（wire protocol） 1.7、controller设计1、controller 架构 2、controller 管理状态 controller维护的状态分类：每台 broker 上的分区副本 和 每个分区的 leader 信息；从维度上看，又分为 副本状态和分区状态；由此 引入 副本状态机 和 分区状态机 3、controller 职责 更新集群元数据信息：分区信息变更时，controller封装变更信息 发送给每个 broker； 创建topic：监控 zk的/brokers/topics 子节点变更，监听到新节点 触发topic创建逻辑； 删除topic：监听到/admin/delete_topics下的节点时，触发删除topic操作； 分区重分配：监听 /admin/reassign_partitions 节点下的变更； prefered leader 副本选举：分区的第一顺位副本作为 leader； topic 分区扩展：用户发起增加分区的操作，会在 /brokers/topics/topic_name 下写入新的分区目录； broker 加入集群：新加入的broker会在 /broker/ids 下创建 新的znode并写入 broker信息，对应的监听器会感知到变更； broker 崩溃：/broker/ids 下的znode小时，监听器拿到变更，执行broker退出逻辑； broker 受控关闭： controller leader 选举：/controller 节点 存储了当前 controller所在的brokerid，集群首次启动，都会抢着创建该节点，胜出的那个broker成为 controller，同时更新 /controller_epoch 节点的值； 4、controller 与 broker 通信 5、controller 组件 6、老板本 controller 的缺陷 7、新版本 controller 1.8、broker 请求处理1、Reactor模型 2、kafka的broker处理模型 1),KafkaServer该类代表了一个kafka Broker的生命周期，处理kafka启动或者停止所需要的所有功能。 2),SocketServer一个NIO 服务中心。线程模型是1个Acceptor线程，用来处理新的链接请求N个加工Processor线程。每个线程拥有一个他们自己的selector，主要负责IO请求及应答。 3),KafkaRequestHandler实际会在KafkaRequestHandlerPool中创建多个对象，负责加工处理request线程。会1创建M个处理Handler线程。负责处理request请求，将responses重新写会加工线程Processor，以便于其写回给客户端。 一个典型的 broker 请求流程如下： （1）启动 broker 启动 acceptor 线程A 启动 3个 process 线程 P1, P2, P3 创建 KafkaRequestHander 线程池 和 8个请求处理线程 H1 ~ H8 （2）broker 启动后，acceptor 线程 不断轮询是否存在 客户端的 新连接；P1~P3实时轮询 是否有 acceptor新发送的 socket连接通道 以及 请求队列 和 响应队列中是否有请求需要处理；H1 ~ H8 则 实时监控 请求队列中的新请求； （3）此时 client 向 broker 发送数据，首先 client 会 创建 与 该 broker 的Socket 连接； （4）acceptor 线程 监听到socket连接，接收，将连接发送给 P1~P3中的一个，假设是 P2； （5）P2 下一次轮询时 发现 acceptor 传送过来的新连接，将其注册到 Selector上 并开始监听其上的 入站请求； （6）现在 client 开始给 broker 发送 producer请求； （7）P2 监听到 有新的请求到来，故获取之，然后发送到请求队列中； （8）由于 H1~H8 实时监听请求队列，故必有一个线程最早发现 producer请求并开始处理，假设是 H5，H5从请求队列中取出并开始处理； （9）H5 线程请求处理完成，将响应结果放入 P2 对应的 响应队列； （10）P2 监听到 它的响应队列 有 响应，将响应取出 发送给 对应的 client； （11）client 接收响应，标记本次 producer 请求处理过程结束； 2、producer 端设计架构2.1、producer 端基本数据结构 2.2、工作流程 用户首先构建发送的消息对象 ProducerRecord，然后调用 KafkaProducer.send 进行发送； KafkaProducer 接收到消息后首先 对其进行序列化； 然后结合本地缓存的元数据信息一起发送给 pointer 去确定目标分区；最后追加 写入内存中的缓存池（accumulator）。此时 KafkaProducer.send 方法成功返回； KafkaProducer 负责 将缓存池 中的消息 分批次 发送给对应的 broker，完成真正的 消息发送逻辑； 3、consumer 端设计架构3.1、consumer group 状态机 3.2、group 管理协议 3.3、rebalance 场景分析 4、实现精确一次处理语义4.1、消息交付语义 最多一次（at more once）： 最少一次（at least once）： 精确一次（exactly once）： KafkaProducer端默认是 at least once，可以通过幂等性 实现 exactly once； KafkaConsumer 跟 位移提交时机有关系 先处理后提交位移，则是 at least once； 先提交位移后处理消息，则是 at more once； 4.2、幂等性producer（idempotent producer）需要 打开 producer 参数 enable.idempotence = true 通过 producer_id，分区号，和消息序列号，实现 单个producer 的 exactly once，多个之间则不能； 4.3、事务（transaction）]]></content>
      <categories>
        <category>mq</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.6 innodb存储引擎]]></title>
    <url>%2Fmysql%2Fmysql-innodb%2F</url>
    <content type="text"><![CDATA[官方文档：https://dev.mysql.com/doc/refman/5.6/en/innodb-storage-engine.html 目录 14.1 InnoDB简介 14.2 InnoDB和ACID模型 14.3 InnoDB多版本 14.4 InnoDB架构 14.5 InnoDB内存结构 14.6 InnoDB磁盘结构 14.7 InnoDB锁定和事务模型 14.8 InnoDB配置 14.9 InnoDB表压缩 14.10 InnoDB文件格式管理 14.11 InnoDB行格式 14.12 InnoDB磁盘I / O和文件空间管理 14.13 InnoDB和在线DDL 14.14 InnoDB启动选项和系统变量 14.15 InnoDB INFORMATION_SCHEMA表 14.16 InnoDB与MySQL性能架构的集成 14.17 InnoDB监视器 14.18 InnoDB备份和恢复 14.19 InnoDB和MySQL复制 14.20 InnoDB memcached插件 14.21 InnoDB故障排除 14.22 InnoDB限制 14.23 InnoDB的限制和限制 14.1 InnoDB简介InnoDB是一种兼顾了高可靠性和高性能的通用存储引擎。在MySQL 5.6中，InnoDB是默认的MySQL存储引擎。除非您配置了其他默认存储引擎，否则发出CREATE TABLE不带ENGINE= 子句的语句将创建一个InnoDB表。 InnoDB 包括适用于MySQL 5.1的InnoDB插件的所有功能，以及特定于MySQL 5.5及更高版本的新功能。 InnoDB的主要优势 它的DML操作遵循 ACID模型，并 具有具有 提交，回滚和 崩溃恢复 功能的事务， 以保护用户数据。有关更多信息，请参见 第14.2节“ InnoDB和ACID模型”。 行级锁定和Oracle风格的一致读取可提高多用户并发性和性能。有关更多信息，请参见第14.7节“ InnoDB锁定和事务模型”。 InnoDB表格将您的数据排列在磁盘上以基于主键优化查询 。每个 InnoDB表都有一个称为聚集索引的主键索引，该索引 组织数据以最小化主键查找的I / O。有关更多信息，请参见第14.6.2.1节“聚集索引和二级索引”。 维护数据 完整性， InnoDB支持 FOREIGN KEY约束。使用外键检查插入，更新和删除操作，以确保它们不会导致不同表之间的不一致。有关更多信息，请参见 第13.1.17.6节“外键约束”。 表14.1 InnoDB存储引擎功能 特征 支持 B树索引 是 备份/时间点恢复（在服务器中而不是在存储引擎中实现。） 是 集群数据库支持 没有 聚集索引 是 压缩数据 是 资料快取 是 加密数据 是（通过加密功能在服务器中实现；在MySQL 5.7和更高版本中，支持静态数据表空间加密。） 外键支持 是 全文搜索索引 是（MySQL 5.6和更高版本提供InnoDB对FULLTEXT索引的支持。） 地理空间数据类型支持 是 地理空间索引支持 是（MySQL 5.7和更高版本提供InnoDB对地理空间索引的支持。） 哈希索引 否（InnoDB在内部将哈希索引用于其自适应哈希索引功能。） 索引缓存 是 锁定粒度 行 MVCC 是 复制支持（在服务器中而不是在存储引擎中实现。） 是 储存限制 64TB T树索引 没有 交易次数 是 更新数据字典的统计信息 是 要将其功能InnoDB与MySQL随附的其他存储引擎进行比较，请参阅第15章，备用存储引擎中的“ 存储引擎功能”表 。 Table 15.1 Storage Engines Feature Summary Feature MyISAM Memory InnoDB Archive NDB B-tree indexes Yes Yes Yes No No Backup/point-in-time recovery (note 1) Yes Yes Yes Yes Yes Cluster database support No No No No Yes Clustered indexes No No Yes No No Compressed data Yes (note 2) No Yes Yes No Data caches No N/A Yes No Yes Encrypted data Yes (note 3) Yes (note 3) Yes (note 4) Yes (note 3) Yes (note 3) Foreign key support No No Yes No Yes (note 5) Full-text search indexes Yes No Yes (note 6) No No Geospatial data type support Yes No Yes Yes Yes Geospatial indexing support Yes No Yes (note 7) No No Hash indexes No Yes No (note 8) No Yes Index caches Yes N/A Yes No Yes Locking granularity Table Table Row Row Row MVCC No No Yes No No Replication support (note 1) Yes Limited (note 9) Yes Yes Yes Storage limits 256TB RAM 64TB None 384EB T-tree indexes No No No No Yes Transactions No No Yes No Yes Update statistics for data dictionary Yes Yes Yes Yes Yes 14.2 InnoDB和ACID模型该ACID模式是一组数据库设计原则强调的是，对于业务数据和关键任务应用重要的可靠性方面。MySQL包含诸如InnoDB存储引擎严格遵循ACID模型，因此数据不会损坏，结果也不会因软件崩溃和硬件故障等异常情况而失真。当您依赖于ACID的功能时，您无需重新发明一致性检查和崩溃恢复机制。如果您有其他软件保护措施，超可靠的硬件或可以容忍少量数据丢失或不一致的应用程序，则可以调整MySQL设置以牺牲一些ACID可靠性，以获得更高的性能或吞吐量。 以下各节讨论MySQL功能（尤其是InnoDB存储引擎）如何 与ACID模型的类别进行交互： 答：原子性。 C：一致性。 我：：隔离。 D：耐久性。 原子性ACID模型 的原子性方面主要涉及InnoDB 事务。相关的MySQL功能包括： 自动提交设置。 COMMIT 声明。 ROLLBACK 声明。 INFORMATION_SCHEMA 表中的 操作数据。 一致性ACID模型 的一致性方面主要涉及内部InnoDB处理，以防止数据崩溃。相关的MySQL功能包括： InnoDB doublewrite缓冲区。 InnoDB 崩溃恢复。 隔离ACID模型 的隔离方面主要涉及InnoDB 事务，尤其是适用于每个事务的隔离级别。相关的MySQL功能包括： 自动提交设置。 SET ISOLATION LEVEL 声明。 InnoDB 锁定 的底层细节。在性能调整期间，您可以通过INFORMATION_SCHEMA表格查看这些详细信息 。 耐用性ACID模型 的持久性方面涉及与特定硬件配置交互的MySQL软件功能。由于取决于您的CPU，网络和存储设备的功能的可能性很多，因此为具体的准则提供最复杂的方面。（这些准则可能采取购买“ 新硬件 ”的形式 。）相关的MySQL功能包括： InnoDB doublewrite buffer，由innodb_doublewrite 配置选项打开和关闭 。 配置选项 innodb_flush_log_at_trx_commit。 配置选项 sync_binlog。 配置选项 innodb_file_per_table。 存储设备（例如磁盘驱动器，SSD或RAID阵列）中的写缓冲区。 存储设备中由电池支持的缓存。 用来运行MySQL的操作系统，特别是它对fsync()系统调用的支持。 不间断电源（UPS）保护运行MySQL服务器并存储MySQL数据的所有计算机服务器和存储设备的电源。 您的备份策略，例如备份的频率和类型以及备份保留期。 对于分布式或托管数据应用程序，MySQL服务器的硬件所在的数据中心的特定特性，以及数据中心之间的网络连接。 14.3 InnoDB MVCCInnoDB是一个 多版本的存储引擎：它保留有关已更改行的旧版本的信息，以支持诸如并发和回滚之类的事务功能 。此信息存储在表空间中的数据结构中，该数据结构称为 回滚段（在Oracle中类似的数据结构之后）。InnoDB 使用回滚段中的信息来执行事务回滚中所需的撤消操作。它还使用该信息来构建行的早期版本，以实现 一致的读取。 在内部，InnoDB向数据库中存储的每一行添加三个字段。6个字节的DB_TRX_ID字段表示插入或更新该行的最后一个事务的事务标识符。同样，删除在内部被视为更新，在该更新中，行中的特殊位被设置为将其标记为已删除。每行还包含一个7字节的 DB_ROLL_PTR字段，称为滚动指针。回滚指针指向写入回滚段的Undo Log记录。如果行已更新，则Undo Log记录将包含在更新行之前重建行内容所必需的信息。一个6字节的DB_ROW_ID字段包含一个行ID，该行ID随着插入新行而单调增加。如果 InnoDB自动生成聚集索引，该索引包含行ID值。否则，该 DB_ROW_ID列不会出现在任何索引中。 回滚段中的Undo Log分为插入和更新Undo Log。插入Undo Log仅在事务回滚时才需要，并且在事务提交后可以立即将其丢弃。更新Undo Log也用于一致的读取中，但是只有在不存在为其InnoDB分配了快照的事务（ 一致的读取可能需要更新Undo Log中的信息来构建数据库的早期版本）后，才可以将其删除行。 定期提交您的事务，包括仅发出一致读取的事务。否则， InnoDB将无法丢弃更新Undo Log中的数据，并且回滚段可能会变得太大而填满表空间。 回滚段中的Undo Log记录的物理大小通常小于相应的插入或更新的行。您可以使用此信息来计算回滚段所需的空间。 在InnoDB多版本方案中，当您使用SQL语句删除行时，并不会立即将其从数据库中物理删除。InnoDB仅在丢弃为删除而编写的更新Undo Log记录时，才物理删除相应的行及其索引记录。此删除操作称为purge，它非常快，通常花费与执行删除操作的SQL语句相同的时间顺序。 如果您以大约相同的速率在表中以较小的批次插入和删除行，则由于所有“ 死 ”行，清除线程可能开始滞后并且表可能会变得越来越大 ，从而使所有内容都受磁盘约束慢。在这种情况下，请限制新行的操作，并通过调整innodb_max_purge_lag系统变量来向清除线程分配更多资源 。有关更多信息，请参见第14.14节“ InnoDB启动选项和系统变量”。 多版本索引和二级索引InnoDB多版本并发控制（MVCC）对二级索引的处理方式不同于聚簇索引。聚簇索引中的记录将就地更新，其隐藏的系统列指向Undo Log条目，可以从中重建记录的早期版本。与聚簇索引记录不同，辅助索引记录不包含隐藏的系统列，也不会就地更新。 更新二级索引列时，将对旧的二级索引记录进行删除标记，插入新记录，并最终清除带有删除标记的记录。当二级索引记录被删除标记或二级索引页被较新的事务更新时，InnoDB在聚集索引中查找数据库记录。在聚集索引中，DB_TRX_ID检查记录的记录，如果在启动读取事务后修改了记录，则从Undo Log中检索记录的正确版本。 如果二级索引记录被标记为删除或二级索引页被更新的事务更新， 则不使用覆盖索引技术。而不是从索引结构中返回值，而是InnoDB在聚集索引中查找记录。 但是，如果启用了 索引条件下推（ICP）优化，并且WHERE只能使用索引中的字段来评估部分条件，则MySQL服务器仍会将WHERE条件的这一部分下推到存储引擎，并使用指数。如果找不到匹配的记录，则避免聚集索引查找。如果找到了匹配的记录，即使在删除标记的记录中，也要在 InnoDB聚簇索引中查找记录。 14.4 InnoDB架构下图显示了构成InnoDB存储引擎体系结构的内存中和磁盘上的结构。有关每种结构的信息，请参见 第14.5节“ InnoDB内存结构”和 第14.6节“ InnoDB磁盘结构”。 图14.1 InnoDB架构 14.5 InnoDB内存结构 14.5.1缓冲池 14.5.2Change Buffer 14.5.3自适应哈希索引 14.5.4日志缓冲区 14.5.1 Buffer PoolBuffer Pool是主内存中的一个区域，在InnoDB访问表和索引数据时会在其中进行 缓存。Buffer Pool允许直接从内存中直接处理经常使用的数据，从而加快了处理速度。在专用服务器上，通常将多达80％的物理内存分配给缓冲池。 为了提高大容量读取操作的效率，Buffer Pool被分为多个页面，这些页面可能包含多个行。为了提高缓存管理的效率，Buffer Pool被实现为页面的链接列表。使用LRU算法的变体将很少使用的数据从缓存中老化掉 。 知道如何利用缓冲池将经常访问的数据保留在内存中是MySQL优化的重要方面。 缓冲池LRU算法使用最近最少使用（LRU）算法的变体，将缓冲池作为列表进行管理。当需要空间以将新页面添加到缓冲池时，将驱逐最近使用最少的页面，并将新页面添加到列表的中间。此中点插入策略将列表视为两个子列表： 最前面是最近访问过的新页面（“ 年轻 ”） 的子列表 在末尾，是最近访问的旧页面的子列表 图14.2缓冲池列表 该算法将大量页面保留在新的子列表中。旧的子列表包含较少使用的页面。这些页面是驱逐的候选对象 。 默认情况下，该算法的运行方式如下： 3/8的缓冲池专用于旧的子列表。 列表的中点是新子列表的尾部与旧子列表的头相交的边界。 当InnoDB将页面读入缓冲池时，它首先将其插入中点（旧子列表的头部）。可以读取页面，因为它是用户启动的操作（例如SQL查询）所必需的，或作为的自动执行的预读操作的一部分 InnoDB。 访问旧子列表中的页面 使其变为“ 年轻 ”，将其移至新子列表的头部。如果由于用户启动的操作而需要读取页面，则将立即进行首次访问，并使页面年轻。如果由于预读操作而读取了该页面，则第一次访问不会立即发生，并且在退出该页面之前可能根本不会发生。 随着数据库的运行，通过移至列表的尾部，缓冲池中未被访问的页面将“ 老化 ”。新的和旧的子列表中的页面都会随着其他页面的更新而老化。随着将页面插入中点，旧子列表中的页面也会老化。最终，未使用的页面到达旧子列表的尾部并被逐出。 默认情况下，查询读取的页面会立即移入新的子列表，这意味着它们在缓冲池中的停留时间更长。例如，针对mysqldump操作或SELECT不带WHERE子句的 语句 执行的表扫描可以将大量数据带入缓冲池，并驱逐同等数量的旧数据，即使不再使用新数据也是如此。同样，由预读后台线程加载且仅访问一次的页面将移到新列表的开头。这些情况可能会将常用页面推送到旧的子列表，在此它们会被逐出。有关优化此行为的信息，请参见 第14.8.3.2节“使缓冲池扫描具有抵抗力”和 第14.8.3.3节“配置InnoDB缓冲池预取（预读）”。 InnoDB标准监视器输出在BUFFER POOL AND MEMORY有关缓冲池LRU算法操作的部分中包含几个字段。有关详细信息，请参阅使用InnoDB Standard Monitor监视缓冲池。 缓冲池配置 那么innodb-buffer-pool-size的大小应该设置为什么呢？下面我们就开始谈到这个。 独立服务器 在一个独立的只使用InnoDB引擎的MySQL服务器中，根据经验，推荐设置innodb-buffer-pool-size为服务器总可用内存的80%。为什么不是90%或者100%呢？因为其它的东西也需要内存： 每个查询至少需要几K的内存(有时候是几M) 有各种其它内部的MySQL结构和缓存 InnoDB有一些结构是不用缓冲池的内存的(字典缓存，文件系统，锁系统和页哈希表等) 也有一些MySQL文件是在OS缓存里的(binary日志，relay日志,innodb事务日志等) 此处，你也必须为操作系统留出些内存 共享服务器 如果你的MySQL服务器与其它应用共享资源，那么上面80%的经验就不那么适用了。在这样的环境下，设置一个对的数字有点难度。首先让我们来统计一下InnoDB表的实际占用大小。执行如下查询： 123456789101112&gt; SELECT engine,&gt; count(*) as TABLES,&gt; concat(round(sum(table_rows)/1000000,2),'M') rows,&gt; concat(round(sum(data_length)/(1024*1024*1024),2),'G') DATA,&gt; concat(round(sum(index_length)/(1024*1024*1024),2),'G') idx,&gt; concat(round(sum(data_length+index_length)/(1024*1024*1024),2),'G') total_size,&gt; round(sum(index_length)/sum(data_length),2) idxfrac&gt; FROM information_schema.TABLES&gt; WHERE table_schema not in ('mysql', 'performance_schema', 'information_schema')&gt; GROUP BY engine&gt; ORDER BY sum(data_length+index_length) DESC LIMIT 10;&gt; 这会给出一个参考，让你知道如果你想缓存整个数据集应该为InnoDB缓冲池设置多少内存合适。不过大多数情况你不需要那样做，你只需要缓存你经常使用的数据集。设置好之后，我们来看看如何检查InnoDB缓冲池大小是否设置足够。在终端中，执行如下命令： 1234567891011121314&gt; $ mysqladmin ext -ri1 | grep Innodb_buffer_pool_reads&gt; | Innodb_buffer_pool_reads | 1832098003 |&gt; | Innodb_buffer_pool_reads | 595 |&gt; | Innodb_buffer_pool_reads | 915 |&gt; | Innodb_buffer_pool_reads | 734 |&gt; | Innodb_buffer_pool_reads | 622 |&gt; | Innodb_buffer_pool_reads | 710 |&gt; | Innodb_buffer_pool_reads | 664 |&gt; | Innodb_buffer_pool_reads | 987 |&gt; | Innodb_buffer_pool_reads | 1287 |&gt; | Innodb_buffer_pool_reads | 967 |&gt; | Innodb_buffer_pool_reads | 1181 |&gt; | Innodb_buffer_pool_reads | 949 |&gt; 你所看到的是从硬盘读取数据到缓冲池的次数(每秒)。上面的数据已经相当高了(幸运的是，这个服务器的IO设备能处理每秒4000的IO操作)，如果这个是OLTP系统，我建议提高innodb缓冲池的大小和如果必要增加服务器内存。 更改InnoDB缓冲池 最后，介绍如何更改innodb-buffer-pool-size。如果你运行的是MySQL 5.7，那么非常幸运，你可以在线更改这个变量，只需要以root身份执行如下查询： 12&gt; mysql&gt; SET GLOBAL innodb_buffer_pool_size=size_in_bytes;&gt; 这还没完，你仍然需要更改my.cnf文件，不过至少你不需要重启服务器让它生效。从mysql的错误日志中我们可以看到它生效的过程： 12345678&gt; [Note] InnoDB: Resizing buffer pool from 134217728 to 21474836480. (unit=134217728)&gt; [Note] InnoDB: disabled adaptive hash index.&gt; [Note] InnoDB: buffer pool 0 : 159 chunks (1302369 blocks) were added.&gt; [Note] InnoDB: buffer pool 0 : hash tables were resized.&gt; [Note] InnoDB: Resized hash tables at lock_sys, adaptive hash index, dictionary.&gt; [Note] InnoDB: Completed to resize buffer pool from 134217728 to 21474836480.&gt; [Note] InnoDB: Re-enabled adaptive hash index.&gt; 在更早的mysql版本就需要重启了，所以： 在my.cnf中设置一个innodb_buffer_pool_size合适的值 重启mysql服务器 您可以配置缓冲池的各个方面以提高性能。 理想情况下，您可以将Buffer Pool的大小设置为与实际一样大的值，从而为服务器上的其他进程留出足够的内存以运行而不会进行过多的分页。缓冲池越大，就越InnoDB像内存数据库一样，从磁盘读取一次数据，然后在后续读取期间从内存访问数据。使用innodb_buffer_pool_size 配置选项配置缓冲池大小 。 在具有足够内存的64位系统上，可以将缓冲池分成多个部分，以最大程度地减少并发操作之间的内存结构争用。有关详细信息，请参见第14.8.3.1节“配置多个缓冲池实例”。 您可以将频繁访问的数据保留在内存中，而不必考虑操作突然导致的活动高峰，这些操作会将大量不经常访问的数据带入缓冲池。有关详细信息，请参见 第14.8.3.2节“使缓冲池扫描具有抵抗力”。 您可以控制何时以及如何执行预读请求，以异步方式将页面预取到缓冲池中，从而预期很快将需要这些页面。有关详细信息，请参见第14.8.3.3节“配置InnoDB缓冲池预取（预读）”。 您可以控制何时进行后台冲洗，以及是否根据工作负荷动态调整冲洗速率。有关详细信息，请参见 第14.8.3.4节“配置缓冲池刷新”。 您可以配置如何InnoDB保留当前缓冲池状态，以免在服务器重新启动后进行冗长的预热。有关详细信息，请参见 第14.8.3.5节“保存和恢复缓冲池状态”。 使用InnoDB标准监视器监视缓冲池InnoDB可以使用访问的标准监视器输出， SHOW ENGINE INNODB STATUS提供有关缓冲池操作的度量。缓冲池度量标准位于BUFFER POOL AND MEMORY“ InnoDB标准监视器”输出中的部分，其 外观类似于以下内容： 123456789101112131415161718192021----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 2197815296; in additional pool allocated 0Dictionary memory allocated 155455Buffer pool size 131071Free buffers 92158Database pages 38770Old database pages 14271Modified db pages 619Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 4, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 322, created 38448, written 420830.00 reads/s, 222.30 creates/s, 159.47 writes/sBuffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead0.00/sLRU len: 38770, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0] 下表描述了InnoDB标准监视器报告的缓冲池度量 标准。 注意 InnoDB 标准监视器输出中 提供的每秒平均值基于自InnoDB上次打印标准监视器输出以来经过的时间 。 表14.2 InnoDB缓冲池指标 名称 描述 分配的总内存 为缓冲池分配的总内存（以字节为单位）。 分配了额外的池 为附加池分配的总内存（以字节为单位）。 分配的字典内存 为InnoDB数据字典分配的总内存，以字节为单位。 缓冲池大小 分配给缓冲池的页面总大小（以页为单位）。 可用缓冲区 缓冲池空闲列表的页面总大小。 数据库页面 缓冲池LRU列表的页面总大小。 旧数据库页面 缓冲池旧LRU子列表的页面总大小。 修改的数据库页面 缓冲池中当前修改的页面数。 待读 等待读入缓冲池的缓冲池页面数。 待写LRU 从LRU列表的底部开始写入的缓冲池中的旧脏页数。 等待写入刷新列表 检查点期间要刷新的缓冲池页面数。 待写单页 缓冲池中暂挂的独立页面写入数。 使页面年轻化 缓冲池LRU列表中变年轻的页面总数（已移至“ new ”页面的子列表的开头）。 页面不年轻 缓冲池LRU列表中未设置为年轻的页面总数（保留在“ old ”子列表中但未设置为年轻的页面）。 青少年 每秒平均访问缓冲池LRU列表中的旧页面所导致的页面年轻。有关更多信息，请参见此表后面的注释。 非年轻 每秒平均访问缓冲池LRU列表中的旧页面导致的页面不年轻。有关更多信息，请参见此表后面的注释。 阅读页面 从缓冲池读取的页面总数。 创建页面 在缓冲池中创建的页面总数。 写的页面 从缓冲池写入的页面总数。 读/秒 每秒平均每秒读取的缓冲池页面数。 创建/秒 每秒平均创建的缓冲池页面的每秒数量。 写/秒 每秒平均缓冲池页面写入数。 缓冲池命中率 从缓冲池内存与磁盘存储读取的页面的缓冲池页面命中率。 年青率 页面访问的平均命中率使页面更年轻。有关更多信息，请参见此表后面的注释。 不（成年率） 页面访问未使页面变年轻的平均命中率。有关更多信息，请参见此表后面的注释。 预读页面 预读操作的每秒平均数。 被逐出的页面无权访问 每秒从缓冲池访问而未访问的页面的平均值。 随机预读 随机预读操作的每秒平均数。 伦 缓冲池LRU列表的页面总大小。 unzip_LRU len 缓冲池unzip_LRU列表的页面总大小。 I / O总和 最近50秒内访问的缓冲池LRU列表页面的总数。 I / O电流 已访问的缓冲池LRU列表页面的总数。 I / O解压缩总和 已访问的缓冲池unzip_LRU列表页面的总数。 I / O解压缩 已访问的缓冲池unzip_LRU列表页面的总数。 名称 描述 注意事项： 该youngs/s指标仅适用于旧页面。它基于对页面的访问次数，而不是页面数。可以对给定页面进行多次访问，所有访问都计入在内。如果youngs/s在不进行大扫描时看到非常低的 值，则可能需要减少延迟时间或增加用于旧子列表的缓冲池的百分比。增加百分比会使旧的子列表变大，因此该子列表中的页面需要更长的时间才能移到尾部，这增加了再次访问这些页面并使它们变年轻的可能性。 该non-youngs/s指标仅适用于旧页面。它基于对页面的访问次数，而不是页面数。可以对给定页面进行多次访问，所有访问都计入在内。如果non-youngs/s执行大型表扫描时看不到较高的值（较高的youngs/s 值），请增加延迟值。 该young-making比率说明了对所有缓冲池页面的访问，而不仅仅是访问了旧子列表中的页面。该young-making速率和 not速率通常不会加总到整个缓冲池的命中率。旧子列表中的页面命中会导致页面移动到新子列表，但是新子列表中的页面命中只会导致页面与列表的头部保持一定距离时才移动到列表的头部。 not (young-making rate)是由于innodb_old_blocks_time未满足所定义的延迟，或者由于新子列表中的页面点击未导致页面移动到头部而导致页面访问未使页面变年轻的平均点击率 。此速率说明了对所有缓冲池页面的访问，而不仅仅是访问旧子列表中的页面。 缓冲池服务器状态变量和 INNODB_BUFFER_POOL_STATS表提供了许多与InnoDB标准监视器输出中相同的缓冲池度量 标准。有关更多信息，请参见 示例14.10，“查询INNODB_BUFFER_POOL_STATS表”。 14.5.2 Change BufferChange Buffer是一种特殊的数据结构，当二级索引页不在缓冲池(Buffer Pool)中时，它们 会缓存这些更改 。当页面通过其他读取操作加载到缓冲池中时，可能由INSERT， UPDATE或 DELETE操作（DML）导致的缓冲更改 将在以后合并。 图14.3Change Buffer 与聚簇索引不同，二级索引通常是非唯一的，并且二级索引中的插入以相对随机的顺序发生。同样，删除和更新可能会影响索引树中不相邻的二级索引页。当稍后通过其他操作将受影响的页读入缓冲池时，合并缓存的更改将避免从磁盘将辅助索引页读入缓冲池所需的大量随机访问I / O。 在系统大部分处于空闲状态或缓慢关闭期间运行的清除操作会定期将更新的索引页写入磁盘。与将每个值立即写入磁盘相比，清除操作可以更有效地为一系列索引值写入磁盘块。 当有许多受影响的行和许多辅助索引要更新时，Change Buffer合并可能需要几个小时。在此期间，磁盘I / O会增加，这可能会导致磁盘绑定查询的速度大大降低。提交事务之后，甚至在服务器关闭并重新启动之后，Change Buffer合并也可能继续发生（ 有关更多信息，请参见第14.21.2节“强制InnoDB恢复”）。 在内存中，Change Buffer占用了缓冲池的一部分。在磁盘上，Change Buffer是系统表空间的一部分，当数据库服务器关闭时，索引更改将存储在其中。 Change Buffer中缓存的数据类型由 innodb_change_buffering变量控制。有关更多信息，请参见 配置变更缓冲。您还可以配置最大Change Buffer大小。有关更多信息，请参见 配置Change Buffer最大大小。 如果索引包含降序索引列或主键包含降序索引列，则辅助索引不支持更改缓冲。 有关Change Buffer的常见问题的答案，请参见第A.16节“ MySQL 5.6 FAQ：InnoDBChange Buffer”。 配置Change Buffer在表上执行，和 操作时INSERT， 索引列的值（尤其是辅助键的值）通常处于未排序的顺序，需要大量的I / O才能使辅助索引保持最新状态。当相关页面不在 缓冲池中时，Change Buffer将 更改缓存到辅助索引条目 ，从而避免了不立即从磁盘读取页面而避免了昂贵的I / O操作。当页面加载到缓冲池中时，缓冲的更改将合并，更新的页面随后将刷新到磁盘。的UPDATEDELETEInnoDB当服务器接近空闲时以及在缓慢关闭期间，主线程会合并缓冲的更改 。 由于Change Buffer功能可以减少磁盘读写操作，因此它对于受I / O约束的工作负载（例如，具有大量DML操作的应用程序，例如批量插入）最有价值。 但是，Change Buffer占用了缓冲池的一部分，从而减少了可用于缓存数据页的内存。如果工作集几乎适合缓冲池，或者您的表具有相对较少的二级索引，则禁用更改缓冲可能很有用。如果工作数据集完全适合缓冲池，则更改缓冲不会带来额外的开销，因为它仅适用于不在缓冲池中的页面。 您可以InnoDB 使用innodb_change_buffering 配置参数来控制执行更改缓冲 的程度。您可以为插入，删除操作（最初将索引记录标记为删除）和清除操作（物理删除索引记录）启用或禁用缓冲。更新操作是插入和删除的组合。默认 innodb_change_buffering值为 all。 允许的innodb_change_buffering 值包括： all 默认值：缓冲区插入，删除标记操作和清除。 none 不要缓冲任何操作。 inserts 缓冲区插入操作。 deletes 缓冲区删除标记操作。 changes 缓冲插入和删除标记操作。 purges 缓冲在后台发生的物理删除操作。 您可以innodb_change_buffering在MySQL选项文件（my.cnf或 my.ini）中设置 参数，或使用SET GLOBAL 语句动态更改参数，该 语句需要足够的权限来设置全局系统变量。请参见 第5.1.8.1节“系统变量特权”。更改设置会影响新操作的缓冲。现有缓冲条目的合并不受影响。 配置Change Buffer最大大小该innodb_change_buffer_max_size 变量允许将Change Buffer的最大大小配置为Buffer Pool总大小的百分比。默认情况下， innodb_change_buffer_max_size设置为25。最大设置为50。 考虑innodb_change_buffer_max_size在具有大量插入，更新和删除活动的MySQL服务器上进行增加 ，其中Change Buffer合并不能跟上新的Change Buffer条目，从而导致Change Buffer达到其最大大小限制。 考虑innodb_change_buffer_max_size在使用静态数据进行报告的MySQL服务器上减少 存储空间，或者Change Buffer消耗的缓冲池共享的内存空间过多，从而导致页面比预期的更快地退出缓冲池。 使用代表性的工作负载测试不同的设置，以确定最佳配置。该 innodb_change_buffer_max_size 设置是动态的，允许在不重新启动服务器的情况下修改设置。 监视Change Buffer以下选项可用于Change Buffer监视： InnoDB标准监视器输出包括Change Buffer状态信息。要查看监视器数据，请发出该SHOW ENGINE INNODB STATUS语句。 1mysql&gt; SHOW ENGINE INNODB STATUS\G Change Buffer状态信息位于INSERT BUFFER AND ADAPTIVE HASH INDEX 标题下， 并显示类似以下内容： 12345678910-------------------------------------INSERT BUFFER AND ADAPTIVE HASH INDEX-------------------------------------Ibuf: size 1, free list len 0, seg size 2, 0 mergesmerged operations: insert 0, delete mark 0, delete 0discarded operations: insert 0, delete mark 0, delete 0Hash table size 4425293, used cells 32, node heap has 1 buffer(s)13577.57 hash searches/s, 202.47 non-hash searches/s 有关更多信息，请参见 第14.17.3节“ InnoDB标准监视器和锁定监视器输出”。 该 INFORMATION_SCHEMA.INNODB_METRICS 表提供了在InnoDB标准监视器输出中找到的大多数数据点 以及其他数据点。要查看Change Buffer度量标准以及每个度量标准的描述，请发出以下查询： 1mysql&gt; SELECT NAME, COMMENT FROM INFORMATION_SCHEMA.INNODB_METRICS WHERE NAME LIKE '%ibuf%'\G 有关INNODB_METRICS表用法的信息，请参见 第14.15.6节“ InnoDB INFORMATION_SCHEMA指标表”。 该 INFORMATION_SCHEMA.INNODB_BUFFER_PAGE 表提供有关缓冲池中每个页面的元数据，包括Change Buffer索引和Change Buffer位图页面。Change Buffer页面由标识 PAGE_TYPE。IBUF_INDEX是Change Buffer索引页面IBUF_BITMAP的页面类型，并且 是Change Buffer位图页面的页面类型。 警告 查询该INNODB_BUFFER_PAGE 表可能会带来很大的性能开销。为避免影响性能，请重现要在测试实例上调查的问题，然后在测试实例上运行查询。 例如，您可以查询该 INNODB_BUFFER_PAGE表以确定缓冲池页面总数中所包含的IBUF_INDEX和 的大概数量 IBUF_BITMAP。 12345678910mysql&gt; SELECT (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE WHERE PAGE_TYPE LIKE 'IBUF%') AS change_buffer_pages, (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE) AS total_pages, (SELECT ((change_buffer_pages/total_pages)*100)) AS change_buffer_page_percentage;+---------------------+-------------+-------------------------------+| change_buffer_pages | total_pages | change_buffer_page_percentage |+---------------------+-------------+-------------------------------+| 25 | 8192 | 0.3052 |+---------------------+-------------+-------------------------------+ 有关该INNODB_BUFFER_PAGE表提供的其他数据的信息 ，请参见 第21.30.1节“ INFORMATION_SCHEMA INNODB_BUFFER_PAGE表”。有关相关用法信息，请参见 第14.15.5节“ InnoDB INFORMATION_SCHEMA缓冲池表”。 Performance Schema 提供了Change Buffer互斥锁等待工具，以进行高级性能监视。要查看Change Buffer检测，请发出以下查询： 123456789mysql&gt; SELECT * FROM performance_schema.setup_instruments WHERE NAME LIKE '%wait/synch/mutex/innodb/ibuf%';+-------------------------------------------------------+---------+-------+| NAME | ENABLED | TIMED |+-------------------------------------------------------+---------+-------+| wait/synch/mutex/innodb/ibuf_bitmap_mutex | YES | YES || wait/synch/mutex/innodb/ibuf_mutex | YES | YES || wait/synch/mutex/innodb/ibuf_pessimistic_insert_mutex | YES | YES |+-------------------------------------------------------+---------+-------+ 有关监视InnoDB 互斥锁等待的信息，请参见 第14.16.1节“使用性能模式监视InnoDB Mutex等待”。 14.5.3自适应哈希索引自适应哈希索引（innodb-adaptive-hash）功能可以InnoDB 在不牺牲事务功能或可靠性的情况下，在工作负载和缓冲池有足够内存的适当组合的系统上，更像是内存数据库。自适应哈希索引功能由innodb_adaptive_hash_index 变量启用 ，或在服务器启动时由禁用 --skip-innodb-adaptive-hash-index。 根据观察到的搜索模式，使用索引关键字的前缀构建哈希索引。该前缀可以是任何长度，并且可能是哈希树索引中仅B树中的某些值出现。哈希索引是根据对经常访问的索引页面的需求而建立的。 如果表几乎完全适合主内存，则散列索引可以通过启用直接查找任何元素的方式来加速查询，从而将索引值转换为某种指针。InnoDB 具有监视索引搜索的机制。如果 InnoDB发现查询可以从构建哈希索引中受益，它会自动这样做。 在某些工作负载下，哈希索引查找的速度大大超过了监视索引查找和维护哈希索引结构的额外工作。在繁重的工作负载（例如多个并发连接）下，访问自适应哈希索引有时可能会成为争用的源。与 LIKE运算符和% 通配符也往往不会受益。对于无法从自适应哈希索引功能中受益的工作负载，将其关闭可减少不必要的性能开销。由于很难预先预测自适应哈希索引功能是否适合特定的系统和工作负载，因此请考虑启用和禁用该功能的基准测试。与早期版本相比，MySQL 5.6中的体系结构更改使其更适合禁用自适应哈希索引功能。 您可以在输出SEMAPHORES部分中 监视自适应哈希索引的使用和争用 SHOW ENGINE INNODB STATUS。如果在中创建的RW锁上有许多线程正在等待btr0sea.c，则禁用自适应哈希索引功能可能很有用。 有关哈希索引的性能特征的信息，请参见第8.3.8节“ B树和哈希索引的比较”。 14.5.4 Log Buffer日志缓冲区是存储区域，用于保存要写入磁盘上的日志文件的数据。日志缓冲区大小由innodb_log_buffer_size变量定义 。默认大小为16MB。日志缓冲区的内容会定期刷新到磁盘。较大的日志缓冲区使大型事务可以运行，而无需在事务提交之前将Redo Log数据写入磁盘。因此，如果您有更新，插入或删除许多行的事务，则增加日志缓冲区的大小可以节省磁盘I / O。 该 innodb_flush_log_at_trx_commit 变量控制如何将日志缓冲区的内容写入并刷新到磁盘。该 innodb_flush_log_at_timeout 变量控制日志刷新频率。 14.6 InnoDB磁盘结构 14.6.1表格 14.6.2索引 14.6.3表空间 14.6.4 InnoDB数据字典 14.6.5Doublewrite Buffer 14.6.6Redo Log 14.6.7Undo Log 本节介绍InnoDB磁盘上的结构和相关主题。 14.6.1 Tables 14.6.1.1创建InnoDB表 14.6.1.2在外部创建表 14.6.1.3导入InnoDB表 14.6.1.4移动或复制InnoDB表 14.6.1.5将表从MyISAM转换为InnoDB 14.6.1.6 InnoDB中的AUTO_INCREMENT处理 14.6.1.1创建InnoDB表要创建InnoDB表，请使用以下 CREATE TABLE语句。 1CREATE TABLE t1 (a INT, b CHAR (20), PRIMARY KEY (a)) ENGINE=InnoDB; 如果将ENGINE=InnoDB 子句InnoDB定义为默认存储引擎（默认情况下为默认引擎），则无需指定该子句。要检查默认存储引擎，请发出以下语句： 123456mysql&gt; SELECT @@default_storage_engine;+--------------------------+| @@default_storage_engine |+--------------------------+| InnoDB |+--------------------------+ ENGINE=InnoDB如果计划使用mysqldump或复制CREATE TABLE在没有默认存储引擎的服务器上重播该语句， 则 可能仍使用子句InnoDB。 一个InnoDB表及其索引可以在创建系统表空间或在一个 文件每个表的 表空间。当 innodb_file_per_table启用，这是默认像MySQL 5.6.6的，一 InnoDB台是隐含在一个单独的文件，每个表的表空间中创建。相反，如果 innodb_file_per_table禁用此选项，则会InnoDB在InnoDB系统表空间中隐式创建表。 创建InnoDB表时，MySQL 在MySQL数据目录下的数据库目录中创建一个.frm文件。有关.frm文件的更多信息，请参见 InnoDB表和.frm文件。对于在每个表文件表空间中创建的表，默认情况下，MySQL还在数据库目录中创建一个 .ibd表空间文件。在 InnoDB系统表空间中创建的表在现有ibdata文件中创建，该文件位于MySQL数据目录中。 在内部，InnoDB将每个表的条目添加到InnoDB数据字典中。该条目包括数据库名称。例如，如果t1在test 数据库中创建了table ，则数据库名称的数据字典条目为 &#39;test/t1&#39;。这意味着您可以t1在不同的数据库中创建一个具有相同名称（）的表，并且该表名不会在内部冲突InnoDB。 InnoDB表和.frm文件MySQL将表的数据字典信息存储在数据库目录中的 .frm文件中。与其他MySQL存储引擎不同， InnoDB它还在系统表空间内的自身内部数据字典中编码有关表的信息。MySQL删除表或数据库时，将删除一个或多个.frm文件以及InnoDB数据字典中的相应条目。您不能InnoDB仅通过移动.frm 文件来在数据库之间移动表。有关移动InnoDB 表的信息，请参见第14.6.1.4节“移动或复制InnoDB表”。 InnoDB表和行格式默认的行格式的的InnoDB表 Compact。尽管此行格式适合基本实验，但请考虑使用 Dynamic or Compressed 格式来利用InnoDB 诸如表压缩和长列值的有效页外存储等功能。使用这些行格式要求将innodb_file_per_table其启用（MySQL 5.6.6中的默认值），并将其 innodb_file_format设置为 Barracuda： 1234SET GLOBAL innodb_file_per_table=1;SET GLOBAL innodb_file_format=barracuda;CREATE TABLE t3 (a INT, b CHAR (20), PRIMARY KEY (a)) ROW_FORMAT=DYNAMIC;CREATE TABLE t4 (a INT, b CHAR (20), PRIMARY KEY (a)) ROW_FORMAT=COMPRESSED; 有关InnoDB行格式的更多信息，请参见第14.11节“ InnoDB行格式”。有关如何确定InnoDB表的行格式以及行格式的物理特性的信息InnoDB ，请参见第14.11节“ InnoDB行格式”。 InnoDB表和主键始终为表定义一个主键InnoDB，并指定一个或多个满足以下条件的列： 被最重要的查询引用。 永远不会空白。 永远不要有重复的值。 一旦插入，就很少更改值。 例如，在包含有关人员的信息的表中，您不会在其上创建主键，(firstname, lastname)因为一个以上的人员可以具有相同的名称，某些人员的姓氏为空白，有时人们会更改其名称。有这么多的约束，通常没有明显的列集可以用作主键，因此您需要创建一个新的具有数字ID的列，以用作全部或部分主键。您可以声明一个 自动增量列，以便在插入行时自动填写升序值： 12345# The value of ID can act like a pointer between related items in different tables.CREATE TABLE t5 (id INT AUTO_INCREMENT, b CHAR (20), PRIMARY KEY (id));# The primary key can consist of more than one column. Any autoinc column must come first.CREATE TABLE t6 (id INT AUTO_INCREMENT, a INT, b CHAR (20), PRIMARY KEY (id,a)); 尽管在没有定义主键的情况下表可以正常工作，但是主键涉及性能的许多方面，并且对于任何大型或经常使用的表都是至关重要的设计方面。建议您始终在CREATE TABLE 语句中指定主键。如果创建表，加载数据，然后稍后运行 ALTER TABLE以添加主键，则该操作比创建表时定义主键要慢得多。 查看InnoDB表属性要查看InnoDB表的属性，请发出一条SHOW TABLE STATUS 语句： 123456789101112131415161718192021mysql&gt; SHOW TABLE STATUS FROM test LIKE 't%' \G;*************************** 1. row *************************** Name: t1 Engine: InnoDB Version: 10 Row_format: Compact Rows: 0 Avg_row_length: 0 Data_length: 16384Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2015-03-16 16:26:52 Update_time: NULL Check_time: NULL Collation: latin1_swedish_ci Checksum: NULL Create_options: Comment:1 row in set (0.00 sec) 有关SHOW TABLE STATUS输出的信息，请参见 第13.7.5.37节“ SHOW TABLE STATUS语句”。 InnoDB还可以使用InnoDBInformation Schema系统表查询表属性： 1234567891011mysql&gt; SELECT * FROM INFORMATION_SCHEMA.INNODB_SYS_TABLES WHERE NAME='test/t1' \G*************************** 1. row *************************** TABLE_ID: 42 NAME: test/t1 FLAG: 1 N_COLS: 5 SPACE: 24 FILE_FORMAT: Antelope ROW_FORMAT: CompactZIP_PAGE_SIZE: 01 row in set (0.02 sec) 有关更多信息，请参见 第14.15.3节“ InnoDB INFORMATION_SCHEMA系统表”。 14.6.1.5将表从MyISAM转换为InnoDB如果您具有MyISAM要转换InnoDB为更好的可靠性和可伸缩性的表，请在转换之前查看以下准则和提示。 调整MyISAM和InnoDB的内存使用量 处理太长或太短的交易 处理死锁 规划存储布局 转换现有表 克隆表的结构 传输现有数据 储存要求 为每个表定义一个主键 应用程序性能注意事项 了解与InnoDB表关联的文件 调整MyISAM和InnoDB的内存使用量从MyISAM表过渡时，降低key_buffer_size配置选项的值 以释放缓存结果不再需要的内存。增加innodb_buffer_pool_size 配置选项的值，该 选项执行与为InnoDB表分配缓存内存类似的作用。该 InnoDB 缓冲池可以缓存表数据和索引数据，加快了查询，查找并保持查询结果在内存中进行再利用。有关缓冲池大小配置的指导，请参见 第8.12.4.1节“ MySQL如何使用内存”。 在繁忙的服务器上，在关闭查询缓存的情况下运行基准测试。该InnoDB缓冲池提供类似的好处，所以查询缓存可能会不必要地占用内存。有关查询缓存的信息，请参见 第8.10.3节“ MySQL查询缓存”。 处理太长或太短的交易因为MyISAM表不支持 事务，所以您可能没有过多注意 autocommit配置选项和COMMITand ROLLBACK 语句。这些关键字对于允许多个会话同时读取和写入InnoDB表很重要，从而在写繁重的工作负载中提供了可伸缩性的实质好处。 当事务打开时，系统会保留数据快照，如在事务开始时看到的那样，如果系统在杂散事务保持运行的同时插入，更新和删除数百万行，则可能导致大量开销。因此，请注意避免事务运行时间过长： 如果您正在使用mysql会话进行交互式实验，请务必 COMMIT在完成时（完成更改）或 ROLLBACK（撤消更改）。关闭交互式会话，而不要长时间打开它们，以免意外使事务长时间打开。 确保您的应用程序中的任何错误处理程序也未 ROLLBACK 完成更改或COMMIT 已完成更改。 ROLLBACK这是一个相对昂贵的操作，因为 INSERT， UPDATE和 DELETE操作会写入到InnoDB之前的表中 COMMIT，并期望大多数更改都能成功提交，并且很少进行回滚。试验大量数据时，请避免对大量行进行更改，然后回滚这些更改。 当使用一系列INSERT语句加载大量数据时 ，请定期 COMMIT执行结果以避免事务持续数小时。在数据仓库的典型加载操作中，如果出现问题，请截断表（使用TRUNCATE TABLE），然后从头开始，而不是执行操作 ROLLBACK。 前面的技巧可以节省在过长的事务中可能浪费的内存和磁盘空间。当事务短于应有的时间时，问题就在于过多的I / O。对于每个 COMMIT，MySQL确保将每个更改安全地记录到磁盘上，其中涉及一些I / O。 对于InnoDB表格的大多数操作，应使用设置 autocommit=0。从效率的角度看，这样就避免了在发出大量连续的不必要的I / O INSERT， UPDATE或 DELETE语句。从安全角度来看，ROLLBACK 如果您在mysql命令行或应用程序中的异常处理程序中出错，则允许您发布 语句以恢复丢失或乱码的数据。 autocommit=1适用于InnoDB表 的时间是运行一系列查询以生成报告或分析统计信息时。在这种情况下，不会存在与COMMIT或 相关的I / O损失ROLLBACK，并且InnoDB可以 自动优化只读工作负载。 如果进行了一系列相关更改，请一次完成所有更改，最后一次 COMMIT完成。例如，如果您将相关的信息插入到多个表中，请COMMIT 在进行所有更改后执行一次。或者，如果您运行许多连续的 INSERT语句，则COMMIT在加载所有数据之后执行一次 ；如果您要执行数百万条 INSERT语句，则可能通过发出COMMIT每万或十万条记录来拆分庞大的事务 ，因此事务不会增长得太大。 请记住，即使是一条SELECT 语句也会打开一个事务，因此在交互式mysql 会话中运行某些报表或调试查询后，请发出a COMMIT 或关闭mysql会话。 处理死锁您可能会在MySQL错误日志中看到警告消息，该警告消息涉及 “ 死锁 ”或的输出 SHOW ENGINE INNODB STATUS。尽管名称听起来很吓人，但是 对于表来说，死锁并不是一个严重的问题InnoDB，并且通常不需要采取任何纠正措施。当两个事务开始修改多个表时，以不同的顺序访问表时，它们可以达到每个事务都在等待另一个的状态，而任何一个都不能继续进行。MySQL立即检测到这种情况并取消（回滚）“ 较小 ”的事务，从而允许其他事务继续进行。 您的应用程序确实需要错误处理逻辑来重新启动像这样被强制取消的事务。当您重新发出与以前相同的SQL语句时，原始的计时问题不再适用。另一笔交易已经完成，您可以继续进行，或者另一笔交易仍在进行中，您的交易会等到完成为止。 如果不断发生死锁警告，则可以检查应用程序代码以一致的方式对SQL操作进行重新排序，或缩短事务。您可以在innodb_print_all_deadlocks启用该选项的情况下进行测试， 以查看MySQL错误日志中的所有死锁警告，而不仅仅是SHOW ENGINE INNODB STATUS输出中的最后一个警告 。 有关更多信息，请参见第14.7.5节“ InnoDB中的死锁”。 规划存储布局为了从InnoDB表中获得最佳性能，您可以调整许多与存储布局相关的参数。 当您将MyISAM是大的，经常访问的，并保持至关重要的数据表，调查和考虑innodb_file_per_table， innodb_file_format以及 innodb_page_size配置选项，以及 ROW_FORMAT 和KEY_BLOCK_SIZE条款中的 CREATE TABLE说法。 在初始实验期间，最重要的设置是 innodb_file_per_table。启用此设置后，这是MySQL 5.6.6中的默认设置，新 InnoDB表将在每表文件表 空间中隐式创建 。与InnoDB系统表空间相比，每表文件表空间允许在表被截断或删除时由操作系统回收磁盘空间。每表文件表空间还支持 梭子鱼文件格式和相关功能，例如表压缩，长变长列的有效页外存储和大索引前缀。有关更多信息，请参见 第14.6.3.2节“每表文件表空间”。 转换现有表要将非InnoDB表转换为使用，请 InnoDB使用ALTER TABLE： 1ALTER TABLE table_name ENGINE=InnoDB; 警告 不要不转换在MySQL系统表 mysql从数据库MyISAM 到InnoDB表。这是不受支持的操作。如果这样做，MySQL将不会重新启动，直到您从备份中还原旧的系统表或通过重新初始化数据目录来重新生成它们（请参见 第2.10.1节“初始化数据目录”）。 克隆表的结构您可以制作一个InnoDB表，该表是MyISAM表的克隆，而不是ALTER TABLE用来执行转换，以便在切换之前并排测试新旧表。 创建InnoDB具有相同的列和索引定义的空表。使用看到完整的 语句来使用。将子句更改为 。 SHOW CREATE TABLE *table_name*\GCREATE TABLEENGINE``ENGINE=INNODB 传输现有数据要将大量数据传输到InnoDB上一节中创建的空 表中，请使用插入行。 INSERT INTO *innodb_table* SELECT * FROM *myisam_table* ORDER BY *primary_key_columns* 您还可以InnoDB 在插入数据后为表创建索引。从历史上看，创建新的二级索引对于InnoDB而言是一项缓慢的操作，但是现在您可以在索引创建步骤中以相对较少的开销加载数据之后创建索引。 如果您UNIQUE对辅助键有限制，则可以通过在导入操作期间暂时关闭唯一性检查来加快表的导入： 12SET unique_checks=0;... import operation ...SET unique_checks=1; 对于大表，这可以节省磁盘I / O，因为 InnoDB可以使用其 Change Buffer将辅助索引记录批量写入。确保数据不包含重复的密钥。 unique_checks允许但不要求存储引擎忽略重复的密钥。 为了更好地控制插入过程，您可以分段插入大表： 12INSERT INTO newtable SELECT * FROM oldtable WHERE yourkey &gt; something AND yourkey &lt;= somethingelse; 插入所有记录后，您可以重命名表。 在大表转换期间，增加InnoDB缓冲池的大小 以减少磁盘I / O，最多可占物理内存的80％。您还可以增加InnoDB日志文件的大小。 储存要求如果打算InnoDB在转换过程中为表中的数据制作几个临时副本， 建议您在每个表文件表空间中创建表，以便在删除表时可以回收磁盘空间。当 innodb_file_per_table 配置选项启用（默认），新创建的 InnoDB表在文件的每个表的表空间隐式创建。 无论您是MyISAM直接转换表还是创建克隆InnoDB表，请确保在此过程中有足够的磁盘空间来容纳旧表和新表。 InnoDB表比MyISAM表需要更多的磁盘空间。 如果ALTER TABLE操作空间不足，则会启动回滚，如果它是磁盘绑定的，则可能要花费数小时。对于插入，InnoDB使用插入缓冲区将二级索引记录合并到批索引中。这样可以节省大量的磁盘I / O。对于回滚，不使用这种机制，回滚所花费的时间可能比插入时间长30倍。 对于失控的回滚，如果数据库中没有有价值的数据，建议您终止数据库进程，而不要等待数百万的磁盘I / O操作完成。有关完整过程，请参见 第14.21.2节“强制InnoDB恢复”。 为每个表定义一个主键该PRIMARY KEY子句是影响MySQL查询性能以及表和索引空间使用的关键因素。主键唯一地标识表中的一行。表中的每一行都必须具有主键值，并且任何两行都不能具有相同的主键值。 这些是主键的指南，后面有更详细的说明。 PRIMARY KEY为每个表 声明一个。通常，它是WHERE查找单行时在子句中引用的最重要的列。 PRIMARY KEY在原始CREATE TABLE 语句中 声明该子句，而不是稍后通过一条ALTER TABLE语句添加它 。 仔细选择列及其数据类型。数字列优先于字符列或字符串列。 如果没有其他稳定，唯一，非空的数字列要使用，请考虑使用自动递增列。 如果不确定主键列的值是否可以更改，则自动增量列也是一个不错的选择。更改主键列的值是一项昂贵的操作，可能涉及重新排列表内和每个二级索引内的数据。 考虑将主键添加到尚无主键的任何表中。根据表的最大投影尺寸使用最小的实用数字类型。这可以使每行稍微紧凑一些，从而可以为大型表节省大量空间。如果表具有任何二级索引，则节省的空间将成倍增加 ，因为在每个二级索引条目中都会重复主键值。除了减小磁盘上的数据大小之外，小的主键还使更多数据适合 缓冲池，从而加快了各种操作并提高了并发性。 如果表在某个较长的列（例如）上已经具有主键VARCHAR，请考虑添加一个新的无符号 AUTO_INCREMENT列，并将主键切换到该列，即使查询中未引用该列。这种设计更改可以在二级索引中节省大量空间。您可以将以前的主键列指定为UNIQUE NOT NULL强制执行与PRIMARY KEY子句相同的约束，即防止所有这些列之间出现重复或空值。 如果将相关信息分布在多个表中，则通常每个表的主键使用同一列。例如，人员数据库可能有几个表，每个表都有员工编号的主键。一个销售数据库可能有一些带有客户编号主键的表，而另一些带有订单编号主键的表。因为使用主键的查找非常快，所以您可以为此类表构造有效的联接查询。 如果您PRIMARY KEY完全忽略该子句，MySQL会为您创建一个不可见的子句。它是一个6字节的值，可能比您需要的时间更长，因此浪费了空间。因为它是隐藏的，所以您不能在查询中引用它。 应用程序性能注意事项与InnoDB等效MyISAM表相比，的可靠性和可伸缩性功能 需要更多的磁盘存储。您可能会略微更改列和索引的定义，以提高空间利用率，减少处理结果集时的I / O和内存消耗，以及更好地利用索引查找来实现更好的查询优化计划。 如果确实为主键设置了数字ID列，请使用该值与任何其他表中的相关值进行交叉引用，尤其是对于联接查询。例如，与其接受一个国家名称作为输入并进行查询来搜索相同的名称，不如进行一次查找以确定国家ID，然后进行其他查询（或单个联接查询）以在多个表中查找相关信息。与其将客户或商品目录号存储为一串数字（可能会用完几个字节），不如将其转换为数字ID以进行存储和查询。4字节无符号 INT列可以索引超过40亿个项目（美国的含义是十亿：十亿）。有关不同整数类型的范围，请参见 第11.2.1节“整数类型（精确值）-INTEGER，INT，SMALLINT，TINYINT，MEDIUMINT，BIGINT”。 了解与InnoDB表关联的文件InnoDB文件比文件需要更多的照顾和计划MyISAM。 您不得删除代表系统表空间的 ibdata文件。 InnoDB 第14.6.1.4节“移动或复制InnoDB表”InnoDB中介绍了 将表移动或复制到其他服务器的方法 。 14.6.2 Index 14.6.2.1聚集索引和二级索引 14.6.2.2 InnoDB索引的物理结构 14.6.2.3 InnoDB全文索引 本节涵盖与InnoDB 索引有关的主题。 14.6.2.1聚集索引和二级索引每个InnoDB表都有一个特殊的索引，称为聚簇索引 ，用于存储行数据。通常，聚簇索引与主键同义 。为了从查询，插入和其他数据库操作中获得最佳性能，您必须了解如何InnoDB使用聚簇索引为每个表优化最常见的查找和DML操作。 在PRIMARY KEY表上定义a 时，InnoDB将其用作聚簇索引。为您创建的每个表定义一个主键。如果没有逻辑唯一且非空的列或列集，请添加一个新的 自动递增 列，其值将自动填充。 如果您没有PRIMARY KEY为表定义，MySQL会UNIQUE在所有键列所在的位置找到第一个索引，NOT NULL并将 InnoDB其用作聚集索引。 如果表没有索引PRIMARY KEY或没有合适的 UNIQUE索引，则在InnoDB 内部生成一个隐藏的聚集索引GEN_CLUST_INDEX，该索引在包含行ID值的合成列上命名 。这些行由InnoDB分配给该表中各行的ID排序 。行ID是一个6字节的字段，随着插入新行而单调增加。因此，按行ID排序的行实际上在插入顺序上。 聚集索引如何加快查询通过聚集索引访问行是快速的，因为索引搜索直接导致包含所有行数据的页面。如果表很大，则与使用不同于索引记录的页面存储行数据的存储组织相比，聚集索引体系结构通常可以节省磁盘I / O操作。 二级索引如何与聚簇索引相关除聚集索引之外的所有索引都称为 辅助索引。在中InnoDB，辅助索引中的每个记录都包含该行的主键列以及为辅助索引指定的列。 InnoDB使用此主键值在聚集索引中搜索行。 如果主键较长，则辅助索引将使用更多空间，因此具有短的主键是有利的。 有关利用InnoDB 聚簇索引和二级索引的准则，请参见 第8.3节“优化和索引”。 14.6.2.2 InnoDB索引的物理结构所有InnoDB索引都是 B树，索引记录存储在树的叶子页中。索引页的默认大小为16KB。 将新记录插入InnoDB 聚集索引时，请 InnoDB尝试使页面的1/16空闲，以备将来插入和更新索引记录。如果按顺序插入索引记录（升序或降序），则所得到的索引页大约为15/16。如果以随机顺序插入记录，则页面将充满1/2到15/16。如果索引页面的填充因子下降到1/2以下，请 InnoDB尝试收缩索引树以释放页面。 您可以通过 在初始化MySQL实例之前设置配置选项来定义MySQL实例中 所有表空间的页面大小。定义实例的页面大小后，如果不重新初始化实例就无法更改它。支持的大小为16KB，8KB和4KB。 InnoDBinnodb_page_size 使用特定InnoDB页面大小的MySQL实例不能使用来自使用不同页面大小的实例的数据文件或日志文件。 14.6.2.3 InnoDB全文索引FULLTEXT索引是在基于文本的列（CHAR， VARCHAR或TEXT列）上创建的， 以帮助加快对这些列中包含的数据的查询和DML操作，而忽略定义为停用词的任何单词。 甲FULLTEXT指数被定义为一个的一部分 CREATE TABLE说明或使用添加到现有的表ALTER TABLE 或CREATE INDEX。 使用MATCH() ... AGAINST语法执行全文搜索。有关用法信息，请参见 第12.9节“全文搜索功能”。 InnoDB FULLTEXT 本节中的以下主题描述了索引： InnoDB全文索引设计 InnoDB全文索引表 InnoDB全文索引缓存 InnoDB全文索引文档ID和FTS_DOC_ID列 InnoDB全文索引删除处理 InnoDB全文索引事务处理 监控InnoDB全文索引 InnoDB全文索引设计InnoDB FULLTEXT索引具有倒排索引设计。倒排索引存储一个单词列表，对于每个单词，存储该单词出现的文档列表。为了支持邻近搜索，每个单词的位置信息也作为字节偏移量存储。 InnoDB全文索引表创建InnoDB FULLTEXT索引时，将创建一组索引表，如以下示例所示： 1234567891011121314151617181920212223242526mysql&gt; CREATE TABLE opening_lines ( id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, opening_line TEXT(500), author VARCHAR(200), title VARCHAR(200), FULLTEXT idx (opening_line) ) ENGINE=InnoDB;mysql&gt; SELECT table_id, name, space from INFORMATION_SCHEMA.INNODB_SYS_TABLES WHERE name LIKE 'test/%';+----------+----------------------------------------------------+-------+| table_id | name | space |+----------+----------------------------------------------------+-------+| 333 | test/FTS_0000000000000147_00000000000001c9_INDEX_1 | 289 || 334 | test/FTS_0000000000000147_00000000000001c9_INDEX_2 | 290 || 335 | test/FTS_0000000000000147_00000000000001c9_INDEX_3 | 291 || 336 | test/FTS_0000000000000147_00000000000001c9_INDEX_4 | 292 || 337 | test/FTS_0000000000000147_00000000000001c9_INDEX_5 | 293 || 338 | test/FTS_0000000000000147_00000000000001c9_INDEX_6 | 294 || 330 | test/FTS_0000000000000147_BEING_DELETED | 286 || 331 | test/FTS_0000000000000147_BEING_DELETED_CACHE | 287 || 332 | test/FTS_0000000000000147_CONFIG | 288 || 328 | test/FTS_0000000000000147_DELETED | 284 || 329 | test/FTS_0000000000000147_DELETED_CACHE | 285 || 327 | test/opening_lines | 283 |+----------+----------------------------------------------------+-------+ 前六个表代表倒排索引，并称为辅助索引表。对传入文档进行标记时，各个单词（也称为 “ 标记 ”）与位置信息和关联的文档ID（DOC_ID）一起插入索引表中。根据单词的第一个字符的字符集排序权重，单词在六个索引表中得到完全排序和分区。 倒排索引分为六个辅助索引表，以支持并行索引创建。默认情况下，两个线程对索引表中的单词和相关数据进行标记，排序和插入。线程数可以使用该innodb_ft_sort_pll_degree 选项配置 。FULLTEXT在大型表上创建索引时，请考虑增加线程数 。 辅助索引表名称以前缀 FTS_和后缀 INDEX_*。每个索引表通过索引表名称中与table_id索引表的匹配的十六进制值与索引表相关联。例如，table_id所述的 test/opening_lines表是 327，为此，十六进制值是0x147。如前面的示例所示，十六进制值“ 147 ”出现在与该test/opening_lines表关联的索引表的名称中。 表示的十六进制值index_id的的 FULLTEXT索引也出现在辅助索引表名。例如，在辅助表名称中 test/FTS_0000000000000147_00000000000001c9_INDEX_1，十六进制值1c9的十进制值为457。可以通过查询 表中的该值（457）来识别opening_lines表（idx） 上定义的索引INFORMATION_SCHEMA.INNODB_SYS_INDEXES。 1234567mysql&gt; SELECT index_id, name, table_id, space from INFORMATION_SCHEMA.INNODB_SYS_INDEXES WHERE index_id=457;+----------+------+----------+-------+| index_id | name | table_id | space |+----------+------+----------+-------+| 457 | idx | 327 | 283 |+----------+------+----------+-------+ innodb_file_per_table启用 索引表后，索引表将存储在其自己的表空间中 。如果 innodb_file_per_table禁用此选项，则索引表存储在 InnoDB系统表空间（空间0）中。 注意 由于MySQL 5.6.5中引入的错误，启用索引表后会在InnoDB系统表空间（空间0） 中创建索引表innodb_file_per_table。该错误已在MySQL 5.6.20和MySQL 5.7.5（Bug＃18635485）中修复。 上一示例中显示的其他索引表称为通用索引表，用于删除处理和存储FULLTEXT索引的内部状态 。与为每个全文索引创建的倒排索引表不同，这组表是在特定表上创建的所有全文索引所共有的。 即使删除了全文索引，也会保留公用辅助表。删除全文索引时，将 FTS_DOC_ID保留为该索引创建的FTS_DOC_ID 列，因为删除该列将需要重建表。需要通用的腋窝表来管理该FTS_DOC_ID 柱。 FTS_*_DELETED 和 FTS_*_DELETED_CACHE 包含已删除但其数据尚未从全文索引中删除的文档的文档ID（DOC_ID）。该FTS_*_DELETED_CACHE是内存版本的FTS_*_DELETED 表。 FTS_*_BEING_DELETED 和 FTS_*_BEING_DELETED_CACHE 包含已删除文档的文档ID（DOC_ID），这些文档的数据当前正在从全文索引中删除。该FTS_*_BEING_DELETED_CACHE表是该 表的内存版本 FTS_*_BEING_DELETED。 FTS_*_CONFIG 存储有关FULLTEXT索引内部状态的信息 。最重要的是，它存储FTS_SYNCED_DOC_ID，用于标识已解析并刷新到磁盘的文档。在崩溃恢复的情况下， FTS_SYNCED_DOC_ID将使用值来标识尚未刷新到磁盘的文档，以便可以重新解析文档并将其添加回 FULLTEXT索引缓存中。要查看此表中的数据，请查询该 INFORMATION_SCHEMA.INNODB_FT_CONFIG 表。 InnoDB全文索引缓存插入文档后，将对其进行标记化，并将各个单词和关联的数据插入 FULLTEXT索引。即使对于小型文档，此过程也可能导致在辅助索引表中进行大量小的插入，从而使对这些表的并发访问成为争用点。为避免此问题，请 InnoDB使用FULLTEXT 索引缓存来临时缓存最近插入的行的索引表插入。此内存中的高速缓存结构将保留插入，直到高速缓存已满，然后将其批量刷新到磁盘（至辅助索引表）。您可以查询该 INFORMATION_SCHEMA.INNODB_FT_INDEX_CACHE 表以查看最近插入的行的标记化数据。 缓存和批处理刷新行为避免了对辅助索引表的频繁更新，这可能导致在繁忙的插入和更新期间并发访问问题。批处理技术还避免了同一单词的多次插入，并最大程度地减少了重复输入。代替单独刷新每个单词，对同一单词的插入进行合并并作为单个条目刷新到磁盘，从而提高了插入效率，同时保持了尽可能小的辅助索引表。 该innodb_ft_cache_size 变量用于配置全文索引缓存大小（基于每个表），这会影响刷新全文索引缓存的频率。您还可以使用该innodb_ft_total_cache_size 选项为给定实例中的所有表定义全局全文索引高速缓存大小限制 。 全文索引缓存存储与辅助索引表相同的信息。但是，全文索引缓存仅缓存最近插入的行的标记化数据。查询时，已刷新到磁盘（全文辅助表）的数据不会带回到全文索引缓存中。直接查询辅助索引表中的数据，并将辅助索引表中的结果与全文索引缓存中的结果合并，然后再返回。 InnoDB全文索引文档ID和FTS_DOC_ID列InnoDB使用称为文档ID（DOC_ID）的唯一文档标识符将全文索引中的单词映射到单词出现的文档记录。映射需要FTS_DOC_ID 在索引表上有一列。如果FTS_DOC_ID 未定义列，则在创建全文索引时InnoDB自动添加一个隐藏的FTS_DOC_ID列。下面的示例演示了此行为。 下表定义不包括 FTS_DOC_ID列： 123456mysql&gt; CREATE TABLE opening_lines ( id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, opening_line TEXT(500), author VARCHAR(200), title VARCHAR(200) ) ENGINE=InnoDB; 当使用CREATE FULLTEXT INDEX语法在表上创建全文索引时，将 返回警告，报告InnoDB正在重建表以添加FTS_DOC_ID 列。 12345678910mysql&gt; CREATE FULLTEXT INDEX idx ON opening_lines(opening_line);Query OK, 0 rows affected, 1 warning (0.19 sec)Records: 0 Duplicates: 0 Warnings: 1mysql&gt; SHOW WARNINGS;+---------+------+--------------------------------------------------+| Level | Code | Message |+---------+------+--------------------------------------------------+| Warning | 124 | InnoDB rebuilding table to add column FTS_DOC_ID |+---------+------+--------------------------------------------------+ 当用于ALTER TABLE向没有FTS_DOC_ID列的表中添加全文索引时，将 返回相同的警告 。如果您一次创建全文索引CREATE TABLE并且未指定FTS_DOC_ID列，则 InnoDB添加隐藏 FTS_DOC_ID列，而不会发出警告。 与FTS_DOC_ID在CREATE TABLE已经加载了数据的表上创建全文索引相比，在时间上定义列 要便宜得多。如果FTS_DOC_ID 在加载数据之前在表上定义了列，则不必重建表及其索引即可添加新列。如果您不关心CREATE FULLTEXT INDEX性能，请忽略该FTS_DOC_ID列来 InnoDB为您创建性能 。 InnoDB创建隐藏的 FTS_DOC_ID列以及FTS_DOC_ID_INDEX该FTS_DOC_ID列上的唯一索引（） 。如果要创建自己的FTS_DOC_ID列，则必须将该列定义为BIGINT UNSIGNED NOT NULL并命名为 FTS_DOC_ID （全部大写），如以下示例所示： 注意 该FTS_DOC_ID列不必定义为AUTO_INCREMENT列，但 AUTO_INCREMENT可以使加载数据更加容易。 123456mysql&gt; CREATE TABLE opening_lines ( FTS_DOC_ID BIGINT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, opening_line TEXT(500), author VARCHAR(200), title VARCHAR(200) ) ENGINE=InnoDB; 如果选择自己定义FTS_DOC_ID列，则您有责任管理该列，以免出现空值或重复值。FTS_DOC_ID值不能重复使用，这意味着FTS_DOC_ID 值必须不断增加。 （可选）您可以FTS_DOC_ID_INDEX在FTS_DOC_ID列上创建所需的唯一 （全部大写） 。 1mysql&gt; CREATE UNIQUE INDEX FTS_DOC_ID_INDEX on opening_lines(FTS_DOC_ID); 如果您未创建FTS_DOC_ID_INDEX， InnoDB则会自动创建。 在MySQL 5.6.31之前，最大使用FTS_DOC_ID值与新 FTS_DOC_ID值之间的允许间隙 为10000。在MySQL 5.6.31及更高版本中，允许间隙为65535。 为避免重建表，FTS_DOC_ID 删除全文索引时将保留该列。 InnoDB全文索引删除处理删除具有全文索引列的记录可能会导致辅助索引表中的许多小删除，从而使对这些表的并发访问成为争用点。为避免此问题，每当从索引表中删除DOC_ID记录时，已删除文档的Document ID（）就会记录在特殊FTS_*_DELETED表中，并且索引记录仍保留在全文索引中。返回查询结果之前，FTS_*_DELETED表格用于过滤删除的文档ID。这种设计的好处是删除既快速又便宜。缺点是删除记录后不会立即减小索引的大小。要删除已删除记录的全文索引条目，请OPTIMIZE TABLE在带有索引的表上 运行innodb_optimize_fulltext_only=ON 以重建全文索引。有关更多信息，请参见 优化InnoDB全文索引。 InnoDB全文索引事务处理InnoDB FULLTEXT索引由于具有缓存和批处理行为，因此具有特殊的事务处理特性。具体来说，FULLTEXT索引的更新和插入是在事务提交时处理的，这意味着 FULLTEXT搜索只能看到提交的数据。下面的示例演示了此行为。该 FULLTEXT搜索只返回插入的行被提交之后的结果。 1234567891011121314151617181920212223242526272829303132333435mysql&gt; CREATE TABLE opening_lines ( id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY, opening_line TEXT(500), author VARCHAR(200), title VARCHAR(200), FULLTEXT idx (opening_line) ) ENGINE=InnoDB;mysql&gt; BEGIN;mysql&gt; INSERT INTO opening_lines(opening_line,author,title) VALUES ('Call me Ishmael.','Herman Melville','Moby-Dick'), ('A screaming comes across the sky.','Thomas Pynchon','Gravity\'s Rainbow'), ('I am an invisible man.','Ralph Ellison','Invisible Man'), ('Where now? Who now? When now?','Samuel Beckett','The Unnamable'), ('It was love at first sight.','Joseph Heller','Catch-22'), ('All this happened, more or less.','Kurt Vonnegut','Slaughterhouse-Five'), ('Mrs. Dalloway said she would buy the flowers herself.','Virginia Woolf','Mrs. Dalloway'), ('It was a pleasure to burn.','Ray Bradbury','Fahrenheit 451');mysql&gt; SELECT COUNT(*) FROM opening_lines WHERE MATCH(opening_line) AGAINST('Ishmael');+----------+| COUNT(*) |+----------+| 0 |+----------+mysql&gt; COMMIT;mysql&gt; SELECT COUNT(*) FROM opening_lines WHERE MATCH(opening_line) AGAINST('Ishmael');+----------+| COUNT(*) |+----------+| 1 |+----------+ 监控InnoDB全文索引您可以InnoDB FULLTEXT通过查询下INFORMATION_SCHEMA 表来监视和检查索引的特殊文本处理方面： INNODB_FT_CONFIG INNODB_FT_INDEX_TABLE INNODB_FT_INDEX_CACHE INNODB_FT_DEFAULT_STOPWORD INNODB_FT_DELETED INNODB_FT_BEING_DELETED 您还可以FULLTEXT通过查询INNODB_SYS_INDEXES和 查看索引和表的 基本信息 INNODB_SYS_TABLES。 有关更多信息，请参见 第14.15.4节“ InnoDB INFORMATION_SCHEMA FULLTEXT索引表”。 14.6.3 Tablespace 14.6.3.1系统表空间 14.6.3.2每表文件表空间 14.6.3.3撤消表空间 本节涵盖与InnoDB 表空间有关的主题。 14.6.3.1系统表空间系统表空间是InnoDB数据字典，Doublewrite Buffer，Change Buffer和 Undo Log的存储区 。如果在系统表空间中创建表，而不是在每个表文件中创建表，则它也可能包含表和索引数据。 系统表空间可以具有一个或多个数据文件。默认情况下，ibdata1在数据目录中创建一个名为的系统表空间数据文件 。系统表空间数据文件的大小和数量由innodb_data_file_path启动选项定义。有关配置信息，请参阅《 系统表空间数据文件配置》。 本节中以下主题下提供了有关系统表空间的其他信息： 调整系统表空间的大小 对系统表空间使用原始磁盘分区 调整系统表空间的大小本节介绍如何增加或减少系统表空间的大小。 增加系统表空间的大小增加系统表空间大小的最简单方法是将其配置为自动扩展。为此，请autoextend为设置中的最后一个数据文件指定 属性innodb_data_file_path ，然后重新启动服务器。例如： 1innodb_data_file_path=ibdata1:10MB:autoextend 当autoextend指定的属性，则数据文件自动大小由8MB增量因为需要空间增加。所述 innodb_autoextend_increment 可变控制增量大小。 您还可以通过添加另一个数据文件来增加系统表空间的大小。为此： 停止MySQL服务器。 如果innodb_data_file_path 使用autoextend 属性定义了设置中的最后一个数据文件，则将 其删除，然后修改size属性以反映当前数据文件的大小。要确定要指定的适当数据文件大小，请检查文件系统中的文件大小，并将该值四舍五入为最接近的MB值，其中MB等于1024 x 1024。 将新的数据文件追加到 innodb_data_file_path 设置中，可以选择指定 autoextend属性。该 autoextend属性只能在最后一个数据文件中指定 innodb_data_file_path 的设置。 启动MySQL服务器。 例如，此表空间具有一个自动扩展数据文件： 12innodb_data_home_dir =innodb_data_file_path = /ibdata/ibdata1:10M:autoextend 假设数据文件随着时间增长到988MB。这是innodb_data_file_path 修改大小属性以反映当前数据文件大小之后，并指定新的50MB自动扩展数据文件之后的设置： 12innodb_data_home_dir =innodb_data_file_path = /ibdata/ibdata1:988M;/disk2/ibdata2:50M:autoextend 添加新数据文件时，请勿指定现有文件名。InnoDB启动服务器时，将创建并初始化新的数据文件。 注意： 您不能通过更改其大小属性来增加现有系统表空间数据文件的大小。例如，在启动服务器时，将innodb_data_file_path设置从更改 ibdata1:10M:autoextend为 ibdata1:12M:autoextend会产生以下错误： 1234&gt; [ERROR] [MY-012263] [InnoDB] The Auto-extending innodb_system &gt; data file &apos;./ibdata1&apos; is of a different size 640 pages (rounded down to MB) than &gt; specified in the .cnf file: initial 768 pages, max 0 (relevant if non-zero) pages!&gt; 该错误表明现有数据文件大小（以InnoDB页表示）与配置文件中指定的大小不同。如果遇到此错误，请恢复先前的 innodb_data_file_path 设置，然后参考系统表空间调整大小说明。 InnoDB页面大小由innodb_page_size变量定义 。默认值为16384字节。 减少InnoDB系统表空间的大小您不能从系统表空间中删除数据文件。要减小系统表空间大小，请使用以下过程： 使用mysqldump转储所有 InnoDB表，包括 模式中的InnoDB表 mysql。使用以下查询标识 模式中的InnoDB表 mysql： 1234567891011mysql&gt; SELECT TABLE_NAME from INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA='mysql' and ENGINE='InnoDB';+----------------------+| table_name |+----------------------+| innodb_index_stats || innodb_table_stats || slave_master_info || slave_relay_log_info || slave_worker_info |+----------------------+5 rows in set (0.00 sec) 停止服务器。 删除所有现有的表空间文件（*.ibd），包括 ibdata和ib_log 文件。不要忘记删除 架构*.ibd 中表的文件mysql。 删除表的所有.frm文件 InnoDB。 为新系统表空间配置数据文件。请参阅 系统表空间数据文件配置。 重新启动服务器。 导入转储文件。 注意 如果您的数据库仅使用InnoDB 引擎，则转储所有数据库，停止服务器，删除所有数据库和InnoDB日志文件，重新启动服务器以及导入转储文件可能更简单 。 对系统表空间使用原始磁盘分区您可以将原始磁盘分区用作InnoDB 系统表空间中的数据文件 。此技术可在Windows以及某些Linux和Unix系统上启用无缓冲I / O，而不会增加文件系统开销。在有和没有原始分区的情况下执行测试，以验证此更改是否确实提高了系统性能。 使用原始磁盘分区时，请确保运行MySQL服务器的用户ID具有该分区的读写特权。例如，如果您以mysql用户身份运行服务器 ，则分区必须可由读取和写入mysql。如果使用该--memlock选项运行服务器，则该服务器必须以身份运行root，因此该分区必须可由读取和写入root。 下述步骤涉及选项文件的修改。有关更多信息，请参见第4.2.2.2节“使用选项文件”。 在Linux和Unix系统上分配原始磁盘分区 When you create a new data file, specify the keyword newraw immediately after the data file size for the innodb_data_file_path option. The partition must be at least as large as the size that you specify. Note that 1MB in InnoDB is 1024 × 1024 bytes, whereas 1MB in disk specifications usually means 1,000,000 bytes. 123[mysqld]innodb_data_home_dir=innodb_data_file_path=/dev/hdd1:3Gnewraw;/dev/hdd2:2Gnewraw 重新启动服务器。InnoDB注意 newraw关键字并初始化新分区。但是，请不要创建或更改任何 InnoDB表。否则，当您下次重新启动服务器时，将InnoDB 重新初始化分区，并且所做的更改将丢失。（为安全起见，InnoDB当newraw指定任何分区时，防止用户修改数据 。） 后InnoDB已初始化新的分区，停止服务器，更改newraw 数据文件规范raw： 123[mysqld]innodb_data_home_dir=innodb_data_file_path=/dev/hdd1:3Graw;/dev/hdd2:2Graw 重新启动服务器。InnoDB现在允许进行更改。 在Windows上分配原始磁盘分区在Windows系统上，适用于Linux和Unix系统的相同步骤和随附的准则，只是innodb_data_file_pathWindows上的 设置略有不同。 创建新的数据文件时，请在newraw该innodb_data_file_path 选项的数据文件大小后立即 指定关键字 ： 123[mysqld]innodb_data_home_dir=innodb_data_file_path=//./D::10Gnewraw 在//./相当于Windows语法\\.\用于访问物理驱动器。在上面的示例中，D:是分区的驱动器号。 重新启动服务器。InnoDB注意 newraw关键字并初始化新分区。 后InnoDB已初始化新的分区，停止服务器，更改newraw 数据文件规范raw： 123[mysqld]innodb_data_home_dir=innodb_data_file_path=//./D::10Graw 重新启动服务器。InnoDB现在允许进行更改。 14.6.3.2 File-Per-Table-Tablespaces每表文件表空间包含单个InnoDB表的数据和索引 ，并存储在文件系统中自己的数据文件中。 每节文件表空间特征在本节的以下主题下描述： 每表文件表空间配置 每表文件表空间数据文件 每表文件表空间的优势 每表文件表空间的缺点 每表文件表空间配置InnoDB默认情况下，在每个表文件表空间中创建表。此行为由innodb_file_per_table变量控制 。禁用在系统表空间中创建表的innodb_file_per_table 原因InnoDB。 的innodb_file_per_table 设置可以在选项文件来指定，或者使用在运行时配置的 SET GLOBAL语句。在运行时更改设置需要足够的特权来设置全局系统变量。请参见第5.1.8.1节“系统变量特权”。 选项文件： 12[mysqld]innodb_file_per_table=ON SET GLOBAL在运行时 使用： 1mysql&gt; SET GLOBAL innodb_file_per_table=ON; innodb_file_per_table在MySQL 5.6和更高版本中默认启用。如果需要考虑与MySQL早期版本的向后兼容性，则可以考虑禁用它。 警告 禁用 innodb_file_per_table 可防止表复制ALTER TABLE操作将驻留在系统表空间中的表隐式移动到每个表文件表空间。表复制ALTER TABLE操作将使用当前innodb_file_per_table 设置重新创建表。添加或删除二级索引时，此行为不适用，也不适用于 ALTER TABLE使用该INPLACE算法的操作。 每表文件表空间数据文件.idb在MySQL数据目录下的架构目录 中的数据文件中，将创建一个每表文件表空间 。该.ibd文件以表（*table_name*.ibd）命名。例如，在MySQL数据目录下的目录中test.t1 创建表的数据文件test： 12345678910mysql&gt; USE test; mysql&gt; CREATE TABLE t1 ( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(100) ) ENGINE = InnoDB; shell&gt; cd /path/to/mysql/data/testshell&gt; ls t1.ibd 您可以使用该语句的DATA DIRECTORY子句 CREATE TABLE在数据目录外部隐式创建每表文件表空间数据文件。有关更多信息，请参见 第14.6.1.2节“在外部创建表”。 每表文件表空间的优势每表文件表空间比共享系统表空间具有以下优点。 截断或删除每个表文件表空间中创建的表后，磁盘空间将返回操作系统。截断或删除存储在系统表空间中的表会在系统表空间内创建可用空间，该可用空间仅可用于 InnoDB数据。换句话说，在表被截断或删除后，系统表空间的大小不会缩小。 ALTER TABLE 对驻留在系统表空间中的表进行表 复制操作可能会增加表空间占用的磁盘空间量。此类操作可能需要与表中的数据加索引一样多的额外空间。该空间不会像每个表文件表空间那样释放回操作系统。 TRUNCATE TABLE 在每个表文件表空间中的表上执行时，性能会更好。 可以在单独的存储设备上创建每表文件表空间数据文件，以进行I / O优化，空间管理或备份。请参见 第14.6.1.2节“在外部创建表”。 您可以从另一个MySQL实例导入位于每个表文件表空间中的表。请参见 第14.6.1.3节“导入InnoDB表”。 在每表文件表空间中创建的表使用梭子鱼文件格式。请参见 第14.10节“ InnoDB文件格式管理”。梭子鱼文件格式启用DYNAMIC与COMPRESSED 行格式关联的功能 。请参见第14.11节“ InnoDB行格式”。 当发生数据损坏，备份或二进制日志不可用或无法重新启动MySQL服务器实例时，存储在单个表空间数据文件中的表可以节省时间并提高成功恢复的机会。 您可以使用MySQL Enterprise Backup快速备份或还原在每表文件表空间中创建的表，而不会中断其他InnoDB 表的使用。这对于具有不同备份计划的表或需要较少备份频率的表很有用。有关详细信息，请参见 进行部分备份。 每表文件表空间允许通过监视表空间数据文件的大小来监视文件系统上的表大小。 当通用的Linux文件系统不允许并发写入到一个单一的文件，如系统表空间的数据文件 innodb_flush_method设置为O_DIRECT。因此，结合使用每表文件表空间和此设置时，可能会提高性能。 共享系统表空间中的表包含InnoDB64TB表空间大小限制，该表包含数据字典和Undo Log等其他结构。相比之下，每个表的每个文件表空间都有64TB的大小限制，这为单个表的大小增加提供了足够的空间。 每表文件表空间的缺点与共享系统表空间相比，每表文件表空间具有以下缺点。 使用每表文件表空间，每个表可能有未使用的空间，只能由同一表的行使用，如果管理不当，则会浪费空间。 fsync对每个表的多个数据文件而不是共享系统表空间数据文件执行操作。由于 fsync操作是针对每个文件的，因此无法合并针对多个表的写操作，这可能导致更多的fsync 操作总数。 mysqld必须为每个表文件空间保留一个打开的文件句柄，如果每个表文件空间中有许多表，则可能会影响性能。 每个表都有其自己的数据文件时，需要更多的文件描述符。 可能存在更多碎片，这可能会影响 DROP TABLE表扫描性能。但是，如果管理碎片，则每表文件表空间可以提高这些操作的性能。 删除驻留在每个表文件表空间中的表时，将扫描缓冲池，对于大型缓冲池可能要花费几秒钟。使用宽泛的内部锁定执行扫描，这可能会延迟其他操作。 该innodb_autoextend_increment 变量定义用于在自动扩展系统表空间文件变满时扩展其大小的增量大小，该 变量不适用于每表文件表空间文件，无论innodb_autoextend_increment 设置如何，该文件都将自动扩展 。每个表的初始文件表扩展名很少，之后扩展名以4MB为增量。 14.6.3.3 Undo Tablespaces撤消表空间包含Undo Log，Undo Log是Undo Log记录的集合，其中包含有关如何通过事务撤消对聚集索引记录的最新更改的信息。Undo Log段中包含Undo Log。该 innodb_rollback_segments变量定义分配给每个撤消表空间的回滚段的数量。 Undo Log可以存储在一个或多个撤消表空间中，而不是 系统表空间中。此布局与默认配置不同，在默认配置中，Undo Log位于系统表空间中。Undo Log的I / O模式使撤消表空间成为SSD存储的理想候选者 ，同时将系统表空间保留在硬盘存储上。 所使用的撤消表空间的数量InnoDB 由innodb_undo_tablespaces 配置选项控制 。仅在初始化MySQL实例时才能配置此选项。此后无法更改。 撤消表空间和这些表空间内的各个 段不能删除。 配置撤消表空间要为MySQL实例配置撤消表空间，请执行以下步骤。假定您在将配置部署到生产系统之前正在测试实例上执行该过程。 重要 撤消表空间的数量只能在初始化MySQL实例时配置，并且在实例生命周期内是固定的。 使用innodb_undo_directory 配置选项为撤消表空间指定目录位置 。如果未指定目录位置，则在数据目录中创建撤消表空间。 使用innodb_rollback_segments 配置选项定义回滚段的数量 。从一个相对较低的值开始，然后随着时间的推移逐渐增加它，以检查对性能的影响。默认设置为 innodb_rollback_segments128，这也是最大值。 一个回退段始终分配给系统表空间。因此，要将回滚段分配给撤消表空间，请设置 innodb_rollback_segments为大于1的值。例如，如果您有两个撤消表空间，则设置 innodb_rollback_segments为3可以为两个撤消表空间中的每一个分配一个回滚段。回滚段以循环方式分布在撤消表空间中。 配置单独的撤消表空间时，系统表空间中的回滚段将变为非活动状态。 使用该innodb_undo_tablespaces 选项定义撤消表空间的数量 。在MySQL实例的生命周期中，指定的还原表空间数量是固定的，因此，如果不确定最佳值，请从高端进行估算。 使用您选择的选项值创建一个新的MySQL测试实例。 在测试实例上使用实际的工作负载，并使用与生产服务器类似的数据量来测试配置。 对I / O密集型工作负载的性能进行基准测试。 定期增加价值 innodb_rollback_segments 并重新运行性能测试，直到I / O性能没有进一步提高。 14.6.4 InnoDB数据字典所述InnoDB数据字典由包含元数据的用于跟踪对象的如表，索引，和表中的列的内部系统表。元数据实际上位于InnoDB系统表空间中。由于历史原因，数据字典元数据在某种程度上与InnoDB表元数据文件（.frm文件）中存储的信息重叠 。 14.6.5 Doublewrite Bufferdoublewrite缓冲区是位于系统表空间中的存储区域，在此之前，InnoDB将从页面InnoDB缓冲池中刷新的页面写入页面，然后再将页面写入数据文件中的适当位置。仅在刷新页面并将页面写入doublewrite缓冲区之后，才 InnoDB将页面写入其适当位置。如果在页面写入过程中发生操作系统，存储子系统或 mysqld进程崩溃，InnoDB以后可以在崩溃恢复期间从doublewrite缓冲区中找到该页面的良好副本。 尽管数据总是被写入两次，但Doublewrite Buffer并不需要两倍的I / O开销或两倍的I / O操作。fsync() 只需对操作系统进行一次调用，就可以将数据作为一个较大的顺序块写入Doublewrite Buffer本身。 默认情况下，Doublewrite Buffer处于启用状态。要禁用Doublewrite Buffer，请设置 innodb_doublewrite为0。 14.6.6 Redo LogRedo Log是基于磁盘的数据结构，在崩溃恢复期间用于纠正不完整事务写入的数据。在正常操作期间，Redo Log对更改请求数据的请求进行编码，这些请求是由SQL语句或低级API调用引起的。在初始化期间以及接受连接之前，会自动重播未完成意外关闭之前未完成更新数据文件的修改。有关Redo Log在崩溃恢复中的作用的信息，请参见 第14.18.2节“ InnoDB恢复”。 默认情况下，Redo Log在磁盘上由两个名为ib_logfile0和的 文件物理表示ib_logfile1。MySQL以循环方式写入Redo Log文件。Redo Log中的数据按照受影响的记录进行编码；此数据统称为重做。通过Redo Log的数据传递以不断增加的LSN值表示。 有关相关信息，请参阅 Redo Log文件配置和 第8.5.4节“优化InnoDBRedo Log”。 更改Redo Log文件的数量或大小要在MySQL 5.6.7或更早版本中更改InnoDB Redo Log文件的数量或大小，请执行以下步骤： 如果innodb_fast_shutdown设置为2，则设置 innodb_fast_shutdown为1： 1mysql&gt; SET GLOBAL innodb_fast_shutdown = 1; 确保 innodb_fast_shutdown未将其设置为2后，停止MySQL服务器并确保其关闭且没有错误（以确保日志中没有未完成事务的信息）。 将旧的日志文件复制到一个安全的地方，以防在关闭过程中出现问题并且需要它们来恢复表空间。 从日志文件目录中删除旧的日志文件。 编辑my.cnf以更改日志文件配置。 再次启动MySQL服务器。mysqld发现InnoDB启动时不存在任何日志文件，并创建了新的日志文件。 该innodb_fast_shutdown 改变的数量或大小时设置不再是相关的InnoDB日志文件。此外，尽管您可能仍希望将旧日志文件复制到一个安全的地方作为备份，但是不再需要删除旧日志文件。要更改InnoDB日志文件的数量或大小 ，请执行以下步骤： 停止MySQL服务器，并确保它关闭且没有错误。 编辑my.cnf以更改日志文件配置。要更改日志文件的大小，请配置 innodb_log_file_size。要增加日志文件的数量，请配置 innodb_log_files_in_group。 再次启动MySQL服务器。 如果InnoDB检测到 innodb_log_file_size与Redo Log文件大小不同，它将编写日志检查点，关闭并删除旧的日志文件，以请求的大小创建新的日志文件，然后打开新的日志文件。 组提交以Redo Log刷新InnoDB像任何其他 符合ACID的数据库引擎一样，在提交事务之前刷新事务的Redo Log。InnoDB 使用组提交 功能将多个此类刷新请求分组在一起，以避免每次提交都进行一次刷新。使用组提交， InnoDB可以对日志文件进行一次写入操作，以对大约同时提交的多个用户事务执行提交操作，从而显着提高了吞吐量。 有关性能COMMIT和其他事务操作的更多信息 ，请参见第8.5.2节“优化InnoDB事务管理”。 14.6.7 Undo Logs回滚段 InnoDB采用回滚段的方式来维护undo log的并发写入和持久化。回滚段实际上是一种 Undo 文件组织方式，每个回滚段又有多个undo log slot。 一共128个回滚段，每个回滚段维护了一个段头页，在该page中又划分了1024个slot (TRX_RSEG_N_SLOTS)，每个slot又对应到一个undo log对象，因此理论上InnoDB最多支持 96 * 1024个普通事务。 rseg0预留在系统表空间ibdata中; rseg 1~rseg 32这32个回滚段存放于临时表的系统表空间中; rseg33~ 则根据配置存放到独立undo表空间中（如果没有打开独立Undo表空间，则存放于ibdata中） Undo Log ​ undo Log是与单个读写事务关联的 undo Log 记录的集合。undo Log 记录包含如何 撤消事务对 聚簇索引 记录的最新更改 的信息。如果另一个事务读取原始数据，就需要在 undo Log记录读取。 ​ undo Log存在于 undo log segment。默认情况下，undo log segment 实际上是 system tablespace 的一部分 ，但它们也可以驻留在 undo log tablespace 中。有关更多信息，请参见第14.6.3.3节“撤消表空间”。 InnoDB支持128个回滚段，通过参数 innodb_rollback_segments 定义； 一个 rollback segment 支持的 事务数 取决（ rollback segment 中 undo slots 的数量） 和 （每个事务所需的 undo log 数）； 一个rollback segment 中 undo slots 的数量 又会因为 innodb_page_size 不同而不同； InnoDB Page Size rollback segment 中的 undo slots 数（innodb_page_size/ 16） 4096 (4KB) 256 8192 (8KB) 512 16384 (16KB) 1024 以下每种操作类型，一个事务最多可以分配两个 undo Log： INSERT 操作 在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除； UPDATE和 DELETE 操作 产生的Undo日志被归成一类，即update_undo； undo Log根据需要分配。例如，一个事务执行INSERT， UPDATE 和 DELETE 操作被分配了两个undo Log，仅执行INSERT操作的事务被分配有一个 undo Log； 分配给事务的Undo Log在其持续时间内始终与事务相关； 给定上述因素，可以使用以下公式来估计InnoDB能够支持的并发读写事务数。 注意：在达到 InnoDB 能够支持的并发读写事务数之前，事务可能会遇到并发事务限制错误。当分配给事务的 rollback segment 用完 undo slots 时，就会发生这种情况。在这种情况下，请尝试重新运行事务 如果每个事务执行任一个 insert 或 update 或 delete 操作，InnoDB支持的 并发读-写事务的数目： 1(innodb_page_size / 16) * innodb_rollback_segments 如果每个事务执行任一个 insert 和（ update 或 delete） 操作，InnoDB支持的 并发读-写事务的数目 1(innodb_page_size / 16 / 2) * innodb_rollback_segments 14.12 InnoDB磁盘I / O和文件空间管理 14.12.1 InnoDB磁盘I / O 14.12.2文件空间管理 14.12.3 InnoDB检查点 14.12.4对表进行碎片整理 14.12.5使用TRUNCATE TABLE回收磁盘空间 作为DBA，您必须管理磁盘I / O以防止I / O子系统饱和，并管理磁盘空间以避免填满存储设备。该ACID设计模型需要一定量的I / O可能似乎是多余的，但有助于确保数据的可靠性。在这些限制内， InnoDB尝试优化数据库工作和磁盘文件的组织，以最大程度地减少磁盘I / O的数量。有时，I / O会推迟到数据库不忙之前，或者直到所有内容都需要进入一致状态为止，例如在快速关闭后重新启动数据库期间。 本节讨论默认类型的MySQL表（也称为InnoDB表）对I / O和磁盘空间的主要注意事项 ： 控制用于提高查询性能的后台I / O数量。 启用或禁用可提供额外耐用性的功能，但需要付出额外的I / O代价。 将表组织成许多小文件，一些大文件或两者的组合。 使Redo Log文件的大小与日志文件已满时发生的I / O活动保持平衡。 如何重组表以获得最佳查询性能。 14.12.1 InnoDB磁盘I / OInnoDB在可能的情况下使用异步磁盘I / O，方法是创建多个线程来处理I / O操作，同时允许其他数据库操作在I / O仍在进行时继续进行。在Linux和Windows平台上，InnoDB使用可用的OS和库函数来执行“ 本机 ”异步I / O。在其他平台上，InnoDB仍然使用I / O线程，但是这些线程实际上可能会等待I / O请求完成。该技术称为“ 模拟 ” 异步I / O。 预读如果InnoDB可以确定很快有可能需要数据的可能性很大，它将执行预读操作将数据带入缓冲池，以便在内存中可用。对连续数据进行一些大的读取请求可能比对几个分散的小请求进行效率更高。有两种预读启发式InnoDB： 在顺序预读中，如果InnoDB 注意到对表空间中某个段的访问模式是顺序的，则它会将一批数据库页的读取提前发布到I / O系统。 在随机预读中，如果InnoDB注意到表空间中的某些区域似乎正在被完全读入缓冲池，则它将剩余的读操作发布到I / O系统。 有关配置预读启发式方法的信息，请参见 第14.8.3.3节“配置InnoDB缓冲池预取（预读）”。 Doublewrite BufferInnoDB使用一种新颖的文件刷新技术，该技术涉及一种称为doublewrite缓冲区的结构 ，默认情况下（innodb_doublewrite=ON）已启用。它增加了崩溃或断电后的恢复安全性，并通过减少fsync()操作需求来提高大多数Unix版本的性能。 在将页面InnoDB 写到数据文件之前，首先将它们写到称为doublewrite缓冲区的连续表空间区域中。仅在完成对双InnoDB 写缓冲区的写入和刷新之后，才将页面写入数据文件中的相应位置。如果在页面写入过程中发生操作系统，存储子系统或 mysqld进程崩溃（导致页面损坏的 情况），InnoDB则以后可以在恢复期间从doublewrite缓冲区中找到该页面的良好副本。 14.12.2文件空间管理您使用innodb_data_file_path 配置选项在配置文件中定义的数据文件 形成InnoDB 系统表空间。这些文件在逻辑上串联在一起形成系统表空间。没有使用中的条带化。您无法定义表在系统表空间中的分配位置。在新创建的系统表空间中，InnoDB从第一个数据文件开始分配空间。 为避免将所有表和索引存储在系统表空间内所带来的问题，可以启用 innodb_file_per_table 配置选项（默认选项），该选项将每个新创建的表存储在单独的表空间文件中（扩展名为 .ibd）。对于以这种方式存储的表，磁盘文件中的碎片较少，并且当表被截断时，该空间将返回给操作系统，而不是仍由InnoDB在系统表空间中保留。 在整理InnoDB存储引擎的索引的时候，发现B+树是离不开页面page的。所以先整理InnoDB的数据存储结构。关键词：Pages, Extents, Segments, and Tablespaces 如何存储表MySQL 使用 InnoDB 存储表时，会将表的定义和数据索引等信息分开存储，其中前者存储在 .frm 文件中，后者存储在 .ibd 文件中，这一节就会对这两种不同的文件分别进行介绍。 .frm 无论在 MySQL 中选择了哪个存储引擎，所有的 MySQL 表都会在硬盘上创建一个 .frm 文件用来描述表的格式或者说定义； .frm 文件的格式在不同的平台上都是相同的。 .ibd 文件InnoDB 中用于存储数据的文件总共有两个部分，一是系统表空间文件，包括 ibdata1、 ibdata2 等文件，其中存储了 InnoDB 系统信息和用户数据库表数据和索引，是所有表公用的。当打开 innodb_file_per_table 选项时， .ibd 文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。 表空间 innodb存储引擎在存储设计上模仿了Oracle的存储结构，其数据是按照表空间进行管理的。新建一个数据库时，innodb存储引擎会初始化一个名为ibdata1 的表空间文件，默认情况下，这个文件会存储所有表的数据，以及我们所熟知但看不到的系统表sys_tables、sys_columns、sys_indexes 、sys_fields等。此外，还会存储用来保证数据完整性的回滚段数据，当然这部分数据在新版本的MySQL中，已经可以通过参数来设置回滚段的存储位置了； ​ innodb存储引擎的设计很灵活，可以通过参数innodb_file_per_table来设置，使得每一个表都对应一个自己的独立表空间文件，而不是存储到公共的ibdata1文件中。独立的表空间文件之存储对应表的B+树数据、索引和插入缓冲等信息，其余信息还是存储在默认表空间中。 这个文件所存储的内容主要就是B+树（索引），一个表可以有多个索引，也就是在一个文件中，可以存储多个索引，而如果一个表没有索引的话，用来存储数据的被称为聚簇索引，也就是说这也是一个索引。最终的结论是，ibd文件存储的就是一个表的所有索引数据。 索引文件有段（segment）,簇（extends）（有的文章翻译为区），页面（page）组成。 关于行记录格式，单独整理一篇。 段（segment 段是表空间文件中的主要组织结构，它是一个逻辑概念，用来管理物理文件，是构成索引、表、回滚段的基本元素。 上图中显示了表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等。InnoDB存储引擎表是索引组织的（index organized），因此数据即索引，索引即数据。那么数据段即为B+树的页节点（上图的leaf node segment），索引段即为B+树的非索引节点（上图的non-leaf node segment）。 创建一个索引（B+树）时会同时创建两个段，分别是内节点段和叶子段，内节点段用来管理（存储）B+树非叶子（页面）的数据，叶子段用来管理（存储）B+树叶子节点的数据；也就是说，在索引数据量一直增长的过程中，所有新的存储空间的申请，都是从“段”这个概念中申请的。 为了介绍索引为目的，所以不展开介绍回滚段等内容。 区/簇（extents） 段是个逻辑概念，innodb引入了簇的概念，在代码中被称为extent； 簇是由64个连续的页组成的，每个页大小为16KB，即每个簇的大小为1MB。簇是构成段的基本元素，一个段由若干个簇构成。一个簇是物理上连续分配的一个段空间，每一个段至少会有一个簇，在创建一个段时会创建一个默认的簇。如果存储数据时，一个簇已经不足以放下更多的数据，此时需要从这个段中分配一个新的簇来存放新的数据。一个段所管理的空间大小是无限的，可以一直扩展下去，但是扩展的最小单位就是簇。页（page）InnoDB有页（page）的概念，可以理解为簇的细化。页是InnoDB磁盘管理的最小单位。 常见的页类型有：数据页（B-tree Node）。Undo页（Undo Log Page）。系统页（System Page）。事务数据页（Transaction system Page）。插入缓冲位图页（Insert Buffer Bitmap）。插入缓冲空闲列表页（Insert Buffer Free List）。未压缩的二进制大对象页（Uncompressed BLOB Page）。压缩的二进制大对象页（Compressed BLOB Page）。 在逻辑上（页面号都是从小到大连续的）及物理上都是连续的。在向表中插入数据时，如果一个页面已经被写完，系统会从当前簇中分配一个新的空闲页面处理使用，如果当前簇中的64个页面都被分配完，系统会从当前页面所在段中分配一个新的簇，然后再从这个簇中分配一个新的页面来使用； 页面，范围，段和表空间每个表空间由数据库页面组成 。MySQL实例中的每个表空间都具有相同的页面大小。默认情况下，所有表空间的页面大小均为16KB；您可以innodb_page_size在创建MySQL实例时通过指定选项将页面大小减小到8KB或4KB 。 这些页面分为 大小为1MB的扩展区（64个连续的16KB页面，128个8KB页面或256个4KB页面）。在 “ 文件 ”表空间内被称为 段在 InnoDB。（这些段与回滚段不同， 回滚段实际上包含许多表空间段。） 当段在表空间内增长时， InnoDB将前32页一次分配给它。之后，InnoDB开始将整个扩展区分配给该段。InnoDB 一次最多可以向一个大段中添加4个扩展区，以确保数据的良好顺序。 为中的每个索引分配了两个段 InnoDB。一个用于 B树的非叶节点，另一个用于叶节点。将叶子节点保持在磁盘上连续可以实现更好的顺序I / O操作，因为这些叶子节点包含实际的表数据。 表空间中的某些页面包含其他页面的位图，因此表空间中的某些扩展数据InnoDB块无法整体分配给段，而只能分配给单个页面。 当您通过发出一条SHOW TABLE STATUS语句在表空间中请求可用空间时，请 InnoDB报告表空间中绝对可用的扩展区。InnoDB始终保留一定程度的清理和其他内部用途；这些保留的范围不包括在可用空间中。 从表中删除数据时，将InnoDB 收缩相应的B树索引。释放的空间是否可供其他用户使用取决于删除模式是否将单个页面或扩展区释放到表空间中。删除表或删除表中的所有行可以保证将空间释放给其他用户，但请记住，删除的行仅通过清除操作才能物理删除， 清除操作会在不再需要事务回滚或一致读取后的一段时间自动发生。 。（请参见 第14.3节“ InnoDB多版本”。） 要查看有关表空间的信息，请使用表空间监视器。请参见第14.17节“ InnoDB监视器”。 页面如何与表格行相关最大行长度略小于数据库页面的一半。例如，对于默认的16KB InnoDB页面大小，最大行长度略小于8KB ，这是由innodb_page_size 配置选项定义的。 如果一行没有超过页面限制的一半，则所有行都将存储在页面内。如果某行超出了半页限制，那么 将选择可变长度列用于外部页外存储，直到该行适合半页之内。可变长度列的外部页外存储因行格式而异： 紧凑和冗余行格式 当将可变长度列选择用于外部页外存储时，InnoDB将前768个字节本地存储在该行中，其余部分从外部存储到溢出页中。每个此类列都有其自己的溢出页面列表。768字节的前缀附带一个20字节的值，该值存储列的真实长度，并指向存储剩余值的溢出列表。请参见 第14.11节“ InnoDB行格式”。 动态和压缩行格式 如果将可变长度列选择用于外部页外存储，InnoDB则在行中本地存储一个20字节的指针，其余部分在外部存储到溢出页中。请参见第14.11节“ InnoDB行格式”。 LONGBLOB和 LONGTEXT列必须小于4GB，并且总行长（包括 BLOB和 TEXT列）必须小于4GB。 14.12.3 InnoDB检查点使你的日志文件非常大时可以减少磁盘I / O的 检查点。通常将日志文件的总大小设置为与缓冲池一样大，甚至更大。尽管从MySQL 5.5开始，过去大型日志文件可能会使崩溃恢复花费大量时间，但崩溃恢复的性能增强使崩溃后可以快速启动使用大型日志文件。（严格来说，对于具有InnoDB插件1.0.7和更高版本的MySQL 5.1，可以实现这种性能改进。对于MySQL 5.5，可以在默认的InnoDB存储引擎中实现此改进。） 检查点处理的工作原理1InnoDB`实现称为[模糊检查点](https://dev.mysql.com/doc/refman/5.6/en/glossary.html#glos_fuzzy_checkpointing)的 [检查点](https://dev.mysql.com/doc/refman/5.6/en/glossary.html#glos_checkpoint)机制。小批量地从缓冲池中刷新已修改的数据库页面。无需单批刷新缓冲池，这会在检查点过程中中断用户SQL语句的处理。 `InnoDB 在崩溃恢复期间， InnoDB查找写入日志文件的检查点标签。它知道在标签之前对数据库的所有修改都存在于数据库的磁盘映像中。然后InnoDB从检查点向前扫描日志文件，将记录的修改应用于数据库。 14.12.4对表进行碎片整理随机插入二级索引或从二级索引中删除可能导致索引碎片化。碎片意味着磁盘上索引页的物理顺序与页面上记录的索引顺序不接近，或者64页块中有许多未使用的页已分配给索引。 碎片的一个症状是表占用的空间超过了它应 “占用”的空间。到底有多少是很难确定的。所有InnoDB数据和索引都存储在B树中，它们的填充因子可能在50％到100％之间变化。碎片化的另一个症状是，这样的表扫描花费的时间比“ 应该 ”花费的时间更多 ： 1SELECT COUNT(*) FROM t WHERE non_indexed_column &lt;&gt; 12345; 前面的查询要求MySQL执行全表扫描，这是大型表的最慢查询类型。 为了加快索引扫描的速度，您可以定期执行 “ null ” ALTER TABLE 操作，这会导致MySQL重建表： 1ALTER TABLE tbl_name ENGINE=INNODB 在MySQL 5.6.3中，你也可以使用 执行 “ 空 ”，重建表alter操作。以前，该选项已被识别但被忽略。 ALTER TABLE *tbl_name* FORCEFORCE 在MySQL 5.6.17，双方 并 使用 在线DDL。有关更多信息，请参见第14.13节“ InnoDB和在线DDL”。 ALTER TABLE *tbl_name* ENGINE=INNODBALTER TABLE *tbl_name* FORCE 执行碎片整理操作的另一种方法是使用 mysqldump将表转储到文本文件，删除表并从转储文件重新加载它。 如果对索引的插入总是递增的，并且记录仅从末尾删除，则InnoDB 文件空间管理算法可确保不会发生索引中的碎片。 14.12.5使用TRUNCATE TABLE回收磁盘空间当回收操作系统的磁盘空间 截断的 InnoDB表，该表必须存放在自己的的.ibd文件。若要将表存储在其自己的.ibd 文件中，innodb_file_per_table必须在创建表时启用该表。此外，被截断的表和其他表之间不能有 外键约束，否则 TRUNCATE TABLE操作将失败。但是，允许在同一表的两列之间使用外键约束。 截断表后，将其删除并在新.ibd文件中重新创建 ，并将释放的空间返回给操作系统。这与InnoDB存储在 InnoDB 系统表空间 （在时创建的表 innodb_file_per_table=OFF）中InnoDB的表被截断相反 ，后者在表被截断后只能 使用释放的空间。 截断表并将磁盘空间返回给操作系统的能力还意味着 物理备份可以更小。截断存储在系统表空间中的表（时创建的表 innodb_file_per_table=OFF）在系统表空间中留下未使用空间的块。 14.17 InnoDB监视器 14.17.1 InnoDB监视器类型 14.17.2启用InnoDB监视器 14.17.3 InnoDB标准监视器和锁定监视器输出 14.17.4 InnoDB表空间监视器输出 14.17.5 InnoDB表监视器输出 InnoDB监视器提供有关 InnoDB内部状态的信息。此信息对于性能调整很有用。 14.17.1 InnoDB监视器类型InnoDB监视器 有四种类型： 标准InnoDB监视器显示以下类型的信息： 主后台线程完成的工作 信号量等待 有关最新外键和死锁错误的数据 锁等待交易 活动交易持有的表和记录锁 待处理的I / O操作和相关统计信息 插入缓冲区和自适应哈希索引统计信息 Redo Log数据 缓冲池统计 行操作数据 该InnoDB锁监控打印附加锁信息作为标准的一部分 InnoDB监视器输出。 的InnoDB表空间监视器打印在共享表文件的段的列表并验证表分配的数据结构。 该InnoDB表监控打印内容InnoDB的内部数据字典。 注意 Tablespace Monitor和Table Monitor已过时，并将在以后的MySQL版本中删除。可以从InnoDB INFORMATION_SCHEMA表中获得表监视器的类似信息 。请参见 第21.30节“ INFORMATION_SCHEMA InnoDB表”。 有关InnoDB表和表空间监视器的其他信息，请参见 Mark Leith：InnoDB表和表空间监视器。 14.17.2启用InnoDB监视器当InnoDB监视器用于周期性输出使能，InnoDB将输出写入 的mysqld服务器标准错误输出（stderr）每15秒，约。 InnoDB将监视器的输出发送到 内存缓冲区，stderr而不是stdout固定大小的缓冲区，以免潜在的缓冲区溢出。 在Windows上，stderr除非另有配置，否则定向到默认日志文件。如果要将输出定向到控制台窗口而不是错误日志，请从控制台窗口中带有--console选项的命令提示符启动服务器 。有关更多信息，请参见第5.4.2.1节“ Windows上的错误记录”。 在Unix和类似Unix的系统上，stderr除非另行配置，否则通常直接指向终端。有关更多信息，请参见第5.4.2.2节“在Unix和类似Unix的系统上记录错误”。 InnoDB仅在您实际希望查看监视器信息时才应启用监视器，因为输出生成会导致性能降低。另外，如果将监视器输出定向到错误日志，则如果以后忘记通过删除监视器表来禁用监视器，则日志可能会变得很大。 注意 为了帮助进行故障排除，请在某些情况下InnoDB 临时启用标准InnoDBMonitor输出。有关更多信息，请参见 第14.21节“ InnoDB故障排除”。 每个监视器都以包含时间戳和监视器名称的标题开头。例如： 123=====================================2014-10-16 16:28:15 7feee43c5700 INNODB MONITOR OUTPUT===================================== 标准InnoDB监视器（INNODB MONITOR OUTPUT）的标头也用于锁定监视器，因为锁定监视器在增加额外的锁定信息的情况下会产生相同的输出。 启用InnoDB标准监视器和锁定监视器以进行定期输出可以通过以下两种方法之一执行： 使用CREATE TABLE语句创建InnoDB与监视器关联的特别命名的表。例如，要启用标准InnoDBMonitor，您将创建一个InnoDB名为的表 innodb_monitor。还使用该CREATE TABLE方法启用了表空间监视器和表监视器。 使用CREATE TABLE语法只是InnoDB 通过MySQL的SQL解析器将命令传递给引擎的一种方法。唯一重要的是表名以及它是一个InnoDB 表。表的结构与创建表的数据库无关。如果关闭服务器，则在重新启动服务器时监视器不会自动重新启动。删除监视器表并发出新的 CREATE TABLE语句以启动监视器。 注意 CREATE TABLE启用InnoDB监视器 的方法已过时，将来的发行版中可能会删除该方法。从MySQL 5.6.16开始，您可以使用和 系统变量启用标准InnoDB Monitor和InnoDBLock Monitor 。 innodb_status_outputinnodb_status_output_locks 使用 MySQL 5.6.16中引入的 innodb_status_output和 innodb_status_output_locks系统变量。 该PROCESS权限才能启用或禁用InnoDB监视器。 启用标准InnoDB监视器要启用标准InnoDB Monitor的定期输出，请创建innodb_monitor表： 1CREATE TABLE innodb_monitor (a INT) ENGINE=INNODB; 要禁用标准InnoDB监视器，请删除表： 1DROP TABLE innodb_monitor; 从MySQL 5.6.16开始，您还可以InnoDB通过将innodb_status_output系统变量设置为来启用标准 Monitor ON。 1SET GLOBAL innodb_status_output=ON; 要禁用标准InnoDB监视器，请设置 innodb_status_output为 OFF。 关闭服务器时，该 innodb_status_output变量将设置为默认OFF值。 启用InnoDB锁定监视器要启用InnoDB锁定监视器以进行定期输出，请创建innodb_lock_monitor表： 1CREATE TABLE innodb_lock_monitor (a INT) ENGINE=INNODB; 要禁用InnoDB锁定监视器，请删除表： 1DROP TABLE innodb_lock_monitor; 从MySQL 5.6.16开始，您还可以InnoDB通过将innodb_status_output_locks系统变量设置为来启用 锁定监视器 ON。与CREATE TABLE启用InnoDB监视器 的 方法一样， 必须同时启用InnoDB标准监视器和 InnoDB锁定监视器，才能 InnoDB定期打印锁定监视器数据： 12SET GLOBAL innodb_status_output=ON;SET GLOBAL innodb_status_output_locks=ON; 关闭服务器时， innodb_status_output和 innodb_status_output_locks 变量将设置为默认OFF值。 要禁用InnoDB锁定监视器，请设置 innodb_status_output_locks为 OFF。设置 innodb_status_output为OFF也将禁用标准InnoDB监视器。 注意 要启用InnoDB锁定监视器的 SHOW ENGINE INNODB STATUS输出，只需启用 innodb_status_output_locks。 获得标准的InnoDB监视器按需输出作为启用标准InnoDBMonitor定期输出的替代方法 ，您可以InnoDB使用SHOW ENGINE INNODB STATUSSQL语句按需获取标准Monitor输出，该语句将输出提取到客户端程序。如果您使用的是mysql 交互式客户端，则将常用的分号语句终止符替换为，则输出更具可读性\G： 1mysql&gt; SHOW ENGINE INNODB STATUS\G SHOW ENGINE INNODB STATUSInnoDB 如果InnoDB启用了锁定监视器，则输出还包括锁定监视器数据。 将标准InnoDB监视器输出定向到状态文件1InnoDB`通过`--innodb-status-file`在启动时指定选项，可以启用 标准监视器输出并将其定向到状态文件 。使用此选项时，`InnoDB`将 在数据目录中创建一个名称为文件的文件 ，并大约每15秒将输出写入其中。 `innodb_status.*`pid`* InnoDB正常关闭服务器后，将删除状态文件。如果发生异常关闭，则可能必须手动删除状态文件。 该--innodb-status-file选项仅供临时使用，因为输出生成会影响性能，并且 文件会随着时间变得很大。 innodb_status.*pid* 启用InnoDB表空间监视器要为InnoDB表空间监视器启用定期输出，请创建 innodb_tablespace_monitor表： 1CREATE TABLE innodb_tablespace_monitor (a INT) ENGINE=INNODB; 要禁用标准InnoDB表空间监视器，请删除表： 1DROP TABLE innodb_tablespace_monitor; 注意 Tablespace Monitor已弃用，并将在以后的MySQL版本中删除。 启用InnoDB表监视器要为InnoDB表监视器启用定期输出，请创建innodb_table_monitor表： 1CREATE TABLE innodb_table_monitor (a INT) ENGINE=INNODB; 要禁用InnoDB表监视器，请删除表： 1DROP TABLE innodb_table_monitor; 注意 Tablespace Monitor已弃用，并将在以后的MySQL版本中删除。 14.17.3 InnoDB标准监视器和锁定监视器输出锁定监视器与标准监视器相同，只是它包含其他锁定信息。为任一监视器启用定期输出会打开同一输出流，但是如果启用了锁定监视器，则该流将包含其他信息。例如，如果启用“标准监视器”和“锁定监视器”，则将打开单个输出流。在禁用锁定监视器之前，流中将包含其他锁定信息。 使用该SHOW ENGINE INNODB STATUS语句生成时，Standard Monitor的输出限制为1MB 。此限制不适用于写入服务器标准错误输出（stderr）的输出。 标准监视器输出示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241mysql&gt; SHOW ENGINE INNODB STATUS\G*************************** 1. row *************************** Type: InnoDB Name:Status:=====================================2014-10-17 10:33:50 7f47bcd64700 INNODB MONITOR OUTPUT=====================================Per second averages calculated from the last 6 seconds-----------------BACKGROUND THREAD-----------------srv_master_thread loops: 167 srv_active, 0 srv_shutdown, 3023 srv_idlesrv_master_thread log flush and writes: 3190----------SEMAPHORES----------OS WAIT ARRAY INFO: reservation count 1040OS WAIT ARRAY INFO: signal count 959Mutex spin waits 677, rounds 20336, OS waits 644RW-shared spins 180, rounds 5400, OS waits 180RW-excl spins 0, rounds 6420, OS waits 214Spin rounds per wait: 30.04 mutex, 30.00 RW-shared, 6420.00 RW-excl------------------------LATEST FOREIGN KEY ERROR------------------------2014-10-17 09:51:31 7f47bcde6700 Transaction:TRANSACTION 436786, ACTIVE 0 sec insertingmysql tables in use 1, locked 14 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 3MySQL thread id 1, OS thread handle 0x7f47bcde6700, query id 96 localhostroot updateINSERT INTO child VALUES (NULL, 1) , (NULL, 2) , (NULL, 3) , (NULL, 4) , (NULL, 5) , (NULL, 6)Foreign key constraint fails for table `mysql`.`child`:, CONSTRAINT `child_ibfk_1` FOREIGN KEY (`parent_id`) REFERENCES `parent` (`id`) ON DELETE CASCADE ON UPDATE CASCADETrying to add in child table, in index `par_ind` tuple:DATA TUPLE: 2 fields; 0: len 4; hex 80000003; asc ;; 1: len 4; hex 80000003; asc ;;But in parent table `mysql`.`parent`, in index `PRIMARY`,the closest match we can find is record:PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 4; hex 80000004; asc ;; 1: len 6; hex 00000006aa26; asc &amp;;; 2: len 7; hex 9d000001610137; asc a 7;;------------------------LATEST DETECTED DEADLOCK------------------------2014-10-17 09:52:38 7f47bcde6700*** (1) TRANSACTION:TRANSACTION 436801, ACTIVE 12 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 2 lock struct(s), heap size 360, 1 row lock(s)MySQL thread id 2, OS thread handle 0x7f47bcda5700, query id 102 localhostroot updatingDELETE FROM t WHERE i = 1*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 3693 page no 3 n bits 72 index `GEN_CLUST_INDEX` oftable `mysql`.`t` trx id 436801 lock_mode X waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; infobits 0 0: len 6; hex 000000003a00; asc : ;; 1: len 6; hex 00000006aa3f; asc ?;; 2: len 7; hex ad0000021d0110; asc ;; 3: len 4; hex 80000001; asc ;;*** (2) TRANSACTION:TRANSACTION 436800, ACTIVE 34 sec starting index readmysql tables in use 1, locked 14 lock struct(s), heap size 1184, 3 row lock(s)MySQL thread id 1, OS thread handle 0x7f47bcde6700, query id 103 localhostroot updatingDELETE FROM t WHERE i = 1*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 3693 page no 3 n bits 72 index `GEN_CLUST_INDEX` oftable `mysql`.`t` trx id 436800 lock mode SRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; infobits 0 0: len 8; hex 73757072656d756d; asc supremum;;Record lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; infobits 0 0: len 6; hex 000000003a00; asc : ;; 1: len 6; hex 00000006aa3f; asc ?;; 2: len 7; hex ad0000021d0110; asc ;; 3: len 4; hex 80000001; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 3693 page no 3 n bits 72 index `GEN_CLUST_INDEX` oftable `mysql`.`t` trx id 436800 lock_mode X waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 4; compact format; infobits 0 0: len 6; hex 000000003a00; asc : ;; 1: len 6; hex 00000006aa3f; asc ?;; 2: len 7; hex ad0000021d0110; asc ;; 3: len 4; hex 80000001; asc ;;*** WE ROLL BACK TRANSACTION (1)------------TRANSACTIONS------------Trx id counter 437661Purge done for trx's n:o &lt; 437657 undo n:o &lt; 0 state: running butidle History list length 371LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 0, not startedMySQL thread id 10, OS thread handle 0x7f47bcd64700, query id 1001 localhostroot initSHOW ENGINE INNODB STATUS---TRANSACTION 436801, not startedMySQL thread id 2, OS thread handle 0x7f47bcda5700, query id 102 localhostroot ceaning up---TRANSACTION 437660, ACTIVE 0 sec insertingmysql tables in use 1, locked 143 lock struct(s), heap size 6544, 6474 row lock(s), undo log entries 7124MySQL thread id 14, OS thread handle 0x7f47bcde6700, query id 1000 localhostroot updateINSERT INTO `dept_emp` VALUES (100258,'d002','1994-03-21','9999-01-01'),(100259, 'd005','1998-11-04','9999-01-01'),(100259,'d008','1988-02-03','1998-11-04'),(100 260,'d005','1998-09-18','9999-01-01'),(100261,'d004','1989-03-11','9999-01-01'), (100262,'d008','1996-08-12','9999-01-01'),(100263,'d002','1998-06-24','1998-10-0 5'),(100264,'d005','1989-11-09','9999-01-01'),(100265,'d001','1992-06-27','9999- 01-01'),(100266,'d009','1990-09-10','9999-01-01'),(100267,'d009','1992-04-14','9 999-01-01'),(100268,'d005','1998-05-01','2000-04-07'),(100269,'d007','1994-01-02','1999-09-18'),(100269,'d009','1999-09---------FILE I/O--------I/O thread 0 state: waiting for completed aio requests (insert buffer thread)I/O thread 1 state: waiting for completed aio requests (log thread)I/O thread 2 state: waiting for completed aio requests (read thread)I/O thread 3 state: waiting for completed aio requests (read thread)I/O thread 4 state: waiting for completed aio requests (read thread)I/O thread 5 state: waiting for completed aio requests (read thread)I/O thread 6 state: waiting for completed aio requests (write thread)I/O thread 7 state: waiting for completed aio requests (write thread)I/O thread 8 state: waiting for completed aio requests (write thread)I/O thread 9 state: waiting for completed aio requests (write thread)Pending normal aio reads: 0 [0, 0, 0, 0] , aio writes: 0 [0, 0, 0, 0] , ibuf aio reads: 0, log i/o's: 0, sync i/o's: 0Pending flushes (fsync) log: 0; buffer pool: 0344 OS file reads, 45666 OS file writes, 4030 OS fsyncs0.00 reads/s, 0 avg bytes/read, 202.80 writes/s, 48.33 fsyncs/s-------------------------------------INSERT BUFFER AND ADAPTIVE HASH INDEX-------------------------------------Ibuf: size 1, free list len 0, seg size 2, 0 mergesmerged operations: insert 0, delete mark 0, delete 0discarded operations: insert 0, delete mark 0, delete 0Hash table size 4425293, node heap has 143 buffer(s)137083.82 hash searches/s, 2495.92 non-hash searches/s---LOG---Log sequence number 3091027710Log flushed up to 3090240098Pages flushed up to 3074432960Last checkpoint at 30508562660 pending log writes, 0 pending chkp writes1187 log i/o's done, 14.67 log i/o's/second----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 2197815296; in additional pool allocated 0Dictionary memory allocated 155455Buffer pool size 131071Free buffers 92158Database pages 38770Old database pages 14271Modified db pages 619Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 4, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 322, created 38448, written 420830.00 reads/s, 222.30 creates/s, 159.47 writes/sBuffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead0.00/sLRU len: 38770, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0]----------------------INDIVIDUAL BUFFER POOL INFO-------------------------BUFFER POOL 0Buffer pool size 65536Free buffers 46120Database pages 19345Old database pages 7121Modified db pages 291Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 3, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 163, created 19182, written 211490.00 reads/s, 103.48 creates/s, 83.15 writes/sBuffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead0.00/sLRU len: 19345, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0]---BUFFER POOL 1Buffer pool size 65535Free buffers 46038Database pages 19425Old database pages 7150Modified db pages 328Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 1, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 159, created 19266, written 209340.00 reads/s, 118.81 creates/s, 76.32 writes/sBuffer pool hit rate 1000 / 1000, young-making rate 0 / 1000 not 0 / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead0.00/sLRU len: 19425, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0]--------------ROW OPERATIONS--------------0 queries inside InnoDB, 0 queries in queue0 read views open inside InnoDBMain thread process no. 54607, id 139946075744000, state: sleepingNumber of rows inserted 12163964, updated 0, deleted 3, read 467807.03 inserts/s, 0.00 updates/s, 0.00 deletes/s, 0.00 reads/s----------------------------END OF INNODB MONITOR OUTPUT============================ 标准监视器输出部分有关标准监视器报告的每个度量的说明，请参阅 《Oracle Enterprise Manager for MySQL数据库用户指南》中的“ 度量”一章 。 Status 本节显示时间戳，监视器名称以及每秒平均值所基于的秒数。秒数是从当前时间到最后一次InnoDB打印监视器输出之间经过的时间。 BACKGROUND THREAD 这些srv_master_thread行显示了由主后台线程完成的工作。 SEMAPHORES 本节报告线程等待信号量的统计信息，并统计线程需要旋转或等待互斥量或rw-lock信号量的次数。等待信号量的大量线程可能是磁盘I / O或内部争用的结果InnoDB。争用可能是由于查询的高度并行性或操作系统线程调度中的问题。innodb_thread_concurrency 在这种情况下，将系统变量设置为 小于默认值可能会有所帮助。该Spin rounds per wait 行显示每个操作系统等待互斥锁的自旋锁轮数。 LATEST FOREIGN KEY ERROR 本节提供有关最新外键约束错误的信息。如果没有发生此类错误，则不存在。内容包括失败的语句以及有关失败的约束以及所引用和引用表的信息。 LATEST DETECTED DEADLOCK 本节提供有关最新死锁的信息。如果没有发生死锁，则不存在。内容显示涉及哪些事务，每个尝试执行的语句，它们拥有和需要的锁，以及哪个事务InnoDB 决定回滚以打破死锁。第14.7.1节“ InnoDB锁定”中说明了本节中报告的锁定模式 。 TRANSACTIONS 如果此部分报告锁定等待，则您的应用程序可能具有锁定争用。输出还可以帮助跟踪事务死锁的原因。 FILE I/O 本节提供有关InnoDB用于执行各种I / O的线程的信息 。其中的前几个专用于常规 InnoDB处理。内容还显示有关挂起的I / O操作的信息和有关I / O性能的统计信息。 这些线程的数量由innodb_read_io_threads和 innodb_write_io_threads 参数控制 。请参见第14.14节“ InnoDB启动选项和系统变量”。 INSERT BUFFER AND ADAPTIVE HASH INDEX 本部分显示 InnoDB插入缓冲区（也称为Change Buffer）和自适应哈希索引的状态。 有关相关信息，请参见 第14.5.2节“Change Buffer”和 第14.5.3节“自适应哈希索引”。 LOG 本部分显示有关InnoDB日志的信息 。内容包括当前日志序列号，已将日志刷新到磁盘的距离以及InnoDB上次执行检查点的位置 。（请参见 第14.12.3节“ InnoDB检查点”。）本节还显示有关挂起的写入和写入性能统计信息。 BUFFER POOL AND MEMORY 本节为您提供有关已读和已写页面的统计信息。您可以从这些数字中计算出查询当前正在执行多少个数据文件I / O操作。 有关缓冲池统计信息的描述，请参阅 使用InnoDB Standard Monitor监视缓冲池。有关缓冲池操作的更多信息，请参见第14.5.1节“缓冲池”。 ROW OPERATIONS 本节显示了主线程在做什么，包括每种行操作的数量和性能比率。 14.17.4 InnoDB表空间监视器输出注意 该InnoDB表空间监视器是过时的，并且可以在未来版本中删除。 的InnoDB表空间监视器打印关于共享表的文件段的信息，并验证该表空间分配数据结构。表空间监视器未描述使用该innodb_file_per_table选项创建的每表文件表空间 。 InnoDB表空间监视器输出 示例： 123456789101112131415161718192021222324252627282930313233343536================================================090408 21:28:09 INNODB TABLESPACE MONITOR OUTPUT================================================FILE SPACE INFO: id 0size 13440, free limit 3136, free extents 28not full frag extents 2: used pages 78, full frag extents 3first seg id not used 0 23845SEGMENT id 0 1 space 0; page 2; res 96 used 46; full ext 0fragm pages 32; free extents 0; not full extents 1: pages 14SEGMENT id 0 2 space 0; page 2; res 1 used 1; full ext 0fragm pages 1; free extents 0; not full extents 0: pages 0SEGMENT id 0 3 space 0; page 2; res 1 used 1; full ext 0fragm pages 1; free extents 0; not full extents 0: pages 0...SEGMENT id 0 15 space 0; page 2; res 160 used 160; full ext 2fragm pages 32; free extents 0; not full extents 0: pages 0SEGMENT id 0 488 space 0; page 2; res 1 used 1; full ext 0fragm pages 1; free extents 0; not full extents 0: pages 0SEGMENT id 0 17 space 0; page 2; res 1 used 1; full ext 0fragm pages 1; free extents 0; not full extents 0: pages 0...SEGMENT id 0 171 space 0; page 2; res 592 used 481; full ext 7fragm pages 16; free extents 0; not full extents 2: pages 17SEGMENT id 0 172 space 0; page 2; res 1 used 1; full ext 0fragm pages 1; free extents 0; not full extents 0: pages 0SEGMENT id 0 173 space 0; page 2; res 96 used 44; full ext 0fragm pages 32; free extents 0; not full extents 1: pages 12...SEGMENT id 0 601 space 0; page 2; res 1 used 1; full ext 0fragm pages 1; free extents 0; not full extents 0: pages 0NUMBER of file segments: 73Validating tablespaceValidation ok---------------------------------------END OF INNODB TABLESPACE MONITOR OUTPUT======================================= 表空间监视器输出包括有关共享表空间整体的信息，其后是一个列表，其中包含表空间内每个段的细分。 在此示例中，使用默认 页面大小，表空间由每个16KB的数据库页面组成。这些页面分为大小为1MB的扩展区（连续64个页面）。 显示总体表空间信息的输出的初始部分具有以下格式： 1234FILE SPACE INFO: id 0size 13440, free limit 3136, free extents 28not full frag extents 2: used pages 78, full frag extents 3first seg id not used 0 23845 表空间的总体信息包括以下值： id 表空间ID。值为0表示共享表空间。 size 当前表空间大小（以页为单位）。 free limit 空闲列表尚未初始化的最小页码。达到或超过此限制的页面是免费的。 free extents 免费范围的数量。 not full frag extents， used pages 未完全填充的片段扩展区数，以及这些扩展区中已分配的页数。 full frag extents 完全完整的片段范围的数量。 first seg id not used 第一个未使用的段ID。 各个细分受众群信息具有以下格式： 12SEGMENT id 0 15 space 0; page 2; res 160 used 160; full ext 2fragm pages 32; free extents 0; not full extents 0: pages 0 细分信息包括以下值： id： 细分ID。 space， page 表空间号和段“ inode ”所在的表空间内的页面。表空间号0表示共享表空间。 InnoDB使用inode来跟踪表空间中的段。为段显示的其他字段（id，res等）从inode中的信息派生。 res 为该段分配（保留）的页面数。 used 段正在使用的已分配页面数。 full ext 完全使用的为段分配的扩展区数。 fragm pages 已分配给该段的初始页面数。 free extents 完全未使用的为段分配的扩展区数。 not full extents 为该段分配的部分使用的扩展区数。 pages 在未满范围内使用的页面数。 当一个段增长时，它从一个页面开始，并 InnoDB一次为其分配前几页，最多32页（这是fragm pages 值）。之后，InnoDB分配完整范围。InnoDB一次最多可以向一个大段中添加4个扩展区，以确保数据的良好顺序。 对于前面显示的示例段，它具有32个片段页面，外加2个完整扩展（每个64页），在分配的160个页面中总共使用了160个页面。以下部分包含32个片段页面和一个部分完整的范围，使用14个页面，在分配的96个页面中总共使用了46个页面： 12SEGMENT id 0 1 space 0; page 2; res 96 used 46; full ext 0fragm pages 32; free extents 0; not full extents 1: pages 14 如果在扩展区分配之后fragm pages某些个别页面已被重新分配，则为其分配扩展区的段的值可能小于32。 14.17.5 InnoDB表监视器输出注意 该InnoDB表监控已弃用，可能在将来的版本中删除。可以从InnoDB INFORMATION_SCHEMA表中获得类似的信息。请参见 第21.30节“ INFORMATION_SCHEMA InnoDB表”。 该InnoDB表监控打印内容InnoDB的内部数据字典。 输出每个表包含一个部分。在 SYS_FOREIGN与 SYS_FOREIGN_COLS部分是维持大约外键的信息的内部数据字典表。表监视器表和每个用户创建的InnoDB表也有一些部分。假设已在test数据库中创建以下两个表 ： 123456789101112131415161718192021222324CREATE TABLE parent( par_id INT NOT NULL, fname CHAR(20), lname CHAR(20), PRIMARY KEY (par_id), UNIQUE INDEX (lname, fname)) ENGINE = INNODB;CREATE TABLE child( par_id INT NOT NULL, child_id INT NOT NULL, name VARCHAR(40), birth DATE, weight DECIMAL(10,2), misc_info VARCHAR(255), last_update TIMESTAMP, PRIMARY KEY (par_id, child_id), INDEX (name), FOREIGN KEY (par_id) REFERENCES parent (par_id) ON DELETE CASCADE ON UPDATE CASCADE) ENGINE = INNODB; 然后，表监视器的输出将类似于以下内容（略微更改）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576===========================================090420 12:09:32 INNODB TABLE MONITOR OUTPUT===========================================--------------------------------------TABLE: name SYS_FOREIGN, id 0 11, columns 7, indexes 3, appr.rows 1 COLUMNS: ID: DATA_VARCHAR DATA_ENGLISH len 0; FOR_NAME: DATA_VARCHAR DATA_ENGLISH len 0; REF_NAME: DATA_VARCHAR DATA_ENGLISH len 0; N_COLS: DATA_INT len 4; DB_ROW_ID: DATA_SYS prtype 256 len 6; DB_TRX_ID: DATA_SYS prtype 257 len 6; INDEX: name ID_IND, id 0 11, fields 1/6, uniq 1, type 3 root page 46, appr.key vals 1, leaf pages 1, size pages 1 FIELDS: ID DB_TRX_ID DB_ROLL_PTR FOR_NAME REF_NAME N_COLS INDEX: name FOR_IND, id 0 12, fields 1/2, uniq 2, type 0 root page 47, appr.key vals 1, leaf pages 1, size pages 1 FIELDS: FOR_NAME ID INDEX: name REF_IND, id 0 13, fields 1/2, uniq 2, type 0 root page 48, appr.key vals 1, leaf pages 1, size pages 1 FIELDS: REF_NAME ID--------------------------------------TABLE: name SYS_FOREIGN_COLS, id 0 12, columns 7, indexes 1, appr.rows 1 COLUMNS: ID: DATA_VARCHAR DATA_ENGLISH len 0; POS: DATA_INT len 4; FOR_COL_NAME: DATA_VARCHAR DATA_ENGLISH len 0; REF_COL_NAME: DATA_VARCHAR DATA_ENGLISH len 0; DB_ROW_ID: DATA_SYS prtype 256 len 6; DB_TRX_ID: DATA_SYS prtype 257 len 6; INDEX: name ID_IND, id 0 14, fields 2/6, uniq 2, type 3 root page 49, appr.key vals 1, leaf pages 1, size pages 1 FIELDS: ID POS DB_TRX_ID DB_ROLL_PTR FOR_COL_NAME REF_COL_NAME--------------------------------------TABLE: name test/child, id 0 14, columns 10, indexes 2, appr.rows 201 COLUMNS: par_id: DATA_INT DATA_BINARY_TYPE DATA_NOT_NULL len 4; child_id: DATA_INT DATA_BINARY_TYPE DATA_NOT_NULL len 4; name: DATA_VARCHAR prtype 524303 len 40; birth: DATA_INT DATA_BINARY_TYPE len 3; weight: DATA_FIXBINARY DATA_BINARY_TYPE len 5; misc_info: DATA_VARCHAR prtype 524303 len 255; last_update: DATA_INT DATA_UNSIGNED DATA_BINARY_TYPE DATA_NOT_NULL len 4; DB_ROW_ID: DATA_SYS prtype 256 len 6; DB_TRX_ID: DATA_SYS prtype 257 len 6; INDEX: name PRIMARY, id 0 17, fields 2/9, uniq 2, type 3 root page 52, appr.key vals 201, leaf pages 5, size pages 6 FIELDS: par_id child_id DB_TRX_ID DB_ROLL_PTR name birth weight misc_info last_update INDEX: name name, id 0 18, fields 1/3, uniq 3, type 0 root page 53, appr.key vals 210, leaf pages 1, size pages 1 FIELDS: name par_id child_id FOREIGN KEY CONSTRAINT test/child_ibfk_1: test/child ( par_id ) REFERENCES test/parent ( par_id )--------------------------------------TABLE: name test/innodb_table_monitor, id 0 15, columns 4, indexes 1, appr.rows 0 COLUMNS: i: DATA_INT DATA_BINARY_TYPE len 4; DB_ROW_ID: DATA_SYS prtype 256 len 6; DB_TRX_ID: DATA_SYS prtype 257 len 6; INDEX: name GEN_CLUST_INDEX, id 0 19, fields 0/4, uniq 1, type 1 root page 193, appr.key vals 0, leaf pages 1, size pages 1 FIELDS: DB_ROW_ID DB_TRX_ID DB_ROLL_PTR i--------------------------------------TABLE: name test/parent, id 0 13, columns 6, indexes 2, appr.rows 299 COLUMNS: par_id: DATA_INT DATA_BINARY_TYPE DATA_NOT_NULL len 4; fname: DATA_CHAR prtype 524542 len 20; lname: DATA_CHAR prtype 524542 len 20; DB_ROW_ID: DATA_SYS prtype 256 len 6; DB_TRX_ID: DATA_SYS prtype 257 len 6; INDEX: name PRIMARY, id 0 15, fields 1/5, uniq 1, type 3 root page 50, appr.key vals 299, leaf pages 2, size pages 3 FIELDS: par_id DB_TRX_ID DB_ROLL_PTR fname lname INDEX: name lname, id 0 16, fields 2/3, uniq 2, type 2 root page 51, appr.key vals 300, leaf pages 1, size pages 1 FIELDS: lname fname par_id FOREIGN KEY CONSTRAINT test/child_ibfk_1: test/child ( par_id ) REFERENCES test/parent ( par_id )-----------------------------------END OF INNODB TABLE MONITOR OUTPUT================================== 对于每个表，表监视器输出包含一个部分，该部分显示有关表的常规信息以及有关其列，索引和外键的特定信息。 每个表的常规信息包括表名（ 内部表以外的格式），其ID，列数和索引数以及大概的行数。 *db_name*/*tbl_name* COLUMNS表部分 的一部分列出了表中的每一列。每列的信息指示其名称和数据类型特征。某些内部列由添加InnoDB，例如 DB_ROW_ID（行ID）， DB_TRX_ID（事务ID）和 DB_ROLL_PTR（指向回滚/撤消数据的指针）。 DATA_*xxx* 这些符号表示数据类型。给定列可能有多个 符号。 DATA_*xxx* prtype 列的“ 精确 ”类型。该字段包括诸如列数据类型，字符集代码，可空性，带符号以及是否为二进制字符串之类的信息。此字段在innobase/include/data0type.h源文件中描述 。 len 列长度（以字节为单位）。 INDEX表部分的 每一部分提供一个表索引的名称和特征： name 索引名称。如果名称为PRIMARY，则索引为主键。如果名称为 GEN_CLUST_INDEX，则索引为聚集索引，如果表定义不包含主键或非NULL 唯一索引，则聚集索引将自动创建。请参见第14.6.2.1节“聚集索引和二级索引”。 id 索引ID。 fields 索引中的字段数， 格式为： *m*/*n* m是用户定义的列数；也就是说，您将在CREATE TABLE 语句的索引定义中看到的列数。 n是索引列的总数，包括内部添加的索引列。对于聚集索引，总数包括表定义中的其他列，以及内部添加的所有列。对于二级索引，总数包括主键中不属于二级索引的列。 uniq 足以唯一确定索引值的前导字段数。 type 索引类型。这是一个位字段。例如，1表示聚集索引，2表示唯一索引，因此聚集索引（始终包含唯一值）的type值将为3。type值为0 的索引 既不是聚集也不是唯一。标志值在innobase/include/dict0mem.h源文件中定义 。 root page 索引根页号。 appr. key vals 近似索引基数。 leaf pages 索引中叶子页的大概数量。 size pages 索引中的大约总页数。 FIELDS 索引中字段的名称。对于自动生成的聚集索引，字段列表以内部DB_ROW_ID（行ID）字段开头。 DB_TRX_ID并且 DB_ROLL_PTR总是在内部构成聚簇索引之后，紧跟构成主键的字段。对于二级索引，最终字段是主键中不属于二级索引的字段。 表部分的末尾列出了FOREIGN KEY适用于表的定义。无论表是引用表还是引用表，都会显示此信息。 14.21 InnoDB故障排除 14.21.1对InnoDB I / O问题进行故障排除 14.21.2强制InnoDB恢复 14.21.3对InnoDB数据字典操作进行故障排除 14.21.4 InnoDB错误处理 以下一般准则适用于故障排除 InnoDB问题： 当操作失败或怀疑有错误时，请查看MySQL服务器错误日志（请参见第5.4.2节“错误日志”）。 第B.3.1节“服务器错误消息参考”提供了有关InnoDB您可能遇到的一些常见特定错误的故障排除信息 。 如果故障与死锁有关 ，请在innodb_print_all_deadlocks 启用该选项的情况下运行， 以便将有关每个死锁的详细信息打印到MySQL服务器错误日志中。有关死锁的信息，请参见第14.7.5节“ InnoDB中的死锁”。 与InnoDB数据字典有关的问题包括失败的CREATE TABLE 语句（孤立表文件），无法打开 InnoDB文件以及系统找不到指定的路径错误。有关此类问题和错误的信息，请参见 第14.21.3节“对InnoDB数据字典操作进行故障排除”。 在进行故障排除时，通常最好从命令提示符下运行MySQL服务器，而不是通过 mysqld_safe或作为Windows服务运行。然后，您可以查看mysqld打印到控制台的内容，因此可以更好地了解正在发生的情况。在Windows上，使用选项启动mysqld以 --console将输出定向到控制台窗口。 使InnoDB监视器能够获取有关问题的信息（请参见 第14.17节“ InnoDB监视器”）。如果问题与性能有关，或者服务器似乎已挂起，则应启用标准监视器以打印有关的内部状态的信息InnoDB。如果问题出在锁上，请启用锁监视器。如果问题出在创建表或其他数据字典操作中，请启用表监视器以打印InnoDB内部数据字典的内容 。要查看表空间信息，请启用表空间监视器。 InnoDB``InnoDB在以下情况下临时启用标准 Monitor输出： 长时间的信号灯等待 InnoDB 在缓冲池中找不到可用的块 超过67％的缓冲池被锁堆或自适应哈希索引占用 如果怀疑表已损坏，请CHECK TABLE在该表上运行 。 14.21.1对InnoDB I / O问题进行故障排除InnoDBI / O问题 的疑难解答步骤取决于问题发生的时间：在MySQL服务器启动期间，或在正常操作过程中，由于文件系统级别的问题而导致DML或DDL语句失败。 初始化问题如果在InnoDB尝试初始化其表空间或日志文件时出现问题，请删除由InnoDB：所有 ibdata文件和所有ib_logfile文件创建的所有 文件。如果已经创建了一些 InnoDB表，还请从MySQL数据库目录中删除.frm这些表的相应 文件，以及.ibd如果使用多个表空间的所有 文件。然后InnoDB再次尝试 创建数据库。为了最简单的故障排除，请从命令提示符启动MySQL服务器，以便了解发生了什么。 运行时问题如果InnoDB在文件操作过程中显示操作系统错误，通常该问题具有以下解决方案之一： 确保InnoDB数据文件目录和InnoDB日志目录存在。 确保mysqld具有在这些目录中创建文件的访问权限。 确保mysqld可以读取正确的 文件my.cnf或my.ini选项文件，以便它以您指定的选项开头。 确保磁盘未满，并且没有超出任何磁盘配额。 确保为子目录和数据文件指定的名称不冲突。 仔细检查innodb_data_home_dir和 innodb_data_file_path值的语法 。特别是，MAX该innodb_data_file_path选项中的任何值 都是硬限制，超过该限制会导致致命错误。 14.21.2强制InnoDB恢复要调查数据库页面损坏，您可以使用来从数据库中转储表 SELECT ... INTO OUTFILE。通常，以这种方式获得的大多数数据都是完整的。严重损坏可能导致语句或 后台操作崩溃或断言，甚至导致前滚恢复崩溃。在这种情况下，您可以使用该 选项在阻止后台操作运行的同时强制启动存储引擎，以便转储表。例如，您可以在重新启动服务器之前将以下行添加到选项文件的部分中： SELECT * FROM *tbl_name*``InnoDB``InnoDBinnodb_force_recoveryInnoDB``[mysqld] 12[mysqld]innodb_force_recovery = 1 有关使用选项文件的信息，请参见 第4.2.2.2节“使用选项文件”。 警告 仅innodb_force_recovery 在紧急情况下设置为大于0的值，以便您可以启动InnoDB和转储表。这样做之前，请确保您拥有数据库的备份副本，以防万一您需要重新创建它。值大于等于4可能会永久损坏数据文件。innodb_force_recovery在数据库的单独物理副本上成功测试设置之后，请仅在生产服务器实例上使用 4或更大的设置。强制InnoDB恢复时，应始终从头开始， innodb_force_recovery=1并根据需要仅逐渐增加该值。 innodb_force_recovery默认情况下为0（正常启动而不强制恢复）。允许的非零值 innodb_force_recovery是1到6。较大的值包括较小值的功能。例如，值3包含值1和2的所有功能。 如果能够转储 innodb_force_recovery值为3或更小的表，则相对安全的是，仅丢失损坏的单个页面上的某些数据。4或更大的值被认为是危险的，因为数据文件可能会永久损坏。值6被认为是过大的，因为数据库页面处于过时状态，这反过来可能会使B树 和其他数据库结构遭受更多破坏。 为了安全起见，请InnoDB防止 INSERT， UPDATE或 DELETE在innodb_force_recovery大于0 时进行操作 。从MySQL 5.6.15开始， 在只读模式下innodb_force_recovery设置4个或更多位置InnoDB。 1 （SRV_FORCE_IGNORE_CORRUPT） 使服务器即使检测到损坏的页面也可以运行 。尝试 跳过损坏的索引记录和页，这有助于转储表。 SELECT * FROM *tbl_name* 2 （SRV_FORCE_NO_BACKGROUND） 阻止主线程和任何清除线程运行。如果在清除操作期间发生崩溃，则此恢复值可防止崩溃 。 3 （SRV_FORCE_NO_TRX_UNDO） 崩溃恢复后 不运行事务 回滚。 4 （SRV_FORCE_NO_IBUF_MERGE） 防止插入缓冲区合并操作。如果它们会导致崩溃，请不要这样做。不计算表 统计信息。此值可能会永久损坏数据文件。使用此值后，准备删除并重新创建所有二级索引。从MySQL 5.6.15开始，设置InnoDB为只读。 5 （SRV_FORCE_NO_UNDO_LOG_SCAN） 启动数据库时 不查看Undo Log： InnoDB甚至将未完成的事务也视为已提交。此值可能会永久损坏数据文件。从MySQL 5.6.15开始，设置InnoDB为只读。 6 （SRV_FORCE_NO_LOG_REDO） 不进行与恢复有关的Redo Log前 滚。此值可能会永久损坏数据文件。使数据库页面处于过时状态，从而可能导致B树和其他数据库结构遭受更多破坏。从MySQL 5.6.15开始，设置InnoDB为只读。 您可以SELECT从表中转储它们。随着 innodb_force_recovery3或更低就可以值DROP或 CREATE表。从MySQL 5.6.27开始， DROP TABLE还支持innodb_force_recovery大于3 的值。 如果您知道给定的表导致回滚崩溃，则可以将其删除。如果遇到由于批量导入或失败而导致的失控回滚ALTER TABLE，则可以终止mysqld进程并设置 innodb_force_recovery为 3不回滚就启动数据库，然后DROP启动导致失控回滚的表。 如果表数据中的损坏阻止您转储整个表内容，则带有子句的查询可能能够转储损坏部分后的表部分。 ORDER BY *primary_key* DESC 如果一个高innodb_force_recovery 值是必需的开始InnoDB，有可能是，可能导致（含有查询的复杂查询损坏的数据结构WHERE，ORDER BY或其它条款）失败。在这种情况下，您可能只能运行基本SELECT * FROM t 查询。 14.21.3对InnoDB数据字典操作进行故障排除有关表定义的信息既存储在.frm文件中，又存储 在InnoDB 数据字典中。如果您移动.frm文件，或者服务器在数据字典操作过程中崩溃，则这些信息源可能会变得不一致。 如果数据字典损坏或一致性问题使您无法启动InnoDB，请参阅 第14.21.2节“强制InnoDB恢复”以获取有关手动恢复的信息。 由于孤立表导致CREATE TABLE失败数据字典不同步的症状是 CREATE TABLE语句失败。如果发生这种情况，请查看服务器的错误日志。如果日志表明InnoDB内部数据字典中已经存在该 表，则InnoDB表空间文件中有一个孤立表，该表没有相应的.frm文件。错误消息如下所示： 12345678910InnoDB: Error: table test/parent already exists in InnoDB internalInnoDB: data dictionary. Have you deleted the .frm fileInnoDB: and not used DROP TABLE? Have you used DROP DATABASEInnoDB: for InnoDB tables in MySQL version &lt;= 3.23.43?InnoDB: See the Restrictions section of the InnoDB manual.InnoDB: You can drop the orphaned table inside InnoDB byInnoDB: creating an InnoDB table with the same name in anotherInnoDB: database and moving the .frm file to the current database.InnoDB: Then MySQL thinks the table exists, and DROP TABLE willInnoDB: succeed. 您可以按照错误消息中给出的说明删除孤立表。如果仍然无法DROP TABLE成功使用 ，则问题可能是由于mysql客户端中的名称完成 。要解决此问题，请使用 选项启动mysql客户端， --skip-auto-rehash然后重试DROP TABLE。（启用名称完成功能后，mysql试图构造一个表名称列表，当存在上述问题时，该列表将失败。） 无法打开文件错误数据字典不同步的另一个症状是MySQL打印出一个无法打开InnoDB文件的错误 ： 1ERROR 1016: Can&apos;t open file: &apos;child2.ibd&apos;. (errno: 1) 在错误日志中，您可以找到以下消息： 1234InnoDB: Cannot find table test/child2 from the internal data dictionaryInnoDB: of InnoDB though the .frm file for the table exists. Maybe youInnoDB: have deleted and recreated InnoDB data files but have forgottenInnoDB: to delete the corresponding .frm files of InnoDB tables? 这意味着存在一个孤立.frm 文件，里面没有对应的表 InnoDB。您可以.frm通过手动删除孤立 文件来删除它。 孤立的中间表如果MySQL在就地ALTER TABLE操作（ALGORITHM=INPLACE）中间退出，则 可能会留下孤立的中间表，该表占用了系统空间。本节介绍如何识别和删除孤立的中间表。 中间表名称以#sql-ib前缀（例如 #sql-ib87-856498050）开头 。随附 .frm文件具有 #sql-*前缀，并且命名不同（例如#sql-36ab_2.frm）。 要识别系统上的孤立中间表，可以查看表监视器的输出或查询 INFORMATION_SCHEMA.INNODB_SYS_TABLES。查找以开头的表名#sql。如果原始表位于 每个 表文件表空间中，#sql-*.ibd则孤立中间表的表空间文件（该 文件）应在数据库目录中可见。 1SELECT * FROM INFORMATION_SCHEMA.INNODB_SYS_TABLES WHERE NAME LIKE '%#sql%'; 要删除孤立的中间表，请执行以下步骤： 在数据库目录中，重命名 #sql-*.frm文件以匹配孤立中间表的基本名称： 1shell&gt; mv #sql-36ab_2.frm #sql-ib87-856498050.frm 注意 如果没有.frm文件，则可以重新创建它。该.frm文件必须具有与孤立中间表相同的表架构（它必须具有相同的列和索引），并且必须放置在孤立中间表的数据库目录中。 通过发出一条DROP TABLE语句，在表名前面加上前缀 #mysql50#并将表名括在反引号中来删除孤立的中间表 。例如： 1mysql&gt; DROP TABLE `#mysql50##sql-ib87-856498050`; 该#mysql50#前缀告诉MySQL忽略file name safe encoding在MySQL 5.1中引入的。要对具有特殊字符（例如，“ ＃ ”）的表名执行SQL语句，需要将表名括在反引号中。 孤儿临时表如果MySQL在表复制ALTER TABLE操作（ALGORITHM=COPY）中间退出，则 可能会留下一个孤立的临时表，该表会占用系统空间。本节介绍如何识别和删除孤立的临时表。 孤立的临时表名称以#sql-前缀（例如 #sql-540_3）开头 。随附 .frm文件的名称与孤立临时表的名称相同。 注意 如果没有.frm文件，则可以重新创建它。该.frm文件必须具有与孤立临时表相同的表架构（它必须具有相同的列和索引），并且必须放置在孤立临时表的数据库目录中。 要在系统上标识孤立的临时表，可以查看 表监视器的输出或查询 INFORMATION_SCHEMA.INNODB_SYS_TABLES。查找以开头的表名#sql。如果原始表位于 每个 表文件表空间中，#sql-*.ibd则孤立临时表的表空间文件（该 文件）应该在数据库目录中可见。 1SELECT * FROM INFORMATION_SCHEMA.INNODB_SYS_TABLES WHERE NAME LIKE '%#sql%'; 要删除孤立的临时表，请通过发出一条DROP TABLE语句来删除该表，在该表 名的前面加上前缀，#mysql50#并将表名括在反引号中。例如： 1mysql&gt; DROP TABLE `#mysql50##sql-540_3`; 该#mysql50#前缀告诉MySQL忽略 file name safe encoding在MySQL 5.1中引入的。要对具有特殊字符（例如，“ ＃ ”）的表名执行SQL语句，需要将表名括在反引号中。 表空间不存在与innodb_file_per_table 启用，如果可能会出现以下消息 .frm或.ibd文件（或两者）丢失： 123456InnoDB: in InnoDB data dictionary has tablespace id N,InnoDB: but tablespace with that id or name does not exist. HaveInnoDB: you deleted or moved .ibd files?InnoDB: This may also be a table created with CREATE TEMPORARY TABLEInnoDB: whose .ibd and .frm files MySQL automatically removed, but theInnoDB: table still exists in the InnoDB internal data dictionary. 如果发生这种情况，请尝试以下步骤解决问题： .frm在其他一些数据库目录中 创建一个匹配文件，并将其复制到孤立表所在的数据库目录中。 DROP TABLE原始表的 发行。那应该成功删除该表，并且 InnoDB应该在错误日志中显示一条警告，指出该.ibd文件已丢失。 恢复每表孤立文件ibd文件此过程描述了如何将每表 孤立文件还原 .ibd到另一个MySQL实例。如果系统表空间丢失或不可恢复，并且要.ibd 在新的MySQL实例上还原文件备份，则可以使用此过程。 该过程假定您只有 .ibd文件备份，并且要恢复到最初创建孤立.ibd文件的MySQL版本 ，并且 .ibd文件备份是干净的。有关创建干净备份的信息，请参见 第14.6.1.4节“移动或复制InnoDB表”。 第14.6.1.3节“导入InnoDB表”中 概述的表导入限制 适用于此过程。 在新的MySQL实例上，在同名数据库中重新创建表。 123456789101112mysql&gt; CREATE DATABASE sakila;mysql&gt; USE sakila;mysql&gt; CREATE TABLE actor ( actor_id SMALLINT UNSIGNED NOT NULL AUTO_INCREMENT, first_name VARCHAR(45) NOT NULL, last_name VARCHAR(45) NOT NULL, last_update TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (actor_id), KEY idx_actor_last_name (last_name) )ENGINE=InnoDB DEFAULT CHARSET=utf8; 丢弃新创建的表的表空间。 1mysql&gt; ALTER TABLE sakila.actor DISCARD TABLESPACE; 将孤立.ibd文件从备份目录复制到新的数据库目录。 1shell&gt; cp /backup_directory/actor.ibd path/to/mysql-5.6/data/sakila/ 确保.ibd文件具有必要的文件权限。 导入孤立.ibd文件。发出警告，指示InnoDB将尝试在不进行模式验证的情况下导入文件。 123456mysql&gt; ALTER TABLE sakila.actor IMPORT TABLESPACE; SHOW WARNINGS; Query OK, 0 rows affected, 1 warning (0.15 sec)Warning | 1810 | InnoDB: IO Read error: (2, No such file or directory)Error opening './sakila/actor.cfg', will attempt to importwithout schema verification 查询表以确认.ibd 文件已成功还原。 123456mysql&gt; SELECT COUNT(*) FROM sakila.actor;+----------+| count(*) |+----------+| 200 |+----------+ 14.21.4 InnoDB错误处理以下各项描述了如何InnoDB 执行错误处理。InnoDB有时仅回滚失败的语句，而其他时候回滚整个事务。 如果表空间中的文件空间用完 ，Table is full则会发生MySQL 错误并 InnoDB回滚SQL语句。 一个事务死锁 导致InnoDB要 回滚整个 事务。发生这种情况时，请重试整个事务。 锁定等待超时导致InnoDB仅回滚正在等待锁定且遇到超时的单个语句。（要使整个事务回滚，请使用--innodb-rollback-on-timeout 选项启动服务器 。）如果使用当前行为，请重试该语句；如果使用，请重试该事务 --innodb-rollback-on-timeout。 死锁和锁等待超时在繁忙的服务器上都是正常的，应用程序必须意识到它们可能发生并通过重试来处理它们。您可以通过在事务期间第一次更改数据和提交之间进行尽可能少的工作来减少它们的可能性，因此将锁保持在最短的时间内，并且行的数量最少。有时，在不同交易之间分配工作可能是实用且有用的。 当由于死锁或锁定等待超时而导致事务回滚时，它将取消事务中语句的影响。但是，如果start-transaction语句为 START TRANSACTION或 BEGIN 语句，则回滚不会取消该语句。进一步的SQL语句成为交易的一部分，直到发生COMMIT， ROLLBACK或某些SQL语句导致隐式提交。 如果未IGNORE在语句中指定选项，则重复键错误会回滚SQL 语句。 一个row too long error回滚SQL语句。 其他错误大多数由MySQL代码层（在InnoDB存储引擎级别之上）检测到，并且它们回滚相应的SQL语句。在单个SQL语句的回滚中不会释放锁。 在隐式回滚期间以及在执行显式 ROLLBACKSQL语句期间，将在相关连接的 列中SHOW PROCESSLIST 显示。 Rolling back``State]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2、kafka中的概念和角色]]></title>
    <url>%2Fmessage-queue%2Fkafka%2F2-kafka-concept-and-role%2F</url>
    <content type="text"><![CDATA[Kafka broker 和 集群kafka架构图： 生产者发送消息给kafka服务器； 消费者从kafka服务器读取消息； kafka服务器依托zookeeper集群进行服务的协调管理； broker —— kafka服务器的官方名字一个独立的kafka服务器被称为broker；broker用来接收来自生产者的消息，为消息设置偏移量，并把消息保存到磁盘； 换句话说，多个kafka实例组成kafka集群； kafka集群、broker、主题及分区、生产者、消费者 关系示意图： broker中央控制器：kafka集群中由多个broker，在kafka启动时，只有一个会被选举为broker中央控制器（controller leader）； broker中央控制器选举过程：当broker启动的时候，都会创建KafkaController对象，但集群中只能由一个leader对外服务，这些每个节点上的KafkaController会在指定的zookeeper路径下创建临时节点，只有第一个成功创建的节点的KafkaController才可以称为leader，其余的都是follower。当leader故障后，所有的follower都会收到通知，再次竞争在该路径下创建节点从而选举新的leader； 中央控制器的工作： 管理整个集群中的分区； 监控副本的状态； 这些工作如： leader副本故障后，由中央控制器 负责为该partition重新选举新的leader副本； 当检测到同步列表发生变化后，由中央控制器 通知集群中所有leader更新其元数据缓存信息； 当增加某个topic分区的时候也会由中央控制器管理 分区的重新分配工作； Kafka 主题 和 分区消息、主题（topic）、分区（partition）、副本（replica） 消息kafka的消息个是经过3次版本变迁，分别被称为 V0，V1，V2； 3个主要字段的含义： Key：消息键，对消息做 partition 时使用，即决定消息被保存在某个 topic 下那个 partition； Value：消息体，保存实际的消息数据； Timestamp：消息发送时间戳，用于流式处理 及 其他依赖时间的处理语义。如果不指定则取当前时间； 属性：一共1Byte，目前只用了最低的3bit来做压缩类型的存储； CRC32校验码 Kafka消息设计的好处： 1、使用 紧凑的二进制数组 Bytebuffer 而不是独立的对象，因此我们至少能够访问多一倍的可用内存； 2、而且不在堆上分配内存，避免了GC的糟糕性能； 3、大量使用页缓存，当broker进程崩溃时，堆内存上的数据也一并消失，但页缓存的数据依然存在；下次broker重启后可以继续提供服务，不用单独在热缓存； 主题（topic）和 分区（partition）topic ：只是一个逻辑概念，代表了一类消息。通常 topic 用来区分实际业务； partition：每个topic都由若干个 partition 组成； partition 是不可修改的有序消息序列，也可以说是有序的消息日志； 每个partition 都有自己专属的 partition号，通常是从0开始的； 用户堆 partition 唯一能做的操作就是 在 消息序列的尾部 追加写入消息； partition 上的每条消息都会被分配一个唯一的序列号，该序列号被称为位移（offset）； partition 实际上并没有太多的业务含义，partition的引入 只是为了提升系统的吞吐量；可以根据集群的实际配置设置具体的partition数，实现整体性能最大化； 位移（offset）topic partition 下的每条消息都被分配一个位移值；kafka 消费端也有 位移（offset）的概念； 要注意区分这两offset 的不同含义： 由此我们可以断言， 一条消息就是一个三元组&lt;topic，partition，offset&gt; 副本（replica）分布式系统为了实现高可靠性，目前主要实现方式还是 冗余机制；即备份多份日志；这些备份日志在 kafka 中被称为 副本（replica）； 副本存在的唯一目的：防止数据丢失； 副本分两类：领导者副本（leader replica） 和 追随者副本（follower replica）； follower replica 不能给客户端提供服务，只是被动的 向 leader replica 获取数据； 一旦 leader replica 所在的 broker 宕机，kafka 会从 剩余的replica中选举出新的 leader 继续提供服务； ISR（in-sync replica）ISR 全称 in-sync replica，翻译过来就是 与 leader replica 保持同步的 replica集合； 注意：不是所有的 replica 都能与 leader replica 保持同步； kafka 为 partition 动态维护一个 replica 集合 —— 该集合中所有的replica保存的消息日志都与leader replica保持同步状态；正常情况下，所有的 replica 都应该和 leader replica 保持同步状态，但由于各种各样的原因如果滞后到一定程度后，kafka 会把这些 replica 踢出 ISR，如果这些 replica 慢慢的追上了 leader replica，kafka会把他们重新加入 ISR； 只有 ISR 集合中的 replica 才能被选举为 leader； kafka承诺，只要 ISR 中 至少存在一个 replica，那些”已提交“的消息就不会丢失； kafka 生产者 和 消费者 生产者（producer）功能：将 消息 发布到相应的 topic 中； 生产者发送模式： 同步方式（synchronized）：调用send方法发送，它会返回一个Future对象，调用future对象就知道消息是否发送成功。（kafka 默认为同步，即producer.type = sync）; 异步方式（asynchronized）：调用send方法并指定一个回调函数，服务器在返回响应时调用该函数； 发送了不管（oneway）：把消息发送给服务器，但并不关心是否正常到达。多数情况下，消息会正常到达，但有时也会丢失一些消息； 消费者（consumer）功能：主题 topic 中的数据被 消费者 使用；]]></content>
      <categories>
        <category>mq</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务]]></title>
    <url>%2Fdistribution%2Fdistributed-transaction%2F</url>
    <content type="text"><![CDATA[Seatamq 最终一致性]]></content>
      <categories>
        <category>distribution</category>
      </categories>
      <tags>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1、kafka概要设计]]></title>
    <url>%2Fmessage-queue%2Fkafka%2F1-kafka-outline-design%2F</url>
    <content type="text"><![CDATA[设计之初的4个问题 吞吐量/延时 消息持久化 负载均衡和故障转移 伸缩性 吞吐量/延时吞吐量：某种处理能力的最大值； 延时：发出操作 到 收到响应 之间的时间； kafka写入 —— 如何做到高吞吐量、低延时这得益于它对于磁盘的使用方法不同。虽然kafka会持久化所有数据到磁盘，但本质上每次写入操作其实都只是把数据写入到操作系统的也缓存（page cache）中，然后由操作系统决定什么时候把也缓存写回到磁盘上。 这样设计的优势： 操作系统的页缓存是在内存中分配的，所以消息写入速度非常快； kafka不必直接与底层文件系统打交道，所有繁琐的I/O操作 都交给操作系统来处理； kafka写入操作采用追加写入（append）的方式，避免了磁盘随机写操作； kafka消费端 —— 如何做到高吞吐量、低延时kafka在读取消息时会首先尝试从OS页缓存中读取，如果命中便把消息经页缓存直接发送到网络的socket上，这个过程利用linux的sendfile系统调用实现，这就是大名鼎鼎的零拷贝（Zero Copy）技术。 零拷贝 1、DMA copy 将 磁盘数据 复制到 kernel buffer 中； 2、向 socket buffer 中追加当前要发送的数据在 kernel buffer 中的位置和偏移量； 3、DMA gather copy 根据 socket buffer 中的位置和偏移量直接将 kernel buffer 中的数据copy到网卡上； 经过上述过程，数据只经过了2次copy就从磁盘传送出去了。（事实上这个Zero copy是针对内核来讲的，数据在内核模式下是Zero－copy的）。 当前许多高性能http server都引入了sendfile机制，如nginx，lighttpd等。 12// Java中的零拷贝FileChannel.transferTo(long position,long count, WriteableByteChannel target); Java NIO中FileChannel.transferTo(long position, long count, WriteableByteChannel target)方法将当前通道中的数据传送到目标通道target中，在支持Zero-Copy的linux系统中，transferTo()的实现依赖于 sendfile()调用; 高吞吐量、低延时总结1、大量使用操作系统页缓存，内存操作速度快，命中率高； 2、kafka不直接参与物理 I/O 操作，而是交由最擅长此事的操作系统来完成； 3、采用顺序追加写入方式，摒弃了缓慢的磁盘随机读/写操作； 4、使用以sendfile为代表的零拷贝技术加强网络间的数据传输效率； 消息持久化持久化的好处 解耦消息发送和消息消费 实现灵活的消息处理 负载均衡和故障转移负载均衡实现方式：智能化的分区领导者选举（partition leader election） 故障转移定义：服务器意外终止时，整个集群可以快速检测到该失效（failure），并立即将该服务器上的应用和服务转移到其他服务器上； 实现方式：心跳或会话机制，kafka通过会话机制来实现。每台kafka服务器把自己注册到zk上，一旦出现故障，与zk的会话不能保持而超时失效。此时，kafka集群会选举出另一台服务器来完全替代这太服务器继续提供服务； 伸缩性伸缩性，即scalability。表示向分布式系统中增加额外的计算资源（CPU、内存、存储、带宽）时吞吐量提升的能力； 阻碍线性扩容的因素：状态的保存。 如果服务器自己保存自己的状态信息，则必须处理一致性问题； 如果服务器时无状态的，状态的保存和管理交给zookeeper这类服务来做，则大大降低维护复杂度。只需要简单的启动新节点进行自动负载均衡就能扩容集群节点；]]></content>
      <categories>
        <category>mq</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka管理器kafka-manager部署安装]]></title>
    <url>%2Fmessage-queue%2Fkafka%2Fkafka-manager-install%2F</url>
    <content type="text"><![CDATA[官网 https://github.com/yahoo/kafka-manager/releases 下载安装启动wget https://github.com/yahoo/kafka-manager/archive/2.0.0.2.tar.gz mv 2.0.0.2.tar.gz kafka-manager.tar.gz tar -zxf kafka-manager.tar.gz mv kafka-manager-2.0.0.2/ kafka-manager vi kafka-manager/conf/application.conf kafka-manager.zkhosts=”127.0.0.1:2181” 安装sbt： curl https://bintray.com/sbt/rpm/rpm &gt; bintray-sbt-rpm.repo sudo mv bintray-sbt-rpm.repo /etc/yum.repos.d/ sudo yum install sbt -y vi ~/.sbt/repositories 1234567[repositories]localaliyun: http://maven.aliyun.com/nexus/content/groups/public/typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnlysonatype-oss-releasesmaven-centralsonatype-oss-snapshots sbt -version[info] [launcher] getting org.scala-sbt sbt 1.3.3 (this may take some time)… 编译： cd kafka-manager/ ./sbt clean dist mkdir -p bin cp /data/sourcecode/kafka-manager/target/universal/kafka-manager-2.0.0.2.zip /data unzip kafka-manager-2.0.0.2.zip cd /data/kafka-manager-2.0.0.2/bin 启动： nohub /data/kafka-manager-2.0.0.2/bin/kafka-manager -Dhttp.port=9090 &amp; /data/kafka-manager-2.0.0.2/bin/kafka-manager -Dhttp.port=9090 开放端口： [root@localhost ~]# firewall-cmd –query-port 9090/tcp[root@localhost ~]# firewall-cmd –permanent –add-port=9090/tcp[root@localhost ~]# firewall-cmd –reload http://172.18.1.51:9090/]]></content>
      <categories>
        <category>mq</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat架构设计]]></title>
    <url>%2Fjava%2Ftomcat%2Ftomcat-architecture-design%2F</url>
    <content type="text"><![CDATA[零、下载源码官网： https://tomcat.apache.org/ Github： https://github.com/apache/tomcat.git 下载指定版本的源码： 1git clone --branch 8.5.49 https://github.com/apache/tomcat.git 导入IDEA： 将 build.xml 导入 Ant Build，点击 deploy Tomcat是非常常用的应用服务器，了解Tomcat的总体架构以及实现细节，对于理解整个java web也是有非常大的帮助。 一、Server1、最简单的服务器结构 最简单的服务器结构如图所示： Server向客户端提供服务，并且服务器可以start或stop。 缺点：如果需要该服务器支持多种网络协议，就很难扩展，因此改进如下： 2、Connector和Container 将容器拆分成Connector以及Container，Connector用于和客户端连接，可以扩展支持多种协议，建立连接之后将socket交给Container处理。 缺点：一个Server存在多个Connector和Container，因此Connector和Container之间就需要建立比较繁琐的映射关系，改进如下： 3、加入Service 一个Server包括多个Service，一个Service可以包含多个Connector和一个Container，这样Connector在获得客户端的socket之后，交给对应的Service，由Service来找到对应的Container，进而处理客户端的相关请求。 缺点：应用服务器是一个运行环境，可能会有很多项目部署在Tomcat中，因此改进如下: 4、引入Context 将Container换成Engine，Context则表示应用本身，同时Engine和Context都包含start、stop方法，代表各个容器启动、关闭的行为，将各个容器加载资源、释放资源的动作解耦。 缺点：通常在一个主机下，提供多个域名的服务，因此引入Host，如下： 5、引入Host 缺点：一个Context中可以包含多个servlet实例，称为Wrapper，改进如下： 6、Wrapper Engine、Host、Context、Wrapper是一类组件，这类组件的作用就是处理接收客户端的请求并且返回响应数据，并且有可能请求到Engine容器，Engine容器委托给子容器Host处理，修改如下： 使用Container代表容器，Engine、Host、Context、Wrapper都是Container的子容器，Container可以维护子容器。backgroundProcess()方法针对后台处理，并且其基础抽象类(ContainerBase)确保在启动组件的同时，异步启动后台处理。 Tomcat的总体架构大致如此。 二、Lifecycle生命周期 所有的容器中都存在start()、stop()等方法，因此抽象出Lifecycle接口，该接口中定义了相关的生命周期的方法。主要方法如下： Lifecycle状态图： 三、Pipeline和Valve 对于应用服务器来说，增强各组件的扩展性以及灵活性是非常重要的，Tomcat采用职责链模式来实现每个Container组件处理请求的功能。 Pipeline代表职责链，后者表示阀门，具体的处理过程，如图所示： 说的简单点，就是每条线路上包含哪些操作，操作按照顺序一个个执行。Tomcat通过这种方式来决定每个容器的执行过程。之前的Tomcat架构图加上职责链如下所示： 简单的来说，Tomcat监听客户端的请求，获得请求后交个各个组件去处理，返回响应数据到客户端，并且Tomcat能支持HTTP、AJP等协议。并且在扩展性、可用性上有着非常优秀的设计。 四、tomcat工作原理我们启动Tomcat时双击的startup.bat文件的主要作用是找到catalina.bat，并且把参数传递给它，而catalina.bat中有这样一段话： Bootstrap.class是整个Tomcat 的入口，我们在Tomcat源码里找到这个类，其中就有我们经常使用的main方法： 这个类有两个作用 ：1.初始化一个守护进程变量、加载类和相应参数。2.解析命令，并执行。 另外tomcat中有一个非常重要的xml，server.xml，来看下server.xml对应的真实的tomcat架构是啥样的。]]></content>
      <categories>
        <category>java</category>
        <category>tomcat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tomcat性能优化]]></title>
    <url>%2Fjava%2Ftomcat%2Ftomcat-performance-optimizing%2F</url>
    <content type="text"><![CDATA[参考： http://blog.csdn.net/zj52hm/article/details/51980194 http://blog.csdn.net/wuliu_forever/article/details/52607177 https://www.cnblogs.com/dengyungao/p/7542604.html https://www.cnblogs.com/ysocean/p/6893446.html#_label1 常用配置详解 1 目录结构 /bin：脚本文件目录。 /common/lib：存放所有web项目都可以访问的公共jar包（使用Common类加载器加载）。 /conf：存放配置文件，最重要的是server.xml。 /logs：存放日志文件。 /server/webapps：来管理Tomcat-web服务用的。仅对TOMCAT可见，对所有的WEB APP都不可见（使用Catalina类加载器加载）。 /shared/lib：仅对所有WEB APP可见，对TOMCAT不可见（使用Shared类加载器加载）。 /temp：Tomcat运行时候存放临时文件用的。 /webapps：web应用发布目录。 /work：Tomcat把各种由jsp生成的servlet文件放在这个目录下。删除后，启动时会自动创建。 2 配置文件 server.xml：主要的配置文件。 web.xml：缺省的web app配置，WEB-INF/web.xml会覆盖该配置。 context.xml：不清楚跟server.xml里面的context是否有关系。 server.xml配置 server标签 port：指定一个端口，这个端口负责监听关闭tomcat的请求。 shutdown：指定向端口发送的命令字符串。 service标签 name：指定service的名字。 Connector(表示客户端和service之间的连接)标签 port：指定服务器端要创建的端口号，并在这个端口监听来自客户端的请求。 minProcessors：服务器启动时创建的处理请求的线程数。 maxProcessors：最大可以创建的处理请求的线程数。 enableLookups：如果为true，则可以通过调用request.getRemoteHost()进行DNS查询来得到远程客户端的实际主机名，若为false则不进行DNS查询，而是返回其ip地址。 redirectPort：指定服务器正在处理http请求时收到了一个SSL传输请求后重定向的端口号。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。 connectionTimeout：指定超时的时间数(以毫秒为单位)。 Engine(表示指定service中的请求处理机，接收和处理来自Connector的请求)标签 defaultHost：指定缺省的处理请求的主机名，它至少与其中的一个host元素的name属性值是一样的。 Context(表示一个web应用程序，通常为WAR文件，关于WAR的具体信息见servlet规范)标签 docBase：该web应用的文档基准目录（Document Base，也称为Context Root），或者是WAR文件的路径。可以使用绝对路径，也可以使用相对于context所属的Host的appBase路径。 path：表示此web应用程序的url的前缀，这样请求的url为http://localhost:8080/path/****。 reloadable：这个属性非常重要，如果为true，则tomcat会自动检测应用程序的/WEB-INF/lib和/WEB-INF/classes目录的变化，自动装载新的应用程序，我们可以在不重起tomcat的情况下改变应用程序。 useNaming：如果希望Catalina为该web应用使能一个JNDI InitialContext对象，设为true。该InitialialContext符合J2EE平台的约定，缺省值为true。 workDir：Context提供的临时目录的路径，用于servlet的临时读/写。利用javax.servlet.context.tempdir属性，servlet可以访问该目录。如果没有指定，使用$CATALINA_HOME/work下一个合适的目录。 swallowOutput：如果该值为true，System.out和System.err的输出被重定向到web应用的logger。如果没有指定，缺省值为false debug：与这个Engine关联的Logger记录的调试信息的详细程度。数字越大，输出越详细。如果没有指定，缺省为0。 host(表示一个虚拟主机)标签 name：指定主机名。 appBase：应用程序基本目录，即存放应用程序的目录。 unpackWARs：如果为true，则tomcat会自动将WAR文件解压，否则不解压，直接从WAR文件中运行应用程序。 Logger(表示日志，调试和错误信息)标签 className：指定logger使用的类名，此类必须实现org.apache.catalina.Logger接口。 prefix：指定log文件的前缀。 suffix：指定log文件的后缀。 timestamp：如果为true，则log文件名中要加入时间，如下例:localhost_log.2001-10-04.txt。 Realm(表示存放用户名，密码及role的数据库)标签 className：指定Realm使用的类名，此类必须实现org.apache.catalina.Realm接口。 Valve(功能与Logger差不多，其prefix和suffix属性解释和Logger 中的一样)标签 className：指定Valve使用的类名，如用org.apache.catalina.valves.AccessLogValve类可以记录应用程序的访问信息。 directory：指定log文件存放的位置。 pattern：有两个值，common方式记录远程主机名或ip地址，用户名，日期，第一行请求的字符串，HTTP响应代码，发送的字节数。combined方式比common方式记录的值更多。 3 配置虚拟目录 1）直接部署到webapps目录下面访问。 2）修改conf/server.xml文件。在中加入。docBase目录默认使用appBase=”webapps”这个目录。也可以是绝对路径。配置主目录，可以将path=””。 3）当项目没有放在webapps目录下时，可以在conf/Catalina/localhost新建一个XXX.XML文件。里面加入。 注意：这里的path属性不需要设置，设置了也不会起作用的。 也可以使用该方法建立主目录指向另一个目录，例如：命名为ROOT.xml，这样默认访问的主目录就被修改过了。 4 配置连接数 maxThreads：Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。 minSpareThreads：Tomcat初始化时创建的线程数。 maxSpareThreads：一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。 enableLookups：是否反查域名，取值为：true或false。为了提高处理能力，应设置为false connectionTimeout：网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。默认可设置为20000毫秒。 web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。 5 配置内存大小 修改bin/catalina.bat中的set CATALINA_OPTS=-Xms64m -Xmx128m。 Xms指最小内存，Xmx指最大内存。 6 安全配置 1）将SHUTDOWN修改为其他一些字符串。否则就容易被人给停止掉了。 2）对应tomcat3.1中，屏蔽目录文件自动列出 修改conf/web.xml中的 default org.apache.catalina.servlets.DefaultServlet debug 0 listings true 1 3）访问日志设置 在server.xml中加入 这样访问日志会记录到Logs中。 4）修改用户名、密码 conf/tomcat-users.xml 5）屏蔽后台管理入口 方法一：从控制用户和权限着手。废掉要管理权限的用户就可以了。 方法二：将conf/Catalina/localhost/manager.xml改名。 6）配置403,404,500错误页面 默认情况下，报出HTTP错误的时候会暴露tomcat版本号。如果不想暴露的话，就需要重新定义错误跳转页面。 401 /401.jsp 404 /404.jsp 500 /500.jsp 注意：在测试的时候碰到一个奇怪的现象，平时项目里面的时候测试正常的。可是今天在tomcat目录里面新建一个测试目录测试并不能跳转到指定错误页面。暂时不知道为什么。 7 配置Log4j日志记录 项目中抛出的异常，抛到tomcat中的异常会被tomcat记录下来，存放至logs/localhost.yyyy-MM-dd.log文件中。 平时我们在项目中使用的log4j记录日志跟tomcat是没有任何关系的，是独立的一个程序，记录的文件是自定义的。 我们可以在tomcat中定义一个log4j的公共日志处理方式，这样在项目中就不需要在定义log4j的配置了。 1）将log4j-1.2.15.jar加入到commonlib目录。 2）将log4j.properties加入到commonclasses目录。 内容例如： 1234567891011121314# Output pattern : date [thread] priority category - messagelog4j.rootLogger=DEUBG, stdout, logfilelog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d [%t] %-5p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.logfile.File=$&#123;catalina.home&#125;/logs/tomcat_app.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d [%t] %-5p [%c] - %m%n#3rd party library levellog4j.logger.org.hibernate.cache=ERROR 注意：我们项目中使用e.printStackTrace();输出的异常会在控制台输出来，但是，不会记录到tomcat日志中。 而且，也不会记录到log4j的日志中。要想记录到log4j日志中，必须使用log4j输出来。 所以，实际上web项目中进行异常处理应该将e.printStackTrace();写写法多改成log4j的形式才对！ 但是，实际项目中很多项目多偷懒使用了e.printStackTrace();方式输出异常。当出现异常的时候在控制台上查看一下就可以了，也不考虑实际运行时候的维护。假如有人不小心关了控制台，那么，你不就看不到异常了吗？ 个人介意使用log4j的形式记入web异常！ 8 Tomcat5乱码问题 Tomcat5跟Tomcat4对参数处理是不一样的，在Tomcat4中get与post的编码是一样的，所以只要在过滤器中通过request.setCharacterEncoding()设定一次就可以解决get与set的问题。然而，在Tomcat5中，get与post的处理是分开的，对get请求使用URIEncoding进行处理，对post使用request.setCharacterEncoding()处理。Tomcat5中，在server.xml的Connector元素增加了以下配置参数： URIEncoding：用来设定通过URI传递的 tomcat优化配置参数优化内存，主要是在bin/catalina.bat/sh 配置文件中进行。linux上，在catalina.sh中添加： JAVA_OPTS=”-server -Xms1G -Xmx2G -Xss256K -Djava.awt.headless=true -Dfile.encoding=utf-8 -XX:MaxPermSize=256m -XX:PermSize=128M -XX:MaxPermSize=256M”其中： • -server：启用jdk的server版本。• -Xms：虚拟机初始化时的最小堆内存。• -Xmx：虚拟机可使用的最大堆内存。 #-Xms与-Xmx设成一样的值，避免JVM因为频繁的GC导致性能大起大落• -XX:PermSize：设置非堆内存初始值,默认是物理内存的1/64。• -XX:MaxNewSize：新生代占整个堆内存的最大值。• -XX:MaxPermSize：Perm（俗称方法区）占整个堆内存的最大值，也称内存最大永久保留区域。1）错误提示：java.lang.OutOfMemoryError:Java heap space Tomcat默认可以使用的内存为128MB，在较大型的应用项目中，这点内存是不够的，有可能导致系统无法运行。常见的问题是报Tomcat内存溢出错误，Outof Memory(系统内存不足)的异常，从而导致客户端显示500错误，一般调整Tomcat的-Xms和-Xmx即可解决问题，通常将-Xms和-Xmx设置成一样，堆的最大值设置为物理可用内存的最大值的80%。 set JAVA_OPTS=-Xms512m-Xmx512m2）错误提示：java.lang.OutOfMemoryError: PermGenspace PermGenspace的全称是Permanent Generationspace,是指内存的永久保存区域，这块内存主要是被JVM存放Class和Meta信息的,Class在被Loader时就会被放到PermGenspace中，它和存放类实例(Instance)的Heap区域不同,GC(Garbage Collection)不会在主程序运行期对PermGenspace进行清理，所以如果你的应用中有很CLASS的话,就很可能出现PermGen space错误，这种错误常见在web服务器对JSP进行precompile的时候。如果你的WEB APP下都用了大量的第三方jar, 其大小超过了jvm默认的大小(4M)那么就会产生此错误信息了。解决方法： setJAVA_OPTS=-XX:PermSize=128M3）在使用-Xms和-Xmx调整tomcat的堆大小时，还需要考虑垃圾回收机制。如果系统花费很多的时间收集垃圾，请减小堆大小。一次完全的垃圾收集应该不超过3-5 秒。如果垃圾收集成为瓶颈，那么需要指定代的大小，检查垃圾收集的详细输出，研究垃圾收集参数对性能的影响。一般说来，你应该使用物理内存的 80% 作为堆大小。当增加处理器时，记得增加内存，因为分配可以并行进行，而垃圾收集不是并行的。 2、连接数优化： #优化连接数，主要是在conf/server.xml配置文件中进行修改。 2.1、优化线程数找到Connector port=”8080” protocol=”HTTP/1.1”，增加maxThreads和acceptCount属性（使acceptCount大于等于maxThreads），如下： &lt;Connector port=”8080” protocol=”HTTP/1.1”connectionTimeout=”20000” redirectPort=”8443”acceptCount=”500” maxThreads=”400” /&gt;其中： • maxThreads：tomcat可用于请求处理的最大线程数，默认是200• minSpareThreads：tomcat初始线程数，即最小空闲线程数• maxSpareThreads：tomcat最大空闲线程数，超过的会被关闭• acceptCount：当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理.默认1002.2、使用线程池在server.xml中增加executor节点，然后配置connector的executor属性，如下： &lt;Executor name=”tomcatThreadPool” namePrefix=”req-exec-“maxThreads=”1000” minSpareThreads=”50”maxIdleTime=”60000”/&gt;&lt;Connector port=”8080” protocol=”HTTP/1.1”executor=”tomcatThreadPool”/&gt;其中： • namePrefix：线程池中线程的命名前缀• maxThreads：线程池的最大线程数• minSpareThreads：线程池的最小空闲线程数• maxIdleTime：超过最小空闲线程数时，多的线程会等待这个时间长度，然后关闭• threadPriority：线程优先级注：当tomcat并发用户量大的时候，单个jvm进程确实可能打开过多的文件句柄，这时会报java.net.SocketException:Too many open files错误。可使用下面步骤检查： • ps -ef |grep tomcat 查看tomcat的进程ID，记录ID号，假设进程ID为10001• lsof -p 10001|wc -l 查看当前进程id为10001的 文件操作数• 使用命令：ulimit -a 查看每个用户允许打开的最大文件数3、Tomcat Connector三种运行模式（BIO, NIO, APR）3.1、三种模式比较：1）BIO：一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下在Linux系统中默认使用这种方式。 2）NIO：利用Java的异步IO处理，可以通过少量的线程处理大量的请求。Tomcat8在Linux系统中默认使用这种方式。Tomcat7必须修改Connector配置来启动（conf/server.xml配置文件）： &lt;Connector port=”8080”protocol=”org.apache.coyote.http11.Http11NioProtocol” connectionTimeout=”20000”redirectPort=”8443”/&gt;3）APR(Apache Portable Runtime)：从操作系统层面解决io阻塞问题。Linux如果安装了apr和native，Tomcat直接启动就支持apr。 3.2、apr模式安装apr以及tomcat-native yum -y install apr apr-devel进入tomcat/bin目录，比如： cd /opt/local/tomcat/bin/tar xzfv tomcat-native.tar.gzcd tomcat-native-1.1.32-src/jni/native./configure –with-apr=/usr/bin/apr-1-configmake &amp;&amp; make install #注意最新版本的tomcat自带tomcat-native.war.gz，不过其版本相对于yum安装的apr过高，configure的时候会报错。 解决：yum remove apr apr-devel –y,卸载yum安装的apr和apr-devel,下载最新版本的apr源码包，编译安装;或者下载低版本的tomcat-native编译安装 安装成功后还需要对tomcat设置环境变量，方法是在catalina.sh文件中增加1行： CATALINA_OPTS=”-Djava.library.path=/usr/local/apr/lib” #apr下载地址：http://apr.apache.org/download.cgi #tomcat-native下载地址：http://tomcat.apache.org/download-native.cgi 修改8080端对应的conf/server.xml 123456789protocol="org.apache.coyote.http11.Http11AprProtocol"&lt;Connector executor="tomcatThreadPool"port="8080"protocol="org.apache.coyote.http11.Http11AprProtocol"connectionTimeout="20000"enableLookups="false"redirectPort="8443"URIEncoding="UTF-8" /&gt; PS:启动以后查看日志 显示如下表示开启 apr 模式 Sep 19, 2016 3:46:21 PM org.apache.coyote.AbstractProtocol startINFO: Starting ProtocolHandler [“http-apr-8081”] tomcat常见面试题 一、Tomcat的缺省是多少，怎么修改 Tomcat的缺省端口号是8080.修改Tomcat端口号： 1.找到Tomcat目录下的conf文件夹 2.进入conf文件夹里面找到server.xml文件 3.打开server.xml文件 4.在server.xml文件里面找到下列信息 maxThreads=”150″ minSpareThreads=”25″ maxSpareThreads=”75″ enableLookups=”false” redirectPort=”8443″ acceptCount=”100″ connectionTimeout=”20000″ disableUploadTimeout=”true” /&gt; 5.把port=”8080″改成port=”8888″，并且保存 6.启动Tomcat，并且在IE浏览器里面的地址栏输入http://127.0.0.1:8888/ 7、tomcat默认采用的BIO模型，在几百并发下性能会有很严重的下降。tomcat自带还有NIO的模型，另外也可以调用APR的库来实现操作系统级别控制。NIO模型是内置的，调用很方便，只需要将上面配置文件中protocol修改成 org.apache.coyote.http11.Http11NioProtocol，重启即可生效。如下面的参数配置，默认的是HTTP/1.1。 123&lt;Connector port=”8080″ protocol=”org.apache.coyote.http11.Http11NioProtocol” connectionTimeout=”20000″ redirectPort=”8443″ maxThreads=”500″ minSpareThreads=”20″ acceptCount=”100″ disableUploadTimeout=”true”enableLookups=”false” URIEncoding=”UTF-8″ /&gt; 二、tomcat 如何优化？ 1、优化连接配置.这里以tomcat7的参数配置为例，需要修改conf/server.xml文件，修改连接数，关闭客户端dns查询。 参数解释： URIEncoding=”UTF-8″ :使得tomcat可以解析含有中文名的文件的url，真方便，不像apache里还有搞个mod_encoding，还要手工编译 maxSpareThreads : 如果空闲状态的线程数多于设置的数目，则将这些线程中止，减少这个池中的线程总数。 minSpareThreads : 最小备用线程数，tomcat启动时的初始化的线程数。 enableLookups : 这个功效和Apache中的HostnameLookups一样，设为关闭。 connectionTimeout : connectionTimeout为网络连接超时时间毫秒数。 maxThreads : maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数，即最大并发数。 acceptCount : acceptCount是当线程数达到maxThreads后，后续请求会被放入一个等待队列，这个acceptCount是这个队列的大小，如果这个队列也满了，就直接refuse connection maxProcessors与minProcessors : 在 Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。多线程帮助程序员写出CPU最 大利用率的高效程序，使空闲时间保持最低，从而接受更多的请求。 通常Windows是1000个左右，Linux是2000个左右。 useURIValidationHack: 我们来看一下tomcat中的一段源码： 12345678910111213【security】if (connector.getUseURIValidationHack()) &#123; String uri = validate(request.getRequestURI()); if (uri == null) &#123; res.setStatus(400); res.setMessage(“Invalid URI”); throw new IOException(“Invalid URI”);&#125; else &#123; req.requestURI().setString(uri); // Redoing the URI decoding req.decodedURI().duplicate(req.requestURI()); req.getURLDecoder().convert(req.decodedURI(), true);&#125; 可以看到如果把useURIValidationHack设成”false”，可以减少它对一些url的不必要的检查从而减省开销。 enableLookups=”false” ： 为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值。 disableUploadTimeout ：类似于Apache中的keeyalive一样 给Tomcat配置gzip压缩(HTTP压缩)功能 compression=”on” compressionMinSize=”2048″ compressableMimeType=”text/html,text/xml,text/javascript,text/css,text/plain” HTTP 压缩可以大大提高浏览网站的速度，它的原理是，在客户端请求网页后，从服务器端将网页文件压缩，再下载到客户端，由客户端的浏览器负责解压缩并浏览。相对于普通的浏览过程HTML,CSS,Javascript , Text ，它可以节省40%左右的流量。更为重要的是，它可以对动态生成的，包括CGI、PHP , JSP , ASP , Servlet,SHTML等输出的网页也能进行压缩，压缩效率惊人。 1)compression=”on” 打开压缩功能 2)compressionMinSize=”2048″ 启用压缩的输出内容大小，这里面默认为2KB 3)noCompressionUserAgents=”gozilla, traviata” 对于以下的浏览器，不启用压缩 4)compressableMimeType=”text/html,text/xml” 压缩类型 最后不要忘了把8443端口的地方也加上同样的配置，因为如果我们走https协议的话，我们将会用到8443端口这个段的配置，对吧？｛ tomcat设置https端口时,8443和443区别: 8443端口在访问时需要加端口号,相当于http的8080,不可通过域名直接访问,需要加上端口号;https://yuming.com:8443。 443端口在访问时不需要加端口号,相当于http的80,可通过域名直接访问;例:https://yuming.com。 *问:https使用域名访问网站,而不显示端口号? 答:将端口号设置为443,即可通过域名直接访问网站｝ 12345&lt;!–enable tomcat ssl–&gt;&lt;Connector port=”8443″ protocol=”HTTP/1.1″ URIEncoding=”UTF-8″ minSpareThreads=”25″ maxSpareThreads=”75″ enableLookups=”false” disableUploadTimeout=”true” connectionTimeout=”20000″ acceptCount=”300″ maxThreads=”300″ maxProcessors=”1000″ minProcessors=”5″ useURIValidationHack=”false” compression=”on” compressionMinSize=”2048″ compressableMimeType=”text/html,text/xml,text/javascript,text/css,text/plain”SSLEnabled=”true”scheme=”https” secure=”true”clientAuth=”false” sslProtocol=”TLS”keystoreFile=”d:/tomcat2/conf/shnlap93.jks” keystorePass=”aaaaaa”/&gt; 好了，所有的Tomcat优化的地方都加上了。 2、优化JDK Tomcat默认可以使用的内存为128MB,Windows下,在文件{tomcat_home}/bin/catalina.bat，Unix下，在文件$CATALINA_HOME/bin/catalina.sh的前面，增加如下设置： JAVA_OPTS=”‘$JAVA_OPTS” -Xms[初始化内存大小] -Xmx[可以使用的最大内存] 或 设置环境变量：export JAVA_OPTS=””$JAVA_OPTS” -Xms[初始化内存大小] -Xmx[可以使用的最大内存]” 一般说来，你应该使用物理内存的 80% 作为堆大小。如果本机上有Apache服务器，可以先折算Apache需要的内存，然后修改堆大小。建议设置为70％；建议设置[[初始化内存大小]等于[可以使用的最大内存]，这样可以减少平凡分配堆而降低性能。 本例使用加入环境变量的方式： 1234vi /etc/profile加入：export JAVA_OPTS=””$JAVA_OPTS” -Xms700 —Xmx700source /etc/profile 【参数说明】 -Xms 是指设定程序启动时占用内存大小。一般来讲，大点，程序会启动的 快一点，但是也可能会导致机器暂时间变慢。 -Xmx 是指设定程序运行期间最大可占用的内存大小。如果程序运行需要占 用更多的内存，超出了这个设置值，就会抛出OutOfMemory 异常。 -Xss 是指设定每个线程的堆栈大小。这个就要依据你的程序，看一个线程 大约需要占用多少内存，可能会有多少线程同时运行等。 -XX:PermSize设置非堆内存初始值，默认是物理内存的1/64 。 -XX:MaxPermSize设置最大非堆内存的大小，默认是物理内存的1/4。 三、tomcat 有那几种Connector 运行模式？ tomcat的运行模式有3种.修改他们的运行模式.3种模式的运行是否成功,可以看他的启动控制台,或者启动日志.或者登录他们的默认页面http://localhost:8080/查看其中的服务器状态。 1)bio 默认的模式,性能非常低下,没有经过任何优化处理和支持. 2)nio 利用java的异步io护理技术,no blocking IO技术. 想运行在该模式下，直接修改server.xml里的Connector节点,修改protocol为 启动后,就可以生效。 3)apr 安装起来最困难,但是从操作系统级别来解决异步的IO问题,大幅度的提高性能. 必须要安装apr和native，直接启动就支持apr。下面的修改纯属多余，仅供大家扩充知识,但仍然需要安装apr和native。如nio修改模式,修改protocol为org.apache.coyote.http11.Http11AprProtocol 四、tomcat调优 JVM参数调优：-Xms 表示JVM初始化堆的大小，-Xmx表示JVM堆的最大值。这两个值的大小一般根据需要进行设置。当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。在catalina.bat中，设置JAVA_OPTS=’-Xms256m -Xmx512m’，表示初始化内存为256MB，可以使用的最大内存为512MB。 禁用DNS查询 当web应用程序想要记录客户端的信息时，它也会记录客户端的IP地址或者通过域名服务器查找机器名转换为IP地址。DNS查询需要占用网络，并且包括可能从很多很远的服务器或者不起作用的服务器上去获取对应的IP的过程，这样会消耗一定的时间。为了消除DNS查询对性能的影响我们可以关闭DNS查询，方式是修改server.xml文件中的enableLookups参数值：Tomcat4 Tomcat5 调整线程数通过应用程序的连接器（Connector）进行性能控制的的参数是创建的处理请求的线程数。Tomcat使用线程池加速响应速度来处理请求。在Java中线程是程序运行时的路径，是在一个程序中与其它控制线程无关的、能够独立运行的代码段。它们共享相同的地址空间。多线程帮助程序员写出CPU最大利用率的高效程序，使空闲时间保持最低，从而接受更多的请求。Tomcat4中可以通过修改minProcessors和maxProcessors的值来控制线程数。这些值在安装后就已经设定为默认值并且是足够使用的，但是随着站点的扩容而改大这些值。minProcessors服务器启动时创建的处理请求的线程数应该足够处理一个小量的负载。也就是说，如果一天内每秒仅发生5次单击事件，并且每个请求任务处理需要1秒钟，那么预先设置线程数为5就足够了。但在你的站点访问量较大时就需要设置更大的线程数，指定为参数maxProcessors的值。maxProcessors的值也是有上限的，应防止流量不可控制（或者恶意的服务攻击），从而导致超出了虚拟机使用内存的大小。如果要加大并发连接数，应同时加大这两个参数。web server允许的最大连接数还受制于操作系统的内核参数设置，通常Windows是2000个左右，Linux是1000个左右。在Tomcat5对这些参数进行了调整，请看下面属性：maxThreads Tomcat使用线程来处理接收的每个请求。这个值表示Tomcat可创建的最大的线程数。acceptCount 指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理。connnectionTimeout 网络连接超时，单位：毫秒。设置为0表示永不超时，这样设置有隐患的。通常可设置为30000毫秒。minSpareThreads Tomcat初始化时创建的线程数。maxSpareThreads 一旦创建的线程超过这个值，Tomcat就会关闭不再需要的socket线程。最好的方式是多设置几次并且进行测试，观察响应时间和内存使用情况。在不同的机器、操作系统或虚拟机组合的情况下可能会不同，而且并不是所有人的web站点的流量都是一样的，因此没有一刀切的方案来确定线程数的值。 Tomcat作为Web服务器，它的处理性能直接关系到用户体验，下面是几种常见的优化措施： 一、掉对web.xml的监视，把jsp提前编辑成Servlet。有富余物理内存的情况，加大tomcat使用的jvm的内存 二、服务器资源 服务器所能提供CPU、内存、硬盘的性能对处理能力有决定性影响。 (1) 对于高并发情况下会有大量的运算，那么CPU的速度会直接影响到处理速度。 (2) 内存在大量数据处理的情况下，将会有较大的内存容量需求，可以用-Xmx -Xms -XX:MaxPermSize等参数对内存不同功能块进行划分。我们之前就遇到过内存分配不足，导致虚拟机一直处于full GC，从而导致处理能力严重下降。 (3) 硬盘主要问题就是读写性能，当大量文件进行读写时，磁盘极容易成为性能瓶颈。最好的办法还是利用下面提到的缓存。 三、利用缓存和压缩 对于静态页面最好是能够缓存起来，这样就不必每次从磁盘上读。这里我们采用了Nginx作为缓存服务器，将图片、css、js文件都进行了缓存，有效的减少了后端tomcat的访问。 另外，为了能加快网络传输速度，开启gzip压缩也是必不可少的。但考虑到tomcat已经需要处理很多东西了，所以把这个压缩的工作就交给前端的Nginx来完成。 除了文本可以用gzip压缩，其实很多图片也可以用图像处理工具预先进行压缩，找到一个平衡点可以让画质损失很小而文件可以减小很多。曾经我就见过一个图片从300多kb压缩到几十kb，自己几乎看不出来区别。 四、采用集群 单个服务器性能总是有限的，最好的办法自然是实现横向扩展，那么组建tomcat集群是有效提升性能的手段。我们还是采用了Nginx来作为请求分流的服务器，后端多个tomcat共享session来协同工作。 五、 优化tomcat参数 这里以tomcat7的参数配置为例，需要修改conf/server.xml文件，主要是优化连接配置，关闭客户端dns查询。 五、Tomcat 部署项目的三种方法 目录 1、下载 Tomcat 服务器2、启动并部署 Tomcat 服务器3、Tomcat 的目录结构4、部署项目的第一种方法（项目直接放入 webapps 目录中）5、部署项目的第二种方法（修改 conf/server.xml 文件 ）6、部署项目的第三种方法（apache-tomcat-7.0.52\conf\Catalina\localhost ）1、下载 Tomcat 服务器 ①、官网下载地址：http://tomcat.apache.org/ ②、tomcat 8.0 64位百度云下载地址：http://pan.baidu.com/s/1slbKPsx 密码：ewui ③、tomcat 8.0 32位百度云下载地址：http://pan.baidu.com/s/1o8G28rS 密码：k11n 2、启动并部署 Tomcat 服务器 ①、解压 tomcat 安装包到一个非中文目录下 ②、配置环境变量。JAVA_HOME(指向 JDK 安装的根目录) ③、双击 apache-tomcat-6.0.16\bin 目录下的 startup.bat，启动服务器(如果一闪而过，那就是没有配置 JAVA_HOME 的环境变量) ④、在浏览器中输入 http://localhost:8080 注意：Tomcat 启动不了的时候注意配置 JAVA_HOME:C:\Program Files\Java\jdk1.6.0_43这是安装 JDK的根目录3、Tomcat 的目录结构 4、部署项目的第一种方法（项目直接放入 webapps 目录中） 1、将编写并编译好的web项目(注意要是编译好的，如果是 eclipse，可以将项目打成 war 包放入)，放入到 webapps 中 2、启动tomcat服务器（双击 apache-tomcat-6.0.16\bin 目录下的 startup.bat，启动服务器） 3、在浏览器输入：http://localhost:8080/项目名/访问的文件名 5、部署项目的第二种方法（修改 conf/server.xml 文件 ） ①、打开tomcat下conf/server.xml，在 标签之间输入项目配置信息 1 path:浏览器访问时的路径名 docBase:web项目的WebRoot所在的路径，注意是WebRoot的路径，不是项目的路径。其实也就是编译后的项目 reloadble:设定项目有改动时，tomcat是否重新加载该项目 ②、双击 startup.bat，启动 tomcat 服务器，然后在浏览器输入访问的项目名称路径 注意：如果你配置的 path=”/xx”,那么访问的时候就是这样： 6、部署项目的第三种方法（apache-tomcat-7.0.52\conf\Catalina\localhost ） ①、进入到 apache-tomcat-7.0.52\conf\Catalina\localhost 目录，新建一个 项目名.xml 文件 ②、在 那个新建的 xml 文件中，增加下面配置语句（和上面的是一样的,但是不需要 path 配置，加上也没什么用） 1 ③、在浏览器输入路径：localhost:8080/xml文件名/访问的文件名 总结： ①、第一种方法比较普通，但是我们需要将编译好的项目重新 copy 到 webapps 目录下，多出了两步操作 ②、第二种方法直接在 server.xml 文件中配置，但是从 tomcat5.0版本开始后，server.xml 文件作为 tomcat 启动的主要配置文件，一旦 tomcat 启动后，便不会再读取这个文件，因此无法再 tomcat 服务启动后发布 web 项目 ③、第三种方法是最好的，每个项目分开配置，tomcat 将以\conf\Catalina\localhost 目录下的 xml 文件的文件名作为 web 应用的上下文路径，而不再理会 中配置的 path 路径，因此在配置的时候，可以不写 path。 通常我们使用第三种方法]]></content>
      <categories>
        <category>java</category>
        <category>tomcat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java磁盘IO]]></title>
    <url>%2Fjava%2Fio%2Fjava-disk-io%2F</url>
    <content type="text"><![CDATA[Java磁盘顺序写 Java磁盘随机写]]></content>
      <tags>
        <tag>disk-io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络IO]]></title>
    <url>%2Fjava%2Fio%2Fjava-network-io%2F</url>
    <content type="text"></content>
      <tags>
        <tag>network-io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【置顶】Java 成神之路]]></title>
    <url>%2Fjava%2Finterview%2Fjava-tech-stack%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4、lucene源码阅读]]></title>
    <url>%2Flucene%2F4-lucene-srcode-analysis%2F</url>
    <content type="text"><![CDATA[Lucene写文档主类：IndexWriter org.apache.lucene.index.IndexWriter API 调用过程： 1234567891011121314IndexWriter.addDocument():-&gt; IndexWriter.updateDocument(): // 局部删除 docWriter.updateDocument(doc, analyzer, delNode);-&gt; DocumentsWriter.updateDocument(): dwpt.updateDocument(doc, analyzer, delNode, flushNotifications);-&gt; DocumentsWriterPerThread.updateDocument(): consumer.processDocument();-&gt; DefaultIndexingChain.processDocument(): fieldCount = processField(field, fieldGen, fieldCount);-&gt; DefaultIndexingChain.processField(): storedFieldsConsumer.writeField(fp.fieldInfo, field);-&gt; StoredFieldsConsumer.writeField(): writer.writeField(info, field); // writer来源于IndexWriterConfig中的Codec-&gt; SimpleTextStoredFieldsWriter.writeField(); // 实际去写文档 1234567891011SimpleTextStoredFieldsWriter.writeField();public void writeField(FieldInfo info, IndexableField field) throws IOException &#123; write(FIELD); write(Integer.toString(info.number)); newLine(); write(NAME); write(field.name()); newLine();&#125; 123public long addDocument(Iterable&lt;? extends IndexableField&gt; doc) &#123; return updateDocument((DocumentsWriterDeleteQueue.Node&lt;?&gt;) null, doc);&#125; 12345678910111213141516171819202122232425private long updateDocument(final DocumentsWriterDeleteQueue.Node&lt;?&gt; delNode, Iterable&lt;? extends IndexableField&gt; doc) throws IOException &#123; ensureOpen();// 确保索引能够打开，因为Lucene允许多线程，可能拿不到锁 boolean success = false; try &#123; // 关键：DocumentsWriter long seqNo = docWriter.updateDocument(doc, analyzer, delNode); if (seqNo &lt; 0) &#123; seqNo = -seqNo; processEvents(true); &#125; success = true; return seqNo; &#125; catch (VirtualMachineError tragedy) &#123; tragicEvent(tragedy, "updateDocument"); throw tragedy; &#125; finally &#123; if (success == false) &#123; if (infoStream.isEnabled("IW")) &#123; infoStream.message("IW", "hit exception updating document"); &#125; &#125; maybeCloseOnTragicEvent(); &#125;&#125; org.apache.lucene.index.DocumentsWriter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546long updateDocument(final Iterable&lt;? extends IndexableField&gt; doc, final Analyzer analyzer, final DocumentsWriterDeleteQueue.Node&lt;?&gt; delNode) throws IOException &#123; boolean hasEvents = preUpdate(); // class DocumentsWriterPerThreadPool.ThreadState extends ReentrantLock // 本质是锁，配合DocumentsWriterPerThread完成Document的写操作 final ThreadState perThread = flushControl.obtainAndLock(); final DocumentsWriterPerThread flushingDWPT; long seqNo; try &#123; // This must happen after we've pulled the ThreadState because IW.close // waits for all ThreadStates to be released: ensureOpen(); ensureInitialized(perThread); assert perThread.isInitialized(); final DocumentsWriterPerThread dwpt = perThread.dwpt; final int dwptNumDocs = dwpt.getNumDocsInRAM(); try &#123; seqNo = dwpt.updateDocument(doc, analyzer, delNode, flushNotifications); &#125; finally &#123; if (dwpt.isAborted()) &#123; flushControl.doOnAbort(perThread); &#125; // We don't know whether the document actually // counted as being indexed, so we must subtract here to // accumulate our separate counter: numDocsInRAM.addAndGet(dwpt.getNumDocsInRAM() - dwptNumDocs); &#125; final boolean isUpdate = delNode != null &amp;&amp; delNode.isDelete(); flushingDWPT = flushControl.doAfterDocument(perThread, isUpdate); assert seqNo &gt; perThread.lastSeqNo: "seqNo=" + seqNo + " lastSeqNo=" + perThread.lastSeqNo; perThread.lastSeqNo = seqNo; &#125; finally &#123; perThreadPool.release(perThread); &#125; if (postUpdate(flushingDWPT, hasEvents)) &#123; seqNo = -seqNo; &#125; return seqNo;&#125; 1234567891011121314startStoredFields(docState.docID); // 开始存储Fields数据try &#123; for (IndexableField field : docState.doc) &#123; fieldCount = processField(field, fieldGen, fieldCount); &#125;&#125; finally &#123; if (docWriter.hasHitAbortingException() == false) &#123; // Finish each indexed field name seen in the document: for (int i=0;i&lt;fieldCount;i++) &#123; fields[i].finish(); &#125; finishStoredFields(); &#125;&#125; Lucene删除文档 123456789101112IndexWriter.deleteDocuments(Term... terms); -&gt; DocumentsWriter.deleteTerms(terms); -&gt; DocumentsWriter.applyDeleteOrUpdate(q -&gt; q.addDelete(terms)); -&gt; DocumentsWriterFlushControl.doOnDelete(); -&gt; FlushByRamOrCountsPolicy.onDelete(this, null); -&gt; FlushByRamOrCountsPolicy.setApplyAllDeletes(); -&gt; AtomicBoolean flushDeletes.set(true); -&gt; DocumentsWriter.applyAllDeletes(deleteQueue); -&gt; DocumentsWriterFlushQueue.addDeletes(deleteQueue); -&gt; org.apache.lucene.index.DocumentsWriterDeleteQueue#freezeGlobalBuffer -&gt; org.apache.lucene.index.DocumentsWriterFlushQueue.FlushTicket#FlushTicket -&gt; 12345678910111213141516package org.apache.lucene.index;final class DocumentsWriter implements Closeable, Accountable &#123; private synchronized long applyDeleteOrUpdate( ToLongFunction&lt;DocumentsWriterDeleteQueue&gt; function) throws IOException &#123; // TODO why is this synchronized? final DocumentsWriterDeleteQueue deleteQueue = this.deleteQueue; long seqNo = function.applyAsLong(deleteQueue); flushControl.doOnDelete(); lastSeqNo = Math.max(lastSeqNo, seqNo); if (applyAllDeletes(deleteQueue)) &#123; seqNo = -seqNo; &#125; return seqNo; &#125;&#125; 123456789class DocumentsWriterDeleteQueue &#123; private final BufferedUpdates globalBufferedUpdates; // 全局索引删除 long addDelete(Term... terms) &#123; long seqNo = add(new TermArrayNode(terms)); tryApplyGlobalSlice(); return seqNo; &#125;&#125; 1234// 便于内存控制final class DocumentsWriterFlushControl implements Accountable &#123; &#125; 123456789101112131415161718192021final class DocumentsWriterFlushQueue &#123; private final Queue&lt;FlushTicket&gt; queue = new LinkedList&lt;&gt;(); // we track tickets separately since count must be present even before the ticket is // constructed ie. queue.size would not reflect it. private final AtomicInteger ticketCount = new AtomicInteger(); private final ReentrantLock purgeLock = new ReentrantLock(); synchronized void addDeletes(DocumentsWriterDeleteQueue deleteQueue) &#123; incTickets();// first inc the ticket count - freeze opens // a window for #anyChanges to fail boolean success = false; try &#123; queue.add(new FlushTicket(deleteQueue.freezeGlobalBuffer(null), false)); success = true; &#125; finally &#123; if (!success) &#123; decTickets(); &#125; &#125; &#125;&#125; globalBufferUpdates 是全局删除，删除Document pendingUpdates 是局部删除， 添加或更新Document 局部指向最自己最后的节点， 全局永远指向 整个链表 的 最后一个节点； Lucene检索过程查询代码： 12345678910111213141516171819202122232425public class QueryParseTest &#123; public static void main(String[] args) throws IOException, ParseException &#123; String field = "title"; Path indexPath = Paths.get("indexdir"); Directory directory = FSDirectory.open(indexPath); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Analyzer analyzer = new IKAnalyzer8x(); QueryParser parser = new QueryParser(field, analyzer); parser.setDefaultOperator(QueryParser.Operator.AND); Query query = parser.parse("农村学生"); System.out.println("Query: " + query.toString()); // 查询关键词 // 返回前10条 TopDocs topDocs = searcher.search(query, 10); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println("DocID: " + scoreDoc.doc); System.out.println("id: " + doc.get("id")); System.out.println("title: " + doc.get("title")); System.out.println("文档评分: " + scoreDoc.score); &#125; directory.close(); reader.close(); &#125;&#125; 查询关键代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package org.apache.lucene.search;public class IndexSearcher &#123; /** * Lower-level search API. * Search all leaves using the given &#123;@link CollectorManager&#125;. In contrast * to &#123;@link #search(Query, Collector)&#125;, this method will use the searcher's * &#123;@link ExecutorService&#125; in order to parallelize execution of the collection * on the configured &#123;@link #leafSlices&#125;. * @see CollectorManager * @lucene.experimental */ public &lt;C extends Collector, T&gt; T search( Query query, CollectorManager&lt;C, T&gt; collectorManager) throws IOException &#123; if (executor == null) &#123; final C collector = collectorManager.newCollector(); search(query, collector); return collectorManager.reduce(Collections.singletonList(collector)); &#125; else &#123; final List&lt;C&gt; collectors = new ArrayList&lt;&gt;(leafSlices.length); boolean needsScores = false; for (int i = 0; i &lt; leafSlices.length; ++i) &#123; final C collector = collectorManager.newCollector(); collectors.add(collector); needsScores |= collector.needsScores(); &#125; query = rewrite(query); // 创建打分 Weight ，默认 1 final Weight weight = createWeight(query, needsScores, 1); // 根据 leafSlices.length 创建 topDocsFutures，线程调度 final List&lt;Future&lt;C&gt;&gt; topDocsFutures = new ArrayList&lt;&gt;(leafSlices.length); for (int i = 0; i &lt; leafSlices.length; ++i) &#123; final LeafReaderContext[] leaves = leafSlices[i].leaves; final C collector = collectors.get(i); topDocsFutures.add(executor.submit(new Callable&lt;C&gt;() &#123; @Override public C call() throws Exception &#123; // 使用线程 去 遍历数据 search(Arrays.asList(leaves), weight, collector); return collector; &#125; &#125;)); &#125; final List&lt;C&gt; collectedCollectors = new ArrayList&lt;&gt;(); for (Future&lt;C&gt; future : topDocsFutures) &#123; try &#123; collectedCollectors.add(future.get()); &#125; catch (InterruptedException e) &#123; throw new ThreadInterruptedException(e); &#125; catch (ExecutionException e) &#123; throw new RuntimeException(e); &#125; &#125; return collectorManager.reduce(collectors); &#125; &#125;&#125; Analysyer 包含两个组件 Tokenizer 分词器（分词 token） TokenFilter 分词过滤器（大小写转换，词根cats） Lucene70Codec =&gt; 实际去写Term 1234567public class Lucene70Codec extends Codec &#123; private final TermVectorsFormat vectorsFormat = new Lucene50TermVectorsFormat(); private final FieldInfosFormat fieldInfosFormat = new Lucene60FieldInfosFormat(); private final SegmentInfoFormat segmentInfosFormat = new Lucene70SegmentInfoFormat(); private final LiveDocsFormat liveDocsFormat = new Lucene50LiveDocsFormat(); private final CompoundFormat compoundFormat = new Lucene50CompoundFormat();&#125;]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene索引查询]]></title>
    <url>%2Flucene%2F3-lucene-query%2F</url>
    <content type="text"><![CDATA[一、Lucene 索引知识扩展 二、索引查询Lucene 构建查询套路 常用 Query 查询类TermQueryBooleanQuery 分页查询 高亮显示 Facets 分类索引 多级分类（类似：省-&gt;市-&gt;县） 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-facet --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-facet&lt;/artifactId&gt; &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt;&lt;/dependency&gt; 下钻查询 123DrillDownQuery drillDownQuery = new DrillDownQuery(config, query);drillDownQuery.add();FacetsCollector facetsCollector = new FacetsConector(); 查询平行维度 1DrillSideways ds = new DrillSideways(searcher, config, query);]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[珍藏多年的git问题和操作清单]]></title>
    <url>%2Ftools%2Fgit-help-doc%2F</url>
    <content type="text"><![CDATA[来源：微信公众号-猿天地 引言本文整理自工作多年以来遇到的所有 Git 问题汇总，之前都是遗忘的时候去看一遍操作，这次重新整理了一下，发出来方便大家收藏以及需要的时候查找答案。 一、必备知识点 仓库 Remote: 远程主仓库； Repository/History： 本地仓库； Stage/Index： Git追踪树,暂存区； workspace： 本地工作区（即你编辑器的代码） 二、git add 提交到暂存区，出错怎么办一般代码提交流程为：工作区 -&gt; git status 查看状态 -&gt; git add . 将所有修改加入暂存区-&gt; git commit -m &quot;提交描述&quot; 将代码提交到 本地仓库 -&gt; git push 将本地仓库代码更新到 远程仓库 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 git checkout 12// 丢弃工作区的修改git checkout -- &lt;文件名&gt; 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存时，想丢弃修改，分两步，第一步用命令 git reset HEAD file，就回到了场景1，第二步按场景1操作。 三、git commit 提交到本地仓库，出错怎么办？1. 提交信息出错更改 commit 信息 1git commit --amend -m“新提交消息” 2. 漏提交commit 时，遗漏提交部分更新，有两种解决方案： 方案一：再次 commit 1git commit -m“提交消息” 此时，git 上会出现两次 commit 方案二：遗漏文件提交到之前 commit 上 12git add missed-file // missed-file 为遗漏提交文件git commit --amend --no-edit --no-edit 表示提交消息不会更改，在 git 上仅为一次提交 3. 提交错误文件，回退到上一个 commit 版本，再 commitgit reset删除指定的 commit 1234567891011// 修改版本库，修改暂存区，修改工作区git reset HEAD &lt;文件名&gt; // 把暂存区的修改撤销掉（unstage），重新放回工作区。// git版本回退，回退到特定的commit_id版本，可以通过git log查看提交历史，以便确定要回退到哪个版本(commit 之后的即为ID);git reset --hard commit_id //将版本库回退1个版本，不仅仅是将本地版本库的头指针全部重置到指定版本，也会重置暂存区，并且会将工作区代码也回退到这个版本git reset --hard HEAD~1// 修改版本库，保留暂存区，保留工作区// 将版本库软回退1个版本，软回退表示将本地版本库的头指针全部重置到指定版本，且将这次提交之后的所有变更都移动到暂存区。git reset --soft HEAD~1 git revert撤销 某次操作，此次操作之前和之后的commit和history都会保留，并且把这次撤销 作为一次最新的提交 123456// 撤销前一次 commitgit revert HEAD// 撤销前前一次 commitgit revert HEAD^// (比如：fa042ce57ebbe5bb9c8db709f719cec2c58ee7ff）撤销指定的版本，撤销也会作为一次提交进行保存。git revert commit git revert是提交一个新的版本，将需要revert的版本的内容再反向修改回去，版本会递增，不影响之前提交的内容 git revert 和 git reset 的区别 git revert是用一次新的commit来回滚之前的commit，git reset是直接删除指定的commit。 在回滚这一操作上看，效果差不多。但是在日后继续merge以前的老版本时有区别。因为git revert是用一次逆向的commit“中和”之前的提交，因此日后合并老的branch时，导致这部分改变不会再次出现，但是git reset是之间把某些commit在某个branch上删除，因而和老的branch再次merge时，这些被回滚的commit应该还会被引入。 git reset 是把HEAD向后移动了一下，而git revert是HEAD继续前进，只是新的commit的内容和要revert的内容正好相反，能够抵消要被revert的内容。 四、常用命令1. 初始开发 git 操作流程 克隆最新主分支项目代码 git clone 地址 创建本地分支 git branch 分支名 查看本地分支 git branch 查看远程分支 git branch -a 切换分支 git checkout 分支名 (一般修改未提交则无法切换，大小写问题经常会有，可强制切换 git checkout 分支名 -f 非必须慎用) 将本地分支推送到远程分支 git push &lt;远程仓库&gt; &lt;本地分支&gt;:&lt;远程分支&gt; 2. git fetch将某个远程主机的更新，全部/分支 取回本地（此时之更新了Repository）它取回的代码对你本地的开发代码没有影响，如需彻底更新需合并或使用git pull 3. git pull拉取远程主机某分支的更新，再与本地的指定分支合并（相当与fetch加上了合并分支功能的操作） 4. git push将本地分支的更新，推送到远程主机，其命令格式与git pull相似 5. 分支操作 使用 Git 下载指定分支命令为：git clone -b 分支名仓库地址 拉取远程新分支 git checkout -b serverfix origin/serverfix 合并本地分支 git merge hotfix：(将 hotfix 分支合并到当前分支) 合并远程分支 git merge origin/serverfix 删除本地分支 git branch -d hotfix：(删除本地 hotfix 分支) 删除远程分支 git push origin --delete serverfix 上传新命名的本地分支：git push origin newName; 创建新分支：git branch branchName：(创建名为 branchName 的本地分支) 切换到新分支：git checkout branchName：(切换到 branchName 分支) 创建并切换分支：git checkout -b branchName：(相当于以上两条命令的合并) 查看本地分支：git branch 查看远程仓库所有分支：git branch -a 本地分支重命名：git branch -m oldName newName 重命名远程分支对应的本地分支：git branch -m oldName newName 把修改后的本地分支与远程分支关联：git branch --set-upstream-to origin/newName 五、优化操作1. 拉取代码 pull –rebase在团队协作过程中，假设你和你的同伴在本地中分别有各自的新提交，而你的同伴先于你 push了代码到远程分支上，所以你必须先执行 git pull 来获取同伴的提交，然后才能push 自己的提交到远程分支。 而按照 Git 的默认策略，如果远程分支和本地分支之间的提交线图有分叉的话（即不是 fast-forwarded），Git 会执行一次 merge 操作，因此产生一次没意义的提交记录，从而造成了像上图那样的混乱。 其实在 pull 操作的时候，，使用 git pull --rebase选项即可很好地解决上述问题。加上 --rebase 参数的作用是，提交线图有分叉的话，Git 会 rebase 策略来代替默认的 merge 策略。 假设提交线图在执行 pull 前是这样的： 123 A---B---C remotes/origin/master /D---E---F---G master 如果是执行 git pull 后，提交线图会变成这样： 123 A---B---C remotes/origin/master / \D---E---F---G---H master 结果多出了 H 这个没必要的提交记录。如果是执行 git pull --rebase 的话，提交线图就会变成这样： 123 remotes/origin/master |D---E---A---B---C---F'---G' master F G 两个提交通过 rebase 方式重新拼接在 C 之后，多余的分叉去掉了，目的达到。 小结大多数时候，使用 git pull --rebase是为了使提交线图更好看，从而方便 code review。 不过，如果你对使用 git 还不是十分熟练的话，我的建议是 git pull --rebase多练习几次之后再使用，因为 rebase 在 git 中，算得上是『危险行为』。 另外，还需注意的是，使用 git pull --rebase比直接 pull 容易导致冲突的产生，如果预期冲突比较多的话，建议还是直接 pull。 注意： git pull = git fetch + git merge git pull –rebase = git fetch + git rebase 2. 合代码 merge –no-ff上述的 git pull --rebase 策略目的是修整提交线图，使其形成一条直线，而即将要用到的git merge --no-ff &lt;branch-name&gt; 策略偏偏是反行其道，刻意地弄出提交线图分叉出来。 假设你在本地准备合并两个分支，而刚好这两个分支是 fast-forwarded 的，那么直接合并后你得到一个直线的提交线图，当然这样没什么坏处，但如果你想更清晰地告诉你同伴：这一系列的提交都是为了实现同一个目的，那么你可以刻意地将这次提交内容弄成一次提交线图分叉。 执行 git merge --no-ff &lt;branch-name&gt; 的结果大概会是这样的： 中间的分叉线路图很清晰的显示这些提交都是为了实现 complete adjusting user domains and tags 更进一步往往我的习惯是，在合并分支之前（假设要在本地将 feature 分支合并到 dev 分支），会先检查 feature 分支是否『部分落后』于远程 dev 分支： 123git checkout devgit pull # 更新 dev 分支git log feature..dev 如果没有输出任何提交信息的话，即表示 feature 对于 dev 分支是 up-to-date 的。如果有输出的话而马上执行了 git merge --no-ff 的话，提交线图会变成这样： 所以这时在合并前，通常我会先执行： 12git checkout featuregit rebase dev 这样就可以将 feature 重新拼接到更新了的 dev 之后，然后就可以合并了，最终得到一个干净舒服的提交线图。 再次提醒：像之前提到的，rebase 是『危险行为』，建议你足够熟悉 git 时才这么做，否则的话是得不偿失啊。 总结使用 git pull --rebase 和 git merge --no-ff 其实和直接使用 git pull git merge 得到的代码应该是一样。 使用 git pull --rebase 主要是为是将提交约线图平坦化，而 git merge --no-ff 则是刻意制造分叉。 六、SSH1. 查看是否生成了 SSH 公钥123$ cd ~/.ssh$ lsid_rsa id_rsa.pub known_hosts 其中 id_rsa 是私钥，id_rsa.pub 是公钥。 2. 如果没有那就开始生成，设置全局的user.name与user.email1234git config --list // 查看是否设置了user.name与user.email，没有的话，去设置// 设置全局的user.name与user.emailgit config --global user.name "XX"git config --global user.email "XX" 3. 输入 ssh-keygen 即可（或ssh-keygen -t rsa -C &quot;email&quot;）12345678$ ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/Users/schacon/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /Users/schacon/.ssh/id_rsa.Your public key has been saved in /Users/schacon/.ssh/id_rsa.pub.The key fingerprint is: 4. 生成之后获取公钥内容，输入 cat ~/.ssh/id_rsa.pub 即可， 复制 ssh-rsa 一直到 .local这一整段内容1234567$ cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@agadorlaptop.local 5. 打开 GitLab 或者 GitHub，点击头像，找到设置页6. 左侧找到 SSH keys 按钮并点击，输入刚刚复制的公钥即可 七、暂存git stash 可用来暂存当前正在进行的工作，比如想 pull 最新代码又不想 commit ， 或者另为了修改一个紧急的 bug ，先 stash，使返回到自己上一个 commit,，改完 bug 之后再 stash pop , 继续原来的工作； 添加缓存栈：git stash ; 查看缓存栈：git stash list ; 推出缓存栈：git stash pop ; 取出特定缓存内容：git stash apply stash@{1} ; 八、文件名过长错误Filename too long warning: Clone succeeded, but checkout failed. 1git config --system core.longpaths true 九、邮箱和用户名查看12git config user.namegit config user.email 修改12git config --global user.name "username"git config --global user.email "email" 十、.gitignore 更新后生效：123git rm -r --cached .git add .git commit -m ".gitignore is now working” 十一、同步Github fork 出来的分支1、配置remote，指向原始仓库 1git remote add upstream https://github.com/InterviewMap/InterviewMap.git 2、上游仓库获取到分支，及相关的提交信息，它们将被保存在本地的 upstream/master 分支 1234567git fetch upstream# remote: Counting objects: 75, done.# remote: Compressing objects: 100% (53/53), done.# remote: Total 62 (delta 27), reused 44 (delta 9)# Unpacking objects: 100% (62/62), done.# From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY# * [new branch] master -&gt; upstream/master 3、切换到本地的 master 分支 12git checkout master# Switched to branch 'master' 4、把 upstream/master 分支合并到本地的 master 分支，本地的 master 分支便跟上游仓库保持同步了，并且没有丢失本地的修改。 12345678git merge upstream/master# Updating a422352..5fdff0f# Fast-forward# README | 9 -------# README.md | 7 ++++++# 2 files changed, 7 insertions(+), 9 deletions(-)# delete mode 100644 README# create mode 100644 README.md 5、上传到自己的远程仓库中 1git push]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发 —— 限流算法]]></title>
    <url>%2Fdistribution%2Fdistributed-current-limiting%2F</url>
    <content type="text"><![CDATA[一、限流的作用由于API接口无法控制调用方的行为，因此当遇到瞬时请求量激增时，会导致接口占用过多服务器资源，使得其他请求响应速度降低或是超时，更有甚者可能导致服务器宕机。 限流(Rate limiting)指对应用服务的请求进行限制，例如某一接口的请求限制为100个每秒,对超过限制的请求则进行快速失败或丢弃。 限流可以应对： 热点业务带来的突发请求； 调用方bug导致的突发请求； 恶意攻击请求。 因此，对于公开的接口最好采取限流措施。 ​ 二、限流算法实现限流有很多办法，在程序中时通常是根据每秒处理的事务数(Transaction per second)来衡量接口的流量。 本文介绍几种最常用的限流算法： 固定窗口计数器； 滑动窗口计数器； 漏桶； 令牌桶； 1、 固定窗口计数器算法 固定窗口计数器算法概念如下： 将时间划分为多个窗口； 在每个窗口内每有一次请求就将计数器加一； 如果计数器超过了限制数量，则本窗口内所有的请求都被丢弃当时间到达下一个窗口时，计数器重置。 固定窗口计数器是最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。考虑如下情况：限制1秒内最多通过5个请求，在第一个窗口的最后半秒内通过了5个请求，第二个窗口的前半秒内又通过了5个请求。这样看来就是在1秒内通过了10个请求。 ​ 2、滑动窗口计数器算法 滑动窗口计数器算法概念如下： 将时间划分为多个区间； 在每个区间内每有一次请求就将计数器加一维持一个时间窗口，占据多个区间； 每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间； 如果当前窗口内区间的请求计数总和超过了限制数量，则本窗口内所有的请求都被丢弃。 滑动窗口计数器是通过将窗口再细分，并且按照时间”滑动”，这种算法避免了固定窗口计数器带来的双倍突发请求，但时间区间的精度越高，算法所需的空间容量就越大。 3、漏桶算法 漏桶算法概念如下： 将每个请求视作”水滴”放入”漏桶”进行存储； “漏桶”以固定速率向外”漏”出请求来执行如果”漏桶”空了则停止”漏水”； 如果”漏桶”满了则多余的”水滴”会被直接丢弃。 漏桶算法多使用队列实现，服务的请求会存到队列中，服务的提供方则按照固定的速率从队列中取出请求并执行，过多的请求则放在队列中排队或直接拒绝。 漏桶算法的缺陷也很明显，当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也都得在队列中等待一段时间才能被响应。 4、令牌桶算法 令牌桶算法概念如下： 令牌以固定速率生成； 生成的令牌放入令牌桶中存放，如果令牌桶满了则多余的令牌会直接丢弃，当请求到达时，会尝试从令牌桶中取令牌，取到了令牌的请求可以执行； 如果桶空了，那么尝试取令牌的请求会被直接丢弃。 令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。 ​ 三、单体应用限流1、信号量Semaphore限流private final Semaphore permit = new Semaphore(10, true); 2、Guava的RateLimiter实现限流RateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个 3、利用Atomic类自己实现限流算法​ 四、接入层限流1、nginx限流1、自带模块 limit_req_zone and limit_reqhttps://www.nginx.com/blog/rate-limiting-nginx/ 2、nginx+lua+redis 实现复杂的限流算法openresty了解一下 ​ 五、分布式限流1、为什么要分布式限流 当应用为单点应用时，只要应用进行了限流，那么应用所依赖的各种服务也都得到了保护。 但线上业务出于各种原因考虑，多是分布式系统，单节点的限流仅能保护自身节点，但无法保护应用依赖的各种服务，并且在进行节点扩容、缩容时也无法准确控制整个服务的请求限制。 而如果实现了分布式限流，那么就可以方便地控制整个服务集群的请求限制，且由于整个集群的请求数量得到了限制，因此服务依赖的各种资源也得到了限流的保护。 2、现有方案而分布式限流常用的则有Hystrix、resilience4j、Sentinel等框架，但这些框架都需引入第三方的类库，对于国企等一些保守的企业，引入外部类库都需要经过层层审批，较为麻烦。 3、代码+redis+lua实现分布式限流本质上是一个集群并发问题，而Redis作为一个应用广泛的中间件，又拥有单进程单线程的特性，天然可以解决分布式集群的并发问题。本文简单介绍一个通过Redis实现单次请求判断限流的功能。 1、脚本编写经过上面的对比，最适合的限流算法就是令牌桶算法。而为实现限流算法，需要反复调用Redis查询与计算，一次限流判断需要多次请求较为耗时。因此我们采用编写Lua脚本运行的方式，将运算过程放在Redis端，使得对Redis进行一次请求就能完成限流的判断。 令牌桶算法需要在Redis中存储桶的大小、当前令牌数量，并且实现每隔一段时间添加新的令牌。最简单的办法当然是每隔一段时间请求一次Redis，将存储的令牌数量递增。 但实际上我们可以通过对限流两次请求之间的时间和令牌添加速度来计算得出上次请求之后到本次请求时，令牌桶应添加的令牌数量。因此我们在Redis中只需要存储上次请求的时间和令牌桶中的令牌数量，而桶的大小和令牌的添加速度可以通过参数传入实现动态修改。 由于第一次运行脚本时默认令牌桶是满的，因此可以将数据的过期时间设置为令牌桶恢复到满所需的时间，及时释放资源。 编写完成的Lua脚本如下： 123456789101112131415161718192021222324252627local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token')local last_time = ratelimit_info[1]local current_token = tonumber(ratelimit_info[2])local max_token = tonumber(ARGV[1])local token_rate = tonumber(ARGV[2])local current_time = tonumber(ARGV[3])local reverse_time = 1000/token_rateif current_token == nil then current_token = max_token last_time = current_timeelse local past_time = current_time-last_time local reverse_token = math.floor(past_time/reverse_time) current_token = current_token+reverse_token last_time = reverse_time*reverse_token+last_time if current_token&gt;max_token then current_token = max_token endendlocal result = 0if(current_token&gt;0) then result = 1 current_token = current_token-1end redis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_token)redis.call('pexpire',KEYS[1],math.ceil(reverse_time*(max_token-current_token)+(current_time-last_time)))return result 2、执行限流这里使用Spring Data Redis来进行Redis脚本的调用。 编写Redis脚本类: 1234567891011121314151617public class RedisReteLimitScript implements RedisScript&lt;String&gt; &#123; private static final String SCRIPT = "local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token') local last_time = ratelimit_info[1] local current_token = tonumber(ratelimit_info[2]) local max_token = tonumber(ARGV[1]) local token_rate = tonumber(ARGV[2]) local current_time = tonumber(ARGV[3]) local reverse_time = 1000/token_rate if current_token == nil then current_token = max_token last_time = current_time else local past_time = current_time-last_time; local reverse_token = math.floor(past_time/reverse_time) current_token = current_token+reverse_token; last_time = reverse_time*reverse_token+last_time if current_token&gt;max_token then current_token = max_token end end local result = '0' if(current_token&gt;0) then result = '1' current_token = current_token-1 end redis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_toke redis.call('pexpire',KEYS[1],math.ceil(reverse_time*(max_tokencurrent_token)+(current_time-last_time))) return result"; @Override public String getSha1() &#123; return DigestUtils.sha1Hex(SCRIPT); &#125; @Override public Class&lt;String&gt; getResultType() &#123; return String.class; &#125; @Override public String getScriptAsString() &#123; return SCRIPT; &#125;&#125; 通过RedisTemplate对象执行脚本： 1234567public boolean rateLimit(String key, int max, int rate) &#123; List&lt;String&gt; keyList = new ArrayList&lt;&gt;(1); keyList.add(key); return "1".equals(stringRedisTemplate .execute(new RedisReteLimitScript(), keyList, Integer.toString(max), Integer.toString(rate), Long.toString(System.currentTimeMillis()))); &#125; rateLimit方法传入的key为限流接口的ID，max为令牌桶的最大大小，rate为每秒钟恢复的令牌数量，返回的boolean即为此次请求是否通过了限流。为了测试Redis脚本限流是否可以正常工作，我们编写一个单元测试进行测试看看。 123456789101112131415161718192021222324@Autowiredprivate RedisManager redisManager;@Testpublic void rateLimitTest() throws InterruptedException &#123; String key = "test_rateLimit_key"; int max = 10; //令牌桶大小 int rate = 10; //令牌每秒恢复速度 AtomicInteger successCount = new AtomicInteger(0); Executor executor = Executors.newFixedThreadPool(10); CountDownLatch countDownLatch = new CountDownLatch(30); for (int i = 0; i &lt; 30; i++) &#123; executor.execute(() -&gt; &#123; boolean isAllow = redisManager.rateLimit(key, max, rate); if (isAllow) &#123; successCount.addAndGet(1); &#125; log.info(Boolean.toString(isAllow)); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); log.info("请求成功&#123;&#125;次", successCount.get());&#125; 设置令牌桶大小为10，令牌桶每秒恢复10个，启动10个线程在短时间内进行30次请求，并输出每次限流查询的结果。日志输出： 12345678910111213141516171819202122232425262728293031[19:12:50,283]true [19:12:50,284]true [19:12:50,284]true [19:12:50,291]true [19:12:50,291]true [19:12:50,291]true [19:12:50,297]true [19:12:50,297]true [19:12:50,298]true [19:12:50,305]true [19:12:50,305]false [19:12:50,305]true [19:12:50,312]false [19:12:50,312]false [19:12:50,312]false [19:12:50,319]false [19:12:50,319]false [19:12:50,319]false [19:12:50,325]false [19:12:50,325]false [19:12:50,326]false [19:12:50,380]false [19:12:50,380]false [19:12:50,380]false [19:12:50,387]false [19:12:50,387]false [19:12:50,387]false [19:12:50,392]false [19:12:50,392]false [19:12:50,392]false [19:12:50,393]请求成功11次 可以看到，在0.1秒内请求的30次请求中，除了初始的10个令牌以及随时间恢复的1个令牌外，剩下19个没有取得令牌的请求均返回了false，限流脚本正确的将超过限制的请求给判断出来了，业务中此时就可以直接返回系统繁忙或接口请求太过频繁等提示。 3、开发中遇到的问题1）Lua变量格式 Lua中的String和Number需要通过tonumber()和tostring()进行转换。 2）Redis入参 Redis的pexpire等命令不支持小数，但Lua的Number类型可以存放小数，因此Number类型传递给 Redis时最好通过math.ceil()等方式转换以避免存在小数导致命令失败。 3）Time命令 由于Redis在集群下是通过复制脚本及参数到所有节点上，因此无法在具有不确定性的命令后面执行写入命令，因此只能请求时传入时间而无法使用Redis的Time命令获取时间。 3.2版本之后的Redis脚本支持redis.replicate_commands()，可以改为使用Time命令获取当前时间。 4）潜在的隐患 由于此Lua脚本是通过请求时传入的时间做计算，因此务必保证分布式节点上获取的时间同步，如果时间不同步会导致限流无法正常运作。]]></content>
      <categories>
        <category>distribution</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>distribution</tag>
        <tag>限流</tag>
        <tag>openresty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.3、线程间通信]]></title>
    <url>%2Fjava%2Fconcurrent-program%2F4-3-%E7%BA%BF%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[4.3、线程间通信通信方式：共享内存、消息传递 4.3.1、volatile 和 synchronized关键字 volatile修饰变量 —— 禁止指令重排 和 变量对 所有线程的可见； synchronized修饰方法或同步块 —— 同一时刻，只有一个线程处于方法或同步块中，保证 线程对变量访问的可见性 和 排他性；synchronized本身没有禁止指令重排的功能，需要配合volatile使用；]]></content>
      <categories>
        <category>java</category>
        <category>java-concurrent-program</category>
      </categories>
      <tags>
        <tag>Java并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式计算文档汇总]]></title>
    <url>%2Fdistribution%2Fdistributed-computing%2Fdistributed-computing-docs%2F</url>
    <content type="text"><![CDATA[分布式计算总站：https://equn.com/wiki/%E9%A6%96%E9%A1%B5 知乎·分布式计算：https://www.zhihu.com/topic/19552071/hot 批计算MapReduce 是一种 分而治之 的计算模式，在分布式领域中，除了典型的 Hadoop 的 MapReduce(Google MapReduce 的开源实现)，还有 Fork-Join，Fork-Join 是 Java 等语言或库提供的原生多线程并行处理框架，采用线程级的分而治之计算模式。它充分利用多核 CPU 的优势，以递归的方式把一个任务拆分成多个“小任务”，把多个“小任务”放到多个处理器上并行执行，即 Fork 操作。当多个“小任务”执行完成之后，再将这些执行结果合并起来即可得到原始任务的结果，即 Join 操作。 虽然 MapReduce 是进程级的分而治之计算模式，但与 Fork-Join 的核心思想是一致的。因此，Fork-Join 又被称为 Java 版的 MapReduce 框架。但，MapReduce 和 Fork-Join 之间有一个本质的区别：Fork-Join 不能大规模扩展，只适用于在单个 Java 虚拟机上运行，多个小任务虽然运行在不同的处理器上，但可以相互通信，甚至一个线程可以“窃取”其他线程上的子任务。 MapReduce 可以大规模扩展，适用于大型计算机集群。通过 MapReduce 拆分后的任务，可以跨多个计算机去执行，且各个小任务之间不会相互通信。 MapReduce 模式的核心思想是，将大任务拆分成多个小任务，针对这些小任务分别计算后，再合并各小任务的结果以得到大任务的计算结果，任务运行完成后整个任务进程就结束了，属于短任务模式。但任务进程的启动和停止是一件很耗时的事儿，因此 MapReduce 对处理实时性的任务就不太合适了 流计算实时性任务主要是针对流数据的处理，对处理时延要求很高，通常需要有常驻服务进程，等待数据的随时到来随时处理，以保证低时延。处理流数据任务的计算模式，在分布式领域中叫作 Stream。近年来，由于网络监控、传感监测、AR/VR 等实时性应用的兴起，一类需要处理流数据的业务发展了起来。比如各种直播平台中，我们需要处理直播产生的音视频数据流等。这种如流水般持续涌现，且需要实时处理的数据，我们称之为流数据。 总结来讲，流数据的特征主要包括以下 4 点： 数据如流水般持续、快速地到达； 海量数据规模，数据量可达到 TB 级甚至 PB 级； 对实时性要求高，随着时间流逝，数据的价值会大幅降低； 数据顺序无法保证，系统无法控制将要处理的数据元素的顺序。 第一步，提交流式计算作业 第二步，加载流式数据进行流计算 第三步，持续输出计算结果。]]></content>
      <categories>
        <category>distribution</category>
        <category>distributed-computing</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink 各版本 changelog]]></title>
    <url>%2Fflink%2Fflink-version-changelog%2F</url>
    <content type="text"><![CDATA[https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/ Flink 1.10Clusters &amp; DeploymentFileSystems should be loaded via Plugin Architecture (FLINK-11956)s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through plugins but now seamlessly integrate with all credential providers. Other filesystems are strongly recommended to be only used as plugins as we will continue to remove relocations. Flink Client respects Classloading Policy (FLINK-13749)The Flink client now also respects the configured classloading policy, i.e., parent-first or child-first classloading. Previously, only cluster components such as the job manager or task manager supported this setting. This does mean that users might get different behaviour in their programs, in which case they should configure the classloading policy explicitly to use parent-first classloading, which was the previous (hard-coded) behaviour. Enable spreading out Tasks evenly across all TaskManagers (FLINK-12122)When FLIP-6 was rolled out with Flink 1.5.0, we changed how slots are allocated from TaskManagers (TMs). Instead of evenly allocating the slots from all registered TMs, we had the tendency to exhaust a TM before using another one. To use a scheduling strategy that is more similar to the pre-FLIP-6 behaviour, where Flink tries to spread out the workload across all currently available TMs, one can set cluster.evenly-spread-out-slots: true in the flink-conf.yaml. Directory Structure Change for highly available Artifacts (FLINK-13633)All highly available artifacts stored by Flink will now be stored under HA_STORAGE_DIR/HA_CLUSTER_ID with HA_STORAGE_DIR configured by high-availability.storageDir and HA_CLUSTER_ID configured by high-availability.cluster-id. Resources and JARs shipped via –yarnship will be ordered in the Classpath (FLINK-13127)When using the --yarnship command line option, resource directories and jar files will be added to the classpath in lexicographical order with resources directories appearing first. Removal of –yn/–yarncontainer Command Line Options (FLINK-12362)The Flink CLI no longer supports the deprecated command line options -yn/--yarncontainer, which were used to specify the number of containers to start on YARN. This option has been deprecated since the introduction of FLIP-6. All Flink users are advised to remove this command line option. Removal of –yst/–yarnstreaming Command Line Options (FLINK-14957)The Flink CLI no longer supports the deprecated command line options -yst/--yarnstreaming, which were used to disable eager pre-allocation of memory. All Flink users are advised to remove this command line option. Mesos Integration will reject expired Offers faster (FLINK-14029)Flink’s Mesos integration now rejects all expired offers instead of only 4. This improves the situation where Fenzo holds on to a lot of expired offers without giving them back to the Mesos resource manager. Scheduler Rearchitecture (FLINK-14651)Flink’s scheduler was refactored with the goal of making scheduling strategies customizable in the future. Using the legacy scheduler is discouraged as it will be removed in a future release. However, users that experience issues related to scheduling can fallback to the legacy scheduler by setting jobmanager.scheduler to legacy in their flink-conf.yaml for the time being. Note, however, that using the legacy scheduler with the Pipelined Region Failover Strategy enabled has the following caveats: Exceptions that caused a job to restart will not be shown on the job overview page of the Web UI (FLINK-15917). However, exceptions that cause a job to fail (e.g., when all restart attempts exhausted) will still be shown. The uptime metric will not be reset after restarting a job due to task failure (FLINK-15918). Note that in the default flink-conf.yaml, the Pipelined Region Failover Strategy is already enabled. That is, users that want to use the legacy scheduler and cannot accept aforementioned caveats should make sure that jobmanager.execution.failover-strategy is set to full or not set at all. Java 11 Support (FLINK-10725)Beginning from this release, Flink can be compiled and run with Java 11. All Java 8 artifacts can be also used with Java 11. This means that users that want to run Flink with Java 11 do not have to compile Flink themselves. When starting Flink with Java 11, the following warnings may be logged: 1234567891011121314151617181920212223WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.core.memory.MemoryUtils (file:/opt/flink/flink-1.10.0/lib/flink-dist_2.11-1.10.0.jar) to constructor java.nio.DirectByteBuffer(long,int)WARNING: Please consider reporting this to the maintainers of org.apache.flink.core.memory.MemoryUtilsWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseWARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/home/flinkuser/.m2/repository/org/apache/flink/flink-core/1.10.0/flink-core-1.10.0.jar) to field java.lang.String.valueWARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleanerWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseWARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/home/flinkuser/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar) to method java.nio.DirectByteBuffer.cleaner()WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseWARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by com.esotericsoftware.kryo.util.UnsafeUtil (file:/home/flinkuser/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar) to constructor java.nio.DirectByteBuffer(long,int,java.lang.Object)WARNING: Please consider reporting this to the maintainers of com.esotericsoftware.kryo.util.UnsafeUtilWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release These warnings are considered harmless and will be addressed in future Flink releases. Lastly, note that the connectors for Cassandra, Hive, HBase, and Kafka 0.8–0.11 have not been tested with Java 11 because the respective projects did not provide Java 11 support at the time of the Flink 1.10.0 release. Memory ManagementNew Task Executor Memory Model (FLINK-13980)With FLIP-49, a new memory model has been introduced for the task executor. New configuration options have been introduced to control the memory consumption of the task executor process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration. The memory model of the job manager process has not been changed yet but it is planned to be updated as well. If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes. Please, check the user documentation for more details. Deprecation and breaking changesThe following options have been removed and have no effect anymore: Deprecated/removed config option Note taskmanager.memory.fraction Check also the description of the new option taskmanager.memory.managed.fraction but it has different semantics and the value of the deprecated option usually has to be adjusted taskmanager.memory.off-heap Support for on-heap managed memory has been removed, leaving off-heap managed memory as the only possibility taskmanager.memory.preallocate Pre-allocation is no longer supported, and managed memory is always allocated lazily The following options, if used, are interpreted as other new options in order to maintain backwards compatibility where it makes sense: Deprecated config option Interpreted as taskmanager.heap.size taskmanager.memory.flink.size for standalone deploymenttaskmanager.memory.process.size for containerized deployments taskmanager.memory.size taskmanager.memory.managed.size taskmanager.network.memory.min taskmanager.memory.network.min taskmanager.network.memory.max taskmanager.memory.network.max taskmanager.network.memory.fraction taskmanager.memory.network.fraction The container cut-off configuration options, containerized.heap-cutoff-ratio and containerized.heap-cutoff-min, have no effect for task executor processes anymore but they still have the same semantics for the JobManager process. RocksDB State Backend Memory Control (FLINK-7289)Together with the introduction of the new Task Executor Memory Model, the memory consumption of the RocksDB state backend will be limited by the total amount of Flink Managed Memory, which can be configured via taskmanager.memory.managed.size or taskmanager.memory.managed.fraction. Furthermore, users can tune RocksDB’s write/read memory ratio (state.backend.rocksdb.memory.write-buffer-ratio, by default 0.5) and the reserved memory fraction for indices/filters (state.backend.rocksdb.memory.high-prio-pool-ratio, by default 0.1). More details and advanced configuration options can be found in the Flink user documentation. Fine-grained Operator Resource Management (FLINK-14058)Config options table.exec.resource.external-buffer-memory, table.exec.resource.hash-agg.memory, table.exec.resource.hash-join.memory, and table.exec.resource.sort.memory have been deprecated. Beginning from Flink 1.10, these config options are interpreted as weight hints instead of absolute memory requirements. Flink choses sensible default weight hints which should not be adjustment by users. Table API &amp; SQLRename of ANY Type to RAW Type (FLINK-14904)The identifier raw is a reserved keyword now and must be escaped with backticks when used as a SQL field or function name. Rename of Table Connector Properties (FLINK-14649)Some indexed properties for table connectors have been flattened and renamed for a better user experience when writing DDL statements. This affects the Kafka Connector properties connector.properties and connector.specific-offsets. Furthermore, the Elasticsearch Connector property connector.hosts is affected. The aforementioned, old properties are deprecated and will be removed in future versions. Please consult the Table Connectors documentation for the new property names. Methods for interacting with temporary Tables &amp; Views (FLINK-14490)Methods registerTable()/registerDataStream()/registerDataSet() have been deprecated in favor of createTemporaryView(), which better adheres to the corresponding SQL term. The scan() method has been deprecated in favor of the from() method. Methods registerTableSource()/registerTableSink() become deprecated in favor of ConnectTableDescriptor#createTemporaryTable(). The ConnectTableDescriptor approach expects only a set of string properties as a description of a TableSource or TableSink instead of an instance of a class in case of the deprecated methods. This in return makes it possible to reliably store those definitions in catalogs. Method insertInto(String path, String... pathContinued) has been removed in favor of in insertInto(String path). All the newly introduced methods accept a String identifier which will be parsed into a 3-part identifier. The parser supports quoting the identifier. It also requires escaping any reserved SQL keywords. Removal of ExternalCatalog API (FLINK-13697)The deprecated ExternalCatalog API has been dropped. This includes: ExternalCatalog (and all dependent classes, e.g., ExternalTable) SchematicDescriptor, MetadataDescriptor, StatisticsDescriptor Users are advised to use the new Catalog API. ConfigurationIntroduction of Type Information for ConfigOptions (FLINK-14493)Getters of org.apache.flink.configuration.Configuration throw IllegalArgumentException now if the configured value cannot be parsed into the required type. In previous Flink releases the default value was returned in such cases. Increase of default Restart Delay (FLINK-13884)The default restart delay for all shipped restart strategies, i.e., fixed-delay and failure-rate, has been raised to 1 s (from originally 0 s). Simplification of Cluster-Level Restart Strategy Configuration (FLINK-13921)Previously, if the user had set restart-strategy.fixed-delay.attempts or restart-strategy.fixed-delay.delay but had not configured the option restart-strategy, the cluster-level restart strategy would have been fixed-delay. Now the cluster-level restart strategy is only determined by the config option restart-strategy and whether checkpointing is enabled. See “Task Failure Recovery” for details. Disable memory-mapped BoundedBlockingSubpartition by default (FLINK-14952)The config option taskmanager.network.bounded-blocking-subpartition-type has been renamed to taskmanager.network.blocking-shuffle.type. Moreover, the default value of the aforementioned config option has been changed from auto to file. The reason is that TaskManagers running on YARN with auto, could easily exceed the memory budget of their container, due to incorrectly accounted memory-mapped files memory usage. Removal of non-credit-based Network Flow Control (FLINK-14516)The non-credit-based network flow control code was removed alongside of the configuration option taskmanager.network.credit-model. Flink will now always use credit-based flow control. Removal of HighAvailabilityOptions#HA_JOB_DELAY (FLINK-13885)The configuration option high-availability.job.delay has been removed since it is no longer used. StateEnable Background Cleanup of State with TTL by default (FLINK-14898)Background cleanup of expired state with TTL is activated by default now for all state backends shipped with Flink. Note that the RocksDB state backend implements background cleanup by employing a compaction filter. This has the caveat that even if a Flink job does not store state with TTL, a minor performance penalty during compaction is incurred. Users that experience noticeable performance degradation during RocksDB compaction can disable the TTL compaction filter by setting the config option state.backend.rocksdb.ttl.compaction.filter.enabled to false. Deprecation of StateTtlConfig#Builder#cleanupInBackground() (FLINK-15606)StateTtlConfig#Builder#cleanupInBackground() has been deprecated because the background cleanup of state with TTL is already enabled by default. Timers are stored in RocksDB by default when using RocksDBStateBackend (FLINK-15637)The default timer store has been changed from Heap to RocksDB for the RocksDB state backend to support asynchronous snapshots for timer state and better scalability, with less than 5% performance cost. Users that find the performance decline critical can set state.backend.rocksdb.timer-service.factory to HEAP in flink-conf.yaml to restore the old behavior. Removal of StateTtlConfig#TimeCharacteristic (FLINK-15605)StateTtlConfig#TimeCharacteristic has been removed in favor of StateTtlConfig#TtlTimeCharacteristic. New efficient Method to check if MapState is empty (FLINK-13034)We have added a new method MapState#isEmpty() which enables users to check whether a map state is empty. The new method is 40% faster than mapState.keys().iterator().hasNext() when using the RocksDB state backend. RocksDB Upgrade (FLINK-14483)We have again released our own RocksDB build (FRocksDB) which is based on RocksDB version 5.17.2 with several feature backports for the Write Buffer Manager to enable limiting RocksDB’s memory usage. The decision to release our own RocksDB build was made because later RocksDB versions suffer from a performance regression under certain workloads. RocksDB Logging disabled by default (FLINK-15068)Logging in RocksDB (e.g., logging related to flush, compaction, memtable creation, etc.) has been disabled by default to prevent disk space from being filled up unexpectedly. Users that need to enable logging should implement their own RocksDBOptionsFactory that creates DBOptions instances with InfoLogLevel set to INFO_LEVEL. Improved RocksDB Savepoint Recovery (FLINK-12785)In previous Flink releases users may encounter an OutOfMemoryError when restoring from a RocksDB savepoint containing large KV pairs. For that reason we introduced a configurable memory limit in the RocksDBWriteBatchWrapper with a default value of 2 MB. RocksDB’s WriteBatch will flush before the consumed memory limit is reached. If needed, the limit can be tuned via the state.backend.rocksdb.write-batch-size config option in flink-conf.yaml. PyFlinkPython 2 Support dropped (FLINK-14469)Beginning from this release, PyFlink does not support Python 2. This is because Python 2 has reached end of life on January 1, 2020, and several third-party projects that PyFlink depends on are also dropping Python 2 support. MonitoringInfluxdbReporter skips Inf and NaN (FLINK-12147)The InfluxdbReporter now silently skips values that are unsupported by InfluxDB, such as Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, Double.NaN, etc. ConnectorsKinesis Connector License Change (FLINK-12847)flink-connector-kinesis is now licensed under the Apache License, Version 2.0, and its artifacts will be deployed to Maven central as part of the Flink releases. Users no longer need to build the Kinesis connector from source themselves. Miscellaneous Interface ChangesExecutionConfig#getGlobalJobParameters() cannot return null anymore (FLINK-9787)ExecutionConfig#getGlobalJobParameters has been changed to never return null. Conversely, ExecutionConfig#setGlobalJobParameters(GlobalJobParameters) will not accept null values anymore. Change of contract in MasterTriggerRestoreHook interface (FLINK-14344)Implementations of MasterTriggerRestoreHook#triggerCheckpoint(long, long, Executor) must be non-blocking now. Any blocking operation should be executed asynchronously, e.g., using the given executor. Client-/ and Server-Side Separation of HA Services (FLINK-13750)The HighAvailabilityServices have been split up into client-side ClientHighAvailabilityServices and cluster-side HighAvailabilityServices. When implementing custom high availability services, users should follow this separation by overriding the factory method HighAvailabilityServicesFactory#createClientHAServices(Configuration). Moreover, HighAvailabilityServices#getWebMonitorLeaderRetriever() should no longer be implemented since it has been deprecated. Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() (FLINK-13977)Implementations of HighAvailabilityServices should implement HighAvailabilityServices#getClusterRestEndpointLeaderElectionService() instead of HighAvailabilityServices#getWebMonitorLeaderElectionService(). Interface Change in LeaderElectionService (FLINK-14287)LeaderElectionService#confirmLeadership(UUID, String) now takes an additional second argument, which is the address under which the leader will be reachable. All custom LeaderElectionService implementations will need to be updated accordingly. Deprecation of Checkpoint Lock (FLINK-14857)The method org.apache.flink.streaming.runtime.tasks.StreamTask#getCheckpointLock() is deprecated now. Users should use MailboxExecutor to run actions that require synchronization with the task’s thread (e.g. collecting output produced by an external thread). The methods MailboxExecutor#yield() or MailboxExecutor#tryYield() can be used for actions that need to give up control to other actions temporarily, e.g., if the current operator is blocked. The MailboxExecutor can be accessed by using YieldingOperatorFactory (see AsyncWaitOperator for an example usage). Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces (FLINK-14926)Interfaces OptionsFactory and ConfigurableOptionsFactory have been deprecated in favor of RocksDBOptionsFactory and ConfigurableRocksDBOptionsFactory, respectively. Flink 1.9Known shortcomings or limitations for new featuresNew Table / SQL Blink plannerFlink 1.9.0 provides support for two planners for the Table API, namely Flink’s original planner and the new Blink planner. The original planner maintains same behaviour as previous releases, while the new Blink planner is still considered experimental and has the following limitations: The Blink planner can not be used with BatchTableEnvironment, and therefore Table programs ran with the planner can not be transformed to DataSet programs. This is by design and will also not be supported in the future. Therefore, if you want to run a batch job with the Blink planner, please use the new TableEnvironment. For streaming jobs, both StreamTableEnvironment and TableEnvironment works. Implementations of StreamTableSink should implement the consumeDataStream method instead of emitDataStream if it is used with the Blink planner. Both methods work with the original planner. This is by design to make the returned DataStreamSink accessible for the planner. Due to a bug with how transformations are not being cleared on execution, TableEnvironment instances should not be reused across multiple SQL statements when using the Blink planner. Table.flatAggregate is not supported Session and count windows are not supported when running batch jobs. The Blink planner only supports the new Catalog API, and does not support ExternalCatalog which is now deprecated. Related issues: FLINK-13708: Transformations should be cleared because a table environment could execute multiple job FLINK-13473: Add GroupWindowed FlatAggregate support to stream Table API (Blink planner), i.e, align with Flink planner FLINK-13735: Support session window with Blink planner in batch mode FLINK-13736: Support count window with Blink planner in batch mode SQL DDLIn Flink 1.9.0, the community also added a preview feature about SQL DDL, but only for batch style DDLs. Therefore, all streaming related concepts are not supported yet, for example watermarks. Related issues: FLINK-13661: Add a stream specific CREATE TABLE SQL DDL FLINK-13568: DDL create table doesn’t allow STRING data type Java 9 supportSince Flink 1.9.0, Flink can now be compiled and run on Java 9. Note that certain components interacting with external systems (connectors, filesystems, metric reporters, etc.) may not work since the respective projects may have skipped Java 9 support. Related issues: FLINK-8033: JDK 9 support Memory managementIn Fink 1.9.0 and prior version, the managed memory fraction of taskmanager is controlled by taskmanager.memory.fraction, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter NewRatio is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value. Related issues: FLINK-14123: Lower the default value of taskmanager.memory.fraction Deprecations and breaking changesScala expression DSL for Table API moved to flink-table-api-scalaSince 1.9.0, the implicit conversions for the Scala expression DSL for the Table API has been moved to flink-table-api-scala. This requires users to update the imports in their Table programs. Users of pure Table programs should define their imports like: 123import org.apache.flink.table.api._TableEnvironment.create(...) Users of the DataStream API should define their imports like: 1234import org.apache.flink.table.api._import org.apache.flink.table.api.scala._StreamTableEnvironment.create(...) Related issues: FLINK-13045: Move Scala expression DSL to flink-table-api-scala Failover strategiesAs a result of completing fine-grained recovery (FLIP-1), Flink will now attempt to only restart tasks that are connected to failed tasks through a pipelined connection. By default, the region failover strategy is used. Users who were not using a restart strategy or have already configured a failover strategy should not be affected. Moreover, users who already enabled the region failover strategy, along with a restart strategy that enforces a certain number of restarts or introduces a restart delay, will see changes in behavior. The region failover strategy now correctly respects constraints that are defined by the restart strategy. Streaming users who were not using a failover strategy may be affected if their jobs are embarrassingly parallel or contain multiple independent jobs. In this case, only the failed parallel pipeline or affected jobs will be restarted. Batch users may be affected if their job contains blocking exchanges (usually happens for shuffles) or the ExecutionMode was set to BATCH or BATCH_FORCED via the ExecutionConfig. Overall, users should see an improvement in performance. Related issues: FLINK-13223: Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml FLINK-13060: FailoverStrategies should respect restart constraints Job termination via CLIWith the support of graceful job termination with savepoints for semantic correctness (FLIP-34), a few changes related to job termination has been made to the CLI. From now on, the stop command with no further arguments stops the job with a savepoint targeted at the default savepoint location (as configured via the state.savepoints.dir property in the job configuration), or a location explicitly specified using the -p option. Please make sure to configure the savepoint path using either one of these options. Since job terminations are now always accompanied with a savepoint, stopping jobs is expected to take longer now. Related issues: FLINK-13123: Align Stop/Cancel Commands in CLI and REST Interface and Improve Documentation FLINK-11458: Add TERMINATE/SUSPEND Job with Savepoint Network stackA few changes in the network stack related to changes in the threading model of StreamTask to a mailbox-based approach requires close attention to some related configuration: Due to changes in the lifecycle management of result partitions, partition requests as well as re-triggers will now happen sooner. Therefore, it is possible that some jobs with long deployment times and large state might start failing more frequently with PartitionNotFound exceptions compared to previous versions. If that’s the case, users should increase the value of taskmanager.network.request-backoff.max in order to have the same effective partition request timeout as it was prior to 1.9.0. To avoid a potential deadlock, a timeout has been added for how long a task will wait for assignment of exclusive memory segments. The default timeout is 30 seconds, and is configurable via taskmanager.network.memory.exclusive-buffers-request-timeout-ms. It is possible that for some previously working deployments this default timeout value is too low and might have to be increased. Please also notice that several network I/O metrics have had their scope changed. See the 1.9 metrics documentation for which metrics are affected. In 1.9.0, these metrics will still be available under their previous scopes, but this may no longer be the case in future versions. Related issues: FLINK-13013: Make sure that SingleInputGate can always request partitions FLINK-12852: Deadlock occurs when requiring exclusive buffer for RemoteInputChannel FLINK-12555: Introduce an encapsulated metric group layout for shuffle API and deprecate old one AsyncIODue to a bug in the AsyncWaitOperator, in 1.9.0 the default chaining behaviour of the operator is now changed so that it is never chained after another operator. This should not be problematic for migrating from older version snapshots as long as an uid was assigned to the operator. If an uid was not assigned to the operator, please see the instructions here for a possible workaround. Related issues: FLINK-13063: AsyncWaitOperator shouldn’t be releasing checkpointingLock Connectors and LibrariesIntroduced KafkaSerializationSchema to fully replace KeyedSerializationSchemaThe universal FlinkKafkaProducer (in flink-connector-kafka) supports a new KafkaSerializationSchema that will fully replace KeyedSerializationSchema in the long run. This new schema allows directly generating Kafka ProducerRecords for sending to Kafka, therefore enabling the user to use all available Kafka features (in the context of Kafka records). Dropped connectors and libraries The Elasticsearch 1 connector has been dropped and will no longer receive patches. Users may continue to use the connector from a previous series (like 1.8) with newer versions of Flink. It is being dropped due to being used significantly less than more recent versions (Elasticsearch versions 2.x and 5.x are downloaded 4 to 5 times more), and hasn’t seen any development for over a year. The older Python APIs for batch and streaming have been removed and will no longer receive new patches. A new API is being developed based on the Table API as part of FLINK-12308: Support python language in Flink Table API. Existing users may continue to use these older APIs with future versions of Flink by copying both the flink-streaming-python and flink-python jars into the /lib directory of the distribution and the corresponding start scripts pyflink-stream.sh and pyflink.sh into the /bin directory of the distribution. The older machine learning libraries have been removed and will no longer receive new patches. This is due to efforts towards a new Table-based machine learning library (FLIP-39). Users can still use the 1.8 version of the legacy library if their projects still rely on it. Related issues: FLINK-11693: Add KafkaSerializationSchema that directly uses ProducerRecord FLINK-12151: Drop Elasticsearch 1 connector FLINK-12903: Remove legacy flink-python APIs FLINK-12308: Support python language in Flink Table API FLINK-12597: Remove the legacy flink-libraries/flink-ml MapR dependency removedDependency on MapR vendor-specific artifacts has been removed, by changing the MapR filesystem connector to work purely based on reflection. This does not introduce any regession in the support for the MapR filesystem. The decision to remove hard dependencies on the MapR artifacts was made due to very flaky access to the secure https endpoint of the MapR artifact repository, and affected build stability of Flink. Related issues: FLINK-12578: Use secure URLs for Maven repositories FLINK-13499: Remove dependency on MapR artifact repository StateDescriptor interface changeAccess to the state serializer in StateDescriptor is now modified from protected to private access. Subclasses should use the StateDescriptor#getSerializer() method as the only means to obtain the wrapped state serializer. Related issues: FLINK-12688: Make serializer lazy initialization thread safe in StateDescriptor Web UI dashboardThe web frontend of Flink has been updated to use the latest Angular version (7.x). The old frontend remains available in Flink 1.9.x, but will be removed in a later Flink release once the new frontend is considered stable. Related issues: FLINK-10705: Rework Flink Web Dashoard Flink 1.8StateContinuous incremental cleanup of old Keyed State with TTLWe introduced TTL (time-to-live) for Keyed state in Flink 1.6 (FLINK-9510). This feature allowed to clean up and make inaccessible keyed state entries when accessing them. In addition state would now also being cleaned up when writing a savepoint/checkpoint. Flink 1.8 introduces continous cleanup of old entries for both the RocksDB state backend (FLINK-10471) and the heap state backend (FLINK-10473). This means that old entries (according to the ttl setting) are continously being cleanup up. New Support for Schema Migration when restoring SavepointsWith Flink 1.7.0 we added support for changing the schema of state when using the AvroSerializer (FLINK-10605). With Flink 1.8.0 we made great progress migrating all built-in TypeSerializers to a new serializer snapshot abstraction that theoretically allows schema migration. Of the serializers that come with Flink, we now support schema migration for the PojoSerializer (FLINK-11485), and Java EnumSerializer (FLINK-11334), As well as for Kryo in limited cases (FLINK-11323). Savepoint compatibilitySavepoints from Flink 1.2 that contain a Scala TraversableSerializer are not compatible with Flink 1.8 anymore because of an update in this serializer (FLINK-11539). You can get around this restriction by first upgrading to a version between Flink 1.3 and Flink 1.7 and then updating to Flink 1.8. RocksDB version bump and switch to FRocksDB (FLINK-10471)We needed to switch to a custom build of RocksDB called FRocksDB because we need certain changes in RocksDB for supporting continuous state cleanup with TTL. The used build of FRocksDB is based on the upgraded version 5.17.2 of RocksDB. For Mac OS X, RocksDB version 5.17.2 is supported only for OS X version &gt;= 10.13. See also: https://github.com/facebook/rocksdb/issues/4862. Maven DependenciesChanges to bundling of Hadoop libraries with Flink (FLINK-11266)Convenience binaries that include hadoop are no longer released. If a deployment relies on flink-shaded-hadoop2 being included in flink-dist, then you must manually download a pre-packaged Hadoop jar from the optional components section of the download page and copy it into the /lib directory. Alternatively, a Flink distribution that includes hadoop can be built by packaging flink-dist and activating the include-hadoop maven profile. As hadoop is no longer included in flink-dist by default, specifying -DwithoutHadoop when packaging flink-dist no longer impacts the build. ConfigurationTaskManager configuration (FLINK-11716)TaskManagers now bind to the host IP address instead of the hostname by default . This behaviour can be controlled by the configuration option taskmanager.network.bind-policy. If your Flink cluster should experience inexplicable connection problems after upgrading, try to set taskmanager.network.bind-policy: name in your flink-conf.yaml to return to the pre-1.8 behaviour. Table APIDeprecation of direct Table constructor usage (FLINK-11447)Flink 1.8 deprecates direct usage of the constructor of the Table class in the Table API. This constructor would previously be used to perform a join with a lateral table. You should now use table.joinLateral() or table.leftOuterJoinLateral() instead. This change is necessary for converting the Table class into an interface, which will make the API more maintainable and cleaner in the future. Introduction of new CSV format descriptor (FLINK-9964)This release introduces a new format descriptor for CSV files that is compliant with RFC 4180. The new descriptor is available as org.apache.flink.table.descriptors.Csv. For now, this can only be used together with the Kafka connector. The old descriptor is available as org.apache.flink.table.descriptors.OldCsv for use with file system connectors. Deprecation of static builder methods on TableEnvironment (FLINK-11445)In order to separate API from actual implementation, the static methods TableEnvironment.getTableEnvironment() are deprecated. You should now use Batch/StreamTableEnvironment.create() instead. Change in the Maven modules of Table API (FLINK-11064)Users that had a flink-table dependency before, need to update their dependencies to flink-table-planner and the correct dependency of flink-table-api-*, depending on whether Java or Scala is used: one of flink-table-api-java-bridge or flink-table-api-scala-bridge. Change to External Catalog Table Builders (FLINK-11522)ExternalCatalogTable.builder() is deprecated in favour of ExternalCatalogTableBuilder(). Change to naming of Table API connector jars (FLINK-11026)The naming scheme for kafka/elasticsearch6 sql-jars has been changed. In maven terms, they no longer have the sql-jar qualifier and the artifactId is now prefixed with flink-sql instead of flink, e.g., flink-sql-connector-kafka.... Change to how Null Literals are specified (FLINK-11785)Null literals in the Table API need to be defined with nullOf(type) instead of Null(type) from now on. The old approach is deprecated. ConnectorsIntroduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (FLINK-8354)For the Flink KafkaConsumers, we introduced a new KafkaDeserializationSchema that gives direct access to the Kafka ConsumerRecord. This subsumes the KeyedSerializationSchema functionality, which is deprecated but still available for now. FlinkKafkaConsumer will now filter restored partitions based on topic specification (FLINK-10342)Starting from Flink 1.8.0, the FlinkKafkaConsumer now always filters out restored partitions that are no longer associated with a specified topic to subscribe to in the restored execution. This behaviour did not exist in previous versions of the FlinkKafkaConsumer. If you wish to retain the previous behaviour, please use the disableFilterRestoredPartitionsWithSubscribedTopics() configuration method on the FlinkKafkaConsumer. Consider this example: if you had a Kafka Consumer that was consuming from topic A, you did a savepoint, then changed your Kafka consumer to instead consume from topic B, and then restarted your job from the savepoint. Before this change, your consumer would now consume from both topic A and B because it was stored in state that the consumer was consuming from topic A. With the change, your consumer would only consume from topic B after restore because we filter the topics that are stored in state using the configured topics. Miscellaneous Interface changesThe canEqual() method was dropped from the TypeSerializer interface (FLINK-9803)The canEqual() methods are usually used to make proper equality checks across hierarchies of types. The TypeSerializer actually doesn’t require this property, so the method is now removed. Removal of the CompositeSerializerSnapshot utility class (FLINK-11073)The CompositeSerializerSnapshot utility class has been removed. You should now use CompositeTypeSerializerSnapshot instead, for snapshots of composite serializers that delegate serialization to multiple nested serializers. Please see here for instructions on using CompositeTypeSerializerSnapshot. Memory managementIn Fink 1.8.0 and prior version, the managed memory fraction of taskmanager is controlled by taskmanager.memory.fraction, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter NewRatio is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value. Flink 1.7Scala 2.12 supportWhen using Scala 2.12 you might have to add explicit type annotations in places where they were not required when using Scala 2.11. This is an excerpt from the TransitiveClosureNaive.scala example in the Flink code base that shows the changes that could be required. Previous code: 123456789val terminate = prevPaths .coGroup(nextPaths) .where(0).equalTo(0) &#123; (prev, next, out: Collector[(Long, Long)]) =&gt; &#123; val prevPaths = prev.toSet for (n &lt;- next) if (!prevPaths.contains(n)) out.collect(n) &#125;&#125; With Scala 2.12 you have to change it to: 123456789val terminate = prevPaths .coGroup(nextPaths) .where(0).equalTo(0) &#123; (prev: Iterator[(Long, Long)], next: Iterator[(Long, Long)], out: Collector[(Long, Long)]) =&gt; &#123; val prevPaths = prev.toSet for (n &lt;- next) if (!prevPaths.contains(n)) out.collect(n) &#125;&#125; The reason for this is that Scala 2.12 changes how lambdas are implemented. They now use the lambda support using SAM interfaces introduced in Java 8. This makes some method calls ambiguous because now both Scala-style lambdas and SAMs are candidates for methods were it was previously clear which method would be invoked. State evolutionBefore Flink 1.7, serializer snapshots were implemented as a TypeSerializerConfigSnapshot (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new TypeSerializerSnapshot interface introduced in 1.7). Moreover, the responsibility of serializer schema compatibility checks lived within the TypeSerializer, implemented in the TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot) method. To be future-proof and to have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. Details and migration guides can be found here. Removal of the legacy modeFlink no longer supports the legacy mode. If you depend on this, then please use Flink 1.6.x. Savepoints being used for recoverySavepoints are now used while recovering. Previously when using exactly-once sink one could get into problems with duplicate output data when a failure occurred after a savepoint was taken but before the next checkpoint occurred. This results in the fact that savepoints are no longer exclusively under the control of the user. Savepoint should not be moved nor deleted if there was no newer checkpoint or savepoint taken. MetricQueryService runs in separate thread poolThe metric query service runs now in its own ActorSystem. It needs consequently to open a new port for the query services to communicate with each other. The query service port can be configured in flink-conf.yaml. Granularity of latency metricsThe default granularity for latency metrics has been modified. To restore the previous behavior users have to explicitly set the granularity to subtask. Latency marker activationLatency metrics are now disabled by default, which will affect all jobs that do not explicitly set the latencyTrackingInterval via ExecutionConfig#setLatencyTrackingInterval. To restore the previous default behavior users have to configure the latency interval in flink-conf.yaml. Relocation of Hadoop’s Netty dependencyWe now also relocate Hadoop’s Netty dependency from io.netty to org.apache.flink.hadoop.shaded.io.netty. You can now bundle your own version of Netty into your job but may no longer assume that io.netty is present in the flink-shaded-hadoop2-uber-*.jar file. Local recovery fixedWith the improvements to Flink’s scheduling, it can no longer happen that recoveries require more slots than before if local recovery is enabled. Consequently, we encourage our users to enable local recovery in flink-conf.yaml. Support for multi slot TaskManagersFlink now properly supports TaskManagers with multiple slots. Consequently, TaskManagers can now be started with an arbitrary number of slots and it is no longer recommended to start them with a single slot. StandaloneJobClusterEntrypoint generates JobGraph with fixed JobIDThe StandaloneJobClusterEntrypoint, which is launched by the script standalone-job.sh and used for the job-mode container images, now starts all jobs with a fixed JobID. Thus, in order to run a cluster in HA mode, one needs to set a different cluster id for each job/cluster. Scala shell does not work with Scala 2.12Flink’s Scala shell does not work with Scala 2.12. Therefore, the module flink-scala-shell is not being released for Scala 2.12. See FLINK-10911 for more details. Limitations of failover strategiesFlink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to &quot;full&quot;. In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details. SQL over window preceding clauseThe over window preceding clause is now optional. It defaults to UNBOUNDED if not specified. OperatorSnapshotUtil writes v2 snapshotsSnapshots created with OperatorSnapshotUtil are now written in the savepoint format v2. SBT projects and the MiniClusterResourceIf you have a sbt project which uses the MiniClusterResource, you now have to add the flink-runtime test-jar dependency explicitly via: 1libraryDependencies += &quot;org.apache.flink&quot; %% &quot;flink-runtime&quot; % flinkVersion % Test classifier &quot;tests&quot; The reason for this is that the MiniClusterResource has been moved from flink-test-utils to flink-runtime. The module flink-test-utils has correctly a test-jar dependency on flink-runtime. However, sbt does not properly pull in transitive test-jar dependencies as described in this sbt issue. Consequently, it is necessary to specify the test-jar dependency explicitly. Flink 1.6Changed Configuration Default ValuesThe default value of the slot idle timeout slot.idle.timeout is set to the default value of the heartbeat timeout (50 s). Changed ElasticSearch 5.x Sink APIPrevious APIs in the Flink ElasticSearch 5.x Sink’s RequestIndexer interface have been deprecated in favor of new signatures. When adding requests to the RequestIndexer, the requests now must be of type IndexRequest, DeleteRequest, or UpdateRequest, instead of the base ActionRequest. Limitations of failover strategiesFlink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to &quot;full&quot;. In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details. Flink 1.5These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5. Update Configuration for Reworked Job DeploymentFlink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos. Flink will automatically determine the number of containers from the parallelism of the application. Although the deployment logic was completely reworked, we aimed to not unnecessarily change the previous behavior to enable a smooth transition. Nonetheless, there are a few options that you should update in your conf/flink-conf.yaml or know about. The allocation of TaskManagers with multiple slots is not fully supported yet. Therefore, we recommend to configure TaskManagers with a single slot, i.e., set taskmanager.numberOfTaskSlots: 1 If you observed any problems with the new deployment mode, you can always switch back to the pre-1.5 behavior by configuring mode: legacy. Please report any problems or possible improvements that you notice to the Flink community, either by posting to a mailing list or by opening a JIRA issue. Note: We plan to remove the legacy mode in the next release. Update Configuration for Reworked Network StackThe changes on the networking stack for credit-based flow control and improved latency affect the configuration of network buffers. In a nutshell, the networking stack can require more memory to run applications. Hence, you might need to adjust the network configuration of your Flink setup. There are two ways to address problems of job submissions that fail due to lack of network buffers. Reduce the number of buffers per channel, i.e., taskmanager.network.memory.buffers-per-channel or Increase the amount of TaskManager memory that is used by the network stack, i.e., increase taskmanager.network.memory.fraction and/or taskmanager.network.memory.max. Please consult the section about network buffer configuration in the Flink documentation for details. In case you experience issues with the new credit-based flow control mode, you can disable flow control by setting taskmanager.network.credit-model: false. Note: We plan to remove the old model and this configuration in the next release. Hadoop Classpath DiscoveryWe removed the automatic Hadoop classpath discovery via the Hadoop binary. If you want Flink to pick up the Hadoop classpath you have to export HADOOP_CLASSPATH. On cloud environments and most Hadoop distributions you would do 1export HADOOP_CLASSPATH=`hadoop classpath`. Breaking Changes of the REST APIIn an effort to harmonize, extend, and improve the REST API, a few handlers and return values were changed. The jobs overview handler is now registered under /jobs/overview (before /joboverview) and returns a list of job details instead of the pre-grouped view of running, finished, cancelled and failed jobs. The REST API to cancel a job was changed. The REST API to cancel a job with savepoint was changed. Please check the REST API documentation for details. Kafka Producer Flushes on Checkpoint by DefaultThe Flink Kafka Producer now flushes on checkpoints by default. Prior to version 1.5, the behaviour was disabled by default and users had to explicitly call setFlushOnCheckpoints(true) on the producer to enable it. Updated Kinesis DependencyThe Kinesis dependencies of Flink’s Kinesis connector have been updated to the following versions. 123&lt;aws.sdk.version&gt;1.11.319&lt;/aws.sdk.version&gt;&lt;aws.kinesis-kcl.version&gt;1.9.0&lt;/aws.kinesis-kcl.version&gt;&lt;aws.kinesis-kpl.version&gt;0.12.9&lt;/aws.kinesis-kcl.version&gt; Limitations of failover strategiesFlink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option jobmanager.execution.failover-strategy from your flink-conf.yaml or set it to &quot;full&quot;. In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See FLINK-10880 for more details.]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Antlr sql语法解析]]></title>
    <url>%2Fjava%2Fsharding-sphere%2Fantlr%2F</url>
    <content type="text"><![CDATA[官网：https://www.antlr.org/ github：https://github.com/antlr/grammars-v4 antlr是指可以根据输入自动生成语法树并可视化的显示出来的开源语法分析器。ANTLR—Another Tool for Language Recognition，其前身是PCCTS，它为包括Java，C++，C#在内的语言提供了一个通过语法描述来自动构造自定义语言的识别器（recognizer），编译器（parser）和解释器（translator）的框架。 1.1 词法分析器（Lexer）词法分析器又称为Scanner，Lexical analyser和Tokenizer。程序设计语言通常由关键字和严格定义的语法结构组成。编译的最终目的是将程序设计语言的高层指令翻译成物理机器或虚拟机可以执行的指令。词法分析器的工作是分析量化那些本来毫无意义的字符流，将他们翻译成离散的字符组（也就是一个一个的Token），包括关键字，标识符，符号（symbols）和操作符供语法分析器使用。 1.2 语法分析器（Parser）编译器又称为Syntactical analyser。在分析字符流的时候，Lexer不关心所生成的单个Token的语法意义及其与上下文之间的关系，而这就是Parser的工作。语法分析器将收到的Tokens组织起来，并转换成为目标语言语法定义所允许的序列。 无论是Lexer还是Parser都是一种识别器，Lexer是字符序列识别器而Parser是Token序列识别器。他们在本质上是类似的东西，而只是在分工上有所不同而已。如下图所示： 字符输入流、tokens和AST之间的关系 1.3 树分析器 (tree parser)树分析器可以用于对语法分析生成的抽象语法树进行遍历，并能执行一些相关的操作。 1.4 ANTLRANTLR将上述结合起来，它允许我们定义识别字符流的词法规则和用于解释Token流的语法分析规则。然后，ANTLR将根据用户提供的语法文件自动生成相应的词法/语法分析器。用户可以利用他们将输入的文本进行编译，并转换成其他形式（如AST—Abstract Syntax Tree，抽象的语法树）。]]></content>
      <categories>
        <category>database</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅谈tcp的半打开连接]]></title>
    <url>%2Fcomputer-network%2Ftcp-half-open%2F</url>
    <content type="text"><![CDATA[原文： https://zhuanlan.zhihu.com/p/32081783 tcp连接一端在进行完三次握手以后进入ESTABLISHED状态，如果连接的对端在某一时刻在网络中消失，而本端没有感知到，还是处于ESTABLISHED状态，那么本端的连接就被称为半打开连接(Half Open)。 连接的对端在网络中消失的情况有好多： 例如对端主机突然断电，tcp连接来不及发送任何信息就消失啦。 还有，连接路径上的某个nat设备aging-time过期，并且nat port被重用，虽然tcp连接的两端都还处于ESTABLISHED状态，可实际上两端的连接已经无法正常通信，此时这两端的连接都是半打开连接。(这种情况是我的猜测，还没有得到实践的检验。如果结论错误，会修改掉！) 还有，listen socket的accept调用缓慢导致积压队列满，client端连接会成为半打开连接。这种情况是本次讨论的主题。 首先说下tcp的三次握手 server端的tcp连接在三次握手阶段会经历SYN_RECV状态到ESTABLISHED状态的变迁，其中SYN_RECV状态到连接存放于listen socket积压队列的半连接队列中，当连接由SYN_RECV状态变为ESTABLISHED状态，连接会被从半连接队列中移到已连接队列中。系统调用accept的作用就是从listen socket的已连接队列中取走一个连接，然后将该连接与进程绑定。 但是，如果listen socket的积压队列(半连接队列与连接队列)全部满后，对于新来的client连接会如何处理呢。答案是，linux不同版本的实现不同。 当前的实验环境： 12zuchunlei@ubuntu14:~$ uname -aLinux ubuntu14 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 服务端代码： 1234In [1]: from socket import *In [2]: sock = socket(AF_INET,SOCK_STREAM)In [3]: sock.bind((&quot;&quot;,10000))In [4]: sock.listen(1) 为了简单，我将listen的backlog设置为1，并且不调用sock.accept方法。这样所有的ESTABLISHED状态的连接都存在积压队列中，并且没有和进程绑定起来。 使用netstat查看10000端口的状态： 1234Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p Sat Dec 16 20:23:03 2017Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timertcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 1578/python off (0.00/0/0) 使用ss查看10000端口的状态： 1234Every 1.0s: ss -tnpoa|sed -n -e 1p -e /10000/p Sat Dec 16 20:25:18 2017State Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 0 1 *:10000 *:* users:((&quot;ipython&quot;,1578,6)) 解析一下，ss命令输出的State=Listen状态的数据时，其中Send-Q的大小表示该listen socket积压队列的长度，Recv-Q代表已完成三次握手，ESTABLISHED状态的连接个数。这样的连接存在于listen socket的已连接队列中。 用nc localhost 10000进行2次连接后，使用netstat查看10000端口的状态： 12345678Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p Sat Dec 16 20:32:45 2017Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timertcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 1578/python off (0.00/0/0)tcp 0 0 127.0.0.1:59890 127.0.0.1:10000 ESTABLISHED 6301/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59890 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59892 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:59892 127.0.0.1:10000 ESTABLISHED 6379/nc off (0.00/0/0) netstat显示当前客户端程序nc连接已经建立完成，服务端的2个连接也处于ESTABLISHED状态，但因为当前没有accept调用，所以服务端的两个连接的进程PID显示为-，表示当前连接没有和进程绑定起来。 使用ss查看10000端口的状态： 12345678Every 1.0s: ss -tnpoa|sed -n -e 1p -e /10000/p Sat Dec 16 20:36:10 2017State Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 2 1 *:10000 *:* users:((&quot;ipython&quot;,1578,6))ESTAB 0 0 127.0.0.1:59890 127.0.0.1:10000 users:((&quot;nc&quot;,6301,3))ESTAB 0 0 127.0.0.1:10000 127.0.0.1:59890ESTAB 0 0 127.0.0.1:10000 127.0.0.1:59892ESTAB 0 0 127.0.0.1:59892 127.0.0.1:10000 users:((&quot;nc&quot;,6379,3)) 通过ss可以看到，当前LISTEN状态的RECV-Q值为2，表示有2个ESTABLISHED状态的连接在已连接队列中等待应用层调用accept取走。 用nc localhost 10000进行第三次连接后，netstat查看10000端口的状态： 12345678910Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p Sat Dec 16 20:41:18 2017Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timertcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 1578/python off (0.00/0/0)tcp 0 0 127.0.0.1:59890 127.0.0.1:10000 ESTABLISHED 6301/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59896 SYN_RECV - on (1.06/3/0)tcp 0 0 127.0.0.1:59896 127.0.0.1:10000 ESTABLISHED 10989/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59890 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59892 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:59892 127.0.0.1:10000 ESTABLISHED 6379/nc off (0.00/0/0) 可以看到对于第三个客户端nc，连接状态为ESTABLISHED，表示3次握手已经正确完成。而对于服务端，当前的连接状态为SYN_RECV，表示半连接状态，因为当前积压队列已经满，没有空间再存放ESTABLISHED连接，所以该连接无法从SYN_RECV状态变为ESTABLISHED状态，虽然能正确接收到nc端的第三个ACK段。 此时使用tcpdump进行抓包： 12345678910111213zuchunlei@ubuntu14:~$ sudo tcpdump -i any tcp port 10000 -nntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes20:50:15.739292 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1340001 ecr 1339751,nop,wscale 7], length 020:50:15.739301 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1340001 ecr 1339751], length 020:50:17.738724 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1340501 ecr 1340001,nop,wscale 7], length 020:50:17.738772 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1340501 ecr 1339751], length 020:50:21.739110 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1341501 ecr 1340501,nop,wscale 7], length 020:50:21.739158 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1341501 ecr 1339751], length 020:50:29.738975 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1343501 ecr 1341501,nop,wscale 7], length 020:50:29.739022 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1343501 ecr 1339751], length 020:50:45.739231 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1347501 ecr 1343501,nop,wscale 7], length 020:50:45.739310 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1347501 ecr 1339751], length 0 对于SYN_RECV状态的连接，linux会启动定时器进行重传三次握手的第二段[S.]，在4次重传后，如果当前listen socket已连接队列中依然没有空间，则将SYN_RECV状态的连接丢弃。 等待4次重传后，使用netstat查看10000端口状态： 123456789Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p Sat Dec 16 20:58:20 2017Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timertcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 1578/python off (0.00/0/0)tcp 0 0 127.0.0.1:59890 127.0.0.1:10000 ESTABLISHED 6301/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59890 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:59896 127.0.0.1:10000 ESTABLISHED 15954/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:59892 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:59892 127.0.0.1:10000 ESTABLISHED 6379/nc off (0.00/0/0) server端将SYN_RECV状态的连接丢弃后，此时第三个nc客户端连接就已经成为了半打开连接。 对半打开连接进行send/recv操作时的影响： 如果此时，第三个nc客户端发送数据，则因为连接对对端不存在，对端会回复RST段，本端收到RST段后也会将连接重置。 如果第三个nc客户端只接收数据的话，则这个客户端永远阻塞在recv调用中无法返回。为了有效解决这种问题，客户端可以启动tcp的keepalive，因为默认tcp发送keepalive probe的间隔时间较长，应用可以通过设置socket option(TCP_KEEPDILE/TCP_KEEPINTVL/TCP_KEEPCNT)将发送keepalive probe的时间设短些。 今早我测试了一下最新版ubuntu16.04的实现，发现如果listen socket的积压队列满后，新来客户端的连接不再成为ESTABLISHED状态，而是在SYN_SENT状态进行进行SYN段的超时重传，而服务端不返回任何tcp段。 新版的测试环境： 12zuchunlei@box:~$ uname -aLinux box 4.10.0-28-generic #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 与之前的测试场景一样，当前只关注第三个nc客户端连接的状态。 使用netstat查看10000端口的状态： 123456789Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p Sat Dec 16 21:21:57 2017Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timertcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 2022/python off (0.00/0/0)tcp 0 0 127.0.0.1:36516 127.0.0.1:10000 ESTABLISHED 2347/nc off (0.00/0/0)tcp 0 1 127.0.0.1:36520 127.0.0.1:10000 SYN_SENT 2522/nc on (5.18/3/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:36518 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:36518 127.0.0.1:10000 ESTABLISHED 2388/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:36516 ESTABLISHED - off (0.00/0/0) 此时，第三个nc客户端连接状态为SYN_SENT，进行超时重传SYN段。 使用tcpdump抓去第三个nc客户端的tcp包： 12345678910zuchunlei@box:~$ sudo tcpdump -i any tcp port 10000 -nntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes21:21:47.357226 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214107076 ecr 0,nop,wscale 7], length 021:21:48.358267 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214107327 ecr 0,nop,wscale 7], length 021:21:50.373837 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214107831 ecr 0,nop,wscale 7], length 021:21:54.565832 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214108879 ecr 0,nop,wscale 7], length 021:22:02.758111 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214110927 ecr 0,nop,wscale 7], length 021:22:18.885934 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214114959 ecr 0,nop,wscale 7], length 021:22:51.141643 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214123023 ecr 0,nop,wscale 7], length 0 可以看到客户端在进行超时重传SYN段的过程中，服务端没有发送一个包。 在客户端SYN_SENT超时后，使用netstat查看10000端口状态： 12345678Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p Sat Dec 16 21:27:36 2017Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timertcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 2022/python off (0.00/0/0)tcp 0 0 127.0.0.1:36516 127.0.0.1:10000 ESTABLISHED 2347/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:36518 ESTABLISHED - off (0.00/0/0)tcp 0 0 127.0.0.1:36518 127.0.0.1:10000 ESTABLISHED 2388/nc off (0.00/0/0)tcp 0 0 127.0.0.1:10000 127.0.0.1:36516 ESTABLISHED - off (0.00/0/0) 客户端连接消失。 在当前新版当linux实现中，由于listen socket积压队列满时，新的客户端连接并不会成为半打开连接，而是在connect调用时进行重传SYN段，如果达到了SYN_SENT状态的阈值后，tcp连接消失，应用层connect调用返回timeout异常！]]></content>
      <categories>
        <category>computer-network</category>
        <category>tcp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[乘积最大子序列]]></title>
    <url>%2Fleetcode%2Fmaximum-product-subarray%2F</url>
    <content type="text"><![CDATA[算法：动态规划 题目https://leetcode-cn.com/problems/maximum-product-subarray/ 给定一个整数数组 nums ，找出一个序列中乘积最大的连续子序列（该序列至少包含一个数）。 示例 1: 输入: [2,3,-2,4]输出: 6解释: 子数组 [2,3] 有最大乘积 6。示例 2: 输入: [-2,0,-1]输出: 0解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 Java解法暴力破解申请空间储存所有的中间结果，再拿中间结果结算下一个位置的所有结果 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public int maxProduct(int[] nums) &#123; if(null==nums || nums.length==0) return 0; ArrayList&lt;TreeSet&lt;Integer&gt;&gt; max = new ArrayList&lt;TreeSet&lt;Integer&gt;&gt;(); max.add(new TreeSet&lt;Integer&gt;()); Integer preMax = Integer.MIN_VALUE; for(int i=0; i&lt;nums.length; ++i) &#123; TreeSet&lt;Integer&gt; curList = new TreeSet&lt;Integer&gt;(); TreeSet&lt;Integer&gt; preList = max.get(i); if (preList==null || preList.size()&lt;=0) &#123; int cur = nums[i]; curList.add(cur); if (preMax &lt; cur) preMax = cur; &#125; else &#123; for (Integer x : preList) &#123; int cur = x * nums[i]; curList.add(cur); int curMax = cur&gt;nums[i]?cur:nums[i]; if (cur != curMax) curList.add(curMax); if (preMax &lt; curMax) preMax = curMax; &#125; &#125; curList.add(nums[i]); max.add(curList); &#125; return preMax; &#125;&#125;public class App &#123; public static void main(String[] args) throws IOException &#123; int x = new Solution().maxProduct(new int[]&#123;2,3,-2,4&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;-2,0,-1&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;-2,3,-4&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;0,2&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;2,-5,-2,-4,3&#125;); System.out.println(x); &#125;&#125; 动态规划遍历数组时计算当前最大值，不断更新令imax为当前最大值，则当前最大值为 imax = max(imax * nums[i], nums[i])由于存在负数，那么会导致最大的变最小的，最小的变最大的。因此还需要维护当前最小值imin，imin = min(imin * nums[i], nums[i])当负数出现时则imax与imin进行交换再进行下一步计算时间复杂度：O(n)O(n) 123456789101112131415161718192021222324252627282930class Solution &#123; public int maxProduct(int[] nums) &#123; int max = Integer.MIN_VALUE, imax = 1, imin = 1; for(int i=0; i&lt;nums.length; i++)&#123; if(nums[i] &lt; 0)&#123; int tmp = imax; imax = imin; imin = tmp; &#125; imax = Math.max(imax*nums[i], nums[i]); imin = Math.min(imin*nums[i], nums[i]); max = Math.max(max, imax); &#125; return max; &#125;&#125;public class App &#123; public static void main(String[] args) throws IOException &#123; int x = new Solution().maxProduct(new int[]&#123;2,3,-2,4&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;-2,0,-1&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;-2,3,-4&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;0,2&#125;); System.out.println(x); x = new Solution().maxProduct(new int[]&#123;2,-5,-2,-4,3&#125;); System.out.println(x); &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不同的子序列统计]]></title>
    <url>%2Fleetcode%2Fdistinct-subsequences%2F</url>
    <content type="text"><![CDATA[算法：动态规划 题目url：https://leetcode-cn.com/problems/distinct-subsequences/ 给定一个字符串 S 和一个字符串 T，计算在 S 的子序列中 T 出现的个数。 一个字符串的一个子序列是指，通过删除一些（也可以不删除）字符且不干扰剩余字符相对位置所组成的新字符串。（例如，”ACE” 是 “ABCDE” 的一个子序列，而 “AEC” 不是） 示例 1： 12345678910111213输入: S = "rabbbit", T = "rabbit"输出: 3解释:如下图所示, 有 3 种可以从 S 中得到 "rabbit" 的方案。(上箭头符号 ^ 表示选取的字母)rabbbit^^^^ ^^rabbbit^^ ^^^^rabbbit^^^ ^^^ 示例 2： 1234567891011121314151617输入: S = "babgbag", T = "bag"输出: 5解释:如下图所示, 有 5 种可以从 S 中得到 "bag" 的方案。 (上箭头符号 ^ 表示选取的字母)babgbag^^ ^babgbag^^ ^babgbag^ ^^babgbag ^ ^^babgbag ^^^ 分析动态规划的本质是穷举法 拿第二个示例分析，S = “babgbag”, T = “bag” b 在S中出现的可能性，存；b后a出现的可能性，存储；在ba的基础上g出现的可能性储存； 二维数组的解法，转化为1位数组的解法 Java解法s的长度为n，t的长度为m，空间复杂度为O(m)的解法如下： 12345678910111213141516171819202122232425262728class Solution &#123; public int numDistinct(String s, String t) &#123; int sl = s.length(); int tl = t.length(); if(sl==0||tl==0)return 0; if(sl&lt;tl)return 0; if(sl==tl)return s.equals(t)?1:0; int[] res = new int[tl+1]; for(int i=0;i&lt;sl; i++)&#123; int pre=0; for(int j=0;j&lt;tl; j++)&#123; if(j==0)pre=1; int temp = res[j+1]; if(s.charAt(i)==t.charAt(j))&#123; res[j+1]=res[j+1]+pre; &#125; pre = temp; &#125; &#125; return res[tl]; &#125;&#125;public class App &#123; public static void main(String[] args) throws IOException &#123; int num = new Solution().numDistinct("babgbag", "bag"); System.out.println(num); &#125;&#125; pre和temp的作用是 记录本轮更改之前的值，用于计算，防止”rabbbit”, “rabbit”这样的数据，一个s中的b，影响多个t中的b的计算]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2Falgorithm%2Fquick-sort%2F</url>
    <content type="text"><![CDATA[快速排序https://www.runoob.com/w3cnote/quick-sort-2.html 快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。 快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。 快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。 快速排序的名字起的是简单粗暴，因为一听到这个名字你就知道它存在的意义，就是快，而且效率高！它是处理大数据最快的排序算法之一了。虽然 Worst Case 的时间复杂度达到了 O(n²)，但是人家就是优秀，在大多数情况下都比平均时间复杂度为 O(n logn) 的排序算法表现要更好，可是这是为什么呢，我也不知道。好在我的强迫症又犯了，查了 N 多资料终于在《算法艺术与信息学竞赛》上找到了满意的答案： 快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。 1. 算法步骤 从数列中挑出一个元素，称为 “基准”（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序； 2、图片演示方法其实很简单：分别从初始序列“6 1 2 7 9 3 4 5 10 8”两端开始“探测”。先从右往左找一个小于6的数，再从左往右找一个大于6的数，然后交换他们。这里可以用两个变量i和j，分别指向序列最左边和最右边。我们为这两个变量起个好听的名字“哨兵i”和“哨兵j”。刚开始的时候让哨兵i指向序列的最左边（即i=1），指向数字6。让哨兵j指向序列的最右边（即=10），指向数字。 首先哨兵j开始出动。因为此处设置的基准数是最左边的数，所以需要让哨兵j先出动，这一点非常重要（请自己想一想为什么）。哨兵j一步一步地向左挪动（即j–），直到找到一个小于6的数停下来。接下来哨兵i再一步一步向右挪动（即i++），直到找到一个数大于6的数停下来。最后哨兵j停在了数字5面前，哨兵i停在了数字7面前。 现在交换哨兵i和哨兵j所指向的元素的值。交换之后的序列如下： 6 1 2 5 9 3 4 7 10 8 到此，第一次交换结束。接下来开始哨兵j继续向左挪动（再友情提醒，每次必须是哨兵j先出发）。他发现了4（比基准数6要小，满足要求）之后停了下来。哨兵i也继续向右挪动的，他发现了9（比基准数6要大，满足要求）之后停了下来。此时再次进行交换，交换之后的序列如下： 6 1 2 5 4 3 9 7 10 8 第二次交换结束，“探测”继续。哨兵j继续向左挪动，他发现了3（比基准数6要小，满足要求）之后又停了下来。哨兵i继续向右移动，糟啦！此时哨兵i和哨兵j相遇了，哨兵i和哨兵j都走到3面前。说明此时“探测”结束。我们将基准数6和3进行交换。交换之后的序列如下： 3 1 2 5 4 6 9 7 10 8 到此第一轮“探测”真正结束。此时以基准数6为分界点，6左边的数都小于等于6，6右边的数都大于等于6。回顾一下刚才的过程，其实哨兵j的使命就是要找小于基准数的数，而哨兵i的使命就是要找大于基准数的数，直到i和j碰头为止。 下面上个霸气的图来描述下整个算法的处理过程： 3、Java实现使用LinkedList实现 12345678910111213141516171819202122232425class QuickSort &#123; public void quickSort(LinkedList&lt;Integer&gt; list, int left, int right) &#123; if(left&lt;right) &#123; int partitionIndex = partition(list, left, right); quickSort(list, left, partitionIndex-1); quickSort(list, partitionIndex+1, right); &#125; &#125; public int partition(LinkedList&lt;Integer&gt; list, int left, int right) &#123; int start = left; int tmp = list.get(left); while(left &lt; right) &#123; while (left &lt; right &amp;&amp; tmp &lt;= list.get(right)) right--; while (left &lt; right &amp;&amp; tmp &gt;= list.get(left)) left++; if (left &lt; right) swap(list, left, right); &#125; swap(list, start, left); return right; &#125; public void swap(LinkedList&lt;Integer&gt; list, int i, int j) &#123; int tmp = list.get(i); list.set(i, list.get(j)); list.set(j, tmp); &#125;&#125; 使用链表实现 1234567891011121314151617181920212223242526272829303132class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; public ListNode sortList(ListNode head) &#123; quickSort(head, null); return head; &#125; public void quickSort(ListNode head, ListNode tail) &#123; if (head == tail || head.next == tail) return; int pivot = head.val; ListNode left = head, cur = head.next; while (cur != tail) &#123; if (cur.val &lt; pivot) &#123; left = left.next; swap(left, cur); &#125; cur = cur.next; &#125; swap(head, left); quickSort(head, left); quickSort(left.next, tail); &#125; public void swap(ListNode a, ListNode b) &#123; int tmp = a.val; a.val = b.val; b.val = tmp; &#125;&#125;]]></content>
      <tags>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序链表]]></title>
    <url>%2Fleetcode%2Fsort-list%2F</url>
    <content type="text"><![CDATA[算法：排序 题目在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。 示例 1: 输入: 4-&gt;2-&gt;1-&gt;3输出: 1-&gt;2-&gt;3-&gt;4示例 2: 输入: -1-&gt;5-&gt;3-&gt;4-&gt;0输出: -1-&gt;0-&gt;3-&gt;4-&gt;5 分析排序算法 Java实现使用快速排序实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; public ListNode sortList(ListNode head) &#123; quickSort(head, null); return head; &#125; public void quickSort(ListNode head, ListNode tail) &#123; if (head == tail || head.next == tail) return; int pivot = head.val; ListNode left = head, cur = head.next; while (cur != tail) &#123; if (cur.val &lt; pivot) &#123; left = left.next; swap(left, cur); &#125; cur = cur.next; &#125; swap(head, left); quickSort(head, left); quickSort(left.next, tail); &#125; public void swap(ListNode a, ListNode b) &#123; int tmp = a.val; a.val = b.val; b.val = tmp; &#125;&#125;public class App &#123; public static int[] stringToIntegerArray(String input) &#123; input = input.trim(); input = input.substring(1, input.length() - 1); if (input.length() == 0) &#123; return new int[0]; &#125; String[] parts = input.split(","); int[] output = new int[parts.length]; for(int index = 0; index &lt; parts.length; index++) &#123; String part = parts[index].trim(); output[index] = Integer.parseInt(part); &#125; return output; &#125; public static ListNode stringToListNode(String input) &#123; // Generate array from the input int[] nodeValues = stringToIntegerArray(input); // Now convert that list into linked list ListNode dummyRoot = new ListNode(0); ListNode ptr = dummyRoot; for(int item : nodeValues) &#123; ptr.next = new ListNode(item); ptr = ptr.next; &#125; return dummyRoot.next; &#125; public static String listNodeToString(ListNode node) &#123; if (node == null) &#123; return "[]"; &#125; String result = ""; while (node != null) &#123; result += Integer.toString(node.val) + ", "; node = node.next; &#125; return "[" + result.substring(0, result.length() - 2) + "]"; &#125; public static void main(String[] args) throws IOException &#123; ListNode head = stringToListNode("[8581,6131,9495,2797,105,3247,16943]"); ListNode ret = new Solution().sortList(head); String out = listNodeToString(ret); System.out.println(out); &#125;&#125;]]></content>
      <tags>
        <tag>快速排序</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LFU缓存机制]]></title>
    <url>%2Fleetcode%2Flfu-cache%2F</url>
    <content type="text"><![CDATA[算法：LFU缓存机制-淘汰最近访问频率最小的元素 题目—— LFU缓存机制url：https://leetcode-cn.com/problems/lfu-cache/ 设计并实现最不经常使用（LFU）缓存的数据结构。它应该支持以下操作：get 和 put。 get(key) - 如果键存在于缓存中，则获取键的值（总是正数），否则返回 -1。put(key, value) - 如果键不存在，请设置或插入值。当缓存达到其容量时，它应该在插入新项目之前，使最不经常使用的项目无效。在此问题中，当存在平局（即两个或更多个键具有相同使用频率）时，最近最少使用的键将被去除。 进阶：你是否可以在 O(1) 时间复杂度内执行两项操作？ 示例: 123456789101112LFUCache cache = new LFUCache( 2 /* capacity (缓存容量) */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // 返回 1cache.put(3, 3); // 去除 key 2cache.get(2); // 返回 -1 (未找到key 2)cache.get(3); // 返回 3cache.put(4, 4); // 去除 key 1cache.get(1); // 返回 -1 (未找到 key 1)cache.get(3); // 返回 3cache.get(4); // 返回 4 分析LFU(Least Frequently Used)：淘汰最近访问频率最小的元素。 缺点：1. 最新加入的数据常常会被踢除，因为其起始方法次数少。 2. 如果频率时间度量是1小时，则平均一天每个小时内的访问频率1000的热点数据可能会被2个小时的一段时间内的访问频率是1001的数据剔除掉； 实现方式： 一个k-v字典存储 数据，同时存储每个item的上次访问时间time 和 累计访问次数 cnt，当容量满时，淘汰最cnt最小的，cnt相同的情况下，淘汰time最早的； Java解法使用HashMap存储数据，使用优先级队列PriorityQueue存储累计访问次数 和 最近访问时间 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import java.util.Comparator;import java.util.HashMap;import java.util.PriorityQueue;class LfuObj &#123; Integer key; Integer val; Integer cnt; Long time; public LfuObj(int key, int val, int cnt, long time) &#123; this.key = key; this.val = val; this.cnt = cnt; this.time = time; &#125; @Override public int hashCode() &#123; return this.key.hashCode(); &#125; @Override public boolean equals(Object obj) &#123; if(null == obj) return false; if(!(obj instanceof LfuObj)) return false; return key.equals(((LfuObj) obj).key); &#125;&#125;class LFUCache &#123; HashMap&lt;Integer, LfuObj&gt; hashMap; PriorityQueue&lt;LfuObj&gt; heap = new PriorityQueue&lt;&gt;(new Comparator&lt;LfuObj&gt;() &#123; @Override public int compare(LfuObj a, LfuObj b) &#123; int c = a.cnt - b.cnt; return (0==c ? (int)((a.time-b.time)%65536):c); &#125; &#125;); Integer capacity; public LFUCache(int capacity) &#123; this.hashMap = new HashMap&lt;&gt;(capacity); this.capacity = capacity; &#125; public int get(int key) &#123; LfuObj lfuObj = hashMap.getOrDefault(key, null); if(null == lfuObj) &#123; return -1; &#125; heap.remove(lfuObj); lfuObj.cnt++; lfuObj.time = System.nanoTime(); heap.offer(lfuObj); hashMap.put(key, lfuObj); return lfuObj.val; &#125; public void put(int key, int value) &#123; if(capacity &lt;= 0) return; LfuObj lfuObj = hashMap.get(key); boolean isUpdate = true; if(null == lfuObj) &#123; isUpdate = false; lfuObj = new LfuObj(key, value, 1, System.nanoTime()); &#125; else &#123; lfuObj.val = value; lfuObj.cnt++; lfuObj.time = System.nanoTime(); &#125; if(hashMap.size() &lt; capacity || isUpdate) &#123; hashMap.put(key, lfuObj); heap.remove(lfuObj); heap.offer(lfuObj); &#125; else &#123; LfuObj tmp = heap.poll(); hashMap.remove(tmp.key); heap.offer(lfuObj); hashMap.put(key, lfuObj); &#125; &#125;&#125;public class App &#123; public static void main(String[] agrs) &#123; LFUCache cache = new LFUCache(3); cache.put(1, 1); cache.put(2, 2); cache.put(3, 3); cache.put(4, 4); System.out.println(cache.get(4)); System.out.println(cache.get(3)); System.out.println(cache.get(2)); System.out.println(cache.get(1)); cache.put(5, 5); System.out.println(cache.get(1)); System.out.println(cache.get(2)); System.out.println(cache.get(3)); System.out.println(cache.get(4)); System.out.println(cache.get(5)); &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>LFU</tag>
        <tag>Cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LRU缓存机制]]></title>
    <url>%2Fleetcode%2Flru-cache%2F</url>
    <content type="text"><![CDATA[算法：LRU缓存机制-最近最少使用 题目—— LRU缓存机制url：https://leetcode-cn.com/problems/lru-cache/ 运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。 进阶: 你是否可以在 O(1) 时间复杂度内完成这两种操作？ 示例: 1234567891011LRUCache cache = new LRUCache( 2 /* 缓存容量 */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // 返回 1cache.put(3, 3); // 该操作会使得密钥 2 作废cache.get(2); // 返回 -1 (未找到)cache.put(4, 4); // 该操作会使得密钥 1 作废cache.get(1); // 返回 -1 (未找到)cache.get(3); // 返回 3cache.get(4); // 返回 4 分析LRU是Least Recently Used的缩写，即最近最少使用，是一种常用的页面置换算法，选择最近最久未使用的页面予以淘汰。 实现方式： 一个k-v字典存储 数据，同时存储每个item的上次访问时间lru_time，当容量满时，淘汰最lru_time最早的； Java解法使用HashMap存储数据，使用优先级队列PriorityQueue存储最近访问时间 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class LruObj &#123; Integer key; Long time; public LruObj(Integer key, Long time) &#123; this.key = key; this.time=time; &#125; @Override public int hashCode() &#123; int result = key.hashCode(); return result; &#125; @Override public boolean equals(Object o) &#123; if(this==o) return true; if(null==o || getClass() != o.getClass()) return false; LruObj lruObj = (LruObj) o; return key.equals(lruObj.key); &#125;&#125;class LRUCache &#123; private int capacity = 0; PriorityQueue&lt;LruObj&gt; queue = new PriorityQueue&lt;LruObj&gt;((a,b)-&gt;(a.time-b.time&gt;0) ? 1 : -1); HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); public LRUCache(int capacity) &#123; this.capacity = capacity; &#125; public int get(int key) &#123; if(map.containsKey(key)) &#123; LruObj obj = new LruObj(key, System.nanoTime()); queue.remove(obj); queue.offer(obj); return map.get(key); &#125; return -1; &#125; public void put(int key, int value) &#123; LruObj obj = new LruObj(key, System.nanoTime()); if(map.containsKey(key)) &#123; queue.remove(obj); queue.offer(obj); map.put(key, value); &#125; else &#123; if(queue.size() &gt;= this.capacity &amp;&amp; queue.size() &gt; 0) &#123; LruObj tmp = queue.poll(); map.remove(tmp.key); &#125; queue.offer(obj); map.put(key, value); &#125; &#125;&#125;public class App &#123; public static void main(String[] agrs) &#123; LRUCache cache = new LRUCache( 2 /* 缓存容量 */ ); cache.put(1, 1); cache.put(2, 2); System.out.println(cache.get(1)); // 返回 1 cache.put(3, 3); // 该操作会使得密钥 2 作废 System.out.println(cache.get(2)); // 返回 -1 (未找到) cache.put(4, 4); // 该操作会使得密钥 1 作废 System.out.println(cache.get(1)); // 返回 -1 (未找到) System.out.println(cache.get(3)); // 返回 3 System.out.println(cache.get(4)); // 返回 4 &#125;&#125; 官方答案： 12345678910111213141516171819202122232425262728293031class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt;&#123; private int capacity; public LRUCache(int capacity) &#123; super(capacity, 0.75F, true); this.capacity = capacity; &#125; public int get(int key) &#123; return super.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; super.put(key, value); &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; capacity; &#125;&#125;public class App &#123; public static void main(String[] agrs) &#123; LRUCache cache = new LRUCache( 2 /* 缓存容量 */ ); cache.put(1, 1); cache.put(2, 2); System.out.println(cache.get(1)); // 返回 1 cache.put(3, 3); // 该操作会使得密钥 2 作废 System.out.println(cache.get(2)); // 返回 -1 (未找到) cache.put(4, 4); // 该操作会使得密钥 1 作废 System.out.println(cache.get(1)); // 返回 -1 (未找到) System.out.println(cache.get(3)); // 返回 3 System.out.println(cache.get(4)); // 返回 4 &#125;&#125; LinkedHashMap内部实现方式 是 LinkedList + HashMap]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Cache</tag>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[课程表 III]]></title>
    <url>%2Fleetcode%2Fcourse-schedule-iii%2F</url>
    <content type="text"><![CDATA[算法：贪心算法&nbsp; 题目url：https://leetcode-cn.com/problems/course-schedule-iii 这里有 n 门不同的在线课程，他们按从 1 到 n 编号。每一门课程有一定的持续上课时间（课程时间）t 以及关闭时间第 d 天。一门课要持续学习 t 天直到第 d 天时要完成，你将会从第 1 天开始。 给出 n 个在线课程用 (t, d) 对表示。你的任务是找出最多可以修几门课。 示例： 12345678输入: [[100, 200], [200, 1300], [1000, 1250], [2000, 3200]]输出: 3解释: 这里一共有 4 门课程, 但是你最多可以修 3 门:首先, 修第一门课时, 它要耗费 100 天，你会在第 100 天完成, 在第 101 天准备下门课。第二, 修第三门课时, 它会耗费 1000 天，所以你将在第 1100 天的时候完成它, 以及在第 1101 天开始准备下门课程。第三, 修第二门课时, 它会耗时 200 天，所以你将会在第 1300 天时完成它。第四门课现在不能修，因为你将会在第 3300 天完成它，这已经超出了关闭日期。 提示: 整数 1 &lt;= d, t, n &lt;= 10,000 。你不能同时修两门课程。 &nbsp; 分析 按照结束时间对课程进行排序; 使用一个大顶堆来储存已经选择的课程的长度; 一旦发现安排了当前课程之后，其结束时间超过了最晚结束时间，那么就从已经安排的课程中，取消掉一门最耗时的 &nbsp; Java解法123456789101112131415161718192021222324252627282930class Solution &#123; public int scheduleCourse(int[][] courses) &#123; Arrays.sort(courses, (a, b) -&gt; a[1] - b[1]); PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;((a, b) -&gt; b - a); int time = 0; for (int[] c: courses) &#123; if(c[0] &gt; c[1]) continue; if (time + c[0] &lt;= c[1]) &#123; queue.offer(c[0]); time += c[0]; &#125; else &#123; if(!queue.isEmpty() &amp;&amp; queue.peek() &gt; c[0])&#123; time += c[0] - queue.poll(); queue.offer(c[0]); &#125; &#125; &#125; return queue.size(); &#125;&#125;public class App &#123; public static void main(String[] args) &#123; //int[][] courses = new int[][] &#123;&#123;100, 200&#125;, &#123;200, 1300&#125;, &#123;1000, 1250&#125;, &#123;2000, 3200&#125;&#125;; int[][] courses = new int[][] &#123;&#123;914,9927&#125;,&#123;333,712&#125;,&#123;163,5455&#125;,&#123;835,5040&#125;,&#123;905,8433&#125;,&#123;417,8249&#125;,&#123;921,9553&#125;,&#123;913,7394&#125;,&#123;303,7525&#125;,&#123;582,8658&#125;,&#123;86,957&#125;,&#123;40,9152&#125;,&#123;600,6941&#125;,&#123;466,5775&#125;,&#123;718,8485&#125;,&#123;34,3903&#125;,&#123;380,9996&#125;,&#123;316,7755&#125;&#125;; int ret = new Solution().scheduleCourse(courses); String out = String.valueOf(ret); System.out.print(out); &#125;&#125; &nbsp;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>贪心算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker和 docker-compose安装配置]]></title>
    <url>%2Fdevops%2Fdocker%2Fdocker-compose-setup%2F</url>
    <content type="text"><![CDATA[docker环境安装 安装yum-utils： 1yum install -y yum-utils device-mapper-persistent-data lvm2 为yum源添加docker仓库位置： 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 安装docker： 1yum install docker-ce 启动docker： 1systemctl start docker docker 源更新12345678910vi /etc/docker/daemon.json&#123; "registry-mirrors" : [ "http://hub-mirror.c.163.com" ], "debug" : true, "experimental" : true&#125;systemctl restart docker docker engine升级一、前言 docker的版本分为社区版docker-ce和企业版dokcer-ee，社区版是免费提供给个人开发者和小型团体使用的，企业版会提供额外的收费服务，比如经过官方测试认证过的基础设施、容器、插件,当然docker的版本更新比较快，截止2018.12最新版本是18.09，如果你的机器上安装了老版本的docker，那么就需要卸载，本文介绍如何完全的卸载老版本以及安装新版本docker。 二、删除老版本 停止docker服务 1systemctl stop docker 查看当前版本 1rpm -qa | grep docker 卸载软件包 1234567891011yum erase docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine \ docker-ce 删除相关配置文件 12345find /etc/systemd -name '*docker*' -exec rm -f &#123;&#125; \;find /etc/systemd -name '*docker*' -exec rm -f &#123;&#125; \;find /lib/systemd -name '*docker*' -exec rm -f &#123;&#125; \;rm -rf /var/lib/docker #删除以前已有的镜像和容器,非必要rm -rf /var/run/docker 三、安装新版本软件包安装 1yum install -y yum-utils device-mapper-persistent-data lvm2 添加yum源 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 查看可安装的版本 1yum list docker-ce --showduplicates | sort -r 安装到指定版本 1yum install docker-ce-18.06.3.ce-3.el7 -y 启动并开机自启 12systemctl start dockersystemctl enable docker 查看docker版本 1docker version 验证docker 1docker run hello-world 参考https://docs.docker.com/install/linux/docker-ce/centos/ docker compose Compose file format Docker Engine 1 1.9.0+ 2.0 1.10.0+ 2.1 1.12.0+ 2.2, 3.0, 3.1, 3.2 1.13.0+ 2.3, 3.3, 3.4, 3.5 17.06.0+ 2.4 17.12.0+ 3.6 18.02.0+ 3.7 18.06.0+ Linux 上我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases。 运行以下命令以下载 Docker Compose 的当前稳定版本： 1$ curl -L https://get.daocloud.io/docker/compose/releases/download/1.24.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 要安装其他版本的 Compose，请替换 1.24.1。 将可执行权限应用于二进制文件： 1$ sudo chmod +x /usr/local/bin/docker-compose 创建软链： 1$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 测试是否安装成功： 12$ docker-compose --versioncker-compose version 1.24.1, build 4667896b 注意： 对于 alpine，需要以下依赖包： py-pip，python-dev，libffi-dev，openssl-dev，gcc，libc-dev，和 make。]]></content>
      <categories>
        <category>devops</category>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用jenkins自动化部署mall-swarm]]></title>
    <url>%2Fmicro-service%2Fspring-cloud%2Fmall-swarm-deploy-by-jenkins%2F</url>
    <content type="text"><![CDATA[在微服务架构中，随着服务越来越多，服务的打包部署就会成为一个相当麻烦的事情。比如说我的mall-swarm项目目前就有8个服务需要部署，有没有什么办法让我们部署一次之后，只要点击执行就可以自动部署呢？当然有！下面我们使用Jenkins来完成一个微服务架构中的自动化部署工作。 1、基础环境部署 mall-swarm运行需要的系统组件如下，Docker容器中安装这些组件的方法直接参考该文章即可：mall在Linux环境下的部署（基于Docker容器） 。 组件 版本号 JDK 1.8 Mysql 5.7 Redis 3.2 Elasticsearch 6.4.0 MongoDb 3.2 RabbitMq 3.7.15 Nginx 1.10 1.1、运行配置要求CenterOS7.6版本，推荐4G以上内存 1.2、部署相关文件 数据库脚本mall.sql：https://gitee.com/macrozheng/mall-swarm/blob/master/document/sql/mall.sql nginx配置文件nginx.conf：https://gitee.com/macrozheng/mall-swarm/blob/master/document/docker/nginx.conf docker-compose-env.yml：https://gitee.com/macrozheng/mall-swarm/tree/master/document/docker/docker-compose-env.yml docker-compose-app.yml：https://gitee.com/macrozheng/mall-swarm/tree/master/document/docker/docker-compose-app.yml 1.3、部署前准备1）下载所有需要安装的Docker镜像设置docker源(https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors) 12345678sudo mkdir -p /etc/dockervi /etc/docker/daemon.json&#123; "registry-mirrors": ["https://k8spv7nq.mirror.aliyuncs.com"]&#125;sudo systemctl daemon-reloadsudo systemctl restart docker 1234567docker pull mysql:5.7docker pull redis:3.2docker pull nginx:1.10docker pull rabbitmq:3.7.15-managementdocker pull elasticsearch:6.4.0docker pull kibana:6.4.0docker pull mongo:3.2 2）elasticsearch 需要设置系统内核参数，否则会因为内存不足无法启动。 1234# 改变设置sysctl -w vm.max_map_count=262144# 使之立即生效sysctl -p 需要创建/carloz/data/elasticsearch/data目录并设置权限，否则会因为无权限访问而启动失败。 1234# 创建目录mkdir -p /carloz/data/elasticsearch/data/# 创建并改变该目录权限chmod 777 /carloz/data/elasticsearch/data 3）nginx需要拷贝nginx配置文件，否则挂载时会因为没有配置文件而启动失败。 123456789# 创建目录之后将nginx.conf文件上传到该目录下面mkdir -p /carloz/data/nginx/docker run -p 80:80 --name nginx \-v /carloz/data/nginx/html:/usr/share/nginx/html \-v /carloz/data/nginx/logs:/var/log/nginx \-d nginx:1.10docker container cp nginx:/etc/nginx/nginx.conf /carloz/data/nginx/docker rm -f nginx 1.4、执行docker-compose-env.yml脚本 将该文件上传的linux服务器上，执行docker-compose up命令即可启动mall所依赖的所有服务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071version: '3'services: mysql: image: mysql:5.7 container_name: mysql command: mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci restart: always environment: MYSQL_ROOT_PASSWORD: root #设置root帐号密码 ports: - 3306:3306 volumes: - /carloz/data/mysql/data/db:/var/lib/mysql #数据文件挂载 - /carloz/data/mysql/data/conf:/etc/mysql/conf.d #配置文件挂载 - /carloz/data/mysql/log:/var/log/mysql #日志文件挂载 redis: image: redis:3.2 container_name: redis command: redis-server --appendonly yes volumes: - /carloz/data/redis/data:/data #数据文件挂载 ports: - 6379:6379 nginx: image: nginx:1.10 container_name: nginx volumes: - /carloz/data/nginx/nginx.conf:/etc/nginx/nginx.conf #配置文件挂载 - /carloz/data/nginx/html:/usr/share/nginx/html #静态资源根目录挂载 - /carloz/data/nginx/log:/var/log/nginx #日志文件挂载 ports: - 80:80 rabbitmq: image: rabbitmq:3.7.15-management container_name: rabbitmq volumes: - /carloz/data/rabbitmq/data:/var/lib/rabbitmq #数据文件挂载 - /carloz/data/rabbitmq/log:/var/log/rabbitmq #日志文件挂载 ports: - 5672:5672 - 15672:15672 elasticsearch: image: elasticsearch:6.4.0 container_name: elasticsearch environment: - "cluster.name=elasticsearch" #设置集群名称为elasticsearch - "discovery.type=single-node" #以单一节点模式启动 - "ES_JAVA_OPTS=-Xms512m -Xmx512m" #设置使用jvm内存大小 volumes: - /carloz/data/elasticsearch/plugins:/usr/share/elasticsearch/plugins #插件文件挂载 - /carloz/data/elasticsearch/data:/usr/share/elasticsearch/data #数据文件挂载 ports: - 9200:9200 kibana: image: kibana:6.4.0 container_name: kibana links: - elasticsearch:es #可以用es这个域名访问elasticsearch服务 depends_on: - elasticsearch #kibana在elasticsearch启动之后再启动 environment: - "elasticsearch.hosts=http://es:9200" #设置访问elasticsearch的地址 ports: - 5601:5601 mongo: image: mongo:3.2 container_name: mongo volumes: - /carloz/data/mongo/db:/data/db #数据文件挂载 ports: - 27017:27017 上传完后在当前目录下执行如下命令： 12345678# Create and start containersdocker-compose -f /carloz/data/docker-compose-env.yml up -d# Stop and remove containers, networks, images, and volumesdocker-compose -f /carloz/data/docker-compose-env.yml down# stopdocker-compose -f /carloz/data/docker-compose-env.yml stop# startdocker-compose -f /carloz/data/docker-compose-env.yml start 1.5、对依赖服务进行以下设置当所有依赖服务启动完成后，需要对以下服务进行一些设置。 1）mysql 需要创建mall数据库并创建一个可以远程访问的对象reader。 将mall.sql文件拷贝到mysql容器的/目录下： 123vim /carloz/data/mall.sql# 下载文件：https://gitee.com/macrozheng/mall-swarm/blob/master/document/sql/mall.sqldocker cp /carloz/data/mall.sql mysql:/ 进入mysql容器并执行如下操作： 123456789101112#进入mysql容器docker exec -it mysql /bin/bash#连接到mysql服务mysql -uroot -proot --default-character-set=utf8#创建远程访问用户grant all privileges on *.* to 'reader' @'%' identified by '123456';#创建mall数据库create database mall character set utf8;#使用mall数据库use mall;#导入mall.sql脚本source /mall.sql; 2）elasticsearch 需要安装中文分词器IKAnalyzer，并重新启动。 1234567891011# 手动下载ik分词器，上传到docker中/usr/share/elasticsearch/plugins目录下cd /carloz/data/elasticsearch/pluginswget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.4.0/elasticsearch-analysis-ik-6.4.0.ziptar -zxf elasticsearch-analysis-ik-v6.4.0.tar.gzrm -f elasticsearch-analysis-ik-v6.4.0.tar.gzdocker exec -it elasticsearch /bin/bash# 检查 /usr/share/elasticsearch/plugins 目录下的ik分词器是否存在docker restart elasticsearchdocker logs -f elasticsearch 3）rabbitmq 需要创建一个mall用户并设置虚拟host为/mall。 访问管理页面地址：http://192.168.145.137:15672/ 输入账号密码并登录：guest guest 创建帐号并设置其角色为管理员：mall mall 创建一个新的虚拟host为：/mall 点击mall用户进入用户配置页面 给mall用户配置该虚拟host的权限 2、可视化管理工具portainer Portainer 是一款轻量级的应用，它提供了图形化界面，用于方便的管理Docker环境，包括单机环境和集群环境，下面我们将用Portainer来管理Docker容器中的应用。 官网地址：https://github.com/portainer/portainer 获取Docker镜像文件： 1docker pull portainer/portainer 使用docker容器运行Portainer： 12345docker run -p 9000:9000 -p 8000:8000 --name portainer \--restart=always \-v /var/run/docker.sock:/var/run/docker.sock \-v /carloz/data/portainer/data:/data \-d portainer/portainer 开放端口 123firewall-cmd --permanent --add-port=9000/tcpfirewall-cmd --permanent --add-port=8000/tcpfirewall-cmd --reload 修改客户端host 12345C:\Windows\System32\drivers\etc\hosts# 文件改为读写192.168.145.137 czportainer.com# 文件改为只读 查看Portainer的DashBoard信息： http://czportainer.com:9000 admin/admin123 3、jenkins安装配置 使用gitee代替gitlab； kubernetes集群（暂时不用）, 使用shell脚本拉取部署镜像 3.1、宿主机软件安装jdk12[root@centos7cz tools]# whereis javajava: /usr/bin/java /carloz/tools/jdk1.8.0_231/bin/java maven 下载解压 1234567cd /carloz/tools/wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gztar -zxf apache-maven-3.6.3-bin.tar.gzmv apache-maven-3.6.3 maven-3.6.3cd maven-3.6.3/pwd/carloz/tools/maven-3.6.3 配置环境变量 12345vi /etc/profileMAVEN_HOME=/carloz/tools/maven-3.6.3export PATH=$&#123;MAVEN_HOME&#125;/bin:$&#123;PATH&#125;source /etc/profile 查看结果 1mvn -v 替换阿里源 1vi /carloz/tools/maven-3.6.3/conf/settings.xml 找到&lt;mirrors&gt;&lt;/mirrors&gt;标签对，添加一下代码： 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt; 指定下载资源位置 123mkdir -p /data/maven-repovi /carloz/tools/maven-3.6.3/conf/settings.xml&lt;localRepository&gt;/data/maven-repo&lt;/localRepository&gt; 指定jdk版本 1234567891011121314vi /carloz/tools/maven-3.6.3/conf/settings.xml&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; git123456[root@centos7cz ~]# yum install -y git[root@centos7cz ~]# whereis gitgit: /usr/bin/git /usr/share/man/man1/git.1.gz[root@centos7cz ~]# git --versiongit version 1.8.3.1[root@centos7cz ~]# 3.2、安装jenkins https://carlo-z.com/devops/jenkins/jenkins-centos7-setup/ 3.3、jenkins做常规配置点击系统管理-&gt;插件管理，进行一些自定义的插件安装： 角色管理权限插件Role-based Authorization Strategy ssh安装ssh插件 配置公钥凭证 在系统管理-&gt;系统配置中添加全局ssh的配置，这样Jenkins使用ssh就可以执行远程的linux脚本了： jdk路径： 系统管理 -&gt; 全局工具配置 -&gt; JDK maven通过系统管理-&gt;全局工具配置来进行全局工具的配置，比如maven的配置 git安装插件： Build With Parameters Git Parameter git可执行文件配置 gitee在线安装 前往 Manage Jenkins -&gt; Manage Plugins -&gt; Available 右侧 Filter 输入： Gitee 下方可选列表中勾选 Gitee（如列表中不存在 Gitee，则点击 Check now 更新插件列表） 点击 Download now and install after restart 添加码云链接配置 前往 Jenkins -&gt; Manage Jenkins -&gt; Configure System -&gt; Gitee Configuration -&gt; Gitee connections 在 Connection name 中输入 Gitee 或者你想要的名字 Gitee host URL 中输入码云完整 URL地址： https://gitee.com （码云私有化客户输入部署的域名） Credentials 中如还未配置码云 APIV5 私人令牌，点击Add- &gt; Jenkins Domain 选择 Global credentials Kind 选择 Gitee API Token Scope 选择你需要的范围 Gitee API Token 输入你的码云私人令牌，获取地址：https://gitee.com/profile/personal_access_tokens ID, Descripiton 中输入你想要的 ID 和描述即可。 Credentials 选择配置好的 Gitee APIV5 Token 点击 Advanced ，可配置是否忽略 SSL 错误（适您的Jenkins环境是否支持），并可设置链接测超时时间（适您的网络环境而定） 点击 Test Connection 测试链接是否成功，如失败请检查以上 3，5，6 步骤。 配置成功后如图所示： 4、mall-swarm部署 4.1、在harbor中新建仓库 harbor安装：https://carlo-z.com/devops/docker/harbor-setup/ 4.2、配置springboot部署参数1、maven配置harbor登录密码 123456789vi /carloz/tools/maven-3.6.3/conf/settings.xml &lt;server&gt; &lt;id&gt;docker-harbor&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;configuration&gt; &lt;email&gt;iceleader@126.com&lt;/email&gt; &lt;/configuration&gt; &lt;/server&gt; 2、修改pom.xml 1234567891011121314151617181920212223242526272829&lt;properties&gt; ... &lt;docker.repostory&gt;czharbor.com&lt;/docker.repostory&gt; &lt;docker.registry.name&gt;mall&lt;/docker.registry.name&gt;&lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.repostory&#125;/$&#123;docker.registry.name&#125;/$&#123;project.artifactId&#125;:$&#123;project.version&#125;&lt;/imageName&gt; &lt;!-- 指定Dockerfile所在的路径 --&gt; &lt;dockerDirectory&gt;$&#123;project.basedir&#125;/src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;serverId&gt;docker-harbor&lt;/serverId&gt; &lt;registryUrl&gt;$&#123;docker.repostory&#125;&lt;/registryUrl&gt; &lt;pushImage&gt;true&lt;/pushImage&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 4.3、执行脚本准备 首先我们先把需要远程执行的脚本准备好。 脚本文件都存放在了mall-swarm项目的/document/sh目录下 上传脚本前在IDEA中修改所有脚本文件的换行符格式为LF，否则脚本会无法执行； 将所有脚本文件上传到指定目录，这里我们上传到/malldata/sh目录下； 1234mkdir -p /malldata/sh# 将所有脚本文件都修改为可执行文件:chmod +x /malldata/sh/mall-* 4.4、Jenkins中创建任务 接下来我们将通过在Jenkins中创建任务来实现自动化部署。由于我们的mall-swarm是个多模块的项目，部署上面和曾经的单模块项目还是有所区别的 mall-registry 由于各个模块的执行任务的创建都大同小异，下面将详细讲解mall-admin模块任务的创建，其他模块将简略讲解。 首先我们选择构建一个自由风格的软件项目，然后输入任务名称为mall-admin，配置其Git仓库地址，这里我直接使用了Gitee上面的地址： 指定代码仓库 点击添加，创建gitee登录凭据 之后我们创建一个构建，构建mall-swarm项目中的依赖模块，否则当构建可运行的服务模块时会因为无法找到这些模块而构建失败； 12# 只install mall-common,mall-mbg,mall-security三个模块clean install -pl mall-common,mall-mbg,mall-security -am 依赖项目构建示意图： 再创建一个构建，单独构建并打包${WORKSPACE}/mall-registry/pom.xml： 添加一个构建来通过SSH去执行远程任务，用于执行mall-registry的运行脚本： 点击保存，完成mall-registry的执行任务创建 mall-admin mall-admin和其他模块与mall-registry创建任务方式基本一致，只需修改构建模块时的pom.xml文件位置和执行脚本位置即可。 我们可以直接从mall-registry模块的任务复制一个过来创建： 修改第二个构建中的pom.xml文件位置，改为：${WORKSPACE}/mall-admin/pom.xml 修改第三个构建中的SSH执行脚本文件位置，改为：/malldata/sh/mall-registry.sh 点击保存，完成mall-admin的执行任务创建。 尝试构建 mall-admin 模块，如果构建成功，再创建其他模块的构建任务 其他模块其他模块的执行任务创建，参考mall-registry和mall-admin的创建即可。 模块启动顺序问题 关于各个模块的启动顺序问题，mall-registry模块必须第一个启动，mall-config模块必须第二个启动，其他模块没有启动顺序限制。 推荐启动顺序： mall-registry mall-config mall-monitor mall-gateway mall-admin mall-portal mall-search mall-demo 12345678docker start mall-registrydocker start mall-configdocker start mall-monitordocker start mall-gatewaydocker start mall-admindocker start mall-portaldocker start mall-searchdocker start mall-demo 4.5、使用jenkins编译 4.6、容器网络问题12345678910111213141516171819202122232425262728293031323334353637383940414243444546docker network create -d bridge mall-networkdocker network disconnect data_default mysqldocker network disconnect data_default redisdocker network disconnect data_default nginxdocker network disconnect data_default rabbitmqdocker network disconnect data_default elasticsearchdocker network disconnect data_default kibanadocker network disconnect data_default mongodocker network inspect data_defaultdocker network disconnect bridge mall-registrydocker network disconnect bridge mall-configdocker network disconnect bridge mall-monitordocker network disconnect bridge mall-gatewaydocker network disconnect bridge mall-admindocker network disconnect bridge mall-portaldocker network disconnect bridge mall-searchdocker network disconnect bridge mall-demodocker network inspect bridgedocker network connect mall-network mysqldocker network connect mall-network redisdocker network connect mall-network nginxdocker network connect mall-network rabbitmqdocker network connect mall-network elasticsearchdocker network connect mall-network kibanadocker network connect mall-network mongodocker network connect mall-network mall-registrydocker network connect mall-network mall-configdocker network connect mall-network mall-monitordocker network connect mall-network mall-gatewaydocker network connect mall-network mall-admindocker network connect mall-network mall-portaldocker network connect mall-network mall-searchdocker network connect mall-network mall-demodocker network lsdocker network inspect mall-networkdocker inspect mysql | grep Networkdocker inspect mall-registry | grep Networkdocker inspect mall-admin | grep Network 5、mall-admin-web前端 http://www.macrozheng.com/#/deploy/mall_deploy_web 1234567git clone https://gitee.com/carloz/mall-admin-web.git$ npm config set registry http://registry.npm.taobao.org/$ npm info underscore$ rm -rf node_modules$ npm install$ npm run dev http://localhost:8090/ 1$ npm run build]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7下安装jenkins]]></title>
    <url>%2Fdevops%2Fjenkins%2Fjenkins-centos7-setup%2F</url>
    <content type="text"><![CDATA[文档：https://jenkins.io/zh/doc/pipeline/tour/getting-started/ 重要经验： 版本一定要下载最新版，历史版本插件支持不完善，很难下载 使用主机安装可以在线升级，使用docker安装不能升级 安装jdk 注意：一定要oraclejdk，openjdk 有 ssl 问题，解决比较麻烦 12345678910111213# 下载https://www.oracle.com/cn/java/technologies/javase-jdk8-downloads.html# 解压# 配置环境变量vi /etc/profileexport JAVA_HOME=/carloz/tools/jdk1.8.0_231export JAVA_BIN=$JAVA_HOME/binexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarsource /etc/profilejava -version 安装jenkins 重要经验：版本一定要下载最新版，历史版本插件支持不完善，很难下载 123sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.reposudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io.keyyum install jenkins -y 配置jenkins123vim /etc/sysconfig/jenkinsJENKINS_PORT="9090"JENKINS_USER="root" 修改目录权限 123chown -R root:root /var/lib/jenkinschown -R root:root /var/cache/jenkinschown -R root:root /var/log/jenkins 启动12345ln -s /carloz/tools/jdk1.8.0_231/bin/java /usr/bin/javaservice jenkins startcat /var/lib/jenkins/secrets/initialAdminPasswordb01d765d389a46d6a1f187b493fdb5b7 此安装向导会引导您完成几个快速“一次性”步骤来解锁Jenkins， 使用插件对其进行自定义，并创建第一个可以继续访问Jenkins的管理员用户。 解锁 Jenkins当您第一次访问新的Jenkins实例时，系统会要求您使用自动生成的密码对其进行解锁。 浏览到 http://192.168.145.137:9090/（或安装时为Jenkins配置的任何端口），并等待 解锁 Jenkins 页面出现 插件安装修改源 123456http://192.168.145.137:9090/pluginManager/advanced# 修改为：http://mirror.serverion.com/jenkins/updates/update-center.jsonhttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.jsonhttp://mirror.esuni.jp/jenkins/updates/update-center.jsonhttp://mirror.xmission.com/jenkins/updates/update-center.json 重新加载配置 1http://192.168.145.137:9090/reload 安装推荐的插件 1http://192.168.145.137:9090/ 修改admin用户的密码为 admin 通过web操作jenkinshttp://192.168.145.137:9090/restarthttp://192.168.145.137:9090/reloadhttp://192.168.145.137:9090/exit]]></content>
      <categories>
        <category>devops</category>
        <category>jenkins</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mongodb 核心设计和集群运维]]></title>
    <url>%2Fmongodb%2Fyunxi%2Fmongodb-3-design-and-cluster%2F</url>
    <content type="text"><![CDATA[引擎 索引 部署 复制 分片 19、MongoDB存储引擎​ MongoDB从3.0开始引入可插拔存储引擎的概念。目前主要有MMAPv1、WiredTiger 存储引擎可供选择。3.2之前，MMAPv1是默认存储引擎，它采用linux的内存映射技术，但一直饱受诟病；3.4以上版本默认的存储引擎是 WiredTiger，相对于MMAPv1其具有如下优势： 读写性能更好，WiredTiger能更好的发挥多核系统的处理能力； MMAPv1殷勤使用表级锁，WiredTiger使用文档级锁； 相比MMAPv1存储，索引时WiredTiger使用前缀压缩，更节省对内存的损耗； 提供压缩算法，可以大大降低对硬盘资源的消耗，节省60%以上的硬盘资源； WiredTiger存储引擎 Concurrency 并发，文档级别 Snapshots and Checkpoints 快照和检查点 Journal 在检查点之间把操作以日志的方式写入journal，用来恢复检查点之间的数据 Compression 压缩算法，节约硬盘 Memory Use 使用内存 Journal 恢复检查点之间的数据，默认压缩算法snappy 最小的记录文件128B 如果内容小于128B不会采用压缩算法 压缩算法 消耗CPU资源进行计算，来减少存储空间的消耗 分块压缩 -&gt; 压缩collection集合的数据 -&gt; snappy压缩算法 -&gt; 索引（前缀压缩） 设置collection压缩算法：storage.wiredTiger.collectionConfig.blockCompressor 设置index压缩算法：storage.wiredTiger.indexConfig.prefixCompression 设置可用内存大小：storage.wiredTiger.engineConfig.cacheSizeGB MMAPv1存储引擎 Journal 每60s从journal写入磁盘一次 storage.syncPeriodSecs 设置写Journal文件的时间间隔 storage.journal.commitIntervalMs 设置写Journal文件的时间间隔 这些是理论值，实际可能更快更频繁的刷新数据 Record Storage MongoDB在磁盘中都是连续的存储，如果数据的修改使得空间变大或变小，就不得不移动数据，非常消耗资源 Memory Use InMemory存储引擎 Concurrency Memory Use 20、Journal工作原理]]></content>
      <categories>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 开发]]></title>
    <url>%2Fmongodb%2Fyunxi%2Fmongodb-2-dev%2F</url>
    <content type="text"><![CDATA[5、MongoDB数据类型]]></content>
      <categories>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 入门]]></title>
    <url>%2Fmongodb%2Fyunxi%2Fmongodb-1-introduce%2F</url>
    <content type="text"><![CDATA[1、为什么要NoSQL关系型数据库难以克服的问题 High performance 高并发读写 Huge Storge 海量数据的高效率存储于访问 High Scalability &amp;&amp; High Availability 高可扩展性和高可用性 关系型数据库的约束 数据库事务一致性需求 数据库的写实时性和读实时性 对复杂的 sql查询，特别是多表关联查询的需求 Nosql的特点优点 处理超大量的数据 运行在便宜的PC服务器集群上 打破性能瓶颈 操作简单 开源支持 缺点 不支持事务 表之间关联查询很难实现 运维的门槛比较高 2、MongoDB简介介于关系型DB 和非关系型DB 之间的一个产品，是非关系型DB中功能最丰富的，最像关系型DB的。它支持的结果非常松散，类似 JSON 的 BSON格式，因此可以存储比较复杂的数据类型。它最大的特点是它支持的查询语言非常强大，其语法类似于面向对象语言的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。他是一个面向集合的、模式自由的文档型数据库； 特点 面向集合文档的存储； 模式自由（自动可以增减）； 强大的查询能力； 支持索引与查询计划； 支持索引与查询计划； 支持自动复制和自动故障转移； 支持二进制数据集 大型对象 （文件）的高效存储； 3、应用场景 经典应用案例： 游戏场景：存储用户信息、装备、积分等； 物流场景：订单信息，订单信息在运输过程中不断更新，以MongoDB内嵌数组的形式来存储，一次查询就能将订单的所有变更读取出来； 社交场景：使用MongoDB存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人，地理位置索引等信息； 物联网场景：存储所有接入的只能设备信息，以及设备上传的日志信息，并对这些信息进行多维度分析 视频直播：存储用户信息、礼物信息等； 不适用的场景： 高度事务性系统（eg：财务系统）； 传统商业智能应用（eg：复杂的关联查询）； 使用SQL更方便的时候；]]></content>
      <categories>
        <category>mongodb</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 文档汇总]]></title>
    <url>%2Fflink%2Fflink-docs%2F</url>
    <content type="text"><![CDATA[开发资料、企业实践：https://ververica.cn/developers-resources]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis 布隆过滤器]]></title>
    <url>%2Fredis%2Fredis-bloomfilter%2F</url>
    <content type="text"><![CDATA[1、安装bloomfiler插件 2、用法1234567[root@dn1 ~]# redis-cli -a 123456127.0.0.1:6379&gt; bf.add cz zhang127.0.0.1:6379&gt; bf.add cz wang127.0.0.1:6379&gt; bf.add cz li127.0.0.1:6379&gt; bf.exists cz wang127.0.0.1:6379&gt; bf.mexists cz wang li 布隆过滤器下载安装首先我们使用布隆过滤器那么我们需要下载布隆过滤器: 下载地址:https://github.com/RedisBloom/RedisBloom 注意: 需要选择和你redis版本适配的版本 下载之后解压即可 启动redis 时加载该模块: 12# 启动redisserver 使用配置文件 使用模型 模型文件位置redis-server redis.conf --loadmodule ../redisbloom.so 布隆过滤器全自动安装启动1234567891011121314151617#创建文件并编辑文件vim redisbloom-install.sh#添加内容cdcd /usr/local/redis1wget https://github.com/RedisLabsModules/rebloom/archive/v1.1.1.tar.gzcd /usr/local/redis1tar zxvf v1.1.1.tar.gzcd /usr/local/redis1/RedisBloom-1.1.1makecdcd /usr/local/redis1./redis-5.0.4/src/redis-server redis-5.0.4/redis.conf --loadmodule RedisBloom-1.1.1/rebloom.so#保存文件变更为可执行文件即可chmod +x redisbloom-install.sh#运行(已经包含启动redis的命令了他会自己启动没必要去管了)./redisbloom-install.sh 什么是布隆过滤器布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率 布隆过滤器的基本使用布隆过滤器有二个基本指令，bf.add 添加元素，bf.exists 查询元素是否存在，如果想要一次添加多个，就需要用到 bf.madd 指令。同样如果需要一次查询多个元素是否存在，就需要用到 bf.mexists 指令。 布隆过滤器的进阶上面的例子中使用的布隆过滤器只是默认参数的布隆过滤器，它在我们第一次使用bf.add 命令时自动创建的。Redis还提供了自定义参数的布隆过滤器，想要尽量减少布隆过滤器的误判，就要设置合理的参数。 在使用bf.add 命令添加元素之前，使用bf.reserve命令创建一个自定义的布隆过滤器。bf.reserve命令有三个参数，分别是： key：键 error_rate：期望错误率，期望错误率越低，需要的空间就越大。 capacity：初始容量，当实际元素的数量超过这个初始化容量时，误判率上升。 如果对应的key已经存在时，在执行bf.reserve命令就会报错。如果不使用bf.reserve命令创建，而是使用Redis自动创建的布隆过滤器，默认的error_rate是 0.001，capacity是 100。 1$ bf.reserve text 0.0001 1000000 #示例 设置 text 精度 0.0001 空间 1000000 布隆过滤器的error_rate越小，需要的存储空间就越大，对于不需要过于精确的场景，error_rate设置稍大一点也可以。布隆过滤器的capacity设置的过大，会浪费存储空间，设置的过小，就会影响准确率，所以在使用之前一定要尽可能地精确估计好元素数量，还需要加上一定的冗余空间以避免实际元素可能会意外高出设置值很多。总之，error_rate和 capacity都需要设置一个合适的数值。 布隆过滤器的原理Redis中布隆过滤器的数据结构就是一个很大的位数组和几个不一样的无偏哈希函数（能把元素的哈希值算得比较平均，能让元素被哈希到位数组中的位置比较随机）。如下图，A、B、C就是三个这样的哈希函数，分别对“yunxitext”和“云析学院”这两个元素进行哈希，位数组的对应位置则被设置为1： 向布隆过滤器中添加元素时，会使用多个无偏哈希函数对元素进行哈希，算出一个整数索引值，然后对位数组长度进行取模运算得到一个位置，每个无偏哈希函数都会得到一个不同的位置。再把位数组的这几个位置都设置为1，这就完成了bf.add命令的操作。 向布隆过滤器查询元素是否存在时，和添加元素一样，也会把哈希的几个位置算出来，然后看看位数组中对应的几个位置是否都为1，只要有一个位为0，那么就说明布隆过滤器里不存在这个元素。如果这几个位置都为1，并不能完全说明这个元素就一定存在其中，有可能这些位置为1是因为其他元素的存在，这就是布隆过滤器会出现误判的原因。 布隆过滤器的应用 解决缓存击穿的问题 一般情况下，先查询缓存是否有该条数据，缓存中没有时，再查询数据库。当数据库也不存在该条数据时，每次查询都要访问数据库，这就是缓存击穿。缓存击穿带来的问题是，当有大量请求查询数据库不存在的数据时，就会给数据库带来压力，甚至会拖垮数据库。 可以使用布隆过滤器解决缓存击穿的问题，把已存在数据的key存在布隆过滤器中。当有新的请求时，先到布隆过滤器中查询是否存在，如果不存在该条数据直接返回；如果存在该条数据再查询缓存查询数据库。 黑名单校验 发现存在黑名单中的，就执行特定操作。比如：识别垃圾邮件，只要是邮箱在黑名单中的邮件，就识别为垃圾邮件。假设黑名单的数量是数以亿计的，存放起来就是非常耗费存储空间的，布隆过滤器则是一个较好的解决方案。把所有黑名单都放在布隆过滤器中，再收到邮件时，判断邮件地址是否在布隆过滤器中即可。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>bloomfilter</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fredis%2Fredis-conf-optimize%2F</url>
    <content type="text"><![CDATA[redis配置解读优化方向文件编码方式 持久化方式]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker三剑客简介]]></title>
    <url>%2Fdevops%2Fdocker%2Fdocker-machine-swarm-compose-introduce%2F</url>
    <content type="text"><![CDATA[docker-machinedocker-machine是解决docker运行环境问题。 ​ docker技术是基于Linux内核的cgroup技术实现的，那么问题来了，如果在非Linux平台上使用docker技术需要依赖安装Linux系统的虚拟机。 ​ docker-machine就是docker公司官方提出的，用于在各种平台上快速创建具有docker服务的虚拟机的技术。你可以把它理解为virtualbox或者vmware，最开始在win7上用得比较多，但是win10开始自带了hyper-v虚拟机，已经不再需要docker-machine了，docker可以直接运行在安装了Linux系统得hyper-v上。 docker-composedcoker-compose主要是解决本地docker容器编排问题。 ​ 一般是通过yaml配置文件来使用它，这个yaml文件里能记录多个容器启动的配置信息（镜像、启动命令、端口映射等），最后只需要执行docker-compose对应的命令就会像执行脚本一样地批量创建和销毁容器。 docker-swarmdocker-swarm是解决多主机多个容器调度部署得问题。 ​ swarm是基于docker平台实现的集群技术，他可以通过几条简单的指令快速的创建一个docker集群，接着在集群的共享网络上部署应用，最终实现分布式的服务。 ​ swarm技术相当不成熟，很多配置功能都无法实现，只能说是个半成品，目前更多的是使用Kubernetes来管理集群和调度容器。 总结 如果你是在非Linux环境下考虑使用docker-compose，当然我更推荐使用hyper-v或者virtualbox； 如果你需要同时操作多个容器，或者希望使用配置文件记录容器启动命令参数，那么推荐使用docker-compose； 如果你需要在多台主机上部署docker容器，并对其进行调度，那么swarm是一种选择，当然更推荐Kubernetes；]]></content>
      <categories>
        <category>devops</category>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis常见场景]]></title>
    <url>%2Fredis%2Fredis-common-scenes%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[缓存穿透、击穿、雪崩]]></title>
    <url>%2Fredis%2Fredis-key-exprie-issues%2F</url>
    <content type="text"><![CDATA[缓存穿透定义client 从 redis中拿某些key，但key不存在（本就不存在），从而流量到达 DB（DB中也不存在），给 DB 造成压力 缓存穿透的概念很简单，用户想要查询一个数据，发现redis内存数据库没有，也就是缓存没有命中，于是向持久层数据库查询。发现也没有，于是本次查询失败。当用户很多的时候，缓存都没有命中，于是都去请求了持久层数据库。这会给持久层数据库造成很大的压力，这时候就相当于出现了缓存穿透。 解决方案1）布隆过滤器将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 2）设定空置如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴 缓存雪崩定义由于redis 大量key同时失效，使得客户端的来的请求 直接到达 DB，打爆 DB； 缓存雪崩，是指在某一个时间段，缓存集中过期失效。 比如在商品抢购的时候，缓存里存的商品都是在晚上12点失效，而这时候刚好大量用户来访问这些商品，这时候数据库的压力就会一下上来了 所以在做电商项目的时候，一般是采取不同分类商品，缓存不同周期。在同一分类中的商品，加上一个随机因子。这样能尽可能分散缓存过期时间，而且，热门类目的商品缓存时间长一些，冷门类目的商品缓存时间短一些，也能节省缓存服务的资源。 场景： 秒杀结束，商品下架 解决方案1）加锁排队（不推荐）加锁排队只是为了减轻数据库的压力，并没有提高系统吞吐量。假设在高并发下，缓存重建期间key是锁着的，这是过来1000个请求999个都在阻塞的。同样会导致用户等待超时，这是个治标不治本的方法！ 注意：加锁排队的解决方式分布式环境的并发问题，有可能还要解决分布式锁的问题；线程还会被阻塞，用户体验很差！因此，在真正的高并发场景下很少使用！ 2）缓存标记（复杂）给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存，实例伪代码如下： 解释说明： 缓存标记：记录缓存数据是否过期，如果过期会触发通知另外的线程在后台去更新实际key的缓存； 缓存数据：它的过期时间比缓存标记的时间延长1倍，例：标记缓存时间30分钟，数据缓存设置为60分钟。 这样，当缓存标记key过期后，实际缓存还能把旧数据返回给调用端，直到另外的线程在后台更新完成后，才会返回新缓存。 关于缓存崩溃的解决方法，这里提出了三种方案：使用锁或队列、设置过期标志更新缓存、为key设置不同的缓存失效时间，还有一各被称为“二级缓存”的解决方法，有兴趣的同学可以自行研究。 3）过期时间加随机数（非重复，间隔值较大）4）延长数据时效5）降级处理 缓存击穿定义热点key 在某一时刻失效 或 不存在（本来应该存在），导致流量打到 DB，打爆 DB 解决方案互斥锁 永不过期 降级 缓存预热在刚启动的缓存系统中，如果缓存中没有任何数据，如果依靠用户请求的方式重建缓存数据，那么对数据库的压力非常大，而且系统的性能开销也是巨大的。 此时，最好的策略是启动时就把热点数据加载好。这样，用户请求时，直接读取的就是缓存的数据，而无需去读取 DB 重建缓存数据。 举个例子，热门的或者推荐的商品，需要提前预热到缓存中。 一般来说，有如下几种方式来实现： 数据量不大时，项目启动时，自动进行初始化。 写个修复数据脚本，手动执行该脚本。 写个管理界面，可以手动点击，预热对应的数据到缓存中。 缓存更新除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： 定时去清理过期的缓存； 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。 两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 缓存降级当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 缓存热点key使用缓存 + 过期时间的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但是有两个问题如果同时出现，可能就会对应用造成致命的危害： 当前 key 是一个热点 key( 可能对应应用的热卖商品、热点新闻、热点评论等），并发量非常大。 重建缓存不能在短时间完成，可能是一个复杂计算，例如复杂的 SQL、多次 IO、多个依赖等。 在缓存失效的瞬间，有大量线程来重建缓存 ( 如下图)，造成后端负载加大，甚至可能会让应用崩溃。 热点 key 失效后大量线程重建缓存 要解决这个问题也不是很复杂，但是不能为了解决这个问题给系统带来更多的麻烦，所以需要制定如下目标： 减少重建缓存的次数 数据尽可能一致 较少的潜在危险 互斥锁 (mutex key)此方法只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即可，整个过程如图 ,使用互斥锁重建缓存 下面代码使用 Redis 的 setnx 命令实现上述功能， 1234567891011121314151617181920212223//伪代码：String get(String key) &#123; //从redis中获取key String value = redis.get(key); //如果value为空则开始重构缓存 if (value == null) &#123; //只允许一个线程重构缓存，使用nx，并设置过期时间ex String mutexKey = "mutex:key" + key; if (redis.set(mutexKey, "1", "ex 180", "nx")) &#123; //从数据源获取数据 value = db.get(key); //回写redis并设置过期时间 redis.set(key, value, timeout); //删除mutexKey redis.del(mutexKey); &#125; else &#123; //其他线程睡眠50秒再重试 Thread.sleep(50); get(key); &#125; &#125; return value;&#125; 从 Redis 获取数据，如果值不为空，则直接返回值。 如果 set(nx 和 ex) 结果为 true，说明此时没有其他线程重建缓存，那么当前线程执行缓存构建逻辑。 如果 setnx(nx 和 ex) 结果为 false，说明此时已经有其他线程正在执行构建缓存的工作，那么当前线程将休息指定时间 (例如这里是 50 毫秒，取决于构建缓存的速度 ) 后，重新执行函数，直到获取到数据。 永远不过期永远不过期”包含两层意思： 从缓存层面来看，确实没有设置过期时间，所以不会出现热点 key 过期后产生的问题，也就是“物理”不过期。 从功能层面来看，为每个 value 设置一个逻辑过期时间，当发现超过逻辑过期时间后，会使用单独的线程去构建缓存。 ” 永远不过期 ” 策略，整个过程如下图所示： 从实战看，此方法有效杜绝了热点 key 产生的问题，但唯一不足的就是重构缓存期间，会出现数据不一致的情况，这取决于应用方是否容忍这种不一致。下面代码使用 Redis 进行模拟： 123456789101112131415161718192021String get(final String key) &#123; V v = redis.get(key); String value = v.getValue(); //逻辑过期时间 final Long logicTimeout = v.getLogicTimeout(); //如果逻辑时间小于当前时间，开始重建缓存 if (logicTimeout &lt;= System.currentTimeMillis()) &#123; final String mutexKey = "mutex:key" + key; if (redis.set(mutexKey, "1", "ex 180", "nx")) &#123; //重建缓存 threadPool.execute(new Runnable() &#123; @Override public void run() &#123; String dbValue = db.get(key); redis.set(key, (dbValue, newLogicTimeout)); redis.del(mutexKey); &#125; &#125;); &#125; &#125; return value;&#125; 作为一个并发量较大的应用，在使用缓存时有三个目标： 第一，加快用户访问速度，提高用户体验。 第二，降低后端负载，减少潜在的风险，保证系统平稳。 第三，保证数据“尽可能”及时更新。下面将按照这三个维度对上述两种解决方案进行分析。 互斥锁 (mutex key)：这种方案思路比较简单，但是存在一定的隐患，如果构建缓存过程出现问题或者时间较长，可能会存在死锁和线程池阻塞的风险，但是这种方法能够较好的降低后端存储负载并在一致性上做的比较好。 ” 永远不过期 “：这种方案由于没有设置真正的过期时间，实际上已经不存在热点 key 产生的一系列危害，但是会存在数据不一致的情况，同时代码复杂度会增大]]></content>
      <categories>
        <category>redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud整合Thrift RPC]]></title>
    <url>%2Fmicro-service%2Fspring-cloud%2Fspringcloud-thrift%2F</url>
    <content type="text"><![CDATA[https://www.jianshu.com/p/66c95dceeb73 前言前面几篇博客，着重对Apache Thrift的使用和原理做了介绍。在微服架构流行的今天，自然而然就会想到Spring Boot和Spring Cloud作为微服务的基础框架。然而，Spring Cloud从诞生以来，就基于HTTP协议的轻量级Restful API作为服务之间的通信方式。 在微服务架构设计中，可以分为外部服务和内部服务。两者主要区别是： 外部服务：基于Restful风格的HTTP协议，通过外网向外部提供服务，相对来说简单并且通用。 内部服务：基于RPC消息通信的TCP/IP协议，提供内网服务与服务之间的调用，以达到减少带宽、降低延迟率、提高性能。 一些应用场景，尤其是内部服务需要高频地调用，就需要考虑是否需要改造为RPC实现，来提高吞吐量和系统性能，比如说鉴权服务一类。]]></content>
      <categories>
        <category>springcloud</category>
      </categories>
      <tags>
        <tag>springcloud</tag>
        <tag>thrift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行相等的最少多米诺旋转]]></title>
    <url>%2Fleetcode%2Fminimum-domino-rotations-for-equal-row%2F</url>
    <content type="text"><![CDATA[算法：贪心算法贪婪算法(贪心算法)是指在对问题进行求解时，在每一步选择中都采取最好或者最优(即最有利)的选择，从而希望能够导致结果是最好或者最优的算法。 贪婪算法所得到的结果往往不是最优的结果(有时候会是最优解)，但是都是相对近似(接近)最优解的结果。 贪婪算法并没有固定的算法解决框架，算法的关键是贪婪策略的选择，根据不同的问题选择不同的策略。 必须注意的是策略的选择必须具备无后效性，即某个状态的选择不会影响到之前的状态，只与当前状态有关，所以对采用的贪婪的策略一定要仔细分析其是否满足无后效性。 比如前边介绍的最短路径问题(广度优先、狄克斯特拉)都属于贪婪算法，只是在其问题策略的选择上，刚好可以得到最优解。 基本思路: 建立数学模型来描述问题; 把求解的问题分成若干个子问题; 对每一子问题求解，得到子问题的局部最优解; 把子问题对应的局部最优解合成原来整个问题的一个近似最优解; 题目：行相等的最少多米诺旋转url：https://leetcode-cn.com/problems/minimum-domino-rotations-for-equal-row/ 1234567在一排多米诺骨牌中，A[i] 和 B[i] 分别代表第 i 个多米诺骨牌的上半部分和下半部分。（一个多米诺是两个从 1 到 6 的数字同列平铺形成的 —— 该平铺的每一半上都有一个数字。）我们可以旋转第 i 张多米诺，使得 A[i] 和 B[i] 的值交换。返回能使 A 中所有值或者 B 中所有值都相同的最小旋转次数。如果无法做到，返回 -1. 示例1： 输入：A = [2,1,2,4,2,2], B = [5,2,6,2,3,2]输出：2解释：图一表示：在我们旋转之前， A 和 B 给出的多米诺牌。如果我们旋转第二个和第四个多米诺骨牌，我们可以使上面一行中的每个值都等于 2，如图二所示。 示例 2：输入：A = [3,5,1,2,3], B = [3,6,3,3,4]输出：-1解释：在这种情况下，不可能旋转多米诺牌使一行的值相等。 提示：1 &lt;= A[i], B[i] &lt;= 62 &lt;= A.length == B.length &lt;= 20000 分析分别统计A数组和B数组中每种牌出现的次数； 如果 ”A中出现的次数 x + B中出现的次数 y“ &gt;= A数组的长度，则算是一种解； 找出所有的解，并返回 最小的 x 或者最小的 y； 如果没有满足条件的解则返回 -1； 上述求出的解是伪最优解，还需要逐个确认每个解是不是真最优解 Java解法1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; public int minDominoRotations(int[] A, int[] B) &#123; TreeMap&lt;Integer,Integer&gt; mapA = new TreeMap&lt;Integer,Integer&gt;(); TreeMap&lt;Integer,Integer&gt; mapB = new TreeMap&lt;Integer,Integer&gt;(); for(int i=0; i&lt;A.length; ++i) &#123; mapA.put(A[i], mapA.getOrDefault(A[i], 0)+1); mapB.put(B[i], mapB.getOrDefault(B[i], 0)+1); &#125; int resA = minDominoRotations(A, B, mapA, mapB); int resB = minDominoRotations(B, A, mapB, mapA); if (-1 != resA &amp;&amp; -1 != resB) &#123; return Math.min(resA, resB); &#125; else &#123; return Math.max(resA, resB); &#125; &#125; public int minDominoRotations(int[] A, int[] B, TreeMap&lt;Integer,Integer&gt; mapA, TreeMap&lt;Integer,Integer&gt; mapB) &#123; int len = A.length, res=-1; TreeMap&lt;Integer,Integer&gt; pseudoPptimalSolution = new TreeMap&lt;&gt;(); for(Map.Entry&lt;Integer,Integer&gt; entry : mapA.entrySet()) &#123; int x = entry.getValue(), y = mapB.getOrDefault(entry.getKey(),0); if(x + y &gt;= len) &#123; pseudoPptimalSolution.put(entry.getKey(), len-x); &#125; &#125; for(Map.Entry&lt;Integer,Integer&gt; entry : pseudoPptimalSolution.entrySet()) &#123; int count = 0; for (int i=0; i&lt;len; ++i) &#123; if (A[i] != entry.getKey() &amp;&amp; B[i] == entry.getKey()) &#123; count++; &#125; &#125; if (count == entry.getValue()) &#123; res = (-1==res) ? count : Math.min(count, res); &#125; &#125; return res; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>贪心算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2Fdistribution%2Fzookeeper%2Fzookeeper-yunxi%2F</url>
    <content type="text"><![CDATA[分布式架构及一致性 ZooKeeper安装配置及使用 ZooKeeper基础核心：数据模型、Node、Watch、版本、ACL ZooKeeper基础核心：服务器角色、会话与客户端、数据与存储 Zookeeper编程手册：ZkClient、Curator ZooKeeper硬核深入：Paxos算法及ZAB协议 ZooKeeper源码深入：Leader选举解析、序列化与通信协议 ZooKeeper应用实战：分布式锁、Master选举、配置中心、服务注册 分布式数据一致性解决方案 分布式协调服务，保证系统的各种数据（配置/状态）在分布式环境中的一致性 硬件、软件 分布式部署在网络上的不同计算机上，消息传递进行通讯和协调 分布式中的问题 故障总会发生：宕机/网络 时钟同步：分布式ID生成 CAP C：Consistency 一致性 A：Availability 可用性 P：Partition Tolerance 分区一致性 BASE理论 —— 解决数据一致性的问题 BA：basic 基本可用 ——&gt; 服务降级 S：soft 软状态 E： 最终一致性 ACID Atomicity：原子性 Consistency：一致性 Isolution：事务隔离性 Durability：持久性 2PC 3PC 三、Zookeeper安装配置环境需求：JDK 1.7以上 集群模式：单机模式、集群模式，伪分布式模式 nnnn同步端口，mmmm选举端口 下载解压 动态变更配置zookeeper 3.5 开始支持 查找主节点命令：[root@dn3 zookeeper]# bin/zkServer.sh status conf/zoo.cfg 1234[root@dn3 ~]# service zookeeper statusZooKeeper JMX enabled by defaultUsing config: /data/soft/new/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: leader 四、核心基础数据如何同步 Leader：读、写、选举 Follower：读、选举 Observer：读 客户端 [root@dn3 zookeeper]# bin/zkCli.sh 过半机制3节点的集群至少有2台集群才能工作，少于两台则会罢工 它的判断机制是 节点数 &gt;= n/2 4台过半，则是2台；所以偶数台则容易脑裂 脑裂过半机制 + 奇数个节点服务器 解决脑裂问题 ZAB协议 ZK命令行1234567891011121314151617181920212223[zk: localhost:2181(CONNECTED) 1] helpZooKeeper -server host:port cmd args stat path [watch] set path data [version] ls path [watch] delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] sync path listquota path rmr path get path [watch] create [-s] [-e] path data acl addauth scheme auth quit getAcl path close connect host:port create 12345678910create [-s] [-e] path data acl-s: 节点是否有序-e: 创建临时节点，临时节点只在session有效，断开就消失[zk: localhost:2181(CONNECTED) 3] create -s -e /cz 001 Created /cz0000000000[zk: localhost:2181(CONNECTED) 4] create -s -e /cz 002Created /cz0000000001[zk: localhost:2181(CONNECTED) 5] create -s -e /cz 003Created /cz0000000002 12345678910111213[zk: localhost:2181(CONNECTED) 7] create -e /cz 005 Created /cz# 临时节点不能有子节点[zk: localhost:2181(CONNECTED) 8] create -e /cz/001 005Ephemerals cannot have children: /cz/001# 持久节点不能级联创建，只能一个一个创建[zk: localhost:2181(CONNECTED) 9] create /czq/001 005 Ephemerals cannot have children: /cz/001[zk: localhost:2181(CONNECTED) 12] create /czq 1Created /czq[zk: localhost:2181(CONNECTED) 13] create /czq/001 123Created /czq/001 12345678910111213141516171819202122232425262728[zk: localhost:2181(CONNECTED) 9] get /czq/001123cZxid = 0x70000000ectime = Fri Mar 06 21:37:11 CST 2020mZxid = 0x70000000emtime = Fri Mar 06 21:37:11 CST 2020pZxid = 0x70000000ecversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0[zk: localhost:2181(CONNECTED) 10] create -e /czq/002 456Created /czq/002[zk: localhost:2181(CONNECTED) 11] get /czq/002456cZxid = 0x700000011ctime = Fri Mar 06 21:43:46 CST 2020mZxid = 0x700000011mtime = Fri Mar 06 21:43:46 CST 2020pZxid = 0x700000011cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x3000000469d0002dataLength = 3numChildren = 0 两大神兽：Znode 和 Watcher 12345678910111213141516171819# 在 session1 启用监听[zk: localhost:2181(CONNECTED) 13] ls /czq watch[001, 002]# 在 session2 创建新节点[zk: localhost:2181(CONNECTED) 1] create -e /czq/003 003Created /czq/003[zk: localhost:2181(CONNECTED) 2]# 查看session1，可以看到，监听到了事件[zk: localhost:2181(CONNECTED) 14] WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/czq# 监听变更[zk: localhost:2181(CONNECTED) 18] get /czq/003 watch[zk: localhost:2181(CONNECTED) 19] WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/czq/003 官方API：注册一次watch只能监听一次事件 客户端 ZooInspector ACL 访问控制等级ACL（Access Control Level） 123create [-s] [-e] path data aclsetAcl path aclgetAcl path 权限模式 Schema 授权对象 ID 权限 Permission 示例 123[zk: localhost:2181(CONNECTED) 3] getAcl /'world,'anyone: cdrwa 权限模式 Schema world 只有一个id， anyone world：anyone Authorware 通过了ZK验证的用户都有权限 username/password Digest username:password, Base64(SHA1(username:password))=username:password ip或ip段 超级管理员 Super：cdrwa 权限 Permission（cdrwa） Create Delete Read Write Admin 12345678910111213[zk: localhost:2181(CONNECTED) 11] setAcl /czq/001 ip:192.168.56.105:cdrw[zk: localhost:2181(CONNECTED) 12] getAcl /czq/001Authentication is not valid : /czq/001[zk: localhost:2181(CONNECTED) 18] addauth digest test:123456[zk: localhost:2181(CONNECTED) 19] create -e /czq/001 001 digest:test:123456:cdrwCreated /czq/001# 没有权限的可以delete，不能rmr[zk: localhost:2181(CONNECTED) 15] delete /czq/001[zk: localhost:2181(CONNECTED) 16] ls /czq[][zk: localhost:2181(CONNECTED) 17] 会话与客户端zk 的 server 和 client 之间是基于 netty 的长连接 tickTime 会话激活会话分桶数据和存储内存数据、磁盘数据（日志、快照） 内存数据 ==&gt; 定期生成快照 操作日志 快照 + 操作日志 ==&gt; 用于崩溃恢复 DataTree ZKDatabase log.xxxxxxxxxxx 当前事务日志的第一条记录zxid（64位，高32位，低32位） 64M，第一条默认是4K大小，超过以后 扩容到64M 解析日志： 五、zk客户端1、zk原生APIzk如何做负载均衡？ 在初始化zk对象的时候，将传入的zk服务器列表打散，形成一个环形队列 2、zkClient3、CuratorAPI使用建造者模式，链式调用，非常友好 六、zk用途1、配置中心 2、3、分布式锁七、深入了解1、Leader选举过程zk内部源码：Leader、Follower、Observer 2、Master选举zk的应用场景 3、分布式锁 4、服务注册发现为了解耦，实现服务的动态发布。动态感知服务上下线 provider，consumer 其他组件：zookeeper、consul、eureka 用到的服务：dubbo， Paxos + ZAB崩溃恢复和Leader选举 ZAB（Zookeeper Atomic Broadcast） 八、源码leader选举 Election：选举接口 FastLeaderElection：选举实现类 ClientConxn：客户端交互 (Packet 包) ServerCnxn： QuorumPeer（extends ZooKeeperThread）：集群节点 QuorumPeerMain：启动入口 ZKDatabase：zk内存数据库 FileTxnSnapLog：事务快照日志 启动入口：org.apache.zookeeper.server.quorum.QuorumPeerMain org/apache/zookeeper/server/quorum/QuorumPeer.java 12345678910111213141516@Overridepublic synchronized void start() &#123; if (!getView().containsKey(myid)) &#123; throw new RuntimeException("My id " + myid + " not in the peer list"); &#125; loadDataBase(); startServerCnxnFactory(); try &#123; adminServer.start(); &#125; catch (AdminServerException e) &#123; LOG.warn("Problem starting AdminServer", e); System.out.println(e); &#125; startLeaderElection(); super.start();&#125; paxos实现lookForLeader]]></content>
      <categories>
        <category>distribution</category>
        <category>zookeeper</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[route：静态路由管理]]></title>
    <url>%2Fcomputer-network%2Froute-manager%2F</url>
    <content type="text"><![CDATA[Centos7 路由管理跟踪路由123456789101112131415161718192021222324252627282930313233[root@nauru-084 ~]# traceroute www.baidu.comtraceroute to www.baidu.com (180.101.49.11), 30 hops max, 60 byte packets 1 gateway (172.18.1.254) 0.299 ms 0.236 ms 0.209 ms 2 172.16.200.12 (172.16.200.12) 0.303 ms 0.243 ms 0.307 ms 3 36.110.61.81 (36.110.61.81) 1.813 ms 4.255 ms 1.630 ms 4 9.235.120.106.static.bjtelecom.net (106.120.235.9) 2.082 ms 1.695 ms * 5 bj141-152-69.bjtelecom.net (219.141.152.69) 2.983 ms 3.016 ms 2.987 ms 6 202.97.97.254 (202.97.97.254) 24.578 ms 202.97.33.30 (202.97.33.30) 29.937 ms 202.97.98.146 (202.97.98.146) 18.506 ms 7 58.213.94.82 (58.213.94.82) 28.225 ms 58.213.95.2 (58.213.95.2) 20.308 ms 58.213.95.106 (58.213.95.106) 19.223 ms 8 * * * 9 58.213.96.110 (58.213.96.110) 21.148 ms 58.213.96.62 (58.213.96.62) 18.620 ms 58.213.96.126 (58.213.96.126) 28.067 ms10 * * *11 * * *12 * * *13 * * *14 * * *15 * * *16 * * *17 * * *18 * * *19 * * *20 * * *21 * * *22 * * *23 * * *24 * * *25 * * *26 * * *27 * * *28 * * *29 * * *30 * * * 查看路由1[root@nauru-084 ~]# route -n 添加路由1route add -host 10.10.87.38 gw 172.18.1.89 删除路由1route del 10.10.87.38 帮助文档12345678910111213141516171819202122232425262728293031323334353637383940[root@localhost ~]# man routeNAME route - show / manipulate the IP routing tableSYNOPSIS route [-CFvnNee] [-A family |-4|-6] route [-v] [-A family |-4|-6] add [-net|-host] target [netmask Nm] [gw Gw] [metric N] [mss M] [window W] [irtt I] [reject] [mod] [dyn] [reinstate] [[dev] If] route [-v] [-A family |-4|-6] del [-net|-host] target [gw Gw] [netmask Nm] [metric N] [[dev] If] route [-V] [--version] [-h] [--help]EXAMPLES route add -net 127.0.0.0 netmask 255.0.0.0 dev lo adds the normal loopback entry, using netmask 255.0.0.0 and associated with the "lo" device (assuming this device was previously set up correctly with ifconfig(8)). route add -net 192.56.76.0 netmask 255.255.255.0 dev eth0 adds a route to the local network 192.56.76.x via "eth0". The word "dev" can be omitted here. route del default deletes the current default route, which is labeled "default" or 0.0.0.0 in the destination field of the current routing table. route add default gw mango-gw adds a default route (which will be used if no other route matches). All packets using this route will be gatewayed through "mango-gw". The device which will actually be used for that route depends on how we can reach "mango-gw" - the static route to "mango-gw" will have to be set up before. route add ipx4 sl0 Adds the route to the "ipx4" host via the SLIP interface (assuming that "ipx4" is the SLIP host). route add -net 192.57.66.0 netmask 255.255.255.0 gw ipx4 This command adds the net "192.57.66.x" to be gatewayed through the former route to the SLIP interface. route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 This is an obscure one documented so people know how to do it. This sets all of the class D (multicast) IP routes to go via "eth0". This is the correct normal configuration line with a multicasting kernel. route add -net 10.0.0.0 netmask 255.0.0.0 reject This installs a rejecting route for the private network "10.x.x.x." Windows 路由管理跟踪路由123456789101112C:\Users\ThinKPad&gt;tracert 10.1.53.2通过最多 30 个跃点跟踪到 ThinKPad-PC [10.1.53.2] 的路由: 1 17 ms 13 ms 11 ms ThinKPad-PC [10.202.0.1] 2 10 ms 9 ms 9 ms bogon [172.23.128.12] 3 22 ms 13 ms 14 ms bogon [172.23.180.113] 4 21 ms 12 ms 14 ms bogon [192.168.111.111] 5 13 ms 13 ms 12 ms ThinKPad-PC [10.1.53.2]跟踪完成。 查看路由1route print -4 添加路由12route add -p 目标IP mask 网段 网关route add -p 10.1.53.79 mask 255.255.255.255 10.10.113.81 删除路由1route delete 10.1.53.79 帮助文档123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172C:\Users\ThinKPad&gt;route \help操作网络路由表。ROUTE [-f] [-p] [-4|-6] command [destination] [MASK netmask] [gateway] [METRIC metric] [IF interface] -f 清除所有网关项的路由表。如果与某个 命令结合使用，在运行该命令前， 应清除路由表。 -p 与 ADD 命令结合使用时，将路由设置为 在系统引导期间保持不变。默认情况下，重新启动系统时， 不保存路由。忽略所有其他命令， 这始终会影响相应的永久路由。Windows 95 不支持此选项。 -4 强制使用 IPv4。 -6 强制使用 IPv6。 command 其中之一: PRINT 打印路由 ADD 添加路由 DELETE 删除路由 CHANGE 修改现有路由 destination 指定主机。 MASK 指定下一个参数为“网络掩码”值。 netmask 指定此路由项的子网掩码值。 如果未指定，其默认设置为 255.255.255.255。 gateway 指定网关。 interface 指定路由的接口号码。 METRIC 指定跃点数，例如目标的成本。用于目标的所有符号名都可以在网络数据库文件 NETWORKS 中进行查找。用于网关的符号名称都可以在主机名称数据库文件 HOSTS 中进行查找。如果命令为 PRINT 或 DELETE。目标或网关可以为通配符，(通配符指定为星号“*”)，否则可能会忽略网关参数。如果 Dest 包含一个 * 或 ?，则会将其视为 Shell 模式，并且只打印匹配目标路由。“*”匹配任意字符串，而“?”匹配任意一个字符。示例: 157.*.1、157.*、127.*、*224*。只有在 PRINT 命令中才允许模式匹配。诊断信息注释: 无效的 MASK 产生错误，即当 (DEST &amp; MASK) != DEST 时。 示例: &gt; route ADD 157.0.0.0 MASK 155.0.0.0 157.55.80.1 IF 1 路由添加失败: 指定的掩码参数无效。 (Destination &amp; Mask) != Destination。示例: &gt; route PRINT &gt; route PRINT -4 &gt; route PRINT -6 &gt; route PRINT 157* .... 只打印那些匹配 157* 的项 &gt; route ADD 157.0.0.0 MASK 255.0.0.0 157.55.80.1 METRIC 3 IF 2 destination^ ^mask ^gateway metric^ ^ Interface^ 如果未给出 IF，它将尝试查找给定网关的最佳 接口。 &gt; route ADD 3ffe::/32 3ffe::1 &gt; route CHANGE 157.0.0.0 MASK 255.0.0.0 157.55.80.5 METRIC 2 IF 2 CHANGE 只用于修改网关和/或跃点数。 &gt; route DELETE 157.0.0.0 &gt; route DELETE 3ffe::/32]]></content>
      <categories>
        <category>computer-network</category>
        <category>route</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jvisualvm 插件]]></title>
    <url>%2Fjava%2Fjvm%2Fjvisualvm-plugins%2F</url>
    <content type="text"><![CDATA[jvisualvm 无法下载插件解决方案各版本 jvisualvm 插件地址： https://visualvm.github.io/pluginscenters.html 更改配置中心的地址： 安装插件在“可用插件”里选择对应的插件即可安装 有用的插件]]></content>
  </entry>
  <entry>
    <title><![CDATA[top + jstack 定位java线程问题]]></title>
    <url>%2Fjava%2Fjvm%2Fjstack-and-top-hp%2F</url>
    <content type="text"><![CDATA[jps -l 定位CPU高问题123456789# 1. 使用 top 定位到占用CPU高的进程PIDtop# 2. 获取线程信息，并找到占用CPU高的线程top -Hp pidps -mp pid -o THREAD,tid,time | sort -rn# 3. 将需要的线程ID转换为16进制格式printf "%x\n" tid# 4. 打印线程的堆栈信息jstack pid | grep tid -A 30 top -Hp pidtop -Hp pid 可以查看某个进程的线程信息 -H 显示线程信息，-p指定pid 可以看到 线程 26096 已经运行了17个小时，将 26096 转化为 16进制：65f0； [root@localhost ~]# printf “%x\n” 27565 线程 27565 对应的 16进制编码为：6bad 各字段含义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970统计信息区前五行是系统整体的统计信息。第一行是任务队列信息，同 uptime 命令的执行结果。其内容如下：01:06:48 当前时间up 1:22 系统运行时间，格式为时:分1 user 当前登录用户数load average: 0.06, 0.60, 0.48 系统负载，即任务队列的平均长度。三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。第二、三行为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下：total 进程总数running 正在运行的进程数sleeping 睡眠的进程数stopped 停止的进程数zombie 僵尸进程数Cpu(s): 0.3% us 用户空间占用CPU百分比1.0% sy 内核空间占用CPU百分比0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比98.7% id 空闲CPU百分比0.0% wa 等待输入输出的CPU时间百分比0.0%hi：硬件CPU中断占用百分比0.0%si：软中断占用百分比0.0%st：虚拟机占用百分比 最后两行为内存信息。内容如下：Mem:191272k total 物理内存总量173656k used 使用的物理内存总量17616k free 空闲内存总量22052k buffers 用作内核缓存的内存量Swap: 192772k total 交换区总量0k used 使用的交换区总量192772k free 空闲交换区总量123988k cached 缓冲的交换区总量,内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小,相应的内存再次被换出时可不必再对交换区写入。 怎么看内存有多少空闲呢？totalfree = free17616 + buffers22052 + cached123988进程信息区统计信息区域的下方显示了各个进程的详细信息。首先来认识一下各列的含义。序号 列名 含义a PID 进程idb PPID 父进程idc RUSER Real user named UID 进程所有者的用户ide USER 进程所有者的用户名f GROUP 进程所有者的组名g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ?h PR 优先级i NI nice值。负值表示高优先级，正值表示低优先级j P 最后使用的CPU，仅在多CPU环境下有意义k %CPU 上次更新到现在的CPU时间占用百分比l TIME 进程使用的CPU时间总计，单位秒m TIME+ 进程使用的CPU时间总计，单位1/100秒n %MEM 进程使用的物理内存百分比o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RESp SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATAr CODE 可执行代码占用的物理内存大小，单位kbs DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kbt SHR 共享内存大小，单位kbu nFLT 页面错误次数v nDRT 最后一次写入到现在，被修改过的页面数。w S 进程状态(D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程)x COMMAND 命令名/命令行y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名z Flags 任务标志，参考 sched.h cpu使用率很高，所以首先找到该进程，通过top命令，监控该进程的使用率，然后通过H，查看各个线程的cpu使用率情况，记下cpu使用率高的线程id，然后通过jstack pid,获取各个线程栈，听过top获取的线程id转化成16进制后 jstack pidjstack 用法1234567891011121314151617/opt/java8/bin/jstackUsage: jstack [-l] &lt;pid&gt; (to connect to running process) 连接活动线程 jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) 连接阻塞线程 jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) 连接dump的文件 jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server) 连接远程服务器Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung) -m to print both java and native frames (mixed mode) -l long listing. Prints additional information about locks -h or -help to print this help message 使用 jstack 打印 进程 26080 的线程堆栈 jstack 26080 &gt; aaa.log 正向推导 jstack 26080 | grep 6bad -A 30 线程 26080（16进制：6bad） 的线程堆栈如下： 各字段含义 pool-1-thread-3：线程名； prio：应该是线程的优先级； os_prio：对应的操作系统的 线程优先级； tid：jvm 中 的线程id； nid： tid映射的操作系统中的线程id,非常有用,不过这里是用16进制的表示。可以通过 printf “%x\n” 十进制数字 查找一个十进制数字的十六进制表示； waiting for condition：等待资源ID； TIMED_WAITING：当前线程状态； 反向推导同理，nid=0x6f44 对应的线程ID 为 28484： top -Hp 26080 | grep 28484 jstack解析示例0、线程状态jstack dump 日志文件中，值得关注的线程状态有： 死锁，Deadlock（重点关注） 执行中，Runnable 等待资源，Waiting on condition（重点关注） 等待获取监视器，Waiting on monitor entry（重点关注） 暂停，Suspended 对象等待中，Object.wait() 或 TIMED_WAITING 阻塞，Blocked（重点关注） 停止，Parked pool-1-thread-3：线程名； prio：应该是线程的优先级； os_prio：对应的操作系统的 线程优先级； tid：jvm 中 的线程id； nid： tid映射的操作系统中的线程id,非常有用,不过这里是用16进制的表示。可以通过 printf “%x\n” 十进制数字 查找一个十进制数字的十六进制表示； waiting for condition：等待资源ID； TIMED_WAITING：当前线程状态； 1、Object.wait() + locked123456"Timer-0" #22 daemon prio=5 os_prio=0 tid=0x00007fc29cb23800 nid=0x65ff in Object.wait() [0x00007fc28474c000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.util.TimerThread.mainLoop(Timer.java:552) - locked &lt;0x00000006c064fa10&gt; (a java.util.TaskQueue) at java.util.TimerThread.run(Timer.java:505) locked：线程拿到了 synchornized 锁 Object； Object.wait()：线程调用了 wait(n)，释放了锁，进入到Wait Set队列； TIMED_WAITING：线程处于 TIMED_WAITING 状态； 2、waiting on condition1234567891011"pool-1-thread-1" #19 prio=5 os_prio=0 tid=0x00007fc220003800 nid=0x65fa waiting on condition [0x00007fc2851bb000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000006c0414240&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) ConditionObject.await：线程调用了 Condition 的 await 方法，释放了锁，同时进入WAITING 状态； 线程在 等待 其他线程 使用 Condition.signal() 将它唤醒 3、runnable123456789"1414013111@qtp-802771878-1 - Acceptor0 SocketConnector@0.0.0.0:8081" #21 prio=5 os_prio=0 tid=0x00007fc29cb14800 nid=0x65fe runnable [0x00007fc28484d000] java.lang.Thread.State: RUNNABLE at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.mortbay.jetty.bio.SocketConnector.accept(SocketConnector.java:99) at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) RUNNABLE：线程处于可运行状态 或 正在运行；]]></content>
      <categories>
        <category>jvm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机操作系统·虚拟内存]]></title>
    <url>%2Foperating-system%2Fos-virtual-memory%2F</url>
    <content type="text"><![CDATA[3.5 虚拟内存的概念、特征以及虚拟内存的实现传统存储管理方式的特征上一节所讨论的各种内存管理策略都是为了同时将多个进程保存在内存中以便允许多道程序设计。它们都具有以下两个共同的特征： 1) 一次性作业必须一次性全部装入内存后，方能开始运行。这会导致两种情况发生： 当作业很大，不能全部被装入内存时，将使该作业无法运行； 当大量作业要求运行时，由于内存不足以容纳所有作业，只能使少数作业先运行，导致多道程序度的下降。 2) 驻留性作业被装入内存后，就一直驻留在内存中，其任何部分都不会被换出，直至作业运行结束。运行中的进程，会因等待I/O而被阻塞，可能处于长期等待状态。 由以上分析可知，许多在程序运行中不用或暂时不用的程序（数据）占据了大量的内存空间，而一些需要运行的作业又无法装入运行，显然浪费了宝贵的内存资源。 局部性原理要真正理解虚拟内存技术的思想，首先必须了解计算机中著名的局部性原理。著名的 Bill Joy (SUN公司CEO)说过：”在研究所的时候，我经常开玩笑地说高速缓存是计算机科学中唯一重要的思想。事实上，髙速缓存技术确实极大地影响了计算机系统的设计。“快表、 页高速缓存以及虚拟内存技术从广义上讲，都是属于高速缓存技术。这个技术所依赖的原理就是局部性原理。局部性原理既适用于程序结构，也适用于数据结构（更远地讲，Dijkstra 著名的关于“goto语句有害”的论文也是出于对程序局部性原理的深刻认识和理解）。 局部性原理表现在以下两个方面： 时间局部性：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。 时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。 虚拟存储器的定义和特征基于局部性原理，在程序装入时，可以将程序的一部分装入内存，而将其余部分留在外存，就可以启动程序执行。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存,然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换出到外存上，从而腾出空间存放将要调入内存的信息。这样，系统好像为用户提供了一个比实际内存大得多的存储器，称为虚拟存储器。 之所以将其称为虚拟存储器，是因为这种存储器实际上并不存在，只是由于系统提供了部分装入、请求调入和置换功能后（对用户完全透明），给用户的感觉是好像存在一个比实际物理内存大得多的存储器。虚拟存储器的大小由计算机的地址结构决定，并非是内存和外存的简单相加。虚拟存储器有以下三个主要特征： 多次性，是指无需在作业运行时一次性地全部装入内存，而是允许被分成多次调入内存运行。 对换性，是指无需在作业运行时一直常驻内存，而是允许在作业的运行过程中，进行换进和换出。 虚拟性，是指从逻辑上扩充内存的容量，使用户所看到的内存容量，远大于实际的内存容量。 虚拟内存技术的实现虚拟内存中，允许将一个作业分多次调入内存。釆用连续分配方式时，会使相当一部分内存空间都处于暂时或“永久”的空闲状态，造成内存资源的严重浪费，而且也无法从逻辑上扩大内存容量。因此，虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。虚拟内存的实现有以下三种方式： 请求分页存储管理。 请求分段存储管理。 请求段页式存储管理。 不管哪种方式，都需要有一定的硬件支持。一般需要的支持有以下几个方面： 一定容量的内存和外存。 页表机制（或段表机制），作为主要的数据结构。 中断机构，当用户程序要访问的部分尚未调入内存，则产生中断。 地址变换机构，逻辑地址到物理地址的变换。 3.6 请求分页管理方式实现虚拟内存请求分页系统建立在基本分页系统基础之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。 在请求分页系统中，只要求将当前需要的一部分页面装入内存，便可以启动作业运行。在作业执行过程中，当所要访问的页面不在内存时，再通过调页功能将其调入，同时还可以通过置换功能将暂时不用的页面换出到外存上，以便腾出内存空间。 为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存的计算机系统，还需要有页表机制、缺页中断机构和地址变换机构。 页表机制请求分页系统的页表机制不同于基本分页系统，请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存的情况，如何发现和处理这种情况是请求分页系统必须解决的两个基本问题。为此，在请求页表项中增加了四个字段，如图3-24所示。 图3-24 请求分页系统中的页表项 增加的四个字段说明如下： 状态位P：用于指示该页是否已调入内存，供程序访问时参考。 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近己有多长时间未被访问，供置换算法换出页面时参考。 修改位M：标识该页在调入内存后是否被修改过。 外存地址：用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。 缺页中断机构在请求分页系统中，每当所要访问的页面不在内存时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。此时应将缺页的进程阻塞（调页完成唤醒)，如果内存中有空闲块，则分配一个块，将要调入的页装入该块，并修改页表中相应页表项，若此时内存中没有空闲块，则要淘汰某页（若被淘汰页在内存期间被修改过，则要将其写回外存)。 缺页中断作为中断同样要经历，诸如保护CPU环境、分析中断原因、转入缺页中断处理程序、恢复CPU环境等几个步骤。但与一般的中断相比，它有以下两个明显的区别： 在指令执行期间产生和处理中断信号，而非一条指令执行完后，属于内部中断。 一条指令在执行期间，可能产生多次缺页中断。 地址变换机构请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存，又增加了某些功能而形成的。 图3-25请求分页中的地址变换过程 如图3-25所示，在进行地址变换时，先检索快表： 若找到要访问的页，便修改页表项中的访问位（写指令则还须重置修改位)，然后利用页表项中给出的物理块号和页内地址形成物理地址。 若未找到该页的页表项，应到内存中去查找页表，再对比页表项中的状态位P，看该页是否已调入内存，未调入则产生缺页中断，请求从外存把该页调入内存。 3.7 页面置换算法进程运行时，若其访问的页面不在内存而需将其调入，但内存已无空闲空间时，就需要从内存中调出一页程序或数据，送入磁盘的对换区。 选择调出页面的算法就称为页面置换算法。好的页面置换算法应有较低的页面更换频率，也就是说，应将以后不会再访问或者以后较长时间内不会再访问的页面先调出。 常见的置换算法有以下四种。 1. 最佳置换算法(OPT)最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。 最佳置换算法可以用来评价其他算法。假定系统为某进程分配了三个物理块，并考虑有以下页面号引用串： 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 进程运行时，先将7, 0, 1三个页面依次装入内存。进程要访问页面2时，产生缺页中断，根据最佳置换算法，选择第18次访问才需调入的页面7予以淘汰。然后，访问页面0时，因为已在内存中所以不必产生缺页中断。访问页面3时又会根据最佳置换算法将页面1淘汰……依此类推，如图3-26所示。从图中可以看出釆用最佳置换算法时的情况。 可以看到，发生缺页中断的次数为9，页面置换的次数为6。 访问页面 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 物理块1 7 7 7 2 2 2 2 2 7 物理块2 0 0 0 0 4 0 0 0 物理块3 1 1 3 3 3 1 1 缺页否 √ √ √ √ √ √ √ √ 图3-26 利用最佳置换算法时的置换图 2. 先进先出(FIFO)页面置换算法优先淘汰最早进入内存的页面，亦即在内存中驻留时间最久的页面。该算法实现简单，只需把调入内存的页面根据先后次序链接成队列，设置一个指针总指向最早的页面。但该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。 访问页面 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 物理块1 7 7 7 2 2 2 4 4 4 0 0 0 7 7 7 物理块2 0 0 0 3 3 3 2 2 2 1 1 1 0 0 物理块3 1 1 1 0 0 0 3 3 3 2 2 2 1 缺页否 √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ 图3-27 利用FIFO置换算法时的置换图 这里仍用上面的实例，釆用FIFO算法进行页面置换。进程访问页面2时，把最早进入内存的页面7换出。然后访问页面3时，再把2, 0, 1中最先进入内存的页换出。由图 3-27可以看出，利用FIFO算法时进行了 12次页面置换，比最佳置换算法正好多一倍。 FIFO算法还会产生当所分配的物理块数增大而页故障数不减反增的异常现象，这是由 Belady于1969年发现，故称为Belady异常，如图3-28所示。只有FIFO算法可能出现Belady 异常，而LRU和OPT算法永远不会出现Belady异常。 访问页面 1 2 3 4 1 2 5 1 2 3 4 5 物理块1 1 1 1 4 4 4 5 ,5’ 5 物理块2 2 2 2 1 1 1 3 3 物理块3 3 3 3 2 2 2 4 缺页否 √ √ √ √ √ √ √ √ √ 1 1 1 5 5 5 5 4 4 物理块2* 2 2 2 2 1 1 1 1 5 物理块3* 3 3 3 3 2 2 2 2 物理块4* 4 4 4 4 3 3 3 缺页否 √ √ √ √ √ √ √ √ √ 图 3-28 Belady 异常 3. 最近最久未使用(LRU)置换算法选择最近最长时间未访问过的页面予以淘汰，它认为过去一段时间内未访问过的页面，在最近的将来可能也不会被访问。该算法为每个页面设置一个访问字段，来记录页面自上次被访问以来所经历的时间，淘汰页面时选择现有页面中值最大的予以淘汰。 再对上面的实例釆用LRU算法进行页面置换，如图3-29所示。进程第一次对页面2访问时，将最近最久未被访问的页面7置换出去。然后访问页面3时，将最近最久未使用的页面1换出。 访问页面 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 物理块1 7 7 7 2 2 4 4 4 0 1 1 1 物理块2 0 0 0 0 0 0 3 3 3 0 0 物理块3 1 1 3 3 2 2 2 2 2 7 缺页否 √ √ √ √ √ √ √ √ √ √ √ √ 图3-29 LRU页面置换算法时的置换图 在图3-29中，前5次置换的情况与最佳置换算法相同，但两种算法并无必然联系。实际上，LRU算法根据各页以前的情况，是“向前看”的，而最佳置换算法则根据各页以后的使用情况，是“向后看”的。 LRU性能较好，但需要寄存器和栈的硬件支持。LRU是堆栈类的算法。理论上可以证明，堆栈类算法不可能出现Belady异常。FIFO算法基于队列实现，不是堆栈类算法。 4. 时钟(CLOCK)置换算法LRU算法的性能接近于OPT,但是实现起来比较困难，且开销大；FIFO算法实现简单，但性能差。所以操作系统的设计者尝试了很多算法，试图用比较小的开销接近LRU的性能，这类算法都是CLOCK算法的变体。 简单的CLOCK算法是给每一帧关联一个附加位，称为使用位。当某一页首次装入主存时，该帧的使用位设置为1;当该页随后再被访问到时，它的使用位也被置为1。对于页替换算法，用于替换的候选帧集合看做一个循环缓冲区，并且有一个指针与之相关联。当某一页被替换时，该指针被设置成指向缓冲区中的下一帧。当需要替换一页时，操作系统扫描缓冲区，以查找使用位被置为0的一帧。每当遇到一个使用位为1的帧时，操作系统就将该位重新置为0；如果在这个过程开始时，缓冲区中所有帧的使用位均为0，则选择遇到的第一个帧替换；如果所有帧的使用位均为1,则指针在缓冲区中完整地循环一周，把所有使用位都置为0，并且停留在最初的位置上，替换该帧中的页。由于该算法循环地检查各页面的情况，故称为CLOCK算法，又称为最近未用(Not Recently Used, NRU)算法。 CLOCK算法的性能比较接近LRU，而通过增加使用的位数目，可以使得CLOCK算法更加高效。在使用位的基础上再增加一个修改位，则得到改进型的CLOCK置换算法。这样，每一帧都处于以下四种情况之一： 最近未被访问，也未被修改(u=0, m=0)。 最近被访问，但未被修改(u=1, m=0)。 最近未被访问，但被修改(u=0, m=1)。 最近被访问，被修改(u=1, m=1)。 算法执行如下操作步骤： 从指针的当前位置开始，扫描帧缓冲区。在这次扫描过程中，对使用位不做任何修改。选择遇到的第一个帧(u=0, m=0)用于替换。 如果第1)步失败，则重新扫描，查找(u=0, m=1)的帧。选择遇到的第一个这样的帧用于替换。在这个扫描过程中，对每个跳过的帧，把它的使用位设置成0。 如果第2)步失败，指针将回到它的最初位置，并且集合中所有帧的使用位均为0。重复第1步，并且如果有必要，重复第2步。这样将可以找到供替换的帧。 改进型的CLOCK算法优于简单CLOCK算法之处在于替换时首选没有变化的页。由于修改过的页在被替换之前必须写回，因而这样做会节省时间。 3.8 页面分配策略：驻留集大小、调入页面的时机以及从何处调入页面驻留集大小对于分页式的虚拟内存，在准备执行时，不需要也不可能把一个进程的所有页都读取到主存，因此，操作系统必须决定读取多少页。也就是说，给特定的进程分配多大的主存空间，这需要考虑以下几点： 分配给一个进程的存储量越小，在任何时候驻留在主存中的进程数就越多，从而可以提高处理机的时间利用效率。 如果一个进程在主存中的页数过少，尽管有局部性原理，页错误率仍然会相对较高。 如桌页数过多，由于局部性原理，给特定的进程分配更多的主存空间对该进程的错误率没有明显的影响。 基于这些因素，现代操作系统通常釆用三种策略： 固定分配局部置换。它为每个进程分配一定数目的物理块，在整个运行期间都不改变。若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后再调入需要的页面。实现这种策略难以确定为每个进程应分配的物理块数目：太少会频繁出现缺页中断，太多又会使CPU和其他资源利用率下降。 可变分配全局置换。这是最易于实现的物理块分配和置换策略，为系统中的每个进程分配一定数目的物理块,操作系统自身也保持一个空闲物理块队列。当某进程发生缺页时，系统从空闲物理块队列中取出一个物理块分配给该进程，并将欲调入的页装入其中。 可变分配局部置换。它为每个进程分配一定数目的物理块，当某进程发生缺页时，只允许从该进程在内存的页面中选出一页换出，这样就不会影响其他进程的运行。如果进程在运行中频繁地缺页，系统再为该进程分配若干物理块，直至该进程缺页率趋于适当程度； 反之，若进程在运行中缺页率特别低，则可适当减少分配给该进程的物理块。 调入页面的时机为确定系统将进程运行时所缺的页面调入内存的时机，可釆取以下两种调页策略： 预调页策略。根据局部性原理，一次调入若干个相邻的页可能会比一次调入一页更高效。但如果调入的一批页面中大多数都未被访问，则又是低效的。所以就需要釆用以预测为基础的预调页策略，将预计在不久之后便会被访问的页面预先调入内存。但目前预调页的成功率仅约50%。故这种策略主要用于进程的首次调入时，由程序员指出应该先调入哪些页。 请求调页策略。进程在运行中需要访问的页面不在内存而提出请求，由系统将所需页面调入内存。由这种策略调入的页一定会被访问，且这种策略比较易于实现，故在目前的虚拟存储器中大多釆用此策略。它的缺点在于每次只调入一页，调入调出页面数多时会花费过多的I/O开销。 从何处调入页面请求分页系统中的外存分为两部分：用于存放文件的文件区和用于存放对换页面的对换区。对换区通常是釆用连续分配方式，而文件区釆用离散分配方式，故对换区的磁盘I/O速度比文件区的更快。这样从何处调入页面有三种情况： 系统拥有足够的对换区空间：可以全部从对换区调入所需页面，以提髙调页速度。为此，在进程运行前，需将与该进程有关的文件从文件区复制到对换区。 系统缺少足够的对换区空间：凡不会被修改的文件都直接从文件区调入；而当换出这些页面时，由于它们未被修改而不必再将它们换出。但对于那些可能被修改的部分，在将它们换出时须调到对换区，以后需要时再从对换区调入。 UNIX方式：与进程有关的文件都放在文件区，故未运行过的页面，都应从文件区调入。曾经运行过但又被换出的页面，由于是被放在对换区，因此下次调入时应从对换区调入。进程请求的共享页面若被其他进程调入内存，则无需再从对换区调入。 3.9 页面抖动(颠簸)和工作集(驻留集)页面抖动（颠簸）在页面置换过程中的一种最糟糕的情形是，刚刚换出的页面马上又要换入主存，刚刚换入的页面马上就要换出主存，这种频繁的页面调度行为称为抖动，或颠簸。如果一个进程在换页上用的时间多于执行时间，那么这个进程就在颠簸。 频繁的发生缺页中断（抖动），其主要原因是某个进程频繁访问的页面数目高于可用的物理页帧数目。虚拟内存技术可以在内存中保留更多的进程以提髙系统效率。在稳定状态，几乎主存的所有空间都被进程块占据，处理机和操作系统可以直接访问到尽可能多的进程。但如果管理不当，处理机的大部分时间都将用于交换块，即请求调入页面的操作，而不是执行进程的指令，这就会大大降低系统效率。 工作集（驻留集）工作集（或驻留集）是指在某段时间间隔内，进程要访问的页面集合。经常被使用的页面需要在工作集中，而长期不被使用的页面要从工作集中被丢弃。为了防止系统出现抖动现象，需要选择合适的工作集大小。 工作集模型的原理是：让操作系统跟踪每个进程的工作集，并为进程分配大于其工作集的物理块。如果还有空闲物理块，则可以再调一个进程到内存以增加多道程序数。如果所有工作集之和增加以至于超过了可用物理块的总数，那么操作系统会暂停一个进程，将其页面调出并且将其物理块分配给其他进程，防止出现抖动现象。 正确选择工作集的大小，对存储器的利用率和系统吞吐量的提嵩，都将产生重要影响。 3.10 内存管理知识点总结分页管理方式和分段管理方式在很多地方相似，比如内存中都是不连续的、都有地址变 换机构来进行地址映射等。但两者也存在着许多区别，表3-20列出了分页管理方式和分段管理方式在各个方面的对比。 分 页 分 段 目 的 页是信息的物理单位，分页是为实现离散分配方式，以消减内存的外零头，提髙内存的利 用率。或者说，分页仅权是由于系统管理的需要而不是用户的需要 是信息的逻辑单位，它含有一组其意义相对完整的信息。分段的目的是为了能更好地满足用户的需要 长 度 页的大小固定且由系统决定，由系统把逻辑地址划分为页号和页内地址两部分，是由机器硬件实现的，因而在系统中只能有一种大小的页面 段的长度不固定，决定于用户所编写的程序， 通常由编译程序在对流程序进行编译时，根据信息的性质来划分 地址空间 作业地址空间是一维的，即单一的线性地址空间，程序员只需利用一个记忆符，即可表示 一个地址 作业地址空间是二维的，程序员在标识一个地址时，既需给出段名，又需给出段内地址 碎 片 有内部碎片 无外部碎片 有外部碎片 无内部碎片 ”共享“和“动态链接” 不容易实现 容易实现]]></content>
      <categories>
        <category>operating-system</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring 核心模块划分]]></title>
    <url>%2Fjava%2Fspring%2Fspring-framework-module-partition%2F</url>
    <content type="text"><![CDATA[从上面可以看出Spring主要分成六个模块： 1、Spring核心容器​ 核心容器是Spring框架的重要组成部分，也可以说是Spring框架的基础。他在整个框架中的作用是负责管理对象的创建，管理，配置等等的操作。其主要包含spring-core，spring-beans，spring-context，spring-expression，spring-context-support组件。 ​ 核心容器提供spring框架的基本功能。Spring以bean的方式组织和管理Java应用中的各个组件及其关系。Spring使用BeanFactory来产生和管理Bean，它是工厂模式的实现。BeanFactory使用控制反转(IoC)模式将应用的配置和依赖性规范与实际的应用程序代码分开。BeanFactory使用依赖注入的方式提供给组件依赖。主要实现控制反转IoC和依赖注入DI、Bean配置以及加载。 spring context​ Spring上下文是一个配置文件，向Spring框架提供上下文信息。Spring上下文包括企业服务，如JNDI、EJB、电子邮件、国际化、校验和调度功能。提供框架式Bean访问方式，其他程序可以通过Context访问Spring的Bean资源。 2、面向切面编程​ Spring框架还提供了面向切面编程的能力，利用面向切面编程，可以实现一些面向对象编程无法很好实现的操作。例如，将日志，事务与具体的业务逻辑解耦。其主要包含spring-aop，spring-aspects组件。 ​ 通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了Spring框架中。所以，可以很容易地使 Spring框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。AOP把一个业务流程分成几部分，例如权限检查、业务处理、日志记录，每个部分单独处理，然后把它们组装成完整的业务流程。每个部分被称为切面或关注点。 AOP的实现原理为动态代理技术，一共有两种代理模式： （1）ProxyFactoryBean代理工厂对象 Spring内置代理类，引入一个中间层，能够创建不同类型的对象，利用它可以实现任何形式的AOP。 （2）TransactionProxyFactoryBean事务代理工厂对象 常用在数据库编程上，Spring利用TransactionProxyFactoryBean对事务进行管理，在指定方法前利用AOP连接数据库并开启事务，然后在指定方法返回后利用AOP提交事务并断开数据库。 3、Instrumentation​ 该模块提供了为JVM添加代理的功能，该模块包含spring-instrument，spring-instrument-tomcat组件，使用较少，不必过分关注。 4、数据访问与集成​ Spring框架为了简化数据访问的操作，包装了很多关于数据访问的操作，提供了相应的模板。同时还提供了使用ORM框架的能力，可以与很多流行的ORM框架进行整合，如hibernate，mybatis等等的著名框架。还实现了数据事务的能力，能够支持事务。包含spring-jdbc，spring-tx，spring-orm，spring-oxm，spring-jms，spring-messaging组件。 Spring ORM（Object Relation Mapper）对象关系映射模块​ Spring 与所有的主要的ORM框架都集成的很好，包括hibernate、JDO实现、TopLink和IBatis SQL Map等。Spring为所有的这些框架提供了模板之类的辅助类，达成了一致的编程风格。Spring的ORM模块对ORM框架如Hibernate等进行了封装，Spring能够管理、维护Hibernate，使用时可直接继承HibernateDaoSupport类，该类内置一个HibernateTemplate。Hibernate的配置也转移到Spring配置文件中。 （注：ORM是通过使用描述对象和数据库之间映射的元数据，ORM框架采用元数据来描述对象–关系映射细节，元数据一般采用xml格式，并且存放在专门的对象–映射文件中） 5、Web和远程调用​ Spring框架支持Web开发，以及与其他应用远程交互调用的方案。包含spring-web，spring-webmvc，spring-websocket，spring-webmvc-portlet组件。 Spring Web模块Web模块建立在应用程序上下文模块之上，为基于Web的应用程序提供了上下文。Web层使用Web层框架，可选的，可以是Spring自己的MVC框架，或者提供的Web框架，如Struts、Webwork、tapestry和jsf。 Web模块用于整合Web框架，将Web框架也纳入Spring的管理之中。如Spring提供继承方式与代理方式整合Struts，继承方式不需要更改任何配置文件，只把Action继承自ActionSupport即可，但会对Spring产生依赖。代理方式需要在struts-config.xml中配置，由Spring全盘代理，因此可以使用Spring的各种资源、拦截器等。 Spring MVCMVC框架是一个全功能的构建Web应用程序的MVC实现。通过策略接口，MVC框架变成为高度可配置的。Spring的MVC框架提供清晰的角色划分：控制器、验证器、命令对象、表单对象和模型对象、分发器、处理器映射和视图解析器。Spring支持多种视图技术。 Spring MVC 的工作流程： （1） 客户端发送请求，请求到达 DispatcherServlet 主控制器。（2） DispatcherServlet 控制器调用 HandlerMapping 处理。（3） HandlerMapping 负责维护请求和 Controller 组件对应关系。 HandlerMapping 根据请求调用对应的 Controller 组件处理。（4） 执行 Controller 组件的业务处理，需要访问数据库，可以调用 DAO 等组件。（5）Controller 业务方法处理完毕后，会返回一个 ModelAndView 对象。该组件封装了模型数据和视图标识。（6）Servlet 主控制器调用 ViewResolver 组件，根据 ModelAndView 信息处理。定位视图资源，生成视图响应信息。（7）控制器将响应信息给用户输出。 6、Spring测试​ Spring框架提供了测试的模块，可以实现单元测试，集成测试等等的测试流程，整合了JUnit或者TestNG测试框架。包含spring-test组件。 ​ 以上就是Spring框架的几个主要模块。而在实际的开发过程中，不必要引入所有的Jar包，只需要引入与自己项目相关的库就行了。例如，只需要Spring框架的容器功能，实现依赖注入，那么只需要引入核心框架的几个库就完成了，其他模块例如Aop模块的库是没有必要引入的。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql select机制]]></title>
    <url>%2Fmysql%2Fmysql-select%2F</url>
    <content type="text"><![CDATA[很多 SQL 查询都是以 SELECT 开始的。不过，最近我跟别人解释什么是窗口函数，我在网上搜索”是否可以对窗口函数返回的结果进行过滤“这个问题，得出的结论是”窗口函数必须在 WHERE 和 GROUP BY 之后，所以不能”。于是我又想到了另一个问题：SQL 查询的执行顺序是怎样的？ 好像这个问题应该很好回答，毕竟自己已经写了上万个 SQL 查询了，有一些还很复杂。但事实是，我仍然很难确切地说出它的顺序是怎样的。 SQL 查询的执行顺序于是我研究了一下，发现顺序大概是这样的。SELECT 并不是最先执行的，而是在第五个。 这张图回答了以下这些问题 这张图与 SQL 查询的语义有关，让你知道一个查询会返回什么，并回答了以下这些问题： 可以在 GRROUP BY 之后使用 WHERE 吗？（不行，WHERE 是在 GROUP BY 之后！） 可以对窗口函数返回的结果进行过滤吗？（不行，窗口函数是 SELECT 语句里，而 SELECT 是在 WHERE 和 GROUP BY 之后） 可以基于 GROUP BY 里的东西进行 ORDER BY 吗？（可以，ORDER BY 基本上是在最后执行的，所以可以基于任何东西进行 ORDER BY） LIMIT 是在什么时候执行？（在最后！） 但数据库引擎并不一定严格按照这个顺序执行 SQL 查询，因为为了更快地执行查询，它们会做出一些优化，这些问题会在以后的文章中解释。 所以： 如果你想要知道一个查询语句是否合法，或者想要知道一个查询语句会返回什么，可以参考这张图； 在涉及查询性能或者与索引有关的东西时，这张图就不适用了。 混合因素：列别名有很多 SQL 实现允许你使用这样的语法： 123SELECT CONCAT(first_name, ' ', last_name) AS full_name, count(*)FROM tableGROUP BY full_name 从这个语句来看，好像 GROUP BY 是在 SELECT 之后执行的，因为它引用了 SELECT 中的一个别名。但实际上不一定要这样，数据库引擎可以把查询重写成这样： 123SELECT CONCAT(first_name, ' ', last_name) AS full_name, count(*)FROM tableGROUP BY CONCAT(first_name, ' ', last_name) 这样 GROUP BY 仍然先执行。 数据库引擎还会做一系列检查，确保 SELECT 和 GROUP BY 中的东西是有效的，所以会在生成执行计划之前对查询做一次整体检查。 数据库可能不按照这个顺序执行查询（优化）在实际当中，数据库不一定会按照 JOIN、WHERE、GROUP BY 的顺序来执行查询，因为它们会进行一系列优化，把执行顺序打乱，从而让查询执行得更快，只要不改变查询结果。 这个查询说明了为什么需要以不同的顺序执行查询： 123SELECT * FROMowners LEFT JOIN cats ON owners.id = cats.ownerWHERE cats.name = 'mr darcy' 如果只需要找出名字叫“mr darcy”的猫，那就没必要对两张表的所有数据执行左连接，在连接之前先进行过滤，这样查询会快得多，而且对于这个查询来说，先执行过滤并不会改变查询结果。 数据库引擎还会做出其他很多优化，按照不同的顺序执行查询，不过我并不是这方面的专家，所以这里就不多说了。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql explain 详解]]></title>
    <url>%2Fmysql%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[explainexplain显示了mysql如何使用索引来处理select语句以及连接表。可以帮助选择更好的索引和写出更优化的查询语句。 使用方法，在select语句前加上explain就可以了, 如： 1234567891011121314151617explain select c_user.*, t.cust_id, ifnull(tcc.tradeamt, 0) tradeamt, ifnull(tcc.commission,0) commission, ifnull(ih.is_hadtrade, 0) is_hadtrade from crm.c_user left join crm.trade_fund_account t on c_user.id = t.c_user_id and t.cust_id &gt; 0 left join (select cust_id, sum(amount_trade) tradeamt, max(commission_ft_current_month) commission from crm.trade_commission tc where tc.bizdate between 20171001 and 20171031 and tc.bid = &apos;00000000000000007&apos; group by cust_id ) tcc on tcc.cust_id = t.cust_id left join (select cust_id, sum(amount_trade), (case when sum(amount_trade) &gt; 0 then 1 else 0 end) is_hadtrade from crm.trade_commission tc where tc.bid = &apos;00000000000000007&apos; and tc.bizdate &lt; 20171022 group by tc.cust_id ) ih on ih.cust_id = t.cust_id where c_user.bapp_broker_id = &apos;00000000000000007&apos; and is_hadtrade = 1 extend​ extended关键字：仅对select语句有效，在explain后使用extended关键字，可以显示filtered列显示了通过条件过滤出的行数的百分比估计值。​ 也可以通过show warnings显示扩展信息，输出中的 Message值SHOW WARNINGS显示优化程序如何限定SELECT语句 中的表名和列名， SELECT应用重写和优化规则后的外观，以及可能有关优化过程的其他说明。 EXPLAIN 各个列的含义id查询的序号，包含一组数字，表示查询中执行select子句或操作表的顺序 两种情况： id相同，执行顺序从上往下; id不同，id值越大，优先级越高，越先执行 select_type查询类型，主要用于区别普通查询，联合查询，子查询等的复杂查询 simple —— 简单的select查询，查询中不包含子查询或者UNION primary —— 查询中若包含任何复杂的子部分，最外层查询被标记 subquery —— 在select或where列表中包含了子查询 derived —— 在from列表中包含的子查询被标记为derived（衍生），MySQL会递归执行这些子查询，把结果放到临时表中 union —— 如果第二个select出现在UNION之后，则被标记为UNION，如果union包含在from子句的子查询中，外层select被标记为derived union result —— UNION 的结果 table 输出的行所引用的表 type显示联结类型，显示查询使用了何种类型，按照从最佳到最坏类型排序 system：表中仅有一行（=系统表）这是const联结类型的一个特例。 const：表示通过索引一次就找到，const用于比较primary key或者unique索引。因为只匹配一行数据，所以如果将主键置于where列表中，mysql能将该查询转换为一个常量 eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于唯一索引或者主键扫描 ref：非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问，它返回所有匹配某个单独值的行，可能会找多个符合条件的行，属于查找和扫描的混合体 range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般就是where语句中出现了between,in等范围的查询。这种范围扫描索引扫描比全表扫描要好，因为它开始于索引的某一个点，而结束另一个点，不用全表扫描 index：index 与all区别为index类型只遍历索引树。通常比all快，因为索引文件比数据文件小很多。 all：遍历全表以找到匹配的行 注意:一般保证查询至少达到range级别，最好能达到ref。 possible_keys 指出MySQL可能使用哪个索引在该表中找到行 key 显示MySQL实际决定使用的键(索引)。如果没有选择索引,键是NULL。查询中如果使用覆盖索引，则该索引和查询的select字段重叠。 key_len 表示索引中使用的字节数，该列计算查询中使用的索引的长度在不损失精度的情况下，长度越短越好。如果键是NULL,则长度为NULL。该字段显示为索引字段的最大可能长度，并非实际使用长度。 ref 显示索引的哪一列被使用了，如果有可能是一个常数，哪些列或常量被用于查询索引列上的值 rows 根据表统计信息以及索引选用情况，大致估算出找到所需的记录所需要读取的行数 filtered 显示了通过条件过滤出的行数的百分比估计值。 Extra包含不适合在其他列中显示，但是十分重要的额外信息 Using filesort：说明mysql会对数据适用一个外部的索引排序。而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成排序操作称为“文件排序” Using temporary：使用了临时表保存中间结果，mysql在查询结果排序时使用临时表。常见于排序order by和分组查询group by。 Using index：表示相应的select操作用使用覆盖索引，避免访问了表的数据行。如果同时出现using where，表名索引被用来执行索引键值的查找；如果没有同时出现using where，表名索引用来读取数据而非执行查询动作。 Using where：表明使用where过滤 using join buffer：使用了连接缓存 impossible where：where子句的值总是false，不能用来获取任何元组 select tables optimized away：在没有group by子句的情况下，基于索引优化Min、max操作或者对于MyISAM存储引擎优化count（*），不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。 distinct：优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL架构设计]]></title>
    <url>%2Fmysql%2F1-mysql-architecture%2F</url>
    <content type="text"><![CDATA[一、MySQL逻辑架构 二、MySQL基本架构​ 对于MySQL来说，虽然经历了多个版本迭代（MySQL5.5,MySQL 5.6,MySQL 5.7,MySQL 8）,但每次的迭代，都是基于MySQL基架的，MySQL基架大致包括如下几大模块组件： 1、MySQL向外提供的交互接口(Connectors)Connectors组件，是MySQL向外提供的交互组件，如java,.net,php等语言可以通过该组件来操作SQL语句，实现与SQL的交互。 2、管理服务组件和工具组件(Management Service &amp; Utilities)提供对MySQL的集成管理，如备份(Backup),恢复(Recovery),安全管理(Security)等 3、连接池组件(Connection Pool) 负责监听对客户端向MySQL Server端的各种请求，接收请求，转发请求到目标模块。每个成功连接MySQL Server的客户请求都会被创建或分配一个线程，该线程负责客户端与MySQL Server端的通信，接收客户端发送的命令，传递服务端的结果信息等。 4、SQL接口组件(SQL Interface)接收用户SQL命令，如DML,DDL和存储过程等，并将最终结果返回给用户。 5、查询分析器组件(Parser)首先分析SQL命令语法的合法性，并尝试将SQL命令解析成抽象语法树，若解析失败，则提示SQL语句不合理 1SELECT id, name FROM t_user WHERE status = 'ACTIVE' AND age &gt; 18 6、优化器组件(Optimizer)对SQL命令按照标准流程进行优化分析。 1234567891011121314151617explain select c_user.*, t.cust_id, ifnull(tcc.tradeamt, 0) tradeamt, ifnull(tcc.commission,0) commission, ifnull(ih.is_hadtrade, 0) is_hadtrade from crm.c_user left join crm.trade_fund_account t on c_user.id = t.c_user_id and t.cust_id &gt; 0 left join (select cust_id, sum(amount_trade) tradeamt, max(commission_ft_current_month) commission from crm.trade_commission tc where tc.bizdate between 20171001 and 20171031 and tc.bid = '00000000000000007' group by cust_id ) tcc on tcc.cust_id = t.cust_id left join (select cust_id, sum(amount_trade), (case when sum(amount_trade) &gt; 0 then 1 else 0 end) is_hadtrade from crm.trade_commission tc where tc.bid = '00000000000000007' and tc.bizdate &lt; 20171022 group by tc.cust_id ) ih on ih.cust_id = t.cust_id where c_user.bapp_broker_id = '00000000000000007' and is_hadtrade = 1 7、缓存主件(Caches &amp; Buffers)缓存和缓冲组件 8、插件式存储引擎(Pluggable Storage Engines)9、物理文件(File System) 三、InnoDB存储引擎 1）、 当事务要修改一条记录时，Innodb需要将该记录从Disk中读到BP（Buffer Pool）中。当事务提交后（commit），BP当中的page的记录被修改，这时BP中page就和Disk中的page不一致了，所以这时候BP中的page就是Dirty Page, 需要等待刷新到Disk中； 2）、假如在BP的Dirty Pages刷新到Disk前，系统断电了那么Dirty Page中的数据修改是不是就全部丢失了呢？答案是不会的，这就是Redo Log(innodb_flush_log_at_trx_commit=1)要做的事情了。在事务提交时候，事务中的更新操作会立即保存到Redo Log. 所以就算BP中的Dirty Page在断电后全部丢失，MySQL在重启时，仍可以根据Redo Log来恢复数据； 3）、好了，现在想象一下，假如Dirty Page在flush到disk的过程中发生断电，导致部分数据丢失，Redo Log还能不能恢复这部分数据。答案是不行的，因为mysql在恢复的时候是根据page的checksum来决定这个page是否需要恢复，而这个checksum就是这个page最后一个事务的事务号。这就是partial write的问题，这时候就需要double write了； partial write是指mysql在将数据写到数据文件的时候, 会出现只写了一半但由于某种原因剩下的数据没有写到innodb file上. 出现这种问题可能是由于系统断电, mysql crash造成的, 而造成这个问题更根本的原因是由于mysql 的page size跟系统文件的page size不一致, 导致在写数据的时候, 系统并不是把整个buffer pool page一次性写到disk上,所以当写到一半时, 系统断电,partial write也就产生了; 如果partial write产生, 会发生什么问题呢? 因为我们知道在flush buffer cache的时候,其实redo log已经写好了. 为什么还需要担心partial write呢? 这是因为mysql在恢复的时候是通过检查page的checksum来决定这个page是否需要恢复的, checksum就是当前这个page最后一个事务的事务号; 如果系统找不到checksum, mysql也就无法对这行数据执行写入操作; 4）、double write的作用就是在mysql将数据刷新到硬盘的时候，先将数据写到double write的空间，然后再某个时刻再把数据写到Disk中。 InnoDB架构 下图显示了组成InnoDB存储引擎架构的内存和磁盘结构。 有关每个结构的信息，请参阅第15.5节“InnoDB内存结构”和第15.6节“InnoDB磁盘结构”。 InnoDB内存结构 缓冲池(buffer pool) 1）、innodb_buffer_pool缓冲池提高数据库整体性能。 2）、innodb读取页操作时，先将从磁盘读到的页放在缓冲池中，下次再次读取，先判断是否缓存命中，若否，则读磁盘中的页 3）、事务提交后,InnoDB首先在buffer pool中找到对应的页,把事务更新到 buffer poll 中. ​ 当刷新脏页到磁盘时,缓冲区都干了什么? 缓冲区把脏页拷贝到doublewrite buffer，doublewirte buffer把脏页刷新到double write磁盘(这也是一次顺序IO),再把脏页刷新到数据文件中。当然buffer pool中还有其他组件,也非常重要,如插入缓冲（change buffer）,该缓冲区是为了高效维护二级非唯一索引所做的优化,把多次IO转化为一次IO来达到快速更新的目的 4）、mysql crash时，buffer pool中的数据都会丢失。 ​ innodb数据文件中，会存储checkpoint信息，redo log中也存储checkpoint信息。InnoDB会对比数据文件ibd和redo log中的checkpoint信息，找出最后一次checkpoint对应的log sequence number，通过redo log的变更记录重新应用到数据库 更改缓冲区（change buffer） 原来就是insert buffer，后来随着update和delete缓冲加入改为change buffer。 缓存对二级索引的变更。二级索引通常是非唯一的，并且二级索引中的插入以相对随机的顺序发生。 log buffer 1、概念1）、InnoDB存储引擎是基于磁盘存储的，并将其中的记录按照页的方式进行管理 bp(buffer pool)：InnoDB数据库page的缓存。 page：对InnoDB中数据的任何修改，首先在bp的page上进行。修改后的page标记为dirty page。 flush list：存放dirty page，后续由专门的thread写入磁盘中。 2）、使用bp的好处：避免每次对page的修改都进行IO操作，将多次对页面的修改合并成一次IO操作。 引入问题：会造成数据的不一致性，RAM和Disk中数据不一致。 解决(数据不一致性)：使用redo log，将所有对page进行的修改操作，都写入一个专门的文件中数据库启动时，会从中恢复。 redo log的影响：延迟了bp中page刷新到磁盘，提高了数据库的吞吐量。 额外的redo log顺序IO的写入开销，以及数据库启动时使用redo log恢复所需时间。 2、redo log相关参数show variables like “%innodb_log%”; innodb_log_file_size 指定redo log日志文件大小，默认48M innodb_log_files_in_group redo log日志文件组中文件的数量，默认2个，ib_logfile0和ib_logfile1 innodb_log_buffer_size redo log buffer的大小，默认为16M 日志组中的文件大小是一致的，以循环的方式运行。ib_logfile0写满时，切换到ib_logfile1，ib_logfile1写满时，再次切换到ib_logfile0。 3、redo log写入过程1）、redo log buffer 向 redo log file 写，是按512个字节，也就是一个扇区的大小进行写入。扇区是写入的最小单位。 ​ 参数innodb_flush_log_at_trx_commit用来控制事务日志刷新到磁盘的策略。 2）、将redo log buffer中的内容刷新到磁盘的redo log file（ib_logfile）中的条件。 保证日志先行，每次事务commit前，都会将redo log刷新到磁盘上。 redo log是一个环(ring)结构，当redo空间快占满时，将会将部分dirty pages flush到disk上，然后释放部分redo log。 4、对比binlog 5、对比undo log1）、Undo Log 是为了实现事务的原子性(rollback)，在MySQL数据库InnoDB存储引擎中，还用Undo Log来实现多版本并发控制(简称：MVCC)。 2）、Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中，但从5.6开始，也可以使用独立的Undo 表空间。 3）、Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。 InnoDB磁盘结构 tablespace：表空间 segment 段： extent 簇：物理上连续的几个页； 1MB一个簇 = 64 * 16KB = 128 * 8KB = 256 * 4KB，一个簇包含多少Page取决于Page的大小 page 页：16KB 索引组织表 doublewrite buffer doublewrite buffer 是 InnoDB 的 system tablespace 中的一个区域。在 把 buffer pool 中的内容写入对应的 data file 之前，先要把数据写入 doublewirte buffer 中； 如果 storage 或者 mysqld 在写 page的过程中 进程崩溃，InnoDB 可以在 doublewrite buffer 中找到完成的 page 的拷贝 用来恢复； 尽管数据总是写两次，但 doublewrite buffer 不需要两倍的 I/O开销 或 两倍的 I/O 操作。数据作为一个大的连续块 写入 doublwrite buffer，只需对操作系统执行一次 fsync() 调用； double write的缺点是什么? ​ 虽然mysql称double write是一个buffer, 但其实它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而我们知道硬盘的fsync性能是很慢的, 所以它会降低mysql的整体性能. 但是并不会降低到原来的50%. 这主要是因为: 1) double write是一个连接的存储空间, 所以硬盘在写数据的时候是顺序写, 而不是随机写, 这样性能更高. 另外将数据从double write buffer写到真正的segment中的时候, 系统会自动合并连接空间刷新的方式, 每次可以刷新多个pages; redo log Redo Log是基于磁盘的数据结构，在崩溃恢复期间用于纠正不完整事务写入的数据。 默认情况下，Redo Log在磁盘上由两个名为ib_logfile0和的 文件物理表示ib_logfile1。MySQL以循环方式写入Redo Log文件。Redo Log中的数据按照受影响的记录进行编码；此数据统称为重做。 undo log 功能：事务回滚、MVCC undo log是与单个读写事务关联的 undo Log 记录的集合。undo Log 记录包含如何 撤消事务对 聚簇索引 记录的最新更改 的信息。如果另一个事务读取原始数据，就需要在 undo Log记录读取。 undo log存在于 undo log segment。默认情况下，undo log segment 实际上是 system tablespace 的一部分 ，但它们也可以驻留在 undolog tablespace 中 •undo:相当于数据修改前的备份 •redo: 相当于数据修改后的备份,为了保证事务的持久化,redo会一直写 •Undo + Redo**事务的简化过程** 假设有A、B两个数据，值分别为1,2. A.事务开始. B.记录A=1到undo log. C.修改A=3. D.记录A=3到redo log. E.记录B=2到undo log. F.修改B=4. G.记录B=4到redo log. H.将redo log写入磁盘 I.事务提交完成 •- Undo + Redo**事务的特点** A. 为了保证持久性，必须在事务提交前将Redo Log持久化 —一般每个事务提交时或每秒刷盘 B. 数据不需要在事务提交前写入磁盘，而是缓存在内存中。 —data在此时不需要写磁盘，但是如果redo文件过小也会触发事务未提交前数据落盘 C. Redo Log 保证事务的持久性 D. Undo Log 保证事务的原子性。 E. 有一个隐含的特点，数据必须要晚于redo log写入持久存储。 一个查询流程图]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Azkaban安装部署]]></title>
    <url>%2Ftools%2Fazkaban-setup%2F</url>
    <content type="text"><![CDATA[Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。 1、简介官网： https://azkaban.github.io/ 下载： https://github.com/azkaban/azkaban/releases 1.1、架构 1.2、三个关键组件的作用 Relational Database：存储元数据，如项目名称、项目描述、项目权限、任务状态、SLA规则等。 AzkabanWebServer：项目管理、权限授权、任务调度、监控executor。 AzkabanExecutorServer：作业流执行的Server。 1.3、三种部署模式1)、solo-server模式​ DB使用的是一个内嵌的H2，Web Server和Executor Server运行在同一个进程里。这种模式包含Azkaban的所有特性，但一般用来学习和测试。 2)、two-server模式​ DB使用的是MySQL，MySQL支持master-slave架构，Web Server和Executor Server运行在不同的进程中。 3)、分布式multiple-executor模式​ DB使用的是MySQL，MySQL支持master-slave架构，Web Server和Executor Server运行在不同机器上，且有多个Executor Server。 2、下载安装2.1、编译环境12# 安装jdk 1.8yum install -y git 2.2、下载源码编译12wget https://github.com/azkaban/azkaban/archive/3.38.0.tar.gz -O azkaban-3.38.0.tar.gztar -zxf azkaban-3.38.0.tar.gz 123cd azkaban-3.38.0./gradlew clean build -x test# ./gradlew build -x test 2.3、列出编译产物123456789101112131415# solo-server模式安装包路径[root@localhost azkaban-3.38.0]# ll azkaban-solo-server/build/distributions/-rw-r--r--. 1 root root 24166624 Nov 21 15:27 azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz-rw-r--r--. 1 root root 24303950 Nov 21 15:27 azkaban-solo-server-0.1.0-SNAPSHOT.zip# two-server模式和multiple-executor模式web-server安装包路径[root@localhost azkaban-3.38.0]# ll azkaban-web-server/build/distributions/-rw-r--r--. 1 root root 20120241 Nov 21 15:21 azkaban-web-server-0.1.0-SNAPSHOT.tar.gz-rw-r--r--. 1 root root 20246140 Nov 21 15:21 azkaban-web-server-0.1.0-SNAPSHOT.zip# two-server模式和multiple-executor模式exec-server安装包路径[root@localhost azkaban-3.38.0]# ll azkaban-exec-server/build/distributions/-rw-r--r--. 1 root root 16055070 Nov 21 15:27 azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz-rw-r--r--. 1 root root 16060378 Nov 21 15:27 azkaban-exec-server-0.1.0-SNAPSHOT.zip# 建表sql[root@localhost azkaban-3.38.0]# ll azkaban-db/build/sql/-rw-r--r--. 1 root root 13887 Nov 21 15:13 create-all-sql-0.1.0-SNAPSHOT.sql 3、multiple-executor集群搭建 1、WebServer 负责管理Job，存储在mysql中 2、mysql存储Job 3、ExecutorServer从mysql中读取任务执行，ExecutorServer要部署在定时任务机器上； Azkaban WebServer挂掉，不影响已经提交的任务执行，主要是不能通过WebUI查看Job、管理Job、跟踪Job状态。因此，对于这个架构，主要是要解决MySQL HA和ExecutorServer HA。官方支持ExecutorServer HA，我们只需要配一个MySQL HA就行了。 12345678910111213141516171819202122232425mkdir -p /data/azkabantar -zxf azkaban-db/build/distributions/azkaban-db-0.1.0-SNAPSHOT.tar.gz -C /data/azkabantar -zxf azkaban-web-server/build/distributions/azkaban-web-server-0.1.0-SNAPSHOT.tar.gz -C /data/azkabantar -zxf azkaban-exec-server/build/distributions/azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz -C /data/azkabantar -zxf azkaban-solo-server/build/distributions/azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz -C /data/azkaban# 结果[root@localhost azkaban]# pwd/data/azkaban[root@localhost azkaban]# lltotal 4drwxr-xr-x. 2 root root 4096 Nov 21 14:50 azkaban-db-0.1.0-SNAPSHOTdrwxr-xr-x. 6 root root 55 Nov 21 15:27 azkaban-exec-server-0.1.0-SNAPSHOTdrwxr-xr-x. 8 root root 77 Nov 21 15:27 azkaban-solo-server-0.1.0-SNAPSHOTdrwxr-xr-x. 6 root root 51 Nov 21 15:21 azkaban-web-server-0.1.0-SNAPSHOTmv azkaban-web-server-0.1.0-SNAPSHOT/ webservermv azkaban-exec-server-0.1.0-SNAPSHOT exec-servermv azkaban-solo-server-0.1.0-SNAPSHOT soloserver[root@localhost azkaban]# lldrwxr-xr-x. 2 root root 4096 Nov 21 14:50 azkaban-db-0.1.0-SNAPSHOTdrwxr-xr-x. 10 root root 202 Nov 22 10:20 exec-serverdrwxr-xr-x. 8 root root 77 Nov 21 15:27 soloserverdrwxr-xr-x. 9 root root 196 Nov 22 10:07 webserver[root@localhost azkaban]# 3.1、建立Azkaban数据库1234mysqladmin create db_azkaban -h 172.18.1.51 -P 3306 -uroot -p123456mysql -h 172.18.1.51 -P 3306 -uroot -p123456 db_azkaban &lt; /data/azkaban/azkaban-db-0.1.0-SNAPSHOT/create-all-sql-0.1.0-SNAPSHOT.sql# 报错：ERROR 1071 (42000) at line 64: Specified key was too long; max key length is 767 bytes# 将数据库编码改为Latin1可解决 3.2、生成SSL12345678910111213141516171819202122[root@localhost azkaban]# cd ~[root@localhost ~]# keytool -keystore keystore -alias jetty -genkey -keyalg RSAEnter keystore password: 123456Re-enter new password: 123456What is your first and last name? [Unknown]: 回车What is the name of your organizational unit? [Unknown]: 回车What is the name of your organization? [Unknown]: 回车What is the name of your City or Locality? [Unknown]: 回车What is the name of your State or Province? [Unknown]: 回车What is the two-letter country code for this unit? [Unknown]: 回车Is CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown correct? [no]: yesEnter key password for &lt;jetty&gt; (RETURN if same as keystore password): 123456Re-enter new password: 123456[root@localhost ~]# 3.3、设置 ExecutorServervi /data/azkaban/exec-server/conf/azkaban.properties 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# Azkaban Personalization Settingsazkaban.name=AzkabanExecutorazkaban.label=AzkabanExecutorazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.jetty.use.ssl=falsejetty.maxThreads=25jetty.port=8181# Where the Azkaban web server is locatedazkaban.webserver.url=http://localhost:8081# mail settingsmail.sender=iceleader@126.commail.host=iceleader@126.com# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081job.failure.email=iceleader@126.comjob.success.email=iceleader@126.comlockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban plugin settingsazkaban.jobtype.plugin.dir=plugins/jobtypes# Azkaban mysql settings by default. Users should configure their own username and password.database.type=mysqlmysql.port=3306mysql.host=172.18.1.51mysql.database=db_azkabanmysql.user=rootmysql.password=123456mysql.numconnections=100# Azkaban Executor settingsexecutor.maxThreads=50executor.flow.threads=30 设置： default.timezone.id 时区配置 user.manager.xml.file 登录用户配置 azkaban.webserver.url 指向 webserver mysql.* mysql 相关配置 mail.sender、mail.host、job.failure.email、job.success.email 邮件相关配置 启动：/data/azkaban/exec-server/bin/start-exec.sh curl http://172.18.1.51:41096/executor?action=activate 1curl http://$&#123;executorHost&#125;:$&#123;executorPort&#125;/executor?action=activate 3.4、设置 WebServer12cd /data/azkaban/webserver/conf/vi /data/azkaban/webserver/conf/azkaban.properties 1234567891011121314# 修改时区default.timezone.id=Asia/Shanghai# 邮箱配置job.failure.email=iceleader@126.comjob.success.email=iceleader@126.com# mysql配置database.type=mysqlmysql.port=3306mysql.host=172.18.1.51mysql.database=db_azkabanmysql.user=rootmysql.password=123456mysql.numconnections=100# SSL密码和文件路径 vi /data/azkaban/webserver/conf/azkaban-users.xml 12345678&lt;azkaban-users&gt; &lt;user groups="azkaban" password="azkaban" roles="admin" username="azkaban"/&gt; &lt;user password="metrics" roles="metrics" username="metrics"/&gt; &lt;user username="admin" password="admin" roles="admin" /&gt; &lt;role name="admin" permissions="ADMIN"/&gt; &lt;role name="metrics" permissions="METRICS"/&gt;&lt;/azkaban-users&gt; mkdir -p /data/azkaban/webserver/plugins/jobtypes vi /data/azkaban/webserver/plugins/jobtypes/commonprivate.properties 12azkaban.native.lib=falseexecute.as.user=false 启动服务器 sh /data/azkaban/webserver/bin/start-web.sh sh /data/azkaban/webserver/bin/shutdown-web.sh 注意：没有 ExecutorServer 启动，WebServer就是退出 select * from db_azkaban.executors update db_azkaban.executors set active = 1 where id = 1; 浏览器访问： http://172.18.1.51:8081/index 使用admin/admin登录 3.5、配置Job1)、创建project 2)、定义任务并上传 每个任务格式如下 123type=commanddependencies=dailysummary,advisorcommand=echo dailysettlement 完成后打包为 dailysettlement.zip， 上传到 Project dailysettlement下 3)、定制执行计划 每小时30分执行一次 4)、查看执行计划 3.6、执行任务的问题1、集群内存不足，去掉webserver中的内存限制 vi webserver/conf/azkaban.properties 12# azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatusazkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus 12345678910111213141516171819202122ERROR [FlowTriggerScheduler] [Azkaban] Unable to get scheduled flow triggersjava.lang.NullPointerException at azkaban.flowtrigger.quartz.FlowTriggerScheduler.getScheduledFlowTriggerJobs(FlowTriggerScheduler.java:139) at azkaban.webapp.servlet.FlowTriggerServlet.ajaxFetchTrigger(FlowTriggerServlet.java:67) at azkaban.webapp.servlet.FlowTriggerServlet.handleAJAXAction(FlowTriggerServlet.java:107) at azkaban.webapp.servlet.FlowTriggerServlet.handleGet(FlowTriggerServlet.java:57) at azkaban.webapp.servlet.LoginAbstractAzkabanServlet.doGet(LoginAbstractAzkabanServlet.java:122) at javax.servlet.http.HttpServlet.service(HttpServlet.java:668) at javax.servlet.http.HttpServlet.service(HttpServlet.java:770) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) 2、ExcutorServer未激活，需要调用url手动去激活一下 executor，方式如下： 1curl http://$&#123;executorHost&#125;:$&#123;executorPort&#125;/executor?action=activate 123456789101112131415161718192021222019/11/22 14:25:23.831 +0800 ERROR [ExecutorServlet] [Azkaban] executor became inactive before setting up the flow 4azkaban.executor.ExecutorManagerException: executor became inactive before setting up the flow 4 at azkaban.execapp.FlowRunnerManager.createFlowRunner(FlowRunnerManager.java:403) at azkaban.execapp.FlowRunnerManager.submitFlow(FlowRunnerManager.java:347) at azkaban.execapp.ExecutorServlet.handleAjaxExecute(ExecutorServlet.java:288) at azkaban.execapp.ExecutorServlet.handleRequest(ExecutorServlet.java:136) at azkaban.execapp.ExecutorServlet.doPost(ExecutorServlet.java:93) at javax.servlet.http.HttpServlet.service(HttpServlet.java:727) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)]]></content>
  </entry>
  <entry>
    <title><![CDATA[rocketmq基准测试_未完]]></title>
    <url>%2Fmessage-queue%2Frocketmq%2Frocketmq-benchmark-test%2F</url>
    <content type="text"></content>
      <categories>
        <category>mq</category>
        <category>rocketmq</category>
      </categories>
      <tags>
        <tag>基准测试</tag>
        <tag>rocketmq</tag>
        <tag>压力测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux页缓存和文件io]]></title>
    <url>%2Flinux%2Flinux-page-cache-and-file-io%2F</url>
    <content type="text"><![CDATA[​ 首先明确的一点是，本文所述的是针对linux引入了虚拟内存管理机制以后所涉及的知识点。linux中页缓存的本质就是对于磁盘中的部分数据在内存中保留一定的副本，使得应用程序能够快速的读取到磁盘中相应的数据，并实现不同进程之间的数据共享。因此，linux中页缓存的引入主要是为了解决两类重要的问题： 磁盘读写速度较慢（ms 级别); 实现不同进程之间或者同一进程的前后不同部分之间对于数据的共享； ​ 如果没有进程之间的共享机制，那么对于系统中所启动的所有进程在打开文件的时候都要将需要的数据从磁盘加载进物理内存空间，这样不仅造成了加载速度变慢（每次都从磁盘中读取数据），而且造成了物理内存的浪费。为了解决以上问题，linux操作系统使用了缓存机制。在虚拟内存机制出现以前，操作系统使用块缓存机制，但是在虚拟内存出现以后操作系统管理IO的粒度更大，因此采用了页缓存机制。此后，和后备存储的数据交互普遍以页为单位。页缓存是基于页的、面向文件的一种缓存机制。 ​ 说到这里，我们只是对于页缓存的重要性做了介绍。但是，还有三个问题（当然也是本文的重点）还没有解释，分别如下： 页缓存究竟是如何实现，其和文件系统是如何关联的？ 页缓存、内存以及文件IO之间的关系是怎样的？ 页缓存中的数据如何实现和后备存储之间的同步？ 接下来我们将对这三个问题进行详细的解释。 页缓存的实现​ 既然页缓存是以页为单位进行数据管理的，那么必须在内核中标识该物理页。其实每个真正存放数据的物理页帧都对应一个管理结构体，称之为struct page，其结构体如下。 12345678910struct page &#123; unsigned long flags; atomic_t _count; atomic_t _mapcount; unsigned long private; struct address_space *mapping; pgoff_t index; struct list_head lru; void* virtual;&#125;; 下面详细介绍一下物理页结构体中各个成员的含义： flags: 描述page当前的状态和其他信息，如当前的page是否是脏页PG_dirty；是否是最新的已经同步到后备存储的页PG_uptodate; 是否处于lru链表上等； _count：引用计数，标识内核中引用该page的次数，如果要操作该page，引用计数会+1，操作完成之后-1。当该值为0时，表示没有引用该page的位置，所以该page可以被解除映射，这在内存回收的时候是有用的； _mapcount: 页表被映射的次数，也就是说page同时被多少个进程所共享，初始值为-1，如果只被一个进程的页表映射了，该值为0。 _mapping有三种含义：a. 如果mapping = 0，说明该page属于交换缓存（swap cache); 当需要地址空间时会指定交换分区的地址空间swapper_space;b. 如果mapping != 0, bit[0] = 0, 说明该page属于页缓存或者文件映射，mapping指向文件的地址空间address_space；c. 如果mapping != 0, bit[0] !=0 说明该page为匿名映射，mapping指向struct anon_vma对象； 注意区分_count和_mapcount，_mapcount表示的是被映射的次数，而_count表示的是被使用的次数；被映射了不一定被使用，但是被使用之前肯定要先被映射）。 index: 在映射的虚拟空间（vma_area)内的偏移；一个文件可能只是映射了一部分，假设映射了1M的空间，那么index指的是1M空间内的偏移，而不是在整个文件内的偏移； private : 私有数据指针； lru:当page被用户态使用或者是当做页缓存使用的时候，将该page连入zone中的lru链表，供内存回收使用； ​ 页缓存就是将一个文件在内存中的所有物理页所组成的一种树形结构，我们称之为基数树，用于管理属于同一个文件在内存中的缓存内容。 ​ 如上所述，一个文件在内存中对应的所有物理页组成了一棵基数树。而一个文件在内存中具有唯一的inode结构标识，inode结构中有该文件所属的设备及其标识符，因而，根据一个inode能够确定其对应的后备设备。为了将文件在物理内存中的页缓存和文件及其后备设备关联起来，linux内核引入了address_space结构体。可以说address_space结构体是将页缓存和文件系统关联起来的桥梁，其组成如下： 123456789101112131415161718struct address_space &#123; struct inode* host;/*指向与该address_space相关联的inode节点*/ struct radix_tree_root page_tree;/*所有页形成的基数树根节点*/ spinlock_t tree_lock;/*保护page_tree的自旋锁*/ unsigned int i_map_writable;/*VM_SHARED的计数*/ struct prio_tree_root i_map; struct list_head i_map_nonlinear; spinlock_t i_map_lock;/*保护i_map的自旋锁*/ atomic_t truncate_count;/*截断计数*/ unsigned long nrpages;/*页总数*/ pgoff_t writeback_index;/*回写的起始位置*/ struct address_space_operation* a_ops;/*操作表*/ unsigned long flags;/*gfp_mask掩码与错误标识*/ struct backing_dev_info* backing_dev_info;/*预读信息*/ spinlock_t private_lock;/*私有address_space锁*/ struct list_head private_list;/*私有address_space链表*/ struct address_space* assoc_mapping;/*相关的缓冲*/&#125; 下面对address_space成员中的变量做相关的解释： host: 指向与该address_space相关联的inode节点，inode节点与address_space之间是一一对应关系； struct radix_tree_root:指向的host文件在该内存中映射的所有物理页形成的基数树的根节点，参考博客。 struct prio_tree_root:与该地址空间相关联的所有进程的虚拟地址区间vm_area_struct所对应的整个进程地址空间mm_struct形成的优先查找树的根节点;vm_area_struct中如果有后备存储，则存在prio_tree_node结构体，通过该prio_tree_node和prio_tree_root结构体，构成了所有与该address_space相关联的进程的一棵优先查找树，便于查找所有与该address_space相关联的进程； 下面列出struct prio_tree_root和struct prio_tree_node的结构体。 1234567891011struct prio_tree_root &#123; struct prio_tree_node* prio_tree_root; unsigned short index_bits;&#125;;struct prio_tree_node &#123; struct prio_tree_node* left; struct prio_tree_node* right; struct prio_tree_node* parent; unsigned long start; unsigned long last;&#125;; 为了便于形成页缓存、文件和进程之间关系的清晰思路，文章画出一幅图，如图2所示 从以上可以解释可以看出，address_space成为构建页缓存和文件、页缓存和共享该文件的所有进程之间的桥梁。 ​ 每个进程的地址空间使用mm_struct结构体标识，该结构体中包含一系列的由vm_area_struct结构体组成的连续地址空间链表。每个vm_area_struct中存在struct file* vm_file用于指向该连续地址空间中所打开的文件，而vm_file通过struct file中的struct path与struct dentry相关联。 struct dentry中通过inode指针指向inode，inode与address_space一一对应，至此形成了页缓存与文件系统之间的关联；为了便于查找与某个文件相关联的所有进程，address_space中的prio_tree_root指向了所有与该页缓存相关联的进程所形成的优先查找树的根节点。关于这种关系的详细思路请参考图1，这里画出其简化图，如图3： ​ 这里需要说明的linux中文件系统的一点是，内核为每个进程在其地址空间中都维护了结构体struct* fd_array[]用于维护该进程地址空间中打开的文件的指针；同时内核为所有被打开的文件还维护了系统级的一个文件描述符表用以记录该系统打开的所有文件，供所有进程之间共享；每个被打开的文件都由一个对应的inode结构体表示，由系统级的文件描述符表指向。所以，进程通过自己地址空间中的打开文件描述符表可以找到系统级的文件描述符表，进而找到文件。 页缓存、内存、文件IO之间的关系​ 关于文件IO我们常说的两句话“普通文件IO需要复制两次，内存映射文件mmap只需要复制一次”。下面，我们对普通文件IO做详细的解释。文章对页缓存和文件IO做了详细的介绍，不过都是英文的，本文在基于对上文理解、翻译的基础上，加入自己对于页缓存的理解。读者可以选择直接去看对应的英文原版说明。 读​ 为了能够深入理解页缓存和文件IO操作之间的关系，假设系统中现在存在一个名为render的进程，该进程打开了文件scene.dat，并且每次读取其中的512B(一个扇区的大小)，将读取的文件数据放入到堆分配的块中（每个进程自己的地址空间对应的物理内存）。先以普通IO为例介绍一下读取数据的过程，第一次读取的过程大致如图4（图4-8来源于该文章）。 进程发起读请求的过程如下： 进程调用库函数read()向内核发起读文件的请求； 内核通过检查进程的文件描述符定位到虚拟文件系统已经打开的文件列表项，调用该文件系统对VFS的read()调用提供的接口； 通过文件表项链接到目录项模块，根据传入的文件路径在目录项中检索，找到该文件的inode； inode中，通过文件内容偏移量计算出要读取的页； 通过该inode的i_mapping指针找到对应的address_space页缓存树—基数树，查找对应的页缓存节点；（1）如果页缓存节点命中，那么直接返回文件内容；（2）如果页缓存缺失，那么产生一个缺页异常，首先创建一个新的空的物理页框，通过该inode找到文件中该页的磁盘地址，读取相应的页填充该页缓存（DMA的方式将数据读取到页缓存），更新页表项；重新进行第5步的查找页缓存的过程； 文件内容读取成功； ​ 也就是说，所有的文件内容的读取（无论一开始是命中页缓存还是没有命中页缓存）最终都是直接来源于页缓存。当将数据从磁盘复制到页缓存之后，还要将页缓存的数据通过CPU复制到read调用提供的缓冲区中，这就是普通文件IO需要的两次复制数据复制过程。其中第一次是通过DMA的方式将数据从磁盘复制到页缓存中，本次过程只需要CPU在一开始的时候让出总线、结束之后处理DMA中断即可，中间不需要CPU的直接干预，CPU可以去做别的事情；第二次是将数据从页缓存复制到进程自己的的地址空间对应的物理内存中，这个过程中需要CPU的全程干预，浪费CPU的时间和额外的物理内存空间。 假如读取了12KB的数据之后，那么render进程的堆地址空间和相关的地址空间如图5所示（ 读取12KB数据之后，进程render的地址空间和页缓存示意图）： ​ 看起来该过程很简单，但是这其中存在着很多的知识点。首先，render使用了常规的read()系统调用读取了12KB的数据，现在scene.dat中三个大小为4KB的页也存在于页缓存中，就像先前所说的所有的文件IO都是通过页缓存进行的。在X86架构的linux体系中，内核以4KB大小的页为单位组织文件中的数据，所以即使你从一个文件中仅仅读取几个字节的数据，那么包含这些字节的整个页的数据都会从硬盘读入页缓存中。这对于提高硬盘的吞吐量很有帮助，并且用户通常每次读取的数据不仅仅是只有几个字节而已。页缓存记录了每个4KB中的页在文件中的位置，如图中的#0， #1等。 ​ 然而，在一次文件读取的过程中，必须将文件的内容从页缓存拷贝到用户的空间。这个过程和缺页异常(通过DMA调入需要的页)不一样，这个拷贝过程需要通过CPU进行，因此浪费了CPU的时间。另一个弊端就是浪费了物理内存，因为需要为同样的数据在内存中维护两个副本，如图6 render进程的heap所对应的堆中的数据和页缓存中的数据存在重复，并且如果系统中有多个这样的进程的话，那么需要为每个进程维护同样的一份数据副本，严重浪费了CPU的时间和物理内存空间。 ​ 好在，通过内存映射IO—mmap，进程不但可以直接操作文件对应的物理内存，减少从内核空间到用户空间的数据复制过程，同时可以和别的进程共享页缓存中的数据，达到节约内存的作用。关于mmap的实现请参考博客。 ​ 当映射一个文件到内存中的时候，内核将虚拟地址直接映射到页缓存中。正如博客4中介绍的，当映射一个文件的时候，如果文件的内容不在物理内存中，操作系统不会将所映射的文件部分的全部内容直接拷贝到物理内存中，而是在使用虚拟地址访问物理内存的时候通过缺页异常将所需要的数据调入内存中。如果文件本身已经存在于页缓存中，则不再通过磁盘IO调入内存。如果采用共享映射的方式，那么数据在内存中的布局如图6所示（文件共享映射示意图）： 写​ 由于页缓存的架构，当一个进程调用write系统调用的时候，对于文件的更新仅仅是被写到了文件的页缓存中，相应的页被标记为dirty。具体过程如下： 前面5步和读文件是一致的，在address_space中查询对应页的页缓存是否存在： 如果页缓存命中，直接把文件内容修改写在页缓存的页中。写文件就结束了。这时候文件修改位于页缓存，并没有写回到磁盘文件中去。 如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过inode找到该文件页的磁盘地址，读取相应的页填充页缓存。此时缓存页命中，进行第6步。 ​ 普通的IO操作需要将写的数据从自己的进程地址空间复制到页缓存中，完成对页缓存的写入；但是mmap通过虚拟地址（指针）可以直接完成对页缓存的写入，减少了从用户空间到页缓存的复制。 ​ 由于写操作只是写到了页缓存中，因此进程并没有被阻塞到磁盘IO发生，因此当计算机崩溃的时候，写操作所引起的改变可能并没有发生在磁盘上。所以，对于一些要求严格的写操作，比如数据库系统，就需要调用fsync等操作及时将数据同步到磁盘上（虽然这中间也可能存在磁盘的驱动程序崩溃的情况）。读操作与写不同，一般会阻塞到进程读取到数据（除非调用非阻塞IO，即使使用IO多路复用技术也是将进程阻塞在多个监听描述符上，本质上还是阻塞的）。为了减轻读操作的这种延迟，linux操作系统的内核使用了”预读”技术，也就是当从磁盘中读取你所需要的数据的时候，内核将会多读取一些页到页缓存中。 ​ 普通文件IO中所有的文件内容的读取（无论一开始是命中页缓存还是没有命中页缓存）最终都是直接来源于页缓存。当将数据通过缺页中断从磁盘复制到页缓存之后，还要将页缓冲的数据通过CPU复制到read调用提供的缓冲区中。这样，必须通过两次数据拷贝过程，才能完成用户进程对文件内容的获取任务。写操作也是一样的，待写入的buffer在用户空间，必须将其先拷贝到内核空间对应的主存中，再写回到磁盘中，也是需要两次数据拷贝。mmap的使用减少了数据从用户空间到页缓存的复制过程，提高了IO的效率，尤其是对于大文件而言；对于比较小的文件而言，由于mmap执行了更多的内核操作，因此其效率可能比普通的文件IO更差。 ​ 在专门介绍mmap的博客中，我们说文件映射分为私有映射(private)和共享映射（shared)两种，二者之间的区别就是一个进程对文件所做的改变能否被其他的进程所看到，且能否同步到后备的存储介质中。那么，如果一个进程仅仅是读取文件中的内容的话，那么共享映射和私有映射对应的物理内存布局如图5所示。但是，如果采用私有映射的方式，且一个进程对文件内容作出了改变，那么会发生怎样的情况呢？内核采用了写时复制技术完成私有映射下对文件内容的改动，下面举例说明。 ​ 假设系统中存在两个进程分别为render和render3d，它们都私有映射同一个文件scene.dat到内存中，然后render进程对映射的文件做出了写操作，如图7所示（私有映射写文件示意图）： ​ 图6中的“只读”标志不是说映射的内容是只读的，这仅仅是内核为了节省物理内存而采用的对于物理内存的一种“欺骗手段”而已。如果两个进程只是读取文件中的内容，不做任何的改动，那么文件只在物理内存中保留一份；但是如果有一个进程，如render，要对文件中的内容做出改动，那么会触发缺页中断，内核采用写时复制技术，为要改动的内容对应的页重新分配一个物理页框，将并将被改动的内容对应的物理页框中的数据复制到新分配的物理页框中，再进行改动。此时新分配的物理页框对于render而言是它自己“私有的”，别的进程是看不到的，也不会被同步到后备的存储中。但是如果是共享映射，所有的进程都是共享同一块页缓存的，此时被映射的文件的数据在内存中只保留一份。任何一个进程对映射区进行读或者写，都不会导致对页缓冲数据的复制。 ​ mmap的系统调用函数原型为void* mmap(void* addr, size_t len, int prot, int flag, int fd, off_t off)。其中，flag指定了是私有映射还是共享映射，私有映射的写会引发缺页中断，然后复制对应的物理页框到新分配的页框中。prot指定了被映射的文件是可读、可写、可执行还是不可访问。如果prot指定的是可读，但是却对映射文件执行写操作，则此时却缺页中断会引起段错误，而不是进行写时复制。 ​ 那么此时存在另一个问题就是当最后一个render进程退出之后，存储scene.dat的页缓存是不是会被马上释放掉？当然不是！在一个进程中打开一个文件使用完之后该进程退出，然后在另一个进程中使用该文件这种情况通常是很常见的，页缓存的管理中必须考虑到这种情况。况且从页缓存中读取数据的时间是ns级别，但是从硬盘中读取数据的时间是ms级别，因此如果能够在使用数据的时候命中页缓存，那么对于系统的性能将非常有帮助。那么，问题来了，什么时候该文件对应的页缓存要被换出内存呢?就是系统中的内存紧张，必须要换出一部分物理页到硬盘中或者交换区中，以腾出更多的空间给即将要使用的数据的时候。所以只要系统中存在空闲的内存，那么页缓存就不会被换出，直到到达页缓存的上限为止。是否换出某一页缓存不是由某一个进程决定的，而是由操作系统在整个系统空间中的资源分配决定的。毕竟，从页缓存中读取数据要比从硬盘上读取数据要快的多了。 ​ 内存映射的一个典型应用就是动态共享库的加载。图8（进程动态共享库的加载）展示了两个同一份程序的两个实例使用动态共享库时，进程的虚拟地址空间及对应的物理内存空间的布局。 页缓存中数据如何实现和后备存储之间的同步？​ 普通文件IO，都是将数据直接写在页缓存上，那么页缓存中的数据何时写回后备存储？怎么写回？ 何时写回 空闲内存的值低于一个指定的阈值的时候，内核必须将脏页写回到后备存储以释放内存。因为只有干净的内存页才可以回收。当脏页被写回之后就变为PG_uptodate标志，变为干净的页，内核就可以将其所占的内存回收； 当脏页在内存中驻留的时间超过一个指定的阈值之后，内核必须将该脏页写回到后备存储，以确定脏页不会在内存中无限期的停留； 当用户进程显式的调用fsync、fdatasync或者sync的时候，内核按照要求执行回写操作。 由谁写回 为了能够不阻塞写操作，并且将脏页及时的写回后备存储。linux在当前的内核版本中使用了flusher线程负责将脏页回写。 为了满足第一个何时回写的条件，内核在可用内存低于一个阈值的时候唤醒一个或者多个flusher线程，将脏页回写； 为了满足第二个条件，内核将通过定时器定时唤醒flusher线程，将所有驻留时间超时的脏页回写。 原文链接：https://blog.csdn.net/GDJ0001/article/details/80136364 页面置换算法操作系统将内存按照页的进行管理，在需要的时候才把进程相应的部分调入内存。当产生缺页中断时，需要选择一个页面写入。如果要换出的页面在内存中被修改过，变成了“脏”页面，那就需要先写会到磁盘。页面置换算法，就是要选出最合适的一个页面，使得置换的效率最高。页面置换算法有很多，简单介绍几个，重点介绍比较重要的LRU及其实现算法。 一、最优页面置换算法最理想的状态下，我们给页面做个标记，挑选一个最远才会被再次用到的页面。当然，这样的算法不可能实现，因为不确定一个页面在何时会被用到。 二、最近未使用页面置换算法（NRU）系统为每一个页面设置两个标志位：当页面被访问时设置R位，当页面（修改）被写入时设置M位。当发生缺页中断时，OS检查所有的页面，并根据它们当前的R和M位的值，分为四类： （1）！R&amp;！M（2）！R&amp;M（3）R&amp;！M（4）R&amp;M 编号越小的类，越被优先换出。即在最近的一个时钟滴答内，淘汰一个没有被访问但是已经被修改的页面，比淘汰一个被频繁使用但是“clean”的页面要好。 三、先进先出页面置换算法（FIFO）及其改进这种算法的思想和队列是一样的，OS维护一个当前在内存中的所有页面的链表，最新进入的页面在尾部，最久的在头部，每当发生缺页中断，就替换掉表头的页面并且把新调入的页面加入到链表末尾。 这个算法的问题，显然是太过于“公正了”，没有考虑到实际的页面使用频率。 一种合理的改进，称为第二次机会算法。即给每个页面增加一个R位，每次先从链表头开始查找，如果R置位，清除R位并且把该页面节点放到链表结尾；如果R是0，那么就是又老又没用到，替换掉。 四、时钟页面置换算法（clock）这种算法只是模型像时钟，其实就是一个环形链表的第二次机会算法，表针指向最老的页面。缺页中断时，执行相同的操作，包括检查R位等。 五、最近最少使用页面置换算法（LRU）缺页中断发生时，置换未使用时间最长的页面，称为LRU（least recently used）。 LRU是可以实现的，需要维护一个包含所有页面的链表，并且根据实时使用情况更新这个链表，代价是比较大的。 于是，需要对这个算法进行一些改进，也可以说是简化。将每一个页面与一个计数器关联，每次时钟终端，扫描所有页面，将每个页面的R位加到计数器上，这样就大致跟踪了每个页面的使用情况。这种方法称为NFU（not frequently used，最不常用）算法。 这样还是存在一个问题，即很久之前的一次使用，与最近的使用权重相等。 所以，再次改进，将计数器在每次时钟滴答时，右移一位，并把R位加在最高位上。这种算法，称为老化（aging）算法，增加了最近使用的比重。 老化算法只能采用有限的位数，所以可能在一定程度上精度会有所损失。 六、工作集算法简单来说，工作集就是在最近k次内存访问所使用过的页面的集合。原始的工作集算法同样代价很大，对它进行简化：在过去Nms的北村访问中所用到的页面的集合。 所以，在实现的时候，可以给每个页面一个计时器。需要置换页面时，同实际时间进行对比，R为1，更新到现在时间；R为0，在规定阈值之外的页面可以被置换。 同样，这个算法也可以用时钟的思想进行改进。 七、Linux使用的页面置换算法Linux区分四种不同的页面：不可回收的、可交换的、可同步的、可丢弃的。 不可回收的：保留的和锁定在内存中的页面，以及内核态栈等。 可交换的：必须在回收之前写回到交换区或者分页磁盘分区。 可同步的：若为脏页面，必须要先写回。 可丢弃的：可以被立即回收的页面。 Linux并没有像我们之前单纯讨论算法时那样，在缺页中断产生的时候才进行页面回收。Linux有一个守护进程kswapd，比较每个内存区域的高低水位来检测是否有足够的空闲页面来使用。每次运行时，仅有一个确定数量的页面被回收。这个阈值是受限的，以控制I/O压力。 每次执行回收，先回收容易的，再处理难的。回收的页面会加入到空闲链表中。 算法是一种改进地LRU算法，维护两组标记：活动/非活动和是否被引用。第一轮扫描清除引用位，如果第二轮运行确定被引用，就提升到一个不太可能回收的状态，否则将该页面移动到一个更可能被回收的状态。 处于非活动列表的页面，自从上次检查未被引用过，因而是移除的最佳选择。被引用但不活跃的页面同样会被考虑回收，是因为一些页面是守护进程访问的，可能很长时间不再使用。 另外，内存管理还有一个守护进程pdflush，会定期醒来，写回脏页面；或者可用内存下降到一定水平后被内核唤醒。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>页缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树、B-树、B+树、B*树之间的关系]]></title>
    <url>%2Falgorithm%2Fvarious-b-tree-compare%2F</url>
    <content type="text"><![CDATA[B树​ B-tree树即B树，B即Balanced，平衡的意思。因为B树的原英文名称为B-tree，而国内很多人喜欢把B-tree译作B-树，其实，这是个非常不好的直译，很容易让人产生[误解]。如人们可能会以为B-树是一种树，而B树又是另一种树。而事实上是，B-tree就是指的B树。特此说明。 二叉搜索树1.所有非叶子结点至多拥有两个儿子（Left和Right）； 2.所有结点存储一个关键字； 3.非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树； 如： 二叉搜索树从根结点开始，如果查询的关键字与结点的关键字相等，那么就命中； 否则，如果查询关键字比结点关键字小，就进入左儿子； 如果比结点关键字大，就进入右儿子; 如果左儿子或右儿子的指针为空，则报告找不到相应的关键字； ​ 如果二叉搜索树的所有非叶子结点的左右子树的结点数目均保持差不多（平衡），那么B树 的搜索性能逼近二分查找；但它比连续内存空间的二分查找的优点是，改变二叉搜索树结构 （插入与删除结点）不需要移动大段的内存数据，甚至通常是常数开销； 但二叉搜索树在经过多次插入与删除后，有可能导致不同的结构： 右边也是一个二叉搜索树，但它的搜索性能已经是线性的了；同样的关键字集合有可能导致不同的 树结构索引；所以，使用二叉搜索树还要考虑尽可能让B树保持左图的结构，和避免右图的结构，也就 是所谓的“平衡”问题； 平衡二叉树​ 实际使用的二叉搜索树都是在原二叉搜索树的基础上加上平衡算法，即“平衡二叉树”；如何保持B树 结点分布均匀的平衡算法是平衡二叉树的关键；平衡算法是一种在二叉搜索树中插入和删除结点的 策略； 红黑树红黑树是一种含有红黑结点并能自平衡的二叉查找树。它必须满足下面性质： 每个节点要么是黑色，要么是红色。 根节点是黑色。 每个叶子节点（NIL）是黑色。 每个红色结点的两个子结点一定都是黑色。 任意一结点到每个叶子结点的路径都包含数量相同的黑结点。 从性质5又可以推出： 性质5.1：如果一个结点存在黑子结点，那么该结点肯定有两个子结点图1就是一颗简单的红黑树。其中Nil为叶子结点，并且它是黑色的。(值得提醒注意的是，在Java中，叶子结点是为null的结点。) 红黑树并不是一个完美平衡二叉查找树，从图1可以看到，根结点P的左子树显然比右子树高，但左子树和右子树的黑结点的层数是相等的，也即任意一个结点到到每个叶子结点的路径都包含数量相同的黑结点(性质5)。所以我们叫红黑树这种平衡为黑色完美平衡。 介绍到此，为了后面讲解不至于混淆，我们还需要来约定下红黑树一些结点的叫法，如图2所示 B树（B-树）B树（B-树）是一种多路搜索树（并不是二叉的）： 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； 如：（M=3） B-树的搜索​ 从根结点开始，对结点内的关键字（有序）序列进行二分查找，如果命中则结束，否则进入查询关键字所属范围的儿子结点；重复，直到所对应的儿子指针为空，或已经是叶子结点； B-树的特性 关键字集合分布在整颗树中； 任何一个关键字出现且只出现在一个结点中； 搜索有可能在非叶子结点结束； 其搜索性能等价于在关键字全集内做一次二分查找； 自动层次控制； 由于限制了除根结点以外的非叶子结点，至少含有M/2个儿子，确保了结点的至少利用率，其最低搜索性能为： ​ 其中，M为设定的非叶子结点最多子树个数，N为关键字总数；所以B-树的性能总是等价于二分查找（与M值无关），也就没有B树平衡的问题； ​ 由于M/2的限制，在插入结点时，如果结点已满，需要将结点分裂为两个各占M/2的结点；删除结点时，需将两个不足M/2的兄弟结点合并； B+树B+树是B-树的变体，也是一种多路搜索树： 其定义基本与B-树同，除了： 非叶子结点的子树指针与关键字个数相同； 非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； 为所有叶子结点增加一个链指针； 所有关键字都在叶子结点出现； 如：（M=3） B+与B-树区别B+树只有达到叶子结点才命中（B-树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找； B+的特性 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 不可能在非叶子结点命中； 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 更适合文件索引系统； B*树B*树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针； B树定义了非叶子结点关键字个数至少为(2/3)M，即块的最低使用率为2/3（代替B+树的1/2）； B+树的分裂当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； B*树的分裂当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针； 所以，B*树分配新结点的概率比B+树要低，空间使用率更高； 小结二叉搜索树：二叉树，每个结点只存储一个关键字，等于则命中，小于走左结点，大于走右结点； B（B-）树：多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键字范围的子结点；所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中； B+树：在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中； B*树：在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3；]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>B树</tag>
        <tag>B+树</tag>
        <tag>B-树</tag>
        <tag>红黑树</tag>
        <tag>二叉搜索树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘IO和网络IO详解]]></title>
    <url>%2Foperating-system%2Fdisk-io-and-socket-io%2F</url>
    <content type="text"><![CDATA[1、缓存 IO​ 缓存 I/O 又称 标准 I/O，大都数文件系统的默认 I/O 操作 都是操作缓存 I/O。在Linux的缓存 I/O 机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到 应用程序的地址空间。 ​ 读操作：操作系统检查内核的缓存区 有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓冲中。 ​ 写操作：将数据从用户空间复制到内核空间的缓冲区中。这时对用户程序来说写操作已经完成，至于什么时候再写到磁盘中由操作系统决定，除非显式调用了sync同步命令（详情操作《【珍藏】linux 同步IO: sync、fsync与fdatasync》） ​ 缓存I/O的优点：① 在一定程度上分离了内核空间和用户空间，保护系统本身的运行安全；② 可以减少读盘的次数，从而提高性能； ​ 缓冲I/O的缺点：在缓存 I/O 机制中，DMA方式可以将数据直接从磁盘读到页缓存中，或者将数据从页缓存直接写回到磁盘上，而不能直接在应用程序地址空间和磁盘之间进行数据传输，这样，数据在传输过程中需要在应用程序的地址空间（用户空间）和缓存（内核空间）之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。 2、直接IO​ 直接 I/O 就是应用程序直接访问磁盘数据，而不经过内核缓冲区，这样做的目的是减少一次内核缓冲区到用户程序缓存的数据复制。比如说 数据库管理系统这类应用，他们更倾向于选择他们自己的缓存机制，因为数据库管理系统往往比操作系统更了解数据库中存在的数据，数据库管理系统可以提供一种更加有效的缓存机制来提高数据库中数据的存取性能。 ​ 直接IO的缺点：如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘加载，这种加载会非常缓慢。通常 直接IO与异步IO结合使用，会得到比较好的性能。 ​ 异步IO：当访问数据的线程发出请求后，线程接着去处理其他事，而不是阻塞等待； ​ 下图分析了写场景下的 DirectIO 和 BufferIO： 首先，磁盘IO主要的延时是由（以15000rpm硬盘为例）： 机械转动延时（机械磁盘的主要性能瓶颈，平均为2ms） + 寻址延时（2~3ms） + 块传输延时（一般4k每块，40m/s的传输速度，延时一般为0.1ms) 决定。（平均为5ms） 而网络IO主要延时由： 服务器响应延时 + 带宽限制 + 网络延时 + 跳转路由延时 + 本地接收延时 决定。（一般为几十到几千毫秒，受环境干扰极大） 所以两者一般来说网络IO延时要大于磁盘IO的延时。 PIO与DMA有必要简单的说说慢速I/O设备和内存之间的数据传输方式。 PIO我们拿磁盘来说，很早以前，磁盘和内存之间的数据传输是需要CPU控制的，也就是说如果我们读取磁盘文件到内存中，数据要经过CPU存储转发，这种方式称为PIO。显然这种方式非常不合理，需要占用大量的CPU时间来读取文件，造成文件访问时系统几乎停止响应。 DMA后来，DMA（直接内存访问，Direct Memory Access）取代了PIO，它可以不经过CPU而直接进行磁盘和内存的数据交换。在DMA模式下，CPU只需要向DMA控制器下达指令，让DMA控制器来处理数据的传送即可，DMA控制器通过系统总线来传输数据，传送完毕再通知CPU，这样就在很大程度上降低了CPU占有率，大大节省了系统资源，而它的传输速度与PIO的差异其实并不十分明显，因为这主要取决于慢速设备的速度。 可以肯定的是，PIO模式的计算机我们现在已经很少见到了。 标准文件访问方式 具体步骤： 当应用程序调用read接口时，操作系统检查在内核的高速缓存有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回，如果没有，则从磁盘中读取，然后缓存在操作系统的缓存中。 应用程序调用write接口时，将数据从用户地址空间复制到内核地址空间的缓存中，这时对用户程序来说，写操作已经完成，至于什么时候再写到磁盘中，由操作系统决定，除非显示调用了sync同步命令。 内存映射 减少数据在用户空间和内核空间之间的拷贝操作,适合大量数据传输 Linux内核提供一种访问磁盘文件的特殊方式，它可以将内存中某块地址空间和我们要指定的磁盘文件相关联，从而把我们对这块内存的访问转换为对磁盘文件的访问，这种技术称为内存映射（Memory Mapping）。 操作系统将内存中的某一块区域与磁盘中的文件关联起来，当要访问内存中的一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据从内核空间缓存到用户空间缓存的数据复制操作，因为这两个空间的数据是共享的。 内存映射是指将硬盘上文件的位置与进程逻辑地址空间中一块大小相同的区域一一对应，当要访问内存中一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据在用户空间和内核空间之间的拷贝操作。当大量数据需要传输的时候，采用内存映射方式去访问文件会获得比较好的效率。 使用内存映射文件处理存储于磁盘上的文件时，将不必再对文件执行I/O操作，这意味着在对文件进行处理时将不必再为文件申请并分配缓存，所有的文件缓存操作均由系统直接管理，由于取消了将文件数据加载到内存、数据从内存到文件的回写以及释放内存块等步骤，使得内存映射文件在处理大数据量的文件时能起到相当重要的作用。 在大多数情况下，使用内存映射可以提高磁盘I/O的性能，它无须使用read()或write()等系统调用来访问文件，而是通过mmap()系统调用来建立内存和磁盘文件的关联，然后像访问内存一样自由地访问文件。有两种类型的内存映射，共享型和私有型，前者可以将任何对内存的写操作都同步到磁盘文件，而且所有映射同一个文件的进程都共享任意一个进程对映射内存的修改；后者映射的文件只能是只读文件，所以不可以将对内存的写同步到文件，而且多个进程不共享修改。显然，共享型内存映射的效率偏低，因为如果一个文件被很多进程映射，那么每次的修改同步将花费一定的开销。 直接I/O 绕过内核缓冲区,自己管理I/O缓存区 在Linux 2.6中，内存映射和直接访问文件没有本质上差异，因为数据从进程用户态内存空间到磁盘都要经过两次复制，即在磁盘与内核缓冲区之间以及在内核缓冲区与用户态内存空间。引入内核缓冲区的目的在于提高磁盘文件的访问性能，因为当进程需要读取磁盘文件时，如果文件内容已经在内核缓冲区中，那么就不需要再次访问磁盘；而当进程需要向文件中写入数据时，实际上只是写到了内核缓冲区便告诉进程已经写成功，而真正写入磁盘是通过一定的策略进行延迟的。 然而，对于一些较复杂的应用，比如数据库服务器，它们为了充分提高性能，希望绕过内核缓冲区，由自己在用户态空间实现并管理I/O缓冲区，包括缓存机制和写延迟机制等，以支持独特的查询机制，比如数据库可以根据更加合理的策略来提高查询缓存命中率。另一方面，绕过内核缓冲区也可以减少系统内存的开销，因为内核缓冲区本身就在使用系统内存。 应用程序直接访问磁盘数据，不经过操作系统内核数据缓冲区，这样做的目的是减少一次从内核缓冲区到用户程序缓存的数据复制。这种方式通常是在对数据的缓存管理由应用程序实现的数据库管理系统中。直接I/O的缺点就是如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘进行加载，这种直接加载会非常缓慢。通常直接I/O跟异步I/O结合使用会得到较好的性能。 Linux提供了对这种需求的支持，即在open()系统调用中增加参数选项O_DIRECT，用它打开的文件便可以绕过内核缓冲区的直接访问，这样便有效避免了CPU和内存的多余时间开销。 顺便提一下，与O_DIRECT类似的一个选项是O_SYNC，后者只对写数据有效，它将写入内核缓冲区的数据立即写入磁盘，将机器故障时数据的丢失减少到最小，但是它仍然要经过内核缓冲区。 sendfile/零拷贝 网络I/O，kafka用到此特性 普通的网络传输步骤如下1、操作系统将数据从磁盘复制到操作系统内核的页缓存中；2、应用将数据从内核缓存复制到应用的缓存中；3、应用将数据写回内核的Socket缓存中；4、操作系统将数据从Socket缓存区复制到网卡缓存，然后将其通过网络发出； 解析： 1、当调用read系统调用时，通过DMA（Direct Memory Access）将数据copy到内核模式；2、然后由CPU控制将内核模式数据copy到用户模式下的 buffer中；3、read调用完成后，write调用首先将用户模式下 buffer中的数据copy到内核模式下的socket buffer中；4、最后通过DMA copy将内核模式下的socket buffer中的数据copy到网卡设备中传送； 从上面的过程可以看出，数据白白从内核模式到用户模式走了一圈，浪费了两次copy，而这两次copy都是CPU copy，即占用CPU资源； sendfile 通过sendfile传送文件只需要一次系统调用，当调用sendfile时： 1、首先通过 DMA copy 将数据从磁盘读取到 kernel buffer 中； 2、然后通过 CPU copy 将数据从 kernel buffer 复制到 socket buffer 中； 3、最终通过 DMA copy 将 socket buffer 中的数据复制到 网卡buffer 中； sendfile与read/write方式相比，少了 一次模式切换一次CPU copy。但是从上述过程中也可以发现从kernel buffer中将数据copy到socket buffer是没必要的。 为此，Linux2.4内核对sendfile做了改进，下图所示： 改进后的处理过程如下： 1、DMA copy 将 磁盘数据 复制到 kernel buffer 中； 2、向 socket buffer 中追加当前要发送的数据在 kernel buffer 中的位置和偏移量； 3、DMA gather copy 根据 socket buffer 中的位置和偏移量直接将 kernel buffer 中的数据copy到网卡上； 经过上述过程，数据只经过了2次copy就从磁盘传送出去了。（事实上这个Zero copy是针对内核来讲的，数据在内核模式下是Zero－copy的）。 当前许多高性能http server都引入了sendfile机制，如nginx，lighttpd等。 12// Java中的零拷贝FileChannel.transferTo(long position,long count, WriteableByteChannel target); Java NIO中FileChannel.transferTo(long position, long count, WriteableByteChannel target)方法将当前通道中的数据传送到目标通道target中，在支持Zero-Copy的linux系统中，transferTo()的实现依赖于 sendfile()调用; 整个数据通路涉及4次数据复制和2个系统调用，如果使用sendfile则可以避免多次数据复制，操作系统可以直接将数据从内核页缓存中复制到网卡缓存，这样可以大大加快整个过程的速度; 大多数时候，我们都在向Web服务器请求静态文件，比如图片、样式表等，根据前面的介绍，我们知道在处理这些请求的过程中，磁盘文件的数据先要经过内核缓冲区，然后到达用户内存空间，因为是不需要任何处理的静态数据，所以它们又被送到网卡对应的内核缓冲区，接着再被送入网卡进行发送。 数据从内核出去，绕了一圈，又回到内核，没有任何变化，看起来真是浪费时间。在Linux 2.4的内核中，尝试性地引入了一个称为khttpd的内核级Web服务器程序，它只处理静态文件的请求。引入它的目的便在于内核希望请求的处理尽量在内核完成，减少内核态的切换以及用户态数据复制的开销。 同时，Linux通过系统调用将这种机制提供给了开发者，那就是sendfile()系统调用。它可以将磁盘文件的特定部分直接传送到代表客户端的socket描述符，加快了静态文件的请求速度，同时也减少了CPU和内存的开销。 在OpenBSD和NetBSD中没有提供对sendfile的支持。通过strace的跟踪看到了Apache在处理151字节的小文件时，使用了mmap()系统调用来实现内存映射，但是在Apache处理较大文件的时候，内存映射会导致较大的内存开销，得不偿失，所以Apache使用了sendfile64()来传送文件，sendfile64()是sendfile()的扩展实现，它在Linux 2.4之后的版本中提供。 这并不意味着sendfile在任何场景下都能发挥显著的作用。对于请求较小的静态文件，sendfile发挥的作用便显得不那么重要，通过压力测试，我们模拟100个并发用户请求151字节的静态文件，是否使用sendfile的吞吐率几乎是相同的，可见在处理小文件请求时，发送数据的环节在整个过程中所占时间的比例相比于大文件请求时要小很多，所以对于这部分的优化效果自然不十分明显。]]></content>
      <categories>
        <category>operating-system</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java IO总览]]></title>
    <url>%2Fjava%2Fio%2Fjava-io%2F</url>
    <content type="text"><![CDATA[一、概览Java的 I/O 大概可以分成以下几类： 磁盘操作：File 字节操作：InputStream 和 OutputStream 字符操作：Reader 和 Writer 对象操作：Serializable 网络操作：Socket 非阻塞输入/输出：NIO 二、磁盘操作File 类可以用于表示文件和目录的信息，但是它不表示文件的内容。 递归的列出一个目录下所有文件： 123456789101112public static void listAllFiles(File dir) &#123; if (dir == null || !dir.exists()) &#123; return; &#125; if (dir.isFile()) &#123; System.out.println(dir.getName()); return; &#125; for (File file : dir.listFiles()) &#123; listAllFiles(file); &#125;&#125; 从 Java7 开始，可以用 Paths 和 Files 代替 File。 三、字节操作实现文件复制1234567891011121314151617public static void copyFile(String src, String dist) throws IOException &#123; FileInputStream in = new FileInputStream(src); FileOutputStream out = new FileOutputStream(dist); byte[] buffer = new byte[20 * 1024]; int cnt; // read() 最多读取 buffer.length 个字节 // 返回的是实际读取的个数 // 返回 -1 的时候表示读到 eof，即文件尾 while ((cnt = in.read(buffer, 0, buffer.length)) != -1) &#123; out.write(buffer, 0, cnt); &#125; in.close(); out.close();&#125; 装饰者模式Java I/O 使用了装饰者模式来实现。以 InputStream 为例， InputStream 是抽象组件； FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作； FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能。例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。 实例化一个具有哦缓存功能的字节流对象是，只需要在FileInputStream对象上再套一层 BufferedInputStream 对象即可。 12FileInputStream fileInputStream = new FileInputStream(filePath);BufferedInputStream bufferedInputStream = new BufferedInputStream(fileInputStream) DataInputStream 装饰者提供了对更多数据类型进行输入的操作，比如 int、double 等基本类型。 四、字符操作编码与解码编码就是吧字符转换为字节，而解码是把字节重新组合成字符； 如果编码和解码过程使用不同的编码方式，那么就出现了乱码； GBK 编码中，中文字符占 2 个字节，英文字符占 1 个字节； UTF-8 编码中，中文字符占 3 个字节，英文字符占 1 个字节； UTF-16be 编码中，中文字符和英文字符各占 2 个字节； UTF-16be 中的 be 指的是 Big Endian，也就是大端。相应的也有 UTF-16le，le 指的是 Little Endian，也就是小端； Java 的内存编码使用 双字节编码 UTF-16be，这不是指 Java 只支持这一种编码方式，而是说 char 这种类型使用 UTF-16be 进行编码。char 类型占16 位，也就是两个字节，Java 使用这种双字节编码是为了让一个中文或者一个英文 都能使用 一个 char 来存储； String 的编码方式String 可以看成是一个字符序列，可以指定一个编码方式，将它编码为字节序列，也可以指定一个编码方式 将一个字节序列 解码为 String。 1234String str1 = "中文";byte[] bytes = str1.getBytes("UTF-8");String str2 = new String(bytes, "UTF-8");System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是 可以使用一个 char 存储中文和英文，而将 String转化为 bytes[] 字节数组 就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。 1byte[] bytes = str1.getBytes(); Reader 和 Writer不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符。但是在程序中操作的通常是字符解释的数据，英雌需要提供对字符进行操作的方法。 InputStreamReader 实现从 字节流 解码成 字符流； OutputStreamWriter 实现从 字符流 编码成 字节流； 实现逐行输出文本文件的内容1234567891011121314public static void readFileContent(String filePath) throws IOException &#123; FileReader fileReader = new FileReader(filePath); BufferedReader bufferedReader = new BufferedReader(fileReader); String line; while ((line = bufferedReader.readLine()) != null) &#123; System.out.println(line); &#125; // 装饰者模式使得 BufferedReader 组合了一个 Reader 对象 // 在调用 BufferedReader 的 close() 方法时会去调用 Reader 的 close() 方法 // 因此只要一个 close() 调用即可 bufferedReader.close();&#125; 五、对象操作序列化序列化就是一个将对象转换成字节序列，方便存储和传输 序列化：ObjectOutputStream.writeObject() 反序列化：ObjectInputStream.readObject() 不会对静态变量进行序列化，因为序列化只是保存对象的状态，静态变量属于类的状态。 Serializable序列化的类需要实现 Serializable 接口，它只是一个标准，没有任何方法需要实现，但是如果不如实现它的话而进行序列化，会抛出异常。 1234567891011121314151617181920212223242526272829public static void main(String[] args) throws IOException, ClassNotFoundException &#123; A a1 = new A(123, "abc"); String objectFile = "file/a1"; ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(objectFile)); objectOutputStream.writeObject(a1); objectOutputStream.close(); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(objectFile)); A a2 = (A) objectInputStream.readObject(); objectInputStream.close(); System.out.println(a2);&#125;private static class A implements Serializable &#123; private int x; private String y; A(int x, String y) &#123; this.x = x; this.y = y; &#125; @Override public String toString() &#123; return "x = " + x + " " + "y = " + y; &#125;&#125; transienttransient 关键字 可以使一些属性不会被序列化。 ArrayList中存储数组的 elementData 是用 transient 修饰的，因为这个数组是动态扩展的，并不是所有的空间都被使用，因此就不需要所有的内容都被序列化。通过重写 序列化 和 反序列化方法，使得可以只序列化数组中有内容的那部分数据。 1private transient Object[] elementData; 六、网络操作Java中的网络支持： InetAddress： 用于表示网络上的硬件资源，即 IP 地址； URL：统一资源定位符； Sockets：使用 TCP 协议实现网络通信； Datagram：使用 UDP 协议实现网络通信； InetAddress没有共有的构造函数，只能通过静态方法来创建实例 12InetAddress.getByName(String host);InetAddress.getByAddress(byte[] address); URL可以直接从 URL 中读取字节流数据 1234567891011121314public static void main(String[] args) throws IOException &#123; URL url = new URL("http://www.baidu.com"); /* 字节流 */ InputStream is = url.openStream(); /* 字符流 */ InputStreamReader isr = new InputStreamReader(is, "utf-8"); /* 提供缓存功能 */ BufferedReader br = new BufferedReader(isr); String line; while ((line = br.readLine()) != null) &#123; System.out.println(line); &#125; br.close();&#125; Sockets ServerSocket：服务器端类； Socket：客户端类； 服务器 和 客户端 通过 InputStream 和 OutputStream 进行输入输出； Datagram DatagramSocket：通信类； DatagramPacket：数据包类； 七、NIO新的输入/输出（NIO）库是在JDK 1.4 中引入的，弥补了原来的 I/O 的不足，提供了高速的，面向块的 I/O。 流与块I/O 与 NIO 最重要的区别是：数据打包和传输的方式，I/O以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 一次处理一个字节数据：一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。 面向块的 I/O 一次处理一个数据块，按块处理数据 比 按流处理数据要快的多。但面向块的 I/O 缺少一些面向流的 I/O 所具有的 优雅性 和 简单性。 I/O 包 和 NIO 已经很好的集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统汇总，处理速度也会更快。 通道与缓冲区1、通道通道 Channel 是 对原 I/O 包中流的模拟，可以通过它读取和写入数据。 通道与流的不同之处在于，流只能在一个方向上移动（一个流必须是InputStream 或者 OutputStream 的子类），而通道是双向的，可以用于读、写或者同时用于读写。 通道包括以下类型： FileChannel：从文件读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个SocketChannel； 2、缓冲区发送给一个通道的所有数据都必须首先放到缓冲区中，同样的，从通道中读取的任何数据都要先读到缓冲区中。也就是说，不会直接对通道进行读写数据，而是先经过缓冲区。 缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 缓冲区包括以下类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 缓冲区状态变量 capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数； 状态变量的改变过程举例： ① 新建一个大小为 8 个字节的缓冲区，此时 position 为 0， 而 limit = capacity = 8。capacity 变量不会改变，下面的讨论会忽略它; ② 从输入通道中读取 5个字节数据写入缓冲区中，此时 position 为 5，limit 保持不变; ③ 在将缓冲区的数据写到输出通道之前，需要先调用 flip() 方法，这个方法将 limit 设置为 当前position，并将 position 设置为 0; ④ 从缓冲区中取 4个字节到输出缓冲中，此时 position 设为 4； ⑤ 最后需要调用 clear() 方法来清空缓冲区，此时 position 和 limit 都被设置为 最初位置。 文件 NIO 实例以下展示了使用 NIO 快速复制文件的实例： 1234567891011121314151617181920212223242526public static void fastCopy(String src, String dist) throws IOException &#123; /* 获得源文件的输入字节流 */ FileInputStream fin = new FileInputStream(src); /* 获取输入字节流的文件通道 */ FileChannel fcin = fin.getChannel(); /* 获取目标文件的输出字节流 */ FileOutputStream fout = new FileOutputStream(dist); /* 获取输出字节流的文件通道 */ FileChannel fcout = fout.getChannel(); /* 为缓冲区分配 1024 个字节 */ ByteBuffer buffer = ByteBuffer.allocateDirect(1024); while (true) &#123; /* 从输入通道中读取数据到缓冲区中 */ int r = fcin.read(buffer); /* read() 返回 -1 表示 EOF */ if (r == -1) &#123; break; &#125; /* 切换读写 */ buffer.flip(); /* 把缓冲区的内容写入输出文件中 */ fcout.write(buffer); /* 清空缓冲区 */ buffer.clear(); &#125;&#125; 选择器SelectorNIO 常常被叫做 非阻塞 IO，主要是因为 NIO 在网络中的非阻塞特性被广泛使用。 NIO 实现了 IO 多路复用中的 Reactor 模型，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel上的事件，从而让一个线程就可以处理多个事件。 通过配置监听的通道 Channel 为非阻塞， 那么当Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮序其他 Channel，找到 IO 事件已经到达的 Channel 执行。 因为创建 和 切换线程的开销很大，因此使用一个线程来处理 多个事件 而不是一个线程处理一个事件，对于 IO 密集型 应用 具有很好的性能。 应该注意的是，只有套接字 Channel 才能配置为 非阻塞，而 FileChannel 不能，为 FileChannel 配置非阻塞也没有意义。 1、创建选择器1Selector selector = Selector.open(); 2、将通道注册到选择器上123ServerSocketChannel ssChannel = ServerSocketChannel.open();ssChannel.configureBlocking(false);ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为 非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其他事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。 在将通道注册到selector上时，还需要指定要注册的具体事件，主要有以下几类： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 它们在 SelectionKey 的定义如下： 1234public static final int OP_READ = 1 &lt;&lt; 0;public static final int OP_WRITE = 1 &lt;&lt; 2;public static final int OP_CONNECT = 1 &lt;&lt; 3;public static final int OP_ACCEPT = 1 &lt;&lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如： 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 3、监听事件1int num = selector.select(); 使用 select() 来监听到达的事件，它会一直阻塞直到有至少一个事件到达。 4、获取达到的事件1234567891011Set&lt;SelectionKey&gt; keys = selector.selectedKeys();Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator();while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove();&#125; 5、事件循环因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。 1234567891011121314while (true) &#123; int num = selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove(); &#125;&#125; 套接字 NIO 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); ServerSocket serverSocket = ssChannel.socket(); InetSocketAddress address = new InetSocketAddress("127.0.0.1", 8888); serverSocket.bind(address); while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; ServerSocketChannel ssChannel1 = (ServerSocketChannel) key.channel(); // 服务器会为每个新连接创建一个 SocketChannel SocketChannel sChannel = ssChannel1.accept(); sChannel.configureBlocking(false); // 这个新连接主要用于从客户端读取数据 sChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel sChannel = (SocketChannel) key.channel(); System.out.println(readDataFromSocketChannel(sChannel)); sChannel.close(); &#125; keyIterator.remove(); &#125; &#125; &#125; private static String readDataFromSocketChannel(SocketChannel sChannel) throws IOException &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); StringBuilder data = new StringBuilder(); while (true) &#123; buffer.clear(); int n = sChannel.read(buffer); if (n == -1) &#123; break; &#125; buffer.flip(); int limit = buffer.limit(); char[] dst = new char[limit]; for (int i = 0; i &lt; limit; i++) &#123; dst[i] = (char) buffer.get(i); &#125; data.append(dst); buffer.clear(); &#125; return data.toString(); &#125;&#125; 123456789public class NIOClient &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket("127.0.0.1", 8888); OutputStream out = socket.getOutputStream(); String s = "hello world"; out.write(s.getBytes()); out.close(); &#125;&#125; 内存映射文件内存映射文件 I/O 是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的 I/O 快得多。 向内存映射文件写入可能是危险的，只是改变数组的单个元素这样的简单操作，就可能会直接修改磁盘上的文件。修改数据与将数据保存到磁盘是没有分开的。 下面代码行将文件的前 1024 个字节映射到内存中，map() 方法返回一个 MappedByteBuffer，它是 ByteBuffer 的子类。因此，可以像使用其他任何 ByteBuffer 一样使用新映射的缓冲区，操作系统会在需要时负责执行映射。 1MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, 0, 1024); 对比NIO 与普通 I/O 的区别主要有以下两点： NIO 是非阻塞的； NIO 是面向块，I/O 面向流； 八、参考资料 IBM: NIO 入门 Java NIO Tutorial Java NIO 浅析 IBM: 深入分析 Java I/O 的工作机制 IBM: 深入分析 Java 中的中文编码问题 IBM: Java 序列化的高级认识 NIO 与传统 IO 的区别 Decorator Design Pattern Socket Multicast]]></content>
      <categories>
        <category>java</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java BIO、NIO、AIO]]></title>
    <url>%2Fjava%2Fio%2Fjava-bio-nio-aio%2F</url>
    <content type="text"><![CDATA[熟练掌握 BIO,NIO,AIO 的基本概念以及一些常见问题是你准备面试的过程中不可或缺的一部分，另外这些知识点也是你学习 Netty 的基础。 BIO,NIO,AIO 总结Java 中的 BIO、NIO和 AIO 理解为是 Java 语言对操作系统的各种 IO 模型的封装。程序员在使用这些 API 的时候，不需要关心操作系统层面的知识，也不需要根据不同操作系统编写不同的代码。只需要使用Java的API就可以了。 在讲 BIO,NIO,AIO 之前先来回顾一下这样几个概念：同步与异步，阻塞与非阻塞。 同步与异步 同步： 同步就是发起一个调用后，被调用者未处理完请求之前，调用不返回。 异步： 异步就是发起一个调用后，立刻得到被调用者的回应表示已接收到请求，但是被调用者并没有返回结果，此时我们可以处理其他的请求，被调用者通常依靠事件，回调等机制来通知调用者其返回结果。 同步和异步的区别最大在于异步的话调用者不需要等待处理结果，被调用者会通过回调等机制来通知调用者其返回结果。 阻塞和非阻塞 阻塞： 阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。 非阻塞： 非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。 举个生活中简单的例子，你妈妈让你烧水，小时候你比较笨啊，在那里傻等着水开（同步阻塞）。等你稍微再长大一点，你知道每次烧水的空隙可以去干点其他事，然后只需要时不时来看看水开了没有（同步非阻塞）。后来，你们家用上了水开了会发出声音的壶，这样你就只需要听到响声后就知道水开了，在这期间你可以随便干自己的事情，你需要去倒水了（异步非阻塞）。 1. BIO (Blocking I/O)同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。 1.1 传统 BIOBIO通信（一请求一应答）模型图如下(图源网络，原出处不明)： 采用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，请求一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接，如上图所示。 如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程（主要原因是socket.accept()、socket.read()、socket.write() 涉及的三个主要函数都是同步阻塞的），也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，线程销毁。这就是典型的 一请求一应答通信模型 。我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制 改善，线程池还可以让线程的创建和回收成本相对较低。使用FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了N(客户端请求数量):M(处理客户端请求的线程数量)的伪异步I/O模型（N 可以远远大于 M），下面一节”伪异步 BIO”中会详细介绍到。 我们再设想一下当客户端并发访问量增加后这种模型会出现什么问题？ 在 Java 虚拟机中，线程是宝贵的资源，线程的创建和销毁成本很高，除此之外，线程的切换成本也是很高的。尤其在 Linux 这样的操作系统中，线程本质上就是一个进程，创建和销毁线程都是重量级的系统函数。如果并发访问量增加会导致线程数急剧膨胀可能会导致线程堆栈溢出、创建新线程失败等问题，最终导致进程宕机或者僵死，不能对外提供服务。 1.2 伪异步 IO为了解决同步阻塞I/O面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化一一一后端通过一个线程池来处理多个客户端的请求接入，形成客户端个数M：线程池最大线程数N的比例关系，其中M可以远远大于N.通过线程池可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽。 伪异步IO模型图(图源网络，原出处不明)： 采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，它的模型图如上图所示。当有新的客户端接入时，将客户端的 Socket 封装成一个Task（该任务实现java.lang.Runnable接口）投递到后端的线程池中进行处理，JDK 的线程池维护一个消息队列和 N 个活跃线程，对消息队列中的任务进行处理。由于线程池可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。 伪异步I/O通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。不过因为它的底层仍然是同步阻塞的BIO模型，因此无法从根本上解决问题。 1.3 代码示例下面代码中演示了BIO通信（一请求一应答）模型。我们会在客户端创建多个线程依次连接服务端并向其发送”当前时间+:hello world”，服务端会为每个客户端线程创建一个线程来处理。代码示例出自闪电侠的博客，原地址如下： https://www.jianshu.com/p/a4e03835921a 客户端 123456789101112131415161718192021222324/** * @author 闪电侠 * @date 2018年10月14日 * @Description:客户端 */public class IOClient &#123; public static void main(String[] args) &#123; // TODO 创建多个线程，模拟多个客户端连接服务端 new Thread(() -&gt; &#123; try &#123; Socket socket = new Socket("127.0.0.1", 3333); while (true) &#123; try &#123; socket.getOutputStream().write((new Date() + ": hello world").getBytes()); Thread.sleep(2000); &#125; catch (Exception e) &#123; &#125; &#125; &#125; catch (IOException e) &#123; &#125; &#125;).start(); &#125;&#125; 服务端 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @author 闪电侠 * @date 2018年10月14日 * @Description: 服务端 */public class IOServer &#123; public static void main(String[] args) throws IOException &#123; // TODO 服务端处理客户端连接请求 ServerSocket serverSocket = new ServerSocket(3333); // 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理 new Thread(() -&gt; &#123; while (true) &#123; try &#123; // 阻塞方法获取新的连接 Socket socket = serverSocket.accept(); // 每一个新的连接都创建一个线程，负责读取数据 new Thread(() -&gt; &#123; try &#123; int len; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); // 按字节流方式读取数据 while ((len = inputStream.read(data)) != -1) &#123; System.out.println(new String(data, 0, len)); &#125; &#125; catch (IOException e) &#123; &#125; &#125;).start(); &#125; catch (IOException e) &#123; &#125; &#125; &#125;).start(); &#125;&#125; 1.4 总结在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 2. NIO (New I/O)2.1 NIO 简介NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。 NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。 2.2 NIO的特性/NIO与IO区别如果是在面试中回答这个问题，我觉得首先肯定要从 NIO 流是非阻塞 IO 而 IO 流是阻塞 IO 说起。然后，可以从 NIO 的3个核心组件/特性为 NIO 带来的一些改进来分析。如果，你把这些都回答上了我觉得你对于 NIO 就有了更为深入一点的认识，面试官问到你这个问题，你也能很轻松的回答上来了。 1)Non-blocking IO（非阻塞IO）IO流是阻塞的，NIO流是不阻塞的。 Java NIO使我们可以进行非阻塞IO操作。比如说，单线程中从通道读取数据到buffer，同时可以继续做别的事情，当数据读取到buffer中后，线程再继续处理数据。写数据也是一样的。另外，非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 Java IO的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了 2)Buffer(缓冲区)IO 面向流(Stream oriented)，而 NIO 面向缓冲区(Buffer oriented)。 Buffer是一个对象，它包含一些要写入或者要读出的数据。在NIO类库中加入Buffer对象，体现了新库与原I/O的一个重要区别。在面向流的I/O中·可以将数据直接写入或者将数据直接读到 Stream 对象中。虽然 Stream 中也有 Buffer 开头的扩展类，但只是流的包装类，还是从流读到缓冲区，而 NIO 却是直接读到 Buffer 中进行操作。 在NIO厍中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。 最常用的缓冲区是 ByteBuffer,一个 ByteBuffer 提供了一组功能用于操作 byte 数组。除了ByteBuffer,还有其他的一些缓冲区，事实上，每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。 3)Channel (通道)NIO 通过Channel（通道） 进行读写。 通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。 4)Selector (选择器)NIO有选择器，而IO没有。 选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。 2.3 NIO 读数据和写数据方式通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。 数据读取和写入操作图示： 2.4 NIO核心组件简单介绍NIO 包含下面几个核心的组件： Channel(通道) Buffer(缓冲区) Selector(选择器) 整个NIO体系包含的类远远不止这三个，只能说这三个是NIO体系的“核心API”。我们上面已经对这三个概念进行了基本的阐述，这里就不多做解释了。 2.5 代码示例代码示例出自闪电侠的博客，原地址如下： https://www.jianshu.com/p/a4e03835921a 客户端 IOClient.java 的代码不变，我们对服务端使用 NIO 进行改造。以下代码较多而且逻辑比较复杂，大家看看就好。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * * @author 闪电侠 * @date 2019年2月21日 * @Description: NIO 改造后的服务端 */public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; // 1. serverSelector负责轮询是否有新的连接，服务端监测到新的连接之后，不再创建一个新的线程， // 而是直接将新连接绑定到clientSelector上，这样就不用 IO 模型中 1w 个 while 循环在死等 Selector serverSelector = Selector.open(); // 2. clientSelector负责轮询连接是否有数据可读 Selector clientSelector = Selector.open(); new Thread(() -&gt; &#123; try &#123; // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(3333)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) &#123; // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = serverSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; try &#123; // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); &#125; finally &#123; keyIterator.remove(); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; while (true) &#123; // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = clientSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isReadable()) &#123; try &#123; SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println( Charset.defaultCharset().newDecoder().decode(byteBuffer).toString()); &#125; finally &#123; keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; &#125; &#125;).start(); &#125;&#125; 为什么大家都不愿意用 JDK 原生 NIO 进行开发呢？从上面的代码中大家都可以看出来，是真的难用！除了编程复杂、编程模型难之外，它还有以下让人诟病的问题： JDK 的 NIO 底层由 epoll 实现，该实现饱受诟病的空轮询 bug 会导致 cpu 飙升 100% 项目庞大之后，自行实现的 NIO 很容易出现各类 bug，维护成本较高，上面这一坨代码我都不能保证没有 bug Netty 的出现很大程度上改善了 JDK 原生 NIO 所存在的一些让人难以忍受的问题。 3. AIO (Asynchronous I/O)AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。（除了 AIO 其他的 IO 类型都是同步的，这一点可以从底层IO线程模型解释，推荐一篇文章：《漫话：如何给女朋友解释什么是Linux的五种IO模型？》 ） 查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。]]></content>
      <categories>
        <category>java</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 5种IO模型]]></title>
    <url>%2Fjava%2Fio%2Flinux-io%2F</url>
    <content type="text"><![CDATA[一讲到网络编程的I/O模型，总会涉及到这几个概念。问了很多人，没几个能清晰地讲出他们之间的区别联系，甚至在网络上也有很多不同的观点，也不知是中国文字释义的博大精深，还是本来这几个概念就是绕人不倦。今天我也来给大家讲解一下我对这几个概念的理解。 既然网络上众说纷纭，不如找个权威参考一下，这个权威就是 《UNIX网络编程：卷一》第六章——I/O复用。书中向我们提及了5种类UNIX下可用的I/O模型： 阻塞式I/O； 非阻塞式I/O； I/O复用（select，poll，epoll…）； 信号驱动式I/O（SIGIO）； 异步I/O（POSIX的aio_系列函数）； 阻塞式I/O模型默认情况下，所有套接字都是阻塞的。怎么理解？先理解这么个流程，一个输入操作通常包括两个不同阶段： （1）等待数据准备好； （2）从内核向进程复制数据。 对于一个套接字上的输入操作， 第一步通常涉及等待数据从网络中到达。 当所有等待分组到达时，它被复制到内核中的某个缓冲区。 第二步就是把数据从内核缓冲区复制到应用程序缓冲区。 好，下面我们以阻塞套接字的recvfrom的的调用图来说明阻塞 标红的这部分过程就是阻塞，直到阻塞结束recvfrom才能返回。 非阻塞式I/O 以下这句话很重要：进程把一个套接字设置成非阻塞是在通知内核，当所请求的I/O操作非得把本进程投入睡眠才能完成时，不要把进程投入睡眠，而是返回一个错误。看看非阻塞的套接字的recvfrom操作如何进行 可以看出recvfrom总是立即返回。 I/O多路复用虽然I/O多路复用的函数也是阻塞的，但是其与以上两种还是有不同的，I/O多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的I/O系统调用如recvfrom之上。如图 信号驱动式I/O用的很少，就不做讲解了。直接上图 异步I/O这类函数的工作机制是告知内核启动某个操作，并让内核在整个操作（包括将数据从内核拷贝到用户空间）完成后通知我们。如图： 注意红线标记处说明在调用时就可以立马返回，等函数操作完成会通知我们。 等等，大家一定要问了，同步这个概念你怎么没涉及啊？别急，您先看总结。 其实前四种I/O模型都是同步I/O操作，他们的区别在于第一阶段，而他们的第二阶段是一样的：在数据从内核复制到应用缓冲区期间（用户空间），进程阻塞于recvfrom调用。 相反，异步I/O模型在这两个阶段都要处理。 再看POSIX对这两个术语的定义： 同步I/O操作：导致请求进程阻塞，直到I/O操作完成； 异步I/O操作：不导致请求进程阻塞。 好，下面我用我的语言来总结一下阻塞，非阻塞，同步，异步 阻塞，非阻塞：进程/线程要访问的数据是否就绪，进程/线程是否需要等待； 同步，异步：访问数据的方式，同步需要主动读写数据，在读写数据的过程中还是会阻塞；异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins文档 —— Pipline]]></title>
    <url>%2Fdevops%2Fjenkins%2Fjenkins-docs-pipline%2F</url>
    <content type="text"><![CDATA[官方文档：https://www.jenkins.io/zh/doc/book/pipeline/ 目录 流水线入门 使用 Jenkinsfile 分支和pull请求 在流水线中使用Docker 扩展共享库 流水线开发工具 流水线语法 规模 Pipelines 流水线入门什么是Jenkins的流水线?Jenkins 流水线 (或简单的带有大写”P”的”Pipeline”) 是一套插件，它支持实现和集成 continuous delivery pipelines 到Jenkins。 _continuous delivery (CD) pipeline_是你的进程的自动表达，用于从版本控制向用户和客户获取软件。 你的软件的每次的变更 (在源代码控制中提交)在它被释放的路上都经历了一个复杂的过程 on its way to being released. 这个过程包括以一种可靠并可重复的方式构建软件, 以及通过多个测试和部署阶段来开发构建好的软件 (c成为 “build”) 。 流水线提供了一组可扩展的工具，通过 Pipeline domain-specific language (DSL) syntax. [1]对从简单到复杂的交付流水线 “作为代码” 进行建模。 对Jenkins 流水线的定义被写在一个文本文件中 (成为 Jenkinsfile)，该文件可以被提交到项目的源代码的控制仓库。 [2] 这是”流水线即代码”的基础; 将CD 流水线作为应用程序的一部分，像其他代码一样进行版本化和审查。 创建 Jenkinsfile并提交它到源代码控制中提供了一些即时的好处: 自动地为所有分支创建流水线构建过程并拉取请求。 在流水线上代码复查/迭代 (以及剩余的源代码)。 对流水线进行审计跟踪。 该流水线的真正的源代码 [3], 可以被项目的多个成员查看和编辑。 While定义流水线的语法, 无论是在 web UI 还是在 Jenkinsfile 中都是相同的, 通常认为在Jenkinsfile 中定义并检查源代码控制是最佳实践 声明式和脚本化的流水线语法Jenkinsfile 能使用两种语法进行编写 - 声明式和脚本化。 声明式和脚本化的流水线从根本上是不同的。 声明式流水线的是 Jenkins 流水线更近的特性: 相比脚本化的流水线语法，它提供更丰富的语法特性, 是为了使编写和读取流水线代码更容易而设计的。 然而，写到Jenkinsfile中的许多单独的语法组件(或者 “步骤”), 通常都是声明式和脚本化相结合的流水线。 在下面的 [pipeline-concepts] 和 [pipeline-syntax-overview] 了解更多这两种语法的不同。 Why Pipeline?本质上，Jenkins 是一个自动化引擎，它支持许多自动模式。 流水线向Jenkins中添加了一组强大的工具, 支持用例 简单的持续集成到全面的CD流水线。通过对一系列的相关任务进行建模, 用户可以利用流水线的很多特性: Code: 流水线是在代码中实现的，通常会检查到源代码控制, 使团队有编辑, 审查和迭代他们的交付流水线的能力。 Durable: 流水线可以从Jenkins的主分支的计划内和计划外的重启中存活下来。 Pausable: 流水线可以有选择的停止或等待人工输入或批准，然后才能继续运行流水线。 Versatile: 流水线支持复杂的现实世界的 CD 需求, 包括fork/join, 循环, 并行执行工作的能力。 Extensible:流水线插件支持扩展到它的DSL [1]的惯例和与其他插件集成的多个选项。 然而， Jenkins一直允许以将自由式工作链接到一起的初级形式来执行顺序任务, [4] 流水线使这个概念成为了Jenkins的头等公民。 构建一个的可扩展的核心Jenkins值, 流水线也可以通过 Pipeline Shared Libraries 的用户和插件开发人员来扩展。 [5] 下面的流程图是一个 CD 场景的示例，在Jenkins中很容易对该场景进行建模: 流水线概念下面的概念是Jenkins流水线很关键的一方面 , 它与流水线语法紧密相连 (参考 overview below). 流水线流水线是用户定义的一个CD流水线模型 。流水线的代码定义了整个的构建过程, 他通常包括构建, 测试和交付应用程序的阶段 。 另外 ， pipeline 块是 声明式流水线语法的关键部分. 节点节点是一个机器 ，它是Jenkins环境的一部分 and is capable of执行流水线。 另外, node块是 脚本化流水线语法的关键部分. 阶段stage 块定义了在整个流水线的执行任务的概念性地不同的的子集(比如 “Build”, “Test” 和 “Deploy” 阶段), 它被许多插件用于可视化 或Jenkins流水线目前的 状态/进展. [6] 步骤本质上 ，一个单一的任务, a step 告诉Jenkins 在特定的时间点要做what (或过程中的 “step”)。 举个例子,要执行shell命令 ，请使用 sh 步骤: sh &#39;make&#39;。当一个插件扩展了流水线DSL, [1] 通常意味着插件已经实现了一个新的 step。 流水线语法概述下面的流水线代码骨架说明了声明式流水线语法和 脚本化流水线语法之间的根本差异。 请注意 阶段 and 步骤 (上面的) 都是声明式和脚本化流水线语法的常见元素。 声明式流水线基础在声明式流水线语法中, pipeline 块定义了整个流水线中完成的所有的工作。 在任何可用的代理上，执行流水线或它的任何阶段。 定义 “Build” 阶段。 执行与 “Build” 阶段相关的步骤。 定义”Test” 阶段。 执行与”Test” 阶段相关的步骤。 定义 “Deploy” 阶段。 执行与 “Deploy” 阶段相关的步骤。 脚本化流水线基础在脚本化流水线语法中, 一个或多个 node 块在整个流水线中执行核心工作。 虽然这不是脚本化流水线语法的强制性要求, 但它限制了你的流水线的在node块内的工作做两件事: 通过在Jenkins队列中添加一个项来调度块中包含的步骤。 节点上的执行器一空闲, 该步骤就会运行。 创建一个工作区(特定为特定流水间建立的目录)，其中工作可以在从源代码控制检出的文件上完成。Caution: 根据你的 Jenkins 配置,在一系列的空闲后，一些工作区可能不会自动清理 。参考 JENKINS-2111 了解更多信息。 在任何可用的代理上，执行流水线或它的任何阶段。 定义 “Build” 阶段。 stage 块 在脚本化流水线语法中是可选的。 然而, 在脚本化流水线中实现 stage 块 ，可以清楚的显示Jenkins UI中的每个 stage 的任务子集。 执行与 “Build” 阶段相关的步骤。 定义 “Test” 阶段。 执行与 “Test” 阶段相关的步骤。 定义 “Deploy” 阶段。 执行与 “Deploy” 阶段相关的步骤。 流水线示例使用 Jenkinsfile分支和pull请求在流水线中使用Docker扩展共享库流水线开发工具流水线语法规模 Pipelines]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式连路追踪 sleuth+zipkin+kafka+elasticsearch]]></title>
    <url>%2Fjava%2Fspring%2Fspring-cloud-sleuth-zipkin%2F</url>
    <content type="text"><![CDATA[sleuth zipkin 采集链路数据发送到kafka zipkin 消费kafka数据写入 elasticsearh 使用zipkin-ui 搜索 elasticsearch 数据]]></content>
      <categories>
        <category>java</category>
        <category>spring-cloud</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志平台搭建 sleuth+flume+kafka+logstash+elasticsearch]]></title>
    <url>%2Fjava%2Fspring%2Fspring-cloud-sleuth-logback%2F</url>
    <content type="text"><![CDATA[sleuth + loaback flume 采集日志发送到kafka logstash 消费kafka数据写入 elasticsearh kibana 搜索 elasticsearch 数据]]></content>
      <categories>
        <category>java</category>
        <category>spring-cloud</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-集群运维和管理]]></title>
    <url>%2Fmessage-queue%2Frabbitmq%2Frabbitmq-3-cluster-and-manager%2F</url>
    <content type="text"><![CDATA[1、rabbitmq管理1.1、多租户与权限1.2、用户管理1.3、Web端管理1.4、应用与集群管理1.4.1、应用管理1234567891011121314151617# 用于停止 rabbitmq 的erlang 虚拟机 和 rabbitmq 应用rabbitmqctl stop [pidfile]# 用于停止 rabbitmq 的erlang 虚拟机 和 rabbitmq 应用(阻塞等待)rabbitmqctl shutdown# 停止 rabbitmq 应用rabbitmqctl stop_apprabbitmqctl start_app# 等待pidfile 中 rabbitmq 应用启动rabbitmqctl wait [pidfile]# 重置节点，从原来的集群中删除此节点，删除所有用户,vhostrabbitmqctl reset# 强制将rabbit目前节点重置还原到最初状态rabbitmqctl force_resetrabbitmqctl rotate_logs &#123;siffix&#125;# 将部分rabbitmq 代码用 HiPE编译rabbitmqctl hipe_compile &#123;directory&#125; 1.4.2、集群管理123456789101112131415161718# 将节点加入指定集群rabbitmqctl join_cluster &#123;cluster_node&#125; [--ram]# 查看集群状态rabbitmqctl cluster_status# 修改集群节点的类型rabbitmqctl change_cluster_node_type &#123;disc|ram&#125;# 将节点从集群中删除，允许离线执行rabbitmqctl forget_cluster_node [--offline]# 在集群中的节点应用启动前 资讯 clusternode节点的最新信息，并更新相应的集群信息rabbitmqctl update_cluster_nodes &#123;clusternode&#125;# 确保节点可以启动，即使它不是最后一个关闭的节点rabbitmqctl force_boot# 指示未同步队列queue 的 slave 镜像 可以同步 master镜像行的内容rabbitmqctl sync_queue [-p vhost] &#123;queue&#125;# 取消队列queue的同步镜像操作rabbitmqctl cancel_sync_queue [-p vhost] &#123;queue&#125;# 设定集群名称rabbitmqctl set_cluster_name &#123;name&#125; 1.5、服务端状态1.6、HTTP API 接口管理 2、rabbitmq运维2.1、集群搭建 2.2、查看服务日志 2.3、单节点故障恢复 2.4、集群迁移2.4.1、元数据重建 2.4.2、数据迁移和客户端连接的切换 2.4.3、自动化迁移 2.5、集群监控 3、集群间通信3.1、Federatoin3.2、Shovel 4、网络分区4.1、网络分区的意义4.2、网络分区的判定4.3、网络分区的模拟4.4、网络分区的影响4.5、手动处理网络分区4.6、自动处理网络分区4.7、案例：多分区情形]]></content>
      <categories>
        <category>message-queue</category>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-设计和功能]]></title>
    <url>%2Fmessage-queue%2Frabbitmq%2Frabbitmq-2-design-and-function%2F</url>
    <content type="text"><![CDATA[1、什么是消息中间件消息队列中间件 (Message Queue Middleware，简称为 MQ) 是指利用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型，它可以在分布式环境下扩展进程间的通信。提供了以松散藕合的灵活方式集成应用程序的一种机制。它们提供了基于存储和转发的应用程序之间的异步数据发送，即应用程序彼此不直接通信，而是与作为中介的消息中间件通信。消息中间件提供了有保证的消息发送，应用程序开发人员无须了解远程过程调用 ( RPC) 和网络通信协议的细节。 2、消息中间件的作用 异步（解决不必要的阻塞）：消息中间件提供了异步处理机制，允许应用把一些消息放入消息中间件中，但并不立即处理它，在之后需要的时候再慢慢处理 。 解耦（降低模块间的耦合关系）：消息中间件在处理过程中间插入了一个隐含的、基于数据的接口层。 削峰（峰值任务的平滑处理）：在访问量剧增的情况下，应用仍然需要继续发挥作用。 冗余（存储、补偿机制）：有些情况下，处理数据的过程会失败。消息中间件可以把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。 顺序保证：数据的处理顺序 3、AMQP协议AMQP协议是一套开放标准，支持不同语言的不同产品。 AMQP组件 生产者：消息的创建者，将消息发送到消息中间件 消费者：连接到消息中间件上，订阅在队列上，进行消息的消费。 消息：包括有效载荷与标签。有效载荷：要传输的数据；标签：描述有效载荷的属性；RabbitMQ通过标签决定谁获得该消息，消费者只能得到有效载荷。 信道：可理解为一个虚拟的连接，建立在真实的TCP/IP连接之上。所有AMQP上的消息都通过信道传输，TCP/IP连接的建立和释放 对服务器有很大的消耗、昂贵的资源。信道的创建没有数量限制，保护资源的利用。 交换器、队列、绑定、路由键：队列通过路由键（routing key）绑定到交换器，生产者把消息发送到交换器，交换器根据绑定的路由键把消息路由到特定的队列中，再由订阅该队列的消费者进行消息的消费。 5、客户端开发5.1、连接rabbitmqconnection channel 123$connection = new AMQPStreamConnection(Yii::$app-&gt;params['AMQP']['host'],Yii::$app-&gt;params['AMQP']['port'], Yii::$app-&gt;params['AMQP']['user'],Yii::$app-&gt;params['AMQP']['password']);$channel = $connection-&gt;channel(); 5.2、声明exchange和queue123456789101112131415161718192021222324public function exchange_declare( $exchange, // string, 交换器名 $type, // string, 例如 'direct', 'fanout', 'topic' $passive = false, // 如果已经存在，是否报错 $durable = false, // 是否持久化 $auto_delete = true, // 所有与该交换器绑定的队列都解绑以后，自动删除 $internal = false, // 是否内置交换器，客户端消息无法发送到内置交换器 $nowait = false, // 是否不需要broker返回结果 $arguments = null, // 参数，如死信队列，ttl等 $ticket = null) &#123;&#125;public function queue_declare( $queue = '', $passive = false, $durable = false, $exclusive = false, // 是否排他，排他队列仅对首次声明它的连接可见，断开时自动删除 $auto_delete = true, // 自动删除，所有与这个队列连接的消费者都断开时自动删除 $nowait = false, $arguments = null, $ticket = null)&#123;&#125;public function queue_bind($queue, $exchange, $routing_key = '', $nowait = false, $arguments = null, $ticket = null)&#123;&#125; 举例 123456$channel-&gt;exchange_declare(self::FXC_EXCHANGE, 'direct', false, true, false);$argsAMQ = new AMQPTable();$argsAMQ-&gt;set('x-dead-letter-exchange', self::FXC_EXCHANGE);$argsAMQ-&gt;set('x-dead-letter-routing-key', self::FXC_ROUTING_ERROR);$channel-&gt;queue_declare(self::FXC_MSG_QUEUE, false, true, false, false, false, $argsAMQ);$channel-&gt;queue_bind(self::FXC_MSG_QUEUE, self::FXC_EXCHANGE, self::FXC_ROUTING_NORMAL); 5.3、发送消息12345678public function basic_publish( $msg, $exchange = '', $routing_key = '', // 路由键 $mandatory = false, // 交换器无法根据 exchange和路由键找到对应的队列时，$mandatory 为 true，则会basic.return给生产者；如果为false，则直接丢弃 $immediate = false, // true-如果消息路由到队列以后，发现队列上没有任何消费者则返回给生产者，有消费者则立刻投递给消费者 $ticket = null) &#123;&#125; 5.4、消费消息1）推模式broker向消费者持续不断的推消息 123456789101112public function basic_consume( $queue = '', // 队列名称 $consumer_tag = '', // 消费者标签，用于区分多个消费者 $no_local = false, // 设为true表示，不能见同一个connection中生产者发送的消息传送给这个connection的消费者 $no_ack = false, // 是否自动确认 $exclusive = false, // 是否排他 $nowait = false, $callback = null, $ticket = null, $arguments = array()) &#123;&#125; 2）拉模式消费者一条条从broker拉取消息 1public function basic_get($queue = '', $no_ack = false, $ticket = null) 举例 1234567while (true) &#123; $getResponse = $channel-&gt;basic_get(self::FXC_MSG_QUEUE, false); if (empty($getResponse)) &#123; break; &#125; $msg = json_decode($getResponse-&gt;body, true);&#125; 3) 幂等性1f(x) = f(f(x)) 5.5、消费端的确认与拒绝确认消息已消费 1public function basic_ack($delivery_tag, $multiple = false)&#123;&#125; 拒绝消息 123456// 拒绝单条信息// requeue true-重新放回队列供其他消费者消费，false-从队列中彻底移除（有死信队列会放入死信队列）public function basic_reject($delivery_tag, $requeue)&#123;&#125;// 拒绝多条信息public function basic_nack($delivery_tag, $multiple = false, $requeue = false)&#123;&#125; 5.6、关闭连接12$channel-&gt;close();$connection-&gt;close(); 6、rabbitmq使用进阶6.1、消息何去何从只有交换器没有绑定队列的消息会发送到哪？ 1）mandatory交换器无法根据 exchange和路由键找到对应的队列时，$mandatory 为 true，则会basic.return给生产者；如果为false，则直接丢弃 2）immediatetrue-如果消息路由到队列以后，发现队列上没有任何消费者则返回给生产者，有消费者则立刻投递给消费者 6.2、过期时间TTL1）设置消息的TTL1x-message-ttl 6000 2）设置队列的TTL1x-expires 180000 当队列上没有任何消费者，且过期时间内没有调用basic.get 则会被删除 如果broker重启了，持久化的队列的过期时间会被重新计算 6.3、死信队列12x-dead-letter-exchangex-dead-letter-routing-key 6.4、延迟队列rabbitmq本身并没有实现延迟队列，不过可以用 TTL + 死信队列 达到延迟队列的效果 6.5、优先级队列1234# 1. 将队列设置成优先级队列x-max-priority 10# 2. 对消息设置优先级priority 6.6、RPC实现 文档：https://www.rabbitmq.com/tutorials/tutorial-six-python.html RPC处理流程如下： 客户端启动时，创建一个匿名回调队列（由rabbitmq自动创建）； 客户端为RPC请求设置2个属性：replyTo用来告知RPC服务端回复请求时的目的队列，即回调队列；correlationId 用来标记一个请求； 请求被发送到 rpc_queue队列中； RPC 服务端 监听 rpc_queue 队列中的请求，当请求到来时，服务端会处理并且把带有结果的消息发送给客户端。接收的队列就是 replyTo设定的回调队列； 客户端监听回调队列，当有消息时，检查correlationId属性，如果与请求匹配，那就是结果了； RPCClient.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import com.rabbitmq.client.AMQP;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.UUID;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeoutException;public class RPCClient implements AutoCloseable &#123; private Connection connection; private Channel channel; private String requestQueueName = "rpc_queue"; public RPCClient() throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setHost("localhost"); connection = factory.newConnection(); channel = connection.createChannel(); &#125; public static void main(String[] argv) &#123; try (RPCClient fibonacciRpc = new RPCClient()) &#123; for (int i = 0; i &lt; 32; i++) &#123; String i_str = Integer.toString(i); System.out.println(" [x] Requesting fib(" + i_str + ")"); String response = fibonacciRpc.call(i_str); System.out.println(" [.] Got '" + response + "'"); &#125; &#125; catch (IOException | TimeoutException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public String call(String message) throws IOException, InterruptedException &#123; final String corrId = UUID.randomUUID().toString(); String replyQueueName = channel.queueDeclare().getQueue(); AMQP.BasicProperties props = new AMQP.BasicProperties .Builder() .correlationId(corrId) .replyTo(replyQueueName) .build(); channel.basicPublish("", requestQueueName, props, message.getBytes("UTF-8")); final BlockingQueue&lt;String&gt; response = new ArrayBlockingQueue&lt;&gt;(1); String ctag = channel.basicConsume(replyQueueName, true, (consumerTag, delivery) -&gt; &#123; if (delivery.getProperties().getCorrelationId().equals(corrId)) &#123; response.offer(new String(delivery.getBody(), "UTF-8")); &#125; &#125;, consumerTag -&gt; &#123; &#125;); String result = response.take(); channel.basicCancel(ctag); return result; &#125; public void close() throws IOException &#123; connection.close(); &#125;&#125; RPCServer.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import com.rabbitmq.client.*;public class RPCServer &#123; private static final String RPC_QUEUE_NAME = "rpc_queue"; private static int fib(int n) &#123; if (n == 0) return 0; if (n == 1) return 1; return fib(n - 1) + fib(n - 2); &#125; public static void main(String[] argv) throws Exception &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setHost("localhost"); try (Connection connection = factory.newConnection(); Channel channel = connection.createChannel()) &#123; channel.queueDeclare(RPC_QUEUE_NAME, false, false, false, null); channel.queuePurge(RPC_QUEUE_NAME); channel.basicQos(1); System.out.println(" [x] Awaiting RPC requests"); Object monitor = new Object(); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; AMQP.BasicProperties replyProps = new AMQP.BasicProperties .Builder() .correlationId(delivery.getProperties().getCorrelationId()) .build(); String response = ""; try &#123; String message = new String(delivery.getBody(), "UTF-8"); int n = Integer.parseInt(message); System.out.println(" [.] fib(" + message + ")"); response += fib(n); &#125; catch (RuntimeException e) &#123; System.out.println(" [.] " + e.toString()); &#125; finally &#123; channel.basicPublish("", delivery.getProperties().getReplyTo(), replyProps, response.getBytes("UTF-8")); channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); // RabbitMq consumer worker thread notifies the RPC server owner thread synchronized (monitor) &#123; monitor.notify(); &#125; &#125; &#125;; channel.basicConsume(RPC_QUEUE_NAME, false, deliverCallback, (consumerTag -&gt; &#123; &#125;)); // Wait and be prepared to consume the message from RPC client. while (true) &#123; synchronized (monitor) &#123; try &#123; monitor.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125;&#125; 6.7、持久化 交换器持久化 队列持久化 消息持久化 只有 queue 和 msg 都持久化，broker重启之后才会存在； queue不持久化，msg持久化，broker重启之后消息也会丢失； 重要的消息要考虑持久化，但持久化会严重影响rabbitmq的性能。在 可靠性和吞吐量之间要做取舍； 持久化并不能保证消息不会丢失。还可以考虑镜像队列、发送方确认、事务机制 6.8、生产者确认 生产者将消息发送出去以后，消息是否到达了 broker呢？ 1）事务机制1234567try &#123; channel.txSelect(); channel.basicPublish(); channel.txCommit();&#125; cacth (Exception e) &#123; channel.txRollback();&#125; 不足：事务机制会吸干rabbitmq的性能。所以有些场景考虑用发送方确认机制来达到相同的目的 2）发送方确认机制​ 生产者将新到设置为confirm模式，一旦进入confirm模式，所有在该信到上发布的消息都会被指派一个唯一ID，一旦消息被投递到匹配的队列，rabbitmq就会发送一个确认（basic.Ack）给生产者（包含唯一ID）； ​ 如果消息和队列是持久化的，则会在消息写入磁盘后发出。 对比 事务机制在一条消息发送之后使发送端阻塞，以等待rabbitmq的回应，之后才能发送下一条消息； 发送方确认最大的好处是它是异步的 同步确认 123456789try &#123; channel.confirmSelect(); channel.basicPublic(); if (!channel.waitForConfirms()) &#123; println("send msg fail."); &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 批量确认 12345678910111213141516171819202122try &#123; channel.confirmSelect(); int msgCount = 0; while(true) &#123; channel.basicPublish(); // TODO：将发送出去的消息放如缓存(List 或 BlockingQueue) if (++msgCount &gt;= BATCH_COUNT) &#123; msgCount = 0; try &#123; if (channel.waitForConfirms()) &#123; // TODO: 将缓存中的消息清空 continue; &#125; // TODO： 将缓存中的消息重新发送 &#125; catch (InterruptedException e) &#123; // TODO: 将缓存中的消息重新发送 &#125; &#125; &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; 异步确认 123456789101112131415161718192021222324channel.confirmSelect();channel.addConfirmListener(new ConfirmListener() &#123; public void handleAck(long deliveryTag, boolean multiple) &#123; if (multiple) &#123; confirmSet.headSet(deliveryTag + 1).clear(); &#125; else &#123; confirmSet.remove(deliveryTag); &#125; &#125; public void handleNack(long deliveryTag, boolean multiple) &#123; if (multiple) &#123; confirmSet.headSet(deliveryTag + 1).clear(); &#125; else &#123; confirmSet.remove(deliveryTag); &#125; // TODO：注意这里需要添加 消息重发的 场景 &#125;&#125;);while(true) &#123; long nextSeqNo = channel.getnnnextPublishSeqNo(); channel.basicPublish(...., ConfirmConfig.msg_10B.getBytes()); confirmSet.add(nextSeqNo);&#125; 6.9、消费端要点确认1）消息分发​ 默认清空下，通过轮询将消息发送给每个消费者，但如果有些消费者负载已经满了，另一些消费者空闲，就会造成整体吞吐量下降。 ​ 可以通过 channel.basicQos(n) 设置信道上最大未确认消息数量，如果未确认消息数量已达到n，则不会再向这个消费者发送消息； 注意：channel.BasicQos 对 拉模式无效 123channel.basicQos(int perfetchCount);channel.basicQos(int perfetchCount, boolean global);channel.basicQos(int prefetchSize, int perfetchCount, boolean global); perfetchCount：预取个数，0表示无限；大于0，则信道跟队列需要协调发送的消息没有超过限定的值； prefetchSize：未确认消息个数，0表示无限；大于0，则需要协调 global参数 AMQP 0-9-1 rabbitmq false 信道上所有消费者都要遵循 prefetchCount 限制 信道上新的消费者需要遵循prefetchCount限定 true 当前链路(Connection)上所有消费者都要遵循 信道上所有消费者都要遵循 prefetchCount 限制 2）消息顺序性在只有一个生产者一个消费者的情况下，且不使用任何rabbitmq的高级特性的情况下，消息是有序的。否则需要自己保证消息有序 3）弃用QueueingConsumer 有内存溢出问题 会拖累一个connection下的所有信道 同步递归调用会产生死锁 不是时间驱动的 6.10、消息传输保障 At most once：最多一次 At least once：至少一次 Exactly once：恰好一次 7、rabbitmq核心设计7.1、存储机制123456789bash-5.0# pwd/var/lib/rabbitmq/mnesia/rabbit@rabbitmq001/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09Lbash-5.0# ls -ltotal 20drwxr-xr-x 2 rabbitmq rabbitmq 4096 Jan 7 06:29 msg_store_persistentdrwxr-xr-x 2 rabbitmq rabbitmq 4096 Jan 7 06:29 msg_store_transientdrwxr-xr-x 3 rabbitmq rabbitmq 4096 Apr 1 04:09 queues-rw-r--r-- 1 rabbitmq rabbitmq 5464 Jan 7 06:29 recovery.detsbash-5.0# 1）队列的结构rabbitmq 中的消息可能处于一下4中状态： alpha：消息内容、索引都存在内存中； beta：消息内容保存在磁盘中，索引保存在内存中； gamma：消息内容保存在磁盘中，消息索引保存在磁盘和内存中； delta：消息内容和索引都在磁盘中； 这4种状态的主要作用是满足不同的内存和CPI需求。 alpha最耗内存，最少耗cpu；delta基本不耗内存，却最耗cpu和I/O； Q1 和 Q4 只包含 alpha 状态的消息 Q2 和 Q3 包含 beta 和 gamma 状态的消息 Delta 只包含 delta 状态的消息 2）惰性队列惰性队列会尽可能的 将消息存入磁盘，减少内存的小号，增加 I/O 的使用 队列具备两种模式：default 和 lazy 1x-queue-mode lazy 7.2、内存和磁盘告警1）内存告警Connection 处于 blocking 或 blocked 状态下 可以通过 rabbitmq.conf 来设置 12vm_memory_high_watermarkvm_memory_high_watermark_paging_ratio 因为erlang虚拟机的垃圾回收可能占用 2被的内存，所以 vm_memory_high_watermark 一般不超过 70%， 取值范围为 [0.4, 0.66] vm_memory_high_watermark_paging_ratio 表示 当 vm_memory_high_watermark 范围内的内存占用超过 vm_memory_high_watermark_paging_ratio时，就会从内存swap到磁盘上 2）磁盘告警默认情况下，当系统磁盘占用只剩 50MB时，就会报警。但50MB可能不够用，一个谨慎的做法是 设置为和操作系统的内存一样大 12# 建议范围 1.0 ~ 2.0 之间disk_free_limit.mem_relative = 1.0 rabbitmq 每 10s 会进行一次检测 7.3、流控1）流控的原理信用证算法（credit-based algorithm）：当消息堆积到一定程度时，就会阻塞 接收上游新消息 当 connection 、channel、queue 处于 flow 状态时，表示触发了限流，在 blocked 和 unblock 之间切换 rabbit_reader：connection 的 处理进程，负责接收、解析 AMQP 协议数据包； rabbit_channel： channel 处理进程，负责处理 AMQP 协议的各种方法，进而进行路由解析； rabbit_amqqueue_process：队列的处理进程，负责实现队列的所有逻辑； rabbit_msg_store：负责实现消息的持久化； 2）打破队列的瓶颈向一个队列中推送消息时，往往会在 rabbit_amqqueue_process 中 产生瓶颈。 提升性能方案： 使用HiPE功能，保守估计可以提升 30%~40% 的 性能（不过erlang的版本至少为 18.X） 使用多个rabbit_amqqueue_process， 负载均衡的方案（创建多个队列，不同的队列绑定不同的路由键） 7.4、镜像队列]]></content>
      <categories>
        <category>message-queue</category>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq-安装和集群部署]]></title>
    <url>%2Fmessage-queue%2Frabbitmq%2Frabbitmq-1-setup%2F</url>
    <content type="text"><![CDATA[1、什么是消息中间件利用可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型，他可以在分布式环境下扩展进程间通信。 应用程序之间异步通信 2、消息中间件的作用异步 解耦 削峰 冗余 3、JMS协议JavaMessageService sun 01年发布的 规范，用来统一所有的mq规范，提供了一个JavaAPI AMQP协议 04年出现的 单播 广播 4、AMQP协议AMQP包含如下几个组件 生产者 消费者 消息：有效载荷与标签 连接、信道：一个tcp连接里有多个信道，节省资源 交换器、队列、路由键、绑定： 交换器和队列通过路由键来绑定 5、RabbitMQ简介RabbitMQ的一些特性： 消息确认机制 队列、消息持久化 消息拒收 默认交换器 mandatory（防止丢失） 官方文档：https://www.rabbitmq.com/documentation.html 6、Centos7下RabbitMQ安装 安装指南：https://www.rabbitmq.com/install-rpm.html 6.1、安装Erlang版本选择：https://www.rabbitmq.com/which-erlang.html 下载地址：https://github.com/rabbitmq/erlang-rpm/releases 选择：rabbitmq官方维护的专为rabbitmq准备的erlang版本 vi /etc/yum.repos.d/rabbitmq_erlang.repo写入: 1234567[rabbitmq-erlang]name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq-erlang/rpm/erlang/22/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascrepo_gpgcheck=0enabled=1 安装：yum install erlang -y 6.2、安装RabbitMQ官网：https://github.com/rabbitmq/rabbitmq-server/releases 123rpm --import https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.ascvi /etc/yum.repos.d/rabbitmq.repo 123456[bintray-rabbitmq-server]name=bintray-rabbitmq-rpmbaseurl=https://dl.bintray.com/rabbitmq/rpm/rabbitmq-server/v3.7.x/el/7/gpgcheck=0repo_gpgcheck=0enabled=1 安装 1yum install -y rabbitmq-server 6.3、开机启动RabbitMQ1chkconfig rabbitmq-server on 6.4、启动运行123456789/sbin/service rabbitmq-server start# 默认会以用户rabbitmq运行，注意日志目录等写权限# /var/lib/rabbitmq -&gt; .erlang.cookie# /var/log/rabbitmq/sbin/service rabbitmq-server stopps aux | grep rabbitmqnetstat -nltp 6.5、防火墙设置 4369: epmd, a peer discovery service used by RabbitMQ nodes and CLI tools 5672, 5671: used by AMQP 0-9-1 and 1.0 clients without and with TLS 25672: used for inter-node and CLI tools communication (Erlang distribution server port) and is allocated from a dynamic range (limited to a single port by default, computed as AMQP port + 20000). Unless external connections on these ports are really necessary (e.g. the cluster uses federation or CLI tools are used on machines outside the subnet), these ports should not be publicly exposed. See networking guide for details. 35672-35682: used by CLI tools (Erlang distribution client ports) for communication with nodes and is allocated from a dynamic range (computed as server distribution port + 10000 through server distribution port + 10010). See networking guide for details. 15672: HTTP API clients, management UI and rabbitmqadmin (only if the management plugin is enabled) 61613, 61614: STOMP clients without and with TLS (only if the STOMP plugin is enabled) 1883, 8883: (MQTT clients without and with TLS, if the MQTT plugin is enabled 15674: STOMP-over-WebSockets clients (only if the Web STOMP plugin is enabled) 15675: MQTT-over-WebSockets clients (only if the Web MQTT plugin is enabled) 15692: Prometheus metrics (only if the Prometheus plugin is enabled) 123firewall-cmd --query-port=5672/tcpfirewall-cmd --permanent --add-port=5672/tcpfirewall-cmd --reload 7、插件安装12[root@dn1 ~]# rabbitmq-plugins disable rabbitmq_management[root@dn1 ~]# rabbitmq-plugins enable rabbitmq_management 8、配置优化官方文档：https://www.rabbitmq.com/configure.html ​ rabbitmq的默认设置往往能够适应大多数开发或OA环境，但是对于生产环境来说还需要一定的优化。 8.1、配置文件位置：https://www.rabbitmq.com/configure.html#config-file-location 1234node : rabbit@examplehome dir : /var/lib/rabbitmqconfig file(s) : /etc/rabbitmq/advanced.config : /etc/rabbitmq/rabbitmq.conf 如果不能确定你的配置文件的路径，查阅 log文件。或者在ManagerUI中查找 12[root@dn1 rabbitmq]# find / -name rabbitmq.co*/usr/share/doc/rabbitmq-server-3.7.24/rabbitmq.config.example 8.2、如何检查配置是否生效？12rabbitmqctl environmentrabbitmqctl [--node &lt;node&gt;] [--longnames] [--quiet] environment 8.3、新老配置文件格式rabbitmq 3.7.0 之前格式 rabbitmq.config，使用erlang 配置格式 1234567[ &#123;rabbit, [&#123;ssl_options, [&#123;cacertfile, "/path/to/ca_certificate.pem"&#125;, &#123;certfile, "/path/to/server_certificate.pem"&#125;, &#123;keyfile, "/path/to/server_key.pem"&#125;, &#123;verify, verify_peer&#125;, &#123;fail_if_no_peer_cert, true&#125;]&#125;]&#125;]. rabbitmq 3.7.0 之后，推荐使用新的 sysctl格式 12345ssl_options.cacertfile = /path/to/ca_certificate.pemssl_options.certfile = /path/to/server_certificate.pemssl_options.keyfile = /path/to/server_key.pemssl_options.verify = verify_peerssl_options.fail_if_no_peer_cert = true 8.4、rabbitmq-env.conf样例 /usr/lib/rabbitmq/lib/rabbitmq_server-3.7.24/sbin/rabbitmq-defaults 配置 1234567#!/bin/sh -e# NODENAME=rabbitCONFIG_FILE=/etc/rabbitmq/rabbitmq.conf# ADVANCED_CONFIG_FILE=/etc/rabbitmq/advanced.config# LOG_BASE=# MNESIA_BASE=RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS="+A 24" 查看 1rabbitmqctl environment 8.5、rabbitmq.conf样例：https://github.com/rabbitmq/rabbitmq-server/blob/v3.7.x/docs/rabbitmq.conf.example 通过 rabbitmq-env.conf 来指定使用哪个配置文件，可以用来指定rabbitmq-dev.conf 或 rabbitmq-prod.conf： 其实是 RABBITMQ_CONFIG_FILE 省略了前缀 RABBITMQ_，就变成了 CONFIG_FILE. 核心配置选项 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061listeners.tcp.default = 5672 # AMQP 对外服务端口num_acceptors.tcp = 10 # 用来 accept tcp连接的 erlang 线程数handshake_timeout = 10000 # AMQP 0-9-1 的 tcp握手超时时间# listeners.ssl # 监听 tls加密的 AMQP 对外端口# num_acceptors.ssl = 10 # 用来 accept tls连接的 erlang 线程数# ssl_handshake_timeout = 5000 # AMQP 0-9-1 的 tls握手超时时间ssl_options = none # 默认不启用 SSLvm_memory_calculation_strategy = allocated # 内存使用报告的统计方式vm_memory_high_watermark.relative = 0.6 # 触发流控的内存阈值比例[0.4~0.66]# vm_memory_high_watermark.absolute = 2GB # 触发流控的内存阈值 绝对值vm_memory_high_watermark_paging_ratio = 0.75 # 当内存水位线达到该百分比时，将消息持久化到硬盘 以释放内存，触发换页操作比例# total_memory_available_override_value # 手动设置内存总量，而不是通过系统函数取，默认不设置# 当空间小于50MB时触发流控机制&#123;disk_free_limit, 50000000&#125;,log.file.level = info# 与客户端协商最大帧大小&#123;frame_max,131072&#125;channel_max = 2047 # 最大信道个数channel_operation_timeout = 15000 # 信道运行超时时间max_message_sizeheartbeat = 60 # 服务端和客户端心跳间隔，0表示禁用心跳default_vhost = /default_user = guestdefault_pass = guestdefault_user_tags.administrator = truedefault_permissions.configure = .*default_permissions.read = .*default_permissions.write = .*&#123;loopback_users,[&lt;&lt;"guest"&gt;&gt;]&#125;, # 只能通过本地网络来访问broker的用户列表cluster_formation.classic_config.nodes.1 = rabbit@hostname1cluster_formation.classic_config.nodes.2 = rabbit@hostname2collect_statistics = nonecollect_statistics_interval = 5000management_db_cache_multiplier = 5auth_mechanisms.1 = PLAINauth_mechanisms.2 = AMQPLAINauth_backends.1 = internalreverse_dns_lookups = falsedelegate_count = 16tcp_listen_options.backlog = 128tcp_listen_options.nodelay = truetcp_listen_options.linger.on = truetcp_listen_options.linger.timeout = 0tcp_listen_options.exit_on_close = falsehipe_compile = false # 即时编译，支持的平台可以提升CPU20%~50%的性能cluster_partition_handling = ignore # 处理网络分区cluster_keepalive_interval = 10000 # 向其他节点发送存活消息频率queue_index_embed_msgs_below = 4096 # 消息的大小小于此值会直接嵌入到队列的索引中mnesia_table_loading_retry_timeout = 30000mnesia_table_loading_retry_limit = 10mirroring_sync_batch_size = 4096queue_master_locator = client-local # 创建队列时以什么策略判断坐落的broker节点proxy_protocol = false vi /etc/rabbitmq/rabbitmq.conf 123456789101112131415listeners.tcp.default = 5672tcp_listen_options.backlog = 2048tcp_listen_options.nodelay = truevm_memory_high_watermark.relative = 0.6vm_memory_high_watermark_paging_ratio = 0.75default_user = admindefault_pass = adminhipe_compile = falsequeue_master_locator = client-localcluster_partition_handling = pause_minoritynet_ticktime = 60management.tcp.port = 15672management.tcp.ip = 0.0.0.0 12345678910[root@dn1 rabbitmq]# chown rabbitmq:rabbitmq rabbitmq.conf[root@dn1 rabbitmq]# chown rabbitmq:rabbitmq rabbitmq-env.conf[root@dn1 rabbitmq]# chmod 666 rabbitmq-env.conf[root@dn1 rabbitmq]# chmod 666 rabbitmq.conf# 配置中遇到错误，查看原因[root@dn1 ~]# service rabbitmq-server restart[root@dn1 ~]# journalctl -xe[root@dn1 ~]# rabbitmqctl cluster_status 8.6、advanced.config样例：https://github.com/rabbitmq/rabbitmq-server/blob/master/docs/advanced.config.example 核心配置选项 12345678&#123;msg_store_index_module, rabbit_msg_store_ets_index&#125;&#123;backing_queue_module, rabbit_variable_queue&#125;&#123;msg_store_file_size_limit, 16777216&#125;&#123;trace_vhosts, []&#125;&#123;msg_store_credit_disc_bound, &#123;4000, 800&#125;&#125;&#123;queue_index_max_journal_entries, 32768&#125;&#123;lazy_queue_explicit_gc_run_operation_threshold, 1000&#125;&#123;queue_explicit_gc_run_operation_threshold, 1000&#125; 8.7、配置加密项 8.8、操作系统Kernel限制123456789101112131415161718192021222324252627# 永久更改最大打开文件描述符数# 第一步# vi /etc/security/limits.conf* soft nofile 1000000* hard nofile 1000000 #星号表示对所有用户生效# 第二步/usr/lib/systemd/system/rabbitmq-server.service[Service]LimitNOFILE=1000000# 第三步/etc/systemd/system/rabbitmq-server.service.d/limits.conf[Service]LimitNOFILE=1000000# 第四步systemctl daemon-reloadservice rabbitmq-server startreboot #重启系统# 第五步（查看是否生效）ulimit -n# sysctl fs.file-max#PID是应用的进程ID，在输出结果中查看"Max open files"的显示值ps aux | grep rabbitmqcat /proc/$&#123;rabbitmq-pid&#125;/limits 123456789101112131415161718192021[root@dn1 ~]# vi /etc/sysctl.conffs.file-max=1000000net.core.somaxconn=65535net.ipv4.tcp_max_syn_backlog=8192net.ipv4.ip_local_port_range=32768 60999net.ipv4.tcp_max_tw_buckets=20000net.ipv4.conf.default.rp_filter=1net.ipv4.tcp_timestamps=1net.ipv4.tcp_tw_reuse=1net.ipv4.tcp_tw_recycle=1net.ipv4.tcp_fin_timeout=30net.ipv4.tcp_keepalive_time=120net.ipv4.tcp_keepalive_intvl=75net.ipv4.tcp_keepalive_probes=4net.ipv4.ip_nonlocal_bind=1 net.ipv4.ip_forward=1[root@dn1 ~]# sysctl -p 8.9、生产环境检查官方文档：https://www.rabbitmq.com/production-checklist.html 9、集群配置 为什么需要集群？ 保证高可用 可以保证消息的万无一失吗？不能，队列跟消息存储在单一节点上。exchange和绑定可以复制。默认不会对队列中的消息进行复制。但可以解决 队列在及集群是如何存在的？默认不会复制 交换器在集群中如何存在的？会复制 集群中的节点有什么要求？ 为了保障高可用，建议至少在集群中设置两个磁盘节点，其他为内存节点，提高集群的吞吐量 9.1、单机多节点集群一台机器上，开不同端口 9.2、多机多节点集群1、前提 只能在内网使用，对网络非常敏感 停止rabbitmq节点：/sbin/service rabbitmq-server stop 2、修改3台节点host文件12345vi /etc/hosts192.168.145.133 dn1192.168.145.134 dn2192.168.145.135 dn3 3、修改 erlang.cookie1234cat /var/lib/rabbitmq/.erlang.cookie# 使用 node1 的 .erlang.cookie 覆盖其他所有节点的值echo 'LLSFPZFATCBYBMCXMMAC' &gt; /var/lib/rabbitmq/.erlang.cookie 4、配置集群1、启动节点 1service rabbitmq-server start 2、查看集群状态 1rabbitmqctl cluster_status 3、将节点2，3加入集群 node2作为磁盘节点加入集群 1234567891011121314151617[root@dn2 ~]# rabbitmqctl stop_appStopping rabbit application on node rabbit@dn2 ...[root@dn2 ~]# rabbitmqctl resetResetting node rabbit@dn2 ...[root@dn2 ~]# rabbitmqctl join_cluster rabbit@dn1Clustering node rabbit@dn2 with rabbit@dn1[root@dn2 ~]# rabbitmqctl start_appStarting node rabbit@dn2 ... completed with 0 plugins.[root@dn2 ~]# rabbitmqctl cluster_statusCluster status of node rabbit@dn2 ...[&#123;nodes,[&#123;disc,[rabbit@dn1,rabbit@dn2]&#125;]&#125;, &#123;running_nodes,[rabbit@dn1,rabbit@dn2]&#125;, &#123;cluster_name,&lt;&lt;"rabbit@dn1"&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit@dn1,[]&#125;,&#123;rabbit@dn2,[]&#125;]&#125;][root@dn2 ~]# node3 作为内存节点加入集群 1234567891011121314151617[root@dn3 ~]# rabbitmqctl stop_appStopping rabbit application on node rabbit@dn3 ...[root@dn3 ~]# rabbitmqctl resetResetting node rabbit@dn3 ...[root@dn3 ~]# rabbitmqctl join_cluster rabbit@dn1 --ramClustering node rabbit@dn3 with rabbit@dn1[root@dn3 ~]# rabbitmqctl start_appStarting node rabbit@dn3 ... completed with 0 plugins.[root@dn3 ~]# rabbitmqctl cluster_statusCluster status of node rabbit@dn3 ...[&#123;nodes,[&#123;disc,[rabbit@dn2,rabbit@dn1]&#125;,&#123;ram,[rabbit@dn3]&#125;]&#125;, &#123;running_nodes,[rabbit@dn1,rabbit@dn2,rabbit@dn3]&#125;, &#123;cluster_name,&lt;&lt;"rabbit@dn1"&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit@dn1,[]&#125;,&#123;rabbit@dn2,[]&#125;,&#123;rabbit@dn3,[]&#125;]&#125;][root@dn3 ~]# 5、踢出单个节点方法一： 12345[root@dn2 ~]# service rabbitmq-server stop[root@dn3 ~]# service rabbitmq-server stop[root@dn2 ~]# rabbitmqctl forget_cluster_node rabbit@dn3 --offline[root@dn2 ~]# service rabbitmq-server start[root@dn2 ~]# rabbitmqctl cluster_status 方法二： 123[root@dn3 ~]# rabbitmqctl start_app[root@dn3 ~]# rabbitmqctl reset[root@dn3 ~]# rabbitmqctl start_app 问题解决： 123456782020-04-03 15:36:52.980 [error] &lt;0.165.0&gt; Mnesia(rabbit@dn2): ** ERROR ** (core dumped to file: "/var/lib/rabbitmq/MnesiaCore.rabbit@dn2_1585_899412_979927") ** FATAL ** Failed to merge schema: Bad cookie in table definition tracked_connection_per_vhost_on_node_rabbit@dn3: rabbit@dn2 = &#123;cstruct,tracked_connection_per_vhost_on_node_rabbit@dn3,set,[rabbit@dn3],[],[],[],0,read_write,false,[],[],false,tracked_connection_per_vhost,[vhost,connection_count],[],[],[],&#123;&#123;1585491411119336579,-576460752303418047,1&#125;,rabbit@dn3&#125;,&#123;&#123;2,0&#125;,[]&#125;&#125;, rabbit@dn3 = &#123;cstruct,tracked_connection_per_vhost_on_node_rabbit@dn3,set,[rabbit@dn3],[],[],[],0,read_write,false,[],[],false,tracked_connection_per_vhost,[vhost,connection_count],[],[],[],&#123;&#123;1585898870953098299,-576460752303421919,1&#125;,rabbit@dn3&#125;,&#123;&#123;2,0&#125;,[]&#125;&#125; # 文件可能被修改了，恢复原状/var/lib/rabbitmq/mnesia/rabbit\@dn2/cluster_nodes.config [root@dn2 ~]# service rabbitmq-server stop[root@dn3 ~]# service rabbitmq-server stop[root@dn2 ~]# rabbitmqctl forget_cluster_node rabbit@dn3 --offline 6、异常处理确保节点可以启动，即使它不是最后一个关闭的节点 1rabbitmqctl force_boot 设置集群名称 123rabbitmqctl set_cluster_name &#123;name&#125;rabbitmqctl cluster_statusrabbitmqctl set_cluster_name rabbitmq_cluster_dev 修改集群节点的类型。在这个命令执行前需要停止RabbitMQ应用 123rabbitmqctl stop_apprabbitmqctl change_cluster_node_type &#123;disc|ram&#125;rabbitmqctl start_app 10、负载均衡10.1、使用HAProxy实现负载均衡配置各个负载的内核参数1234567vim /etc/sysctl.conf#开启允许绑定非本机的IP，haporxy启动忽视VIP存在net.ipv4.ip_nonlocal_bind = 1 #内核是否转发数据包net.ipv4.ip_forward = 1sysctl -p 安装Haproxy安装依赖包 1yum install gcc openssl-devel readline-devel systemd-devel make pcre-devel -y 123456789101112131415cd /data/soft/new/wget https://github.com/lua/lua/archive/v5.3.5.tar.gz -O lua-5.3.5.tag.gztar xf lua-5.3.5.tag.gzcd lua-5.3.5wget https://github.com/haproxy/haproxy/archive/v1.9.0.tar.gz -O haproxy-1.9.0.tar.gztar -zxf haproxy-1.9.0.tar.gzcd haproxy-1.9.0pwd# /data/soft/new/haproxy-1.9.0make TARGET=linux2628 PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxycp -r examples/errorfiles/ /usr/local/haproxy/ PREFIX 为指定的安装路径 TARGET则根据当前操作系统内核版本指定 - linux22 for Linux 2.2 - linux24 for Linux 2.4 and above (default) - linux24e for Linux 2.4 with support for a working epoll (&gt; 0.21) - linux26 for Linux 2.6 and above - linux2628 for Linux 2.6.28, 3.x, and above (enables splice and tproxy) 本文的操作系统内核版本为3.10.0，TARGET指定为 linux2628。 make TARGET=linux2628 USE_PCRE=1 USE_OPENSSL=1 USE_ZLIB=1 # # USE_PCRE=1 开启正则 USE_OPENSSL=1 开启openssl USE_ZLIB=1 # # USE_CPU_AFFINITY=1 为开启haproxy进程与CPU核心绑定，USE_SYSTEMD=1为支持使用 -Ws参数（systemd-aware master-worker 模式）启动Haproxy，从而实现单主进程多子进程运行模式。 配置Haproxy123456789101112131415161718192021222324252627282930313233343536373839mkdir /etc/haproxycat &gt; /etc/haproxy/haproxy.cfg &lt;&lt; EOFglobal log 127.0.0.1 local0 info maxconn 4096 chroot /usr/local/haproxy daemon pidfile /run/haproxy.piddefaults log global mode tcp option tcplog option dontlognull retries 3 maxconn 1000000 timeout connect 5s timeout client 120s timeout server 120slisten rabbitmq_cluster bind 0.0.0.0:5671 mode tcp balance roundrobin server rmq_dn1 dn1:5672 check inter 5000 rise 2 fall 3 weight 1 server rmq_dn2 dn2:5672 check inter 5000 rise 2 fall 3 weight 1 server rmq_dn3 dn3:5672 check inter 5000 rise 2 fall 3 weight 1listen rabbitmq_monitor bind 0.0.0.0:8100 mode http option httplog stats enable stats uri /stats stats refresh 5sEOF$ haproxy -c -f /etc/haproxy/haproxy.cfg 日志配置 123456789打开rsyslog配置：vi /etc/rsyslog.conf去掉下面两行前面的#号$ModLoad imudp$UDPServerRun 514并添加下面一行local0.* /var/log/haproxy.log重启rsyslogsystemctl restart rsyslog 准备system启动脚本12345678cp /usr/local/haproxy/sbin/haproxy /usr/sbin/\cp ./examples/haproxy.init /etc/init.d/haproxychmod 755 /etc/init.d/haproxyuseradd -r haproxychkconfig --add haproxysystemctl daemon-reload 如果服务器安装后修改了hostname，出现了 unable to connect to epmd (port 4369) on bogon: nxdomain (non-existing domain) 这个问题，请看下是否 /etc/hosts 文件、/etc/hostname文件和环境变量$HOSTNAME 是否一致。 如果启动HAProxy出现 /etc/rc.d/init.d/haproxy: line 26: [: =: unary operator expected 这个错误，修改/etc/init.d/haproxy 文件的26行 [ ${NETWORKING} = “no” ] &amp;&amp; exit 0 为 [ “${NETWORKING}” = “no” ] &amp;&amp; exit 0 启动haproxy1234567891011121314151617181920212223242526272829# 检查配置文件语法haproxy -c -f /etc/haproxy/haproxy.cfg# 以daemon模式启动，以systemd管理的daemon模式启动haproxy -D -f /etc/haproxy/haproxy.cfg [-p /var/run/haproxy.pid]haproxy -Ds -f /etc/haproxy/haproxy.cfg [-p /var/run/haproxy.pid]# 启动调试功能，将显示所有连接和处理信息在屏幕haproxy -d -f /etc/haproxy/haproxy.cfg# restart。需要使用st选项指定pid列表haproxy -f /etc/haproxy.cfg [-p /var/run/haproxy.pid] -st `cat /var/run/haproxy.pid`# graceful restart，即reload。需要使用sf选项指定pid列表haproxy -f /etc/haproxy.cfg [-p /var/run/haproxy.pid] -sf `cat /var/run/haproxy.pid`# 显示haproxy编译和启动信息haproxy -vvsystemctl start haproxy.servicesystemctl stop haproxy.servicesystemctl status haproxy.service -lhaproxy &#123;start|stop|restart|reload|condrestart|status|check&#125;service haproxy reloadservice haproxy stopservice haproxy checkservice haproxy statusservice haproxy start 开机启动 1/sbin/chkconfig haproxy on 安装keepalived官网：https://www.keepalived.org/download.html 下载安装 1234567891011cd /data/soft/newwget https://www.keepalived.org/software/keepalived-1.4.5.tar.gztar -zxf keepalived-1.4.5.tar.gzcd keepalived-1.4.5[root@dn1 keepalived-1.4.5]# yum -y install libnl libnl-devel[root@dn1 keepalived-1.4.5]# yum install -y libnfnetlink-devel[root@dn1 keepalived-1.4.5]# ./configure --prefix=/usr/local/keepalived --with-init=SYSV[root@dn1 keepalived-1.4.5]# make[root@dn1 keepalived-1.4.5]# make install 配置系统服务 123456789[root@dn1 keepalived-1.4.5]# cd ~[root@dn1 ~]# cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/[root@dn1 ~]# cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig[root@dn1 ~]# cp /usr/local/keepalived/sbin/keepalived /usr/sbin/[root@dn1 ~]# chmod +x /etc/init.d/keepalived[root@dn1 ~]# chkconfig --add keepalived[root@dn1 ~]# chkconfig keepalived on[root@dn1 ~]# mkdir -p /etc/keepalived[root@dn1 ~]# cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/ 配置keepalived 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@dn1 ~]# vi /etc/keepalived/check_haproxy.sh#!/bin/bashif [ $(ps -C haproxy --no-header | wc -l) -eq 0 ];then /usr/sbin/haproxy -D -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pidfisleep 2if [ $(ps -C haproxy --no-header | wc -l) -eq 0 ];then service keepalived stopfi[root@dn1 ~]# chmod +x /etc/keepalived/check_haproxy.sh[root@dn1 ~]# &gt; /etc/keepalived/keepalived.conf[root@dn1 ~]# vim /etc/keepalived/keepalived.confglobal_defs &#123; router_id node1 script_user root enable_script_security&#125;vrrp_script chk_haproxy &#123; script "/etc/keepalived/check_haproxy.sh" interval 5 weight 2&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 1 priority 100 advert_int 1 unicast_src_ip 192.168.145.13 unicast_peer &#123; 192.168.145.13 192.168.145.13 &#125; authentication &#123; auth_type PASS auth_pass root1111 &#125; track_script &#123; chk_haproxy &#125; virtual_ipaddress &#123; 192.168.145.200 &#125;&#125;[root@dn2 ~]# vim /etc/keepalived/keepalived.confglobal_defs &#123; router_id node2 # 每个节点必须不一样 script_user root enable_script_security &#125;vrrp_script chk_haproxy &#123; script "/etc/keepalived/check_haproxy.sh" interval 5 weight 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 # 根据情况变换 virtual_router_id 1 priority 90 # 每个节点必须不一样 advert_int 1 authentication &#123; auth_type PASS auth_pass root1111 &#125; track_script &#123; chk_haproxy &#125; virtual_ipaddress &#123; 192.168.145.200 &#125;&#125; 启动 1234service keepalived start &amp;&amp; systemctl status keepalived.service -lservice keepalived stopservice keepalived statusservice keepalived restart &amp;&amp; systemctl status keepalived.service -l 1[root@dn1 ~]# ip add show 11、性能测试11.1、RabbitMQ PerfTestPerfTest 是一个基于Java的吞吐量测试工具 1)、下载安装1234# 文档https://rabbitmq.github.io/rabbitmq-perf-test/snapshot/htmlsingle/# 下载地址https://github.com/rabbitmq/rabbitmq-perf-test/releases 12345cd /data/message-queue/rabbitmqwget https://github.com/rabbitmq/rabbitmq-perf-test/releases/download/v2.11.0/rabbitmq-perf-test-2.11.0-bin.tar.gztar -zxf rabbitmq-perf-test-2.11.0-bin.tar.gzcd rabbitmq-perf-test-2.11.0/bin./runjava com.rabbitmq.perf.PerfTest --help 2）、开始压测单压各个节点 1-p， 2-c：发现内存节点和磁盘节点差距不大，应该是消费速度足够快，不需要落到磁盘上 12345678cd /data/message-queue/rabbitmq/rabbitmq-perf-test-2.11.0/bin# 单压内存节点./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-1" -a --id "test 1"# 结果id: test 1, sending rate avg: 36934 msg/sid: test 1, receiving rate avg: 36607 msg/s 12345678910111213141516171819202122232425./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 5 -y 10 -u "throughput-test-2" -a --id "test 2"./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-3" --id "test 3"./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-4" --id "test 4" -s 4000./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-5" --id "test-5" -f persistent./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-6" --id "test-6" -f persistent --multi-ack-every 100./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-7" --id "test-7" -f persistent --multi-ack-every 200 -q 500./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-8" --id "test-8" -f persistent -q 500 -c 500./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-10" --id "test-10" -f persistent -q 500 -pmessages 100000./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-11" --id "test-11" -f persistent -q 500 --rate 5000./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-12" --id "test-12" -f persistent --rate 5000 --consumer-rate 2000./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x 1 -y 2 -u "throughput-test-13" --id "test-13" -f persistent -z 30./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -y0 -p -u "throughput-test-14" -s 1000 -C 1000000 --id "test-14" -f persistent./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.135:5672 -x0 -y10 -p -u "throughput-test-14" --id "test-15" 1 1234567# 压整个集群# 1个生产者，2个消费者，使用队列 throughput-test-1，自动确认./runjava com.rabbitmq.perf.PerfTest -h amqp://admin:admin@192.168.145.200:5671 -x 1 -y 2 -u "throughput-test-1" -a --id "test 1"# 结果：id: test 1, sending rate avg: 30106 msg/sid: test 1, receiving rate avg: 29751 msg/s 使用vmstat 观察到cpu不够用，加cpu以后，效率直线上升 123netstat -tan |grep ^tcp |awk '&#123;++a[$6]&#125; END&#123;for (i in a) print i, a[i]&#125;'netstat -s | grep rejectnetstat -s | grep TCPBacklogDrop]]></content>
      <categories>
        <category>message-queue</category>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[haproxy+keepalived]]></title>
    <url>%2Fhaproxy%2Fhaproxy-keepalived%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>haproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx + keepalived + rabbitmq]]></title>
    <url>%2Fnginx%2Fnginx-keepalived-rabbitmq%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>nginx</tag>
        <tag>keepalived</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi删除空行或注释行]]></title>
    <url>%2Flinux%2Flinux-vi%2F</url>
    <content type="text"><![CDATA[删除空行 1:g/^$/d 删除空行以及只有空格的行 1:g/^\s*$/d 删除以 # 开头或 空格# 或 tab#开头的行 1:g/^\s*#/d 对于 php.ini 配置文件，注释为 ; 开头 1:g/^\s*;/d 使用正则表达式删除行 如果当前行包含 bbs ，则删除当前行 1:/bbs/d 删除从第二行到包含 bbs 的区间行 1:2,/bbs/d 删除从包含 bbs 的行到最后一行区间的行 1:/bbs/,$d 删除所有包含 bbs 的行 1:g/bbs/d 删除匹配 bbs 且前面只有一个字符的行 1:g/.bbs/d 删除匹配 bbs 且以它开头的行 1:g/^bbs/d 删除匹配 bbs 且以它结尾的行 1:g/bbs$/d .ini 的注释是以 ; 开始的，如果注释不在行开头，那么删除 ; 及以后的字符 1:%s/\;.\+//g 删除 # 之后所有字符 1%s/\#.*//g]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最大子序和]]></title>
    <url>%2Fleetcode%2Fmaximum-subarray%2F</url>
    <content type="text"><![CDATA[算法：动态规划​ 题目url：https://leetcode-cn.com/problems/maximum-subarray/ 12345678910给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。示例:输入: [-2,1,-3,4,-1,2,1,-5,4],输出: 6解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。进阶:如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。 分析： 根据 最大和求最长子序列 长度为1的最大和子序列 =&gt; 长度为2的最大和子序列 =&gt; 长度为3的最大和子序列 ​ Java解法1、动态规划求出每个位置（及其之前累计）的最大和： max(i-1) &gt;0 : max(i) = max(i-1) + nums[i] max(i-1) &lt;=0 : max(i) = nums[i] 123456789101112131415161718192021222324252627282930313233class MaxKV &#123; int i; int val; public MaxKV(int i, int val) &#123; this.i = i; this.val = val; &#125;&#125;class Solution &#123; public int maxSubArray(int[] nums) &#123; int maxVal=Integer.MIN_VALUE; MaxKV[] maxKV = new MaxKV[nums.length]; for(int i=0; i&lt;nums.length; ++i) &#123; if(0==i) &#123; maxKV[i] = new MaxKV(i, nums[i]); &#125; else &#123; if(maxKV[i-1].val &gt; 0) &#123; maxKV[i] = new MaxKV(maxKV[i-1].i, nums[i]+maxKV[i-1].val); &#125; else &#123; maxKV[i] = new MaxKV(i, nums[i]); &#125; &#125; if(maxVal &lt; maxKV[i].val) maxVal = maxKV[i].val; &#125; return maxVal; &#125;&#125;public class App &#123; public static void main(String[] args) &#123; int[] nums = new int[] &#123;-1&#125;;// int[] nums = new int[] &#123;-2,1,-3,4,-1,2,1,-5,4&#125;; System.out.println(new Solution().maxSubArray(nums)); &#125;&#125; ​ 2、分治法]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab+Jenkins+Harbor+Kubernetes集成应用]]></title>
    <url>%2Ftools%2Fjenkins-gitlab-harbor-k8s%2F</url>
    <content type="text"><![CDATA[docker安装jenkinsjenkins官网：http://mirrors.jenkins.io/war-stable/ docker仓库：https://hub.docker.com/_/jenkins docker pull jenkins:2.7.4-alpine docker run -d -p 8180:8080 -p 50100:50000 –name jenkins-dev jenkins:2.7.4-alpine 访问：http://172.18.1.84:8180 docker exec -it jenkins-dev /bin/bash cd /var/jenkins_home/ cat /var/jenkins_home/secrets/initialAdminPassword e8ad89a38a244e80a174b9be916e0f12 选择推荐安装，安装插件 -&gt; 设置管理员账号admin/admin -&gt; 开始使用 centos7部署jenkins.war12345678910111213141516echo $JAVA_HOME/usr/lib/jvm/java-1.8-openjdkmkdir -p /data/jenkinschmod 666 /data/jenkinscd /data/jenkins/java -version# https://jenkins.io/zh/download/ 下载jenkins.warjava -Xmx512m -jar jenkins.war --httpPort=9090# java -Djenkins.install.runSetupWizard=false -jar jenkins.war --httpPort=90901136d6ccb876433a9d4204578a73bdfd# 跳过插件安装# 设置用户名密码# 修改站点升级镜像地址vi ~/.jenkins/hudson.model.UpdateCenter.xml jenkins 插件安装使用国内插件镜像路径： jenkins -&gt; 插件管理 -&gt; 高级 -&gt; 升级站点，修改地址： https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 安装Gitlab插件 安装Docker插件 安装k8s插件 打通jenkins与k8s-master两台机器ssh免密登陆Jenkins Kubernetes配置]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用cronlog 对 tomcat 的 catalina.out 日志自动切割]]></title>
    <url>%2Fjava%2Ftomcat%2Ftomcat-log-catalina-cronlog%2F</url>
    <content type="text"><![CDATA[一、cronlog 采用cronlolog工具对日志拆分的方式处理该问题 1、下载cronolog工具，我下载的版本是cronolog-1.6.2 （yum安装:yum install cronolog） 2、将下载好的文件解压，tar xvzf cronolog-1.6.2.tar.gz 3、切换到解压后的文件目录下：cd cronolog-1.6.2 4、初始化和编译安装 12345./configure --prefix=/usr/local/cronolog#make#make install#ln -s /usr/local/cronolog/sbin/* /usr/local/sbin/ 5、查看安装版本 1#cronolog --version 6、修改tomcat的启动文件（tomcat目录/bin/catalina.sh） 1vi catalina.sh （1）修改输出日志路径 修改： 123 if [ -z "$CATALINA_OUT" ] ; then CATALINA_OUT="$CATALINA_BASE"/logs/catalina.outfi 为： 123 if [ -z "$CATALINA_OUT" ] ; then CATALINA_OUT="$CATALINA_BASE"/logs/catalina.%Y-%m-%d.outfi （2）删除生成日志文件 注释： 123touch "$CATALINA_OUT" 为：#touch "$CATALINA_OUT" （3）修改启动脚本参数（两项） 修改： org.apache.catalina.startup.Bootstrap &quot;$@&quot; start \ &gt;&gt; &quot;$CATALINA_OUT&quot; 2&gt;&amp;1 &quot;&amp;&quot; 为： org.apache.catalina.startup.Bootstrap &quot;$@&quot; start 2&gt;&amp;1 \ | /usr/local/sbin/cronolog &quot;$CATALINA_OUT&quot; &gt;&gt; /dev/null &amp; 7、重启tomcat tomcat输出日志文件分割成功，输出log文件格式为：catalina.2015-06-30.out 二、logrotate 利用Linux自带的logrotate程序来解决catalina.out的日志轮转问题 1.首先编辑logrotate.conf文件，打开compress选项（去掉注释） 123456[root@localhost ~]# cat /etc/logrotate.conf | grep -v "^$"| grep -v "#"weeklyrotate 4createdateextcompress &lt;&lt;这一项 2.添加指定文件，在/etc/logrotate.d/目录下新建一个名为tomcat的文件 12345678910[root@localhost ~]# cat &gt; /etc/logrotate.d/tomcat &lt;&lt;EOF/home/tomcat/utr/logs/catalina.out&#123; #要轮转的文件 copytruncate # 创建新的catalina.out副本后，截断源catalina.out文件 daily # 每天进行catalina.out文件的轮转 rotate 7 # 至多保留7个副本 missingok # 如果要轮转的文件丢失了，继续轮转而不报错 compress # 使用压缩的方式（节省硬盘空间；一个2~3GB的日志文件可以压缩成60MB左右） size 16M # 当catalina.out文件大于16MB时，就轮转&#125;EOF 参数说明： copytruncate #备份日志并截断源文件 nocopytruncate # 备份日志文件不截断 dateext #使用当期日期作为命名格式 notifempty #当日志文件为空时，不进行轮转 daily # 每天进行文件的轮转 size 16M # 当文件大于16MB时，就会轮转 rotate 30 #指定日志文件删除之前转储的次数 3.执行方式 ①自动执行原理 12341.每天晚上crond守护进程会运行在/etc/cron.daily目录中的任务列表；2.与logrotate相关的脚本也在/etc/cron.daily目录中。运行的方式为"/usr/bin/logrotate /etc/logrotate.conf"；3./etc/logrotate.conf文件include了/etc/logrotate.d/目录下的所有文件。还包括我们上面刚创建的tomcat文件；4./etc/logrotate.d/tomcat文件会触发/usr/local/apache-tomcat-8.0.28/logs/catalina.out文件的轮转。 ②手动执行：logrotate /etc/logrotate.conf ③只轮转刚刚的tomcat配置文件:logrotate –force /etc/logrotate.d/tomcat 三、log4j Tomcat7.0.55下使用Log4j 接管 catalina.out 日志文件生成方式，按天存放，解决catalina.out日志文件过大问题 准备jar包： log4j-1.2.17.jar （从 http://www.apache.org/dist/logging/log4j/1.2.17/ 下载） tomcat-juli.jar, tomcat-juli-adapters.jar （从http://www.apache.org/dist/tomcat/tomcat-7/v7.0.55/bin/extras/下载，根据你的Tomcat版本选择对应的分支） 将上面的三个jar包拷贝到 Tomcat 的 lib 目录下； 将 tomcat-juli.jar 拷贝到 Tomcat 的 bin 目录下，替换原有的jar包； 修改 Tomcat 的 conf/context.xml 文件，将为 （增加 swallowOutput=”true” 的属性配置，只有这样才能完全的把tomcat的stdout给接管过来。这一步很关键 在官网及网上找了许多资料都没有提及。）； 删除 Tomcat 的 conf/logging.properties 文件（或者重命名-建议）； 在 Tomcat 的 lib 目录下创建 log4j.properties 文件，然后重启服务器： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051log4j.rootLogger=INFO, CATALINAlog4j.logger.org.apache=INFO, CATALINAlog4j.logger.org.[hibernate](http://lib.csdn.net/base/javaee)=WARN, CATALINAlog4j.logger.org.springframework=WARN, CATALINA # Define all the appenderslog4j.appender.CATALINA=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.CATALINA.File=$&#123;catalina.base&#125;/logs/catalina.outlog4j.appender.CATALINA.Append=truelog4j.appender.CATALINA.Encoding=UTF-8# Roll-over the log once per daylog4j.appender.CATALINA.DatePattern='.'yyyy-MM-dd'.log'log4j.appender.CATALINA.layout = org.apache.log4j.PatternLayoutlog4j.appender.CATALINA.layout.ConversionPattern = %d [%t] %-5p %c- %m%n log4j.appender.LOCALHOST=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.LOCALHOST.File=$&#123;catalina.base&#125;/logs/localhost.log4j.appender.LOCALHOST.Append=truelog4j.appender.LOCALHOST.Encoding=UTF-8log4j.appender.LOCALHOST.DatePattern='.'yyyy-MM-dd'.log'log4j.appender.LOCALHOST.layout = org.apache.log4j.PatternLayoutlog4j.appender.LOCALHOST.layout.ConversionPattern = %d [%t] %-5p %c- %m%n log4j.appender.MANAGER=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.MANAGER.File=$&#123;catalina.base&#125;/logs/manager.log4j.appender.MANAGER.Append=truelog4j.appender.MANAGER.Encoding=UTF-8log4j.appender.MANAGER.DatePattern='.'yyyy-MM-dd'.log'log4j.appender.MANAGER.layout = org.apache.log4j.PatternLayoutlog4j.appender.MANAGER.layout.ConversionPattern = %d [%t] %-5p %c- %m%n log4j.appender.HOST-MANAGER=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.HOST-MANAGER.File=$&#123;catalina.base&#125;/logs/host-manager.log4j.appender.HOST-MANAGER.Append=truelog4j.appender.HOST-MANAGER.Encoding=UTF-8log4j.appender.HOST-MANAGER.DatePattern='.'yyyy-MM-dd'.log'log4j.appender.HOST-MANAGER.layout = org.apache.log4j.PatternLayoutlog4j.appender.HOST-MANAGER.layout.ConversionPattern = %d [%t] %-5p %c- %m%n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppenderlog4j.appender.CONSOLE.Encoding=UTF-8log4j.appender.CONSOLE.layout = org.apache.log4j.PatternLayoutlog4j.appender.CONSOLE.layout.ConversionPattern = %d [%t] %-5p %c- %m%n # Configure which loggers log to which appenders# Configure which loggers log to which appenderslog4j.logger.org.apache.catalina.core.ContainerBase.[Catalina].[localhost]=INFO, LOCALHOSTlog4j.logger.org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager]=\ INFO, MANAGERlog4j.logger.org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/host-manager]=\ INFO, HOST-MANAGER 四、shell脚本切割 通过shell脚本的方式切割每天的日志 windows部署在windows服务器下的tomcat，将tomcat控制台日志记录到日志文件中 https://www.cnblogs.com/linaGh/p/7777915.html 对于抛出的异常错误，不会被存到文件中的，可以修改代码或者修改cmd属性-布局中的高度，默认是300 高度即行数，设置大点就可以看到没有存到文件中的日志信息 作者：Rooot链接：https://www.jianshu.com/p/396675386a43来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>java</category>
        <category>tomcat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP协议中backlog参数]]></title>
    <url>%2Fcomputer-network%2Ftcp-backlog%2F</url>
    <content type="text"><![CDATA[TCP建立连接是要进行三次握手，但是否完成三次握手后，服务器就处理（accept）呢？ backlog其实是一个连接队列，在Linux内核2.2之前，backlog大小包括半连接状态和全连接状态两种队列大小。 半连接状态为：服务器处于Listen状态时收到客户端SYN报文时放入半连接队列中，即SYN queue（服务器端口状态为：SYN_RCVD）。 全连接状态为：TCP的连接状态从服务器（SYN+ACK）响应客户端后，到客户端的ACK报文到达服务器之前，则一直保留在半连接状态中；当服务器接收到客户端的ACK报文后，该条目将从半连接队列搬到全连接队列尾部，即 accept queue （服务器端口状态为：ESTABLISHED）。 在Linux内核2.2之后，分离为两个backlog来分别限制半连接（SYN_RCVD状态）队列大小和全连接（ESTABLISHED状态）队列大小。 SYN queue 队列长度由 /proc/sys/net/ipv4/tcp_max_syn_backlog 指定，默认为2048。 Accept queue 队列长度由 /proc/sys/net/core/somaxconn 和使用listen函数时传入的参数，二者取最小值。默认为128。在Linux内核2.4.25之前，是写死在代码常量 SOMAXCONN ，在Linux内核2.4.25之后，在配置文件 /proc/sys/net/core/somaxconn 中直接修改，或者在 /etc/sysctl.conf 中配置 net.core.somaxconn = 128 。 可以通过ss命令来显示 1234567[root@localhost ~]# ss -lState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:http *:* LISTEN 0 128 :::ssh :::* LISTEN 0 128 *:ssh *:* LISTEN 0 100 ::1:smtp :::* LISTEN 0 100 127.0.0.1:smtp *:* 在LISTEN状态，其中 Send-Q 即为Accept queue的最大值，Recv-Q 则表示Accept queue中等待被服务器accept()。 另外客户端connect()返回不代表TCP连接建立成功，有可能此时accept queue 已满，系统会直接丢弃后续ACK请求；客户端误以为连接已建立，开始调用等待至超时；服务器则等待ACK超时，会重传SYN+ACK 给客户端，重传次数受限 net.ipv4.tcp_synack_retries ，默认为5，表示重发5次，每次等待30~40秒，即半连接默认时间大约为180秒，该参数可以在tcp被洪水攻击是临时启用这个参数。 查看SYN queue 溢出 12[root@localhost ~]# netstat -s | grep LISTEN102324 SYNs to LISTEN sockets dropped 查看Accept queue 溢出 12[root@localhost ~]# netstat -s | grep TCPBacklogDropTCPBacklogDrop: 2334 1[root@nauru-084 ~]# netstat -s 参考资料： TCP SOCKET中backlog参数的用途是什么？ TCP/IP协议中backlog分析与设置以及TCP状态变化 TCP3次握手和backlog溢出 TCP queue 的一些问题 TCP洪水攻击（SYN Flood）的诊断和处理]]></content>
      <categories>
        <category>computer-network</category>
        <category>tcp/ip</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx 文档]]></title>
    <url>%2Fnginx%2Fnginx-docs%2F</url>
    <content type="text"><![CDATA[官网：http://nginx.org/ 官方文档：https://www.nginx.com/resources/wiki/ 中文文档：http://www.nginx.cn/doc/]]></content>
      <categories>
        <category>nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[那些让你起飞的计算机基础知识]]></title>
    <url>%2Foperating-system%2Fos-basic-concept%2F</url>
    <content type="text"><![CDATA[1、信息的表示和处理计算机如何表示整数：有符号数和无符号数，尤其是如何用补码表示负数，数字的取值范围。 计算机如何表示浮点数，为什么小数的二进制表示法只能近似表示十进制小数。 数值的转换、移位 这几点非常重要，因为几乎所有的编程语言都有数据类型，而最基本数据类型必然包括整数和浮点数。 搞不清这些表示和运算，在编程中就会遇到一些稀奇古怪的问题。 2、从汇编层面理解程序的执行顺序、分支、循环、函数调用、数组、结构体等在汇编层面是怎么实现的，寄存器和内存是怎么使用的。 理解了这些其实也就理解了冯诺依曼计算机体系结构，这是计算机学科一个基础性的东西。 知道程序在底层是怎么运转的， 对于学习各种虚拟机有很大的帮助，比如 JVM，它要解析执行的是字节码，字节码本质上要表达的就是这些东西，只不过有所扩展。 理解了栈帧，就能理解函数调用的本质，递归，以及尾递归的实现。还有安全相关的概念，如缓冲区溢出这个臭名卓著的漏洞及其防范办法。 3、进程和线程程序员必备的知识，不了解这个，简直是无法编程。 需要掌握进程的地址空间，代码在哪里，堆在哪里，栈在哪里。 要准确理解进程和线程之间的关系，为什么说进程是拥有资源的基本单位， 线程是CPU调度的基本单位？ 进程切换和线程切换之间的区别和联系。 他们是如何创建，执行，有哪些状态，状态之间的转换。 由此会涉及到 并发和并行，线程之间的竞争和合作。 锁 的本质（硬件层面），乐观锁，悲观锁，死锁等问题。 线程的实现方式，用户级线程和内核级线程的对应方式。 在编程的过程中，有些知识点会直接使用，如多线程编程，锁。 还有一些概念能用到很多地方，例如CAS，不仅仅是编程语言的概念，还能在更新数据库时使用。再比如你理解了线程的实现方式，迅速就能掌握go语言中并发的手段：goroutine。 4、存储器的层次结构Tomcat用了多线程执行请求，Redis用了单线程来处理请求，Node.js也用了单线程来，这是为什么？ 秘密都在存储器的层次结构。 人类制造的计算机设备之间有着巨大的速度差异： 总之，CPU超级快，内存比较快，硬盘非常慢，网络更慢， 这个速度差异是IT行业的一个核心问题，人类想了很多办法试图去弥补这个差异：多线程，缓存(局部性原理)，异步，多路复用，硬件层面的DMA。 记着下面这张图，每当你遇到某个软件的特性的时候，想一想和它有什么关系： 5、数据结构和算法它的重要性我罗嗦过很多次了，不用再重复了， 我就举个最简单的例子： 理解了B+ Tree才能理解MySQL的InnoDB的索引，理解了索引才能更好地优化查询，对吧？ 6、计算机网络现在的程序基本上都是网络程序， 所以这也是一个必备的基础知识，学习计算机网络的一大好处就是和工作直接相关，能直接使用，比较有动力。 HTTP协议肯定跑不掉，TCP, UDP也得会，尤其是TCP可靠传输的原理：如何在一个不可靠的网络中进行可靠的传输， 这是无数前辈总结的经验，一定得掌握。 要理解什么是通信协议，也许某一天你自己就需要定制一个协议来传输数据。 分组交换是什么意思？ 协议分层的本质是什么？ 什么叫无状态的协议？ Socket相关的编程更是重点，尤其是涉及到服务器端高并发的时候，如何维持和处理这些海量的socket， epoll等技术就得上场了。 还有非常重要的https的基本原理，也是网络安全的精华所在：对称加密，非对称加密，消息摘要，数字证书，中间人攻击。 7、数据库不多说，关系模型、范式、SQL、索引、事务等知识都得掌握，尤其是要了解他们的实现方式。 8、分布式的基础知识这些已经偏向应用层面了，但是现在很多系统都是分布式的了，分布式就变成了一种基础知识。 系统通信：RPC, 消息队列等 负载均衡的原理 CAP原理，BASE原理，幂等性，一致性模型（强一致性，最终一致性…）和相关协议(两阶段提交，Raft，Paxos…) 数据分片：取模算法，一致性Hash，虚拟桶 9、基本的设计思想下面这几种设计思想对我影响很大，需要大家特别注意。但是掌握起来却很不容易，需要在实践中不断地体会： 正交：各个概念之间可以独立变化 抽象：抛弃细节，找到本质和共性 《深入理解计算机系统》一书中提到：“指令集是对CPU的抽象， 文件是对输入/输出设备的抽象， 虚拟存储器是对程序存储的抽象， 进程是对一个正在运行的程序的抽象， 而虚拟机是对整个计算机（包括操作系统、处理器和程序）的抽象。 如果你对这句话透彻理解了，说明对计算机系统的认识已经很深刻了。 分层：我只想和我的邻居打交道， 如网络协议，Web应用开发。 分而治之：大事化小，小事化了，架构设计必备。 10、关键点来了，怎么学习呢？我原来的方式是先看书，看了很多书，数据结构，操作系统，汇编，网络… 这种办法的最大问题就是枯燥（嗯，那时候还没有码农翻身这样用故事讲解技术的文章）。 理论多，实践少，很多知识点体会不深， 等到参与的项目多了，Coding多了，这些知识点才慢慢地鲜活起来。 一种更加有效的办法是从工作中用到的知识点出发，从这个知识点向外扩展，由点到线，由线到面，然后让各个层次都连接起来，形成一个立体的网络。 切记，学习是一个螺旋上升的过程，想要上升就得深度思考，多问几个为什么。 比如工作中用到了Redis，你在学习过程中发现这个Redis用了单线程来处理读写请求，为什么要这么做？ 对于成千上万的请求它是如何处理的？ 然后再联想一下别的软件：Tomcat为什么不这么干？ 想回答这些问题，需要发掘很多基础知识。 这样做的次数多了，积累到一定程度，量变就会引起质变，整个系统就被你看透了，你的知识又扩大了一圈，更多的疑问出现了…]]></content>
      <categories>
        <category>operating-system</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring IoC]]></title>
    <url>%2Fjava%2Fspring%2Fspring-ioc%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>控制反转</tag>
        <tag>依赖注入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS分别在什么情况下使用UDP和TCP]]></title>
    <url>%2Fcomputer-network%2Fdns-base-tcp-and-udp%2F</url>
    <content type="text"><![CDATA[DNS同时占用UDP和TCP端口53是公认的，这种单个应用协议同时使用两种传输协议的情况在TCP/IP栈也算是个另类。但很少有人知道DNS分别在什么情况下使用这两种协议。 如果用wireshark、sniffer或古老些的tcpdump抓包分析，会发现几乎所有的情况都是在使用UDP，使用TCP的情况非常罕见，神秘兮兮。其实当解析器发出一个request后，返回的response中的tc删节标志比特位被置1时，说明反馈报文因为超长而有删节。这是因为UDP的报文最大长度为512字节。解析器发现后，将使用TCP重发request，TCP允许报文长度超过512字节。既然TCP能将data stream分成多个segment，它就能用更多的segment来传送任意长度的数据。 另外一种情况是，当一个域的辅助域名服务器启动时，将从该域的主域名服务器primary DNS server执行区域传送。除此之外，辅域名服务器也会定时（一般时3小时）向PDS进行查询以便了解SOA的数据是否有变动。如有变动，也会执行一次区域传送。区域传送将使用TCP而不是UDP，因为传送的数据量比一个request或response多得多。 DNS主要还是使用UDP，解析器还是服务端都必须自己处理重传和超时。DNS往往需要跨越广域网或互联网，分组丢失率和往返时间的不确定性要更大些，这对于DNS客户端来说是个考验，好的重传和超时检测就显得更重要了。 DNS为什么用TCP和UDPDNS同时占用UDP和TCP端口53是公认的，这种单个应用协议同时使用两种传输协议的情况在TCP/IP栈也算是个另类。但很少有人知道DNS分别在什么情况下使用这两种协议。 先简单介绍下TCP与UDP TCP是一种面向连接的协议，提供可靠的数据传输，一般服务质量要求比较高的情况，使用这个协议。 UDP—用户数据报协议，是一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务。 TCP与UDP的区别​ UDP和TCP协议的主要区别是两者在如何实现信息的可靠传递方面不同。TCP协议中包含了专门的传递保证机制，当数据接收方收到发送方传来的信息时，会自动向发送方发出确认消息；发送方只有在接收到该确认消息之后才继续传送其它信息，否则将一直等待直到收到确认信息为止。 与TCP不同，UDP协议并不提供数据传送的保证机制。如果在从发送方到接收方的传递过程中出现数据报的丢失，协议本身并不能做出任何检测或提示。因此，通常人们把UDP协议称为不可靠的传输协议。相对于TCP协议，UDP协议的另外一个不同之处在于如何接收突发性的多个数据报。不同于TCP，UDP并不能确保数据的发送和接收顺序。事实上，UDP协议的这种乱序性基本上很少出现，通常只会在网络非常拥挤的情况下才有可能发生。 ​ 既然UDP是一种不可靠的网络协议，那么还有什么使用价值或必要呢？其实不然，在有些情况下UDP协议可能会变得非常有用。因为UDP具有TCP所望尘莫及的速度优势。虽然TCP协议中植入了各种安全保障功能，但是在实际执行的过程中会占用大量的系统开销，无疑使速度受到严重的影响。反观UDP由于排除了信息可靠传递机制，将安全和排序等功能移交给上层应用来完成，极大降低了执行时间，使速度得到了保证。 DNS在进行区域传输的时候使用TCP协议，其它时候则使用UDP协议； ​ DNS的规范规定了2种类型的DNS服务器，一个叫主DNS服务器，一个叫辅助DNS服务器。在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做区传送（zone transfer）。 为什么既使用TCP又使用UDP？首先了解一下TCP与UDP传送字节的长度限制： UDP报文的最大长度为512字节，而TCP则允许报文长度超过512字节。当DNS查询超过512字节时，协议的TC标志出现删除标志，这时则使用TCP发送。通常传统的UDP报文一般不会大于512字节。 区域传送时使用TCP主要有一下两点考虑： 辅域名服务器会定时（一般时3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，则会执行一次区域传送，进行数据同步。区域传送将使用TCP而不是UDP，因为数据同步传送的数据量比一个请求和应答的数据量要多得多。 TCP是一种可靠的连接，保证了数据的准确性。 域名解析时使用UDP协议​ 客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，这样DNS服务器负载更低，响应更快。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。 UDP​ UDP 与 TCP 的主要区别在于 UDP 不一定提供可靠的数据传输。事实上，该协议不能保证数据准确无误地到达目的地。UDP 在许多方面非常有效。当某个程序的目标是尽快地传输尽可能多的信息时（其中任意给定数据的重要性相对较低），可使用 UDP。ICQ 短消息使用 UDP 协议发送消息。 ​ 许多程序将使用单独的TCP连接和单独的UDP连接。重要的状态信息随可靠的TCP连接发送，而主数据流通过UDP发送。 TCP​ TCP的目的是提供可靠的数据传输，并在相互进行通信的设备或服务之间保持一个虚拟连接。TCP在数据包接收无序、丢失或在交付期间被破坏时，负责数据恢复。它通过为其发送的每个数据包提供一个序号来完成此恢复。记住，较低的网络层会将每个数据包视为一个独立的单元，因此，数据包可以沿完全不同的路径发送，即使它们都是同一消息的组成部分。这种路由与网络层处理分段和重新组装数据包的方式非常相似，只是级别更高而已。 ​ 为确保正确地接收数据，TCP要求在目标计算机成功收到数据时发回一个确认（即 ACK）。如果在某个时限内未收到相应的 ACK，将重新传送数据包。如果网络拥塞，这种重新传送将导致发送的数据包重复。但是，接收计算机可使用数据包的序号来确定它是否为重复数据包，并在必要时丢弃它。 TCP与UDP的选择​ 如果比较UDP包和TCP包的结构，很明显UDP包不具备TCP包复杂的可靠性与控制机制。与TCP协议相同，UDP的源端口数和目的端口数也都支持一台主机上的多个应用。一个16位的UDP包包含了一个字节长的头部和数据的长度，校验码域使其可以进行整体校验。（许多应用只支持UDP，如：多媒体数据流，不产生任何额外的数据，即使知道有破坏的包也不进行重发。） ​ 很明显，当数据传输的性能必须让位于数据传输的完整性、可控制性和可靠性时，TCP协议是当然的选择。当强调传输性能而不是传输的完整性时，如：音频和多媒体应用，UDP是最好的选择。在数据传输时间很短，以至于此前的连接过程成为整个流量主体的情况下，UDP也是一个好的选择，如：DNS交换。把SNMP建立在UDP上的部分原因是设计者认为当发生网络阻塞时，UDP较低的开销使其有更好的机会去传送管理数据。TCP丰富的功能有时会导致不可预料的性能低下，但是我们相信在不远的将来，TCP可靠的点对点连接将会用于绝大多数的网络应用。 ​ TCP（Transmission Control Protocol，传输控制协议）是基于连接的协议，也就是说，在正式收发数据前，必须和对方建立可靠的连接。一个TCP连接必须要经过三次“对话”才能建立起来。三次对话的简单过程：主机A向主机B发出连接请求数据包：“我想给你发数据，可以吗？”，这是第一次对话；主机B向主机A发送同意连接和要求同步（同步就是两台主机一个在发送，一个在接收，协调工作）的数据包：“可以，你什么时候发？”，这是第二次对话；主机A再发出一个数据包确认主机B的要求同步：“我现在就发，你接着吧！”，这是第三次对话。三次“对话”的目的是使数据包的发送和接收同步，经过三次“对话”之后，主机A才向主机B正式发送数据。 ​ UDP（User Data Protocol，用户数据报协议）是与TCP相对应的协议。它是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发送过去！ UDP适用于一次只传送少量数据、对可靠性要求不高的应用环境。比如，我们经常使用“ping”命令来测试两台主机之间TCP/IP通信是否正常，其实“ping”命令的原理就是向对方主机发送UDP数据包，然后对方主机确认收到数据包，如果数据包是否到达的消息及时反馈回来，那么网络就是通的。例如，在默认状态下，一次“ping”操作发送4个数据包（如图2所示）。大家可以看到，发送的数据包数量是4包，收到的也是4包（因为对方主机收到后会发回一个确认收到的数据包）。这充分说明了UDP协议是面向非连接的协议，没有建立连接的过程。正因为UDP协议没有连接的过程，所以它的通信效果高；但也正因为如此，它的可靠性不如TCP协议高。QQ就使用UDP发消息，因此有时会出现收不到消息的情况。HTTP是用TCP协议传输的。 TCP协议与UDP协议的区别TCP基于面向连接的协议，数据传输可靠，传输速度慢，适用于传输大量数据，可靠性要求高的场合。 UDP协议面向非连接协议，数据传输不可靠，传输速度快，适用于一次只传送少量数据、对可靠性要求不高的应用环境。 面向连接的TCP “面向连接”就是在正式通信前必须要与对方建立起连接。比如你给别人打电话，必须等线路接通了、对方拿起话筒才能相互通话。 TCP协议能为应用程序提供可靠的通信连接，使一台计算机发出的字节流无差错地发往网络上的其他计算机，对可靠性要求高的数据通信系统往往使用TCP协议传输数据。 面向非连接的UDP协议 “面向非连接”就是在正式通信前不必与对方先建立连接，不管对方状态就直接发送。这与现在风行的手机短信非常相似：你在发短信的时候，只需要输入对方手机号就OK了。 UDP适用于一次只传送少量数据、对可靠性要求不高的应用环境 UDP协议是面向非连接的协议，没有建立连接的过程。正因为UDP协议没有连接的过程，所以它的通信效果高；但也正因为如此，它的可靠性不如TCP协议高。QQ就使用UDP发消息，因此有时会出现收不到消息的情况。 TCP**协议与UDP协议支持的应用协议** TCP支持的应用协议主要有：Telnet**、FTP、SMTP**等； UDP支持的应用层协议主要有：NFS（网络文件系统）、SNMP（简单网络管理协议）、DNS（主域名称系统）、TFTP（通用文件传输协议）等。 TCP和UDP都是位于OSI模型中的传输层中。 TCP优点:面向连接的,具有实时性,就象打电话一样,两者必须建立连接. 它保证你所传输的东西是准确到达的,并且收方要给你一个收到或没有\ 收到的回复,所以它具有安全性的特点..UDP优点:面向无连接的,就象给某人寄信一样,对方不需要在邮局等着你的信到. 所以说,它没有保障性,不能确保你一定能收到信,不象TCP那样,,但是 它比TCP好的一点,就是速度快,因为他不需要双方交流是否收到,对发的东西有一个确认的过程.]]></content>
  </entry>
  <entry>
    <title><![CDATA[spring三个核心思想详解]]></title>
    <url>%2Fjava%2Fspring%2Fspring-ioc-di-aop%2F</url>
    <content type="text"><![CDATA[Spring核心思想分三大类：控制反转（IOC），依赖注入（DI）和面向切面（AOP） 控制反转通俗讲，控制权由应用代码中转到了外部容器，控制权的转移，是所谓反转。也就是说，正常我们都是新建对象，才可以调用对象。现在不需要了，交给容器来管理，我们只需要通过一些配置来完成把实体类交给容器这么个过程。这样可以减少代码量，简化开发的复杂度和耦合度。 这里，我要解释下几个概念：1.控制反转只是一个概念，我理解为一种设计模式。2.控制反转的主要形式有两种：依赖查找和依赖注入 依赖查找：容器提供回调接口和上下文条件给组件。EJB和Apache Avalon 都使用这种方式。这样一来，组件就必须使用容器提供的API来查找资源和协作对象，仅有的控制反转只体现在那些回调方法上（也就是上面所说的 类型1）：容器将调用这些回调方法，从而让应用代码获得相关资源。依赖注入：组件不做定位查询，只提供普通的Java方法让容器去决定依赖关系。容器全权负责的组件的装配，它会把符合依赖关系的对象通过JavaBean属性或者构造函数传递给需要的对象。通过JavaBean属性注射依赖关系的做法称为设值方法注入(Setter Injection)；将依赖关系作为构造函数参数传入的做法称为构造器注入（Constructor Injection）。 依赖注入上面也解释了这个概念，其实现方式有三种：属性注入（setter注入），构造器注入和自动装配。 方式一、属性注入123456789101112131415161718192021222324252627282930313233343536package com.spring.demo02.entity; public class Programmer &#123; private String name; private String sex; // 在这里定义要依赖的computer属性，加上set方法 private Computer computer; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public Computer getComputer() &#123; return computer; &#125; /** * 加上Setter方法 * */ public void setComputer(Computer computer) &#123; this.computer = computer; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435package com.spring.demo02.entity; public class Computer &#123; private String brand; private String color; private String size; public void coding() &#123; System.out.println("Computer is coding!!!"); &#125; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; public String getSize() &#123; return size; &#125; public void setSize(String size) &#123; this.size = size; &#125; &#125; 看上面的代码，可以发现： Programmer类里面，有3个属性，name，sex，computer，并且都有对应的getter、setter方法； Computer类里面也有三个属性，分别是品牌、颜色、尺寸，也都有对应的getter、setter方法； 这只是第一步，在类里面声明属性并且实现set方法。 接下来，要写一个spring的xml配置文件，如下： 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd"&gt; &lt;bean id="programmer" class="com.spring.demo2.entity.Programmer"&gt; &lt;property name="name" value="小李"&gt;&lt;/property&gt; &lt;property name="sex" value="男"&gt;&lt;/property&gt; &lt;property name="computer" ref="computer"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id="computer" class="com.spring.demo2.entity.Computer"&gt; &lt;property name="brand" value="hp"&gt;&lt;/property&gt; &lt;property name="color" value="黑"&gt;&lt;/property&gt; &lt;property name="size" value="14"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 解读一下这个xml文件： 声明一个bean，可以理解为实例化了一个对象。那这里实例化了两个对象（programmer和computer），各个属性都已经赋值上去。 id为programmer的bean，其实就是Programmer类；通过给property赋值，Spring就会通过Programmer类的各个属性的set方法，逐一给Programmer的属性赋值。 在programmer里面，有一个属性是computer的，可以看到它属性值是 ref=”computer”，这就说明computer这个属性是个引用，这里ref后面的值其实就是指向另一个bean的id值，所以这里引用的是id为computer的bean。 以上，就是属性注入了。关键的是在类里面声明属性，写set方法，然后在xml里面配置bean和property的值。 方式二、构造器注入构造器注入，顾名思义，就是在构造器里面注入依赖对象。那是怎么实现的呢？其实跟属性注入差不多，定义一个有参构造器，然后配置xml文件就行了。看代码： 1234567891011package com.spring.demo03.entity; import com.spring.demo02.entity.Computer; public class Programmer &#123; private Computer computer; public Programmer(Computer computer)&#123; this.computer = computer; &#125; &#125; 1234567891011121314package com.spring.demo03.entity; public class Computer &#123; private String brand; private String color; private String size; public Computer(String brand, String color, String size) &#123; this.brand = brand; this.color = color; this.size = size; &#125; &#125; 上面两个类都有一个有参的构造器，接下来，在xml里面配置这两个bean，然后再配置构造器的参数值就可以了 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd"&gt; &lt;bean id="programmer" class="com.spring.demo3.entity.Programmer"&gt; &lt;constructor-arg ref="computer"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 构造器里面没有name字段，只有value，是根据构造器的方法参数顺序来定义的 --&gt; &lt;bean id="computer" class="com.spring.demo3.entity.Computer"&gt; &lt;constructor-arg value="联想"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg value="红色"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg value="15.6寸"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;/beans&gt; 方式三：自动装配1234567891011package com.spring.demo04.entity; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; @Component public class Programmer &#123; @Autowired Computer computer;&#125; 12345678910111213141516171819202122232425262728293031323334package com.spring.demo04.entity; import org.springframework.stereotype.Component; @Component public class Computer &#123; private String brand; private String color; private String size; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; public String getSize() &#123; return size; &#125; public void setSize(String size) &#123; this.size = size; &#125; &#125; 123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/beans/spring-beans-3.0.xsd"&gt; &lt;context:component-scan base-pakage="com.spring.demo04"&gt; &lt;/beans&gt; 关键点： 在类前面加注解：@Component； 在需要注入的类里面加注解：@Autowired，这样xml里面的自动扫描就会扫描到这些加了注解的类和属性，在实例化bean的时候，Spring容器会把加了@Component的类实例化；在实际运行时，会给加了@Autowired的属性注入对应的实例。@Autowired方式是通过反射来设置属性值的； 具体参考：https://blog.csdn.net/wenluoxicheng/article/details/73608657 面向切面面向切面是一个概念，通常用来为许多其他的类提供相同的服务，而且是固定的服务，是独立的。提升了代码的复用性，减少了代码的耦合度，减轻程序员的工作负担，把程序的重心放在了核心的逻辑上。 它是可以通过预编译方式和运行期动态代理实现在不修改源代码的情况下给程序动态统一添加功能的一种技术。 应用场景有日志记录，性能统计，安全控制，事务处理，异常处理等。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP]]></title>
    <url>%2Fjava%2Fspring%2Fspring-aop%2F</url>
    <content type="text"><![CDATA[AOP基础知识概念和术语 连接点：应用程序执行期间明确定义的一个点。eg：方法调用、方法调用本身、类初始化、对象实例化； 通知（Advice）：在特定连接点执行的代码就是通知。通知类型举例：前置通知（在连接点之前执行），后置通知（在连接点之后执行）； 切入点（Pointcut）：用于定义何时执行的 连接点集合 切面（Advisor）：封装 类中 通知和切入点的集合； 织入：在适当的位置 将切面插入到 应用程序 代码中的过程；编译时AOP：在生成时完成；运行时AOP：在运行态执行；AspectJ还支持另一种加载时织入（LTW）的植入机制；拦截底层的JVM类加载器，在加载字节码时织入； 目标对象（target）：由AOP进程修改的对象 称为 目标对象；通常也被称为 被通知对象； 引入：通过引入其他方法 来改变 对象结构 的过程。可以通过引入 AOP来使任何对象实现特定的接口，而无需对象的类显式实现该接口； AOP类型静态AOP织入过程 是 应用程序生成的一个步骤； 修改 实际的字节码 来实现 AOP织入过程，最终结果是Java字节码，并且运行时无需特殊技巧确定应该何时执行； 缺点：对切面做的任何修改都需要 重新编译整个程序； AspectJ的编译时织入 就是 静态AOP； 动态AOP织入过程 在运行时 动态执行； 缺点：性能不如静态AOP； 优点：可以轻松修改 应用程序的整个切面集，而无需重新编译主应用程序代码； 选择AOP类型静态AOP和动态AOP各有优点 静态AOP： 实现时间长，倾向于实现更多功能丰富的实现；具有更多的可用连接点；而且如果性能很重要，就倾向于静态AOP； 如果需要SpringAOP未实现的功能 就需要 AspectJ； Spring AOP示例Spring AOP实现两个逻辑部分： 1、AOP内核：完全解耦的纯编程 AOP功能； 2、使 AOP 更易于使用的 一组框架服务； AOP Alliance许多开源项目共同定制的 一组AOP标准接口。 Spring使用了AOP Alliance 接口 官网：http://aopalliance.sourceforge.net/ AOP Alliance示例1、pom.xml中引入jar包 12345678910&lt;dependency&gt; &lt;groupId&gt;aopalliance&lt;/groupId&gt; &lt;artifactId&gt;aopalliance&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt; 2、定义目标对象 12345public class Agent &#123; public void speak() &#123; System.out.print("Bond"); &#125;&#125; 3、定义增强类 AgentDecorator 目标：打印 James Bond！ 1234567891011import org.aopalliance.intercept.MethodInterceptor;import org.aopalliance.intercept.MethodInvocation;public class AgentDecorator implements MethodInterceptor &#123; public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.print("James "); Object ret = invocation.proceed(); System.out.println("!"); return ret; &#125;&#125; 实现 AOP Alliance 标准接口 MethodInterceptor，围绕 invocation.proceed() 实现 环绕通知（around advice）； 4、将invocation.proceed()方法 织入 目标对象 中 123456789101112131415import org.springframework.aop.framework.ProxyFactory;public class AgentAOPDemo &#123; public static void main(String[] args) &#123; Agent target = new Agent(); target.speak(); System.out.println(); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.addAdvice(new AgentDecorator()); proxyFactory.setTarget(target); Agent proxy = ((Agent) proxyFactory.getProxy()); proxy.speak(); &#125;&#125; 运行结果： 12BondJames Bond! Spring AOP架构Spring AOP的核心架构 基于 代理。 想要创建一个类的被通知实例时，必须使用ProxyFactory创建该类的代理实例； 大多数情况下，我们可以使用声明式AOP配置(ProxyFactoryBean、AOP名称空间和@AspectJ样式注解)来完成代理的创建； 但我们需要了解代理的创建过程；因此，首先延时代理创建的编程方法，再讨论 Spring的声明式AOP配置； 运行时，Spring会分析为 ApplicationContext中bean定义的横切关注点，并动态生成代理 bean（封装了底层的目标bean）。不会直接调用目标bean，而是将调用者注入代理bean。然后代理bean分析运行条件，并相应的织入适当的通知。 Spring中的两种代理实现：JDK动态代理、CGLIB代理 Spring中的连接点Spring只支持一种连接点类型：方法调用； 如果需要其他连接点：则可以使用Spring和AspectJ； Spring中的切面在Spring AOP中，切面由实现了 Advisor 接口的类的实例表示。 Advisor有两个子接口：PointcutAdvisor 和 IntroductionAdvisor。 关于ProxyFactory类ProxyFactory 控制 SpringAOP 中织入和代理创建过程； 1Agent proxy = ((Agent) proxyFactory.getProxy()); ProxyFactory将代理创建委托给 DefaultProxyFactory的一个实例，继而又委托给 ObjenesisCglibAopProxy 或 JdkDynamicAopProxy 1proxyFactory.addAdvice(new AgentDecorator()); addAdvice将传入的通知封装到 DefaultPointcutAdvisor的一个实例中，并使用默认的方法进行配置 ProxyFactory可以创建多个代理，每个代理都有不同的切面 1234proxyFactory.addAdvice();proxyFactory.removeAdvice();proxyFactory.addAdvisor();proxyFactory.removeAdvisor(); 在Spring中创建通知Spring中支持6中通知 通知名称 接口 描述 前置通知 org.springframework.aop.MethodBeforeAdvice 在连接点之前执行 后置返回通知 org.springframework.aop.AfterReturningAdvice 在连接点return之后 执行 后置通知 org.springframework.aop.AfterAdvice 连接点执行完成之后执行 环绕通知 org.aopalliance.intercept.MethodInterceptor 在连接点前后都执行 异常通知 org.springframework.aop.ThrowsAdvice 在异常执行以后执行，捕获特定异常 引入通知 org.springframework.aop.IntroductionAdvisor 将引入建模为特殊类型的拦截器 通知（Advice）的接口通知（Advice）和顾问（Advisor）的区别在于：Advisor可以携带相关切入点的通知，从而更细致的控制在哪个连接点上拦截通知。 Advice的层次结构图： 创建前置通知（MethodBeforeAdvice）创建目标对象： 12345678public interface Singer &#123; void sing();&#125;public class Guitarist implements Singer &#123; public void sing() &#123; System.out.println("You're gonna live forever in me."); &#125;&#125; 创建前置通知： 12345678import org.springframework.aop.MethodBeforeAdvice;import java.lang.reflect.Method;public class SimpleBeforeAdvice implements MethodBeforeAdvice &#123; public void before(Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("Before '" + method.getName() + "' , tune guitar."); &#125;&#125; 设置切入点并运行示例： 123456789101112import org.springframework.aop.framework.ProxyFactory;public class SimpleBeforeAdviceDemo &#123; public static void main(String[] args) &#123; Guitarist johnMayer = new Guitarist(); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.addAdvice(new SimpleBeforeAdvice()); proxyFactory.setTarget(johnMayer); Guitarist proxy = ((Guitarist) proxyFactory.getProxy()); proxy.sing(); &#125;&#125; 运行结果： 12Before 'sing' , tune guitar.You're gonna live forever in me. 创建后置返回通知12345678import org.springframework.aop.AfterReturningAdvice;import java.lang.reflect.Method;public class SimpleAfterReturningAdvice implements AfterReturningAdvice &#123; public void afterReturning(Object returnValue, Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("After "+ method.getName() +" return , return value: " + returnValue); &#125;&#125; 创建异常通知(ThrowsAdvice)ThrowsAdvice内置四个接口，通过反射调用： 1234public void afterThrowing(Exception ex)public void afterThrowing(RemoteException)public void afterThrowing(Method method, Object[] args, Object target, Exception ex)public void afterThrowing(Method method, Object[] args, Object target, ServletException ex) 示例： 1234567891011121314151617181920212223242526272829303132333435import org.springframework.aop.ThrowsAdvice;import org.springframework.aop.framework.ProxyFactory;import java.lang.reflect.Method;public class SimpleThrowsAdvice implements ThrowsAdvice &#123; public void afterThrowing(Exception ex) throws Throwable &#123; System.out.println("*****"); System.out.println("Caught: " + ex.getClass().getName()); System.out.println("Message: " + ex.getMessage()); &#125; public void afterThrowing(Method method, Object[] args, Object target, Exception ex) &#123; System.out.println("*****"); System.out.println("Caught2: " + ex.getClass().getName()); System.out.println("Message2: " + ex.getMessage()); &#125; public static void main(String[] args) &#123; ErrBean errBean = new ErrBean(); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(errBean); proxyFactory.addAdvice(new SimpleThrowsAdvice()); ErrBean proxy = ((ErrBean) proxyFactory.getProxy()); try &#123; proxy.errMethod(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;class ErrBean &#123; public void errMethod() throws Exception &#123; throw new Exception("My Exception"); &#125;&#125; 执行结果： 可以看到只执行了第二个方法，通过Cglib代理； 使用顾问（Advisor）和切入点（Pointcut）避免硬编码 将方法检查 放入通知，尽可能使用切入点来控制通知目标上方法的适用性； 如果通知跟目标 之间有 目标关联性，则在通知内部检查也可以； Pointcut接口1234567package org.springframework.aop;public interface Pointcut &#123; ClassFilter getClassFilter(); MethodMatcher getMethodMatcher(); Pointcut TRUE = TruePointcut.INSTANCE;&#125; 123456package org.springframework.aop;// 判断 Pointcut是否适用于特定的方法public interface ClassFilter &#123; boolean matches(Class&lt;?&gt; clazz); ClassFilter TRUE = TrueClassFilter.INSTANCE;&#125; 123456789package org.springframework.aop;import java.lang.reflect.Method;public interface MethodMatcher &#123; boolean matches(Method method, Class&lt;?&gt; targetClass); boolean isRuntime(); boolean matches(Method method, Class&lt;?&gt; targetClass, Object[] args); MethodMatcher TRUE = TrueMethodMatcher.INSTANCE;&#125; MethodMatcher 分为 静态MethodMatcher 和 动态 MethodMatcher； Spring 通过 isRuntime() 判断 MethodMatcher是 静态（false），还是动态（true）； 1、静态切入点： Spring 针对目标上的每个方法，调用一次MethodMatcher的matches(Method, Class&lt;?&gt; )方法，并缓存返回值； 2、动态切入点： Spring先调用 matches(Method, Class&lt;？&gt; )方法进行静态检查，返回true时，再调用 matches(Method, Class&lt;？&gt;, Object[]) 对每个参数进一步检查； 比较： 静态Pointcut 执行的比较好，动态Pointcut更灵活；推荐尽量使用静态Pointcut； 使用动态Pointcut 可以 避免 不必要的通知调用；通知调用对性能影响很大； 可用的Pointcut实现4.0开始Spring提供了八个Pointcut接口的实现：2个用作创建静态、动态切入点的抽象类，6个具体类； 每个具体类可以完成的操作： 一起构成多个切入点 处理控制流切入点 执行简单的基于名称的匹配 使用正则表达式定义切入点 使用AspectJ定义切入点 定义在类或者方法级别查找特定注解的切入点 实现类 描述 org.springframework.aop.support.annotation.AnnotationMatchingPointcut 在类或方法上查找指定Java注解 org.springframework.aop.aspectj.AspectJExpressionPointcut 使用AspectJ植入器以AspectJ语法评估切入点表达式 org.springframework.aop.support.ComposablePointcut ComposablePointcut类使用诸如union()和intersection()等操作组合两个或更多的切入点 org.springframework.aop.support.ControlFlowPointcut ControlFlowPointcut是一种特殊的切入点，他们匹配另一个方法的控制流中的所有方法，即任何作为另一个方法的结果而直接或间接调用的方法 org.springframework.aop.support.DynamicMethodMatcherPointcut 构建动态切入点的基类 org.springframework.aop.support.JdkRegexpMethodPointcut 正则表达式支持定义切入点 org.springframework.aop.support.NameMatchMethodPointcut 创建一个切入点，对方法名称列表执行简单匹配 org.springframework.aop.support.StaticMethodMatcherPointcut 构建静态切入点的基础 使用DefaultPointcutAdvisor使用Pointcut之前，必须先创建 Advisor接口的实例，具体的说是创建一个PointcutAdvisor接口； Advisor是Spring中某个切面的表示，它是 通知 和 切入点 的 结合体； DefaultPointcutAdvisor 将 点那个Pointcut 和 Advice 相关联； 1、使用 StaticMethodMatcherPointcut 创建静态切入点思路：创建两个目标对象类GoodGuitarist和GreatGuitarist，通过DefaultPointcutAdvisor创建这两个类的代理，但通知 只应用到GoodGuitarist； 创建两个目标对象类： 12345678910public class GoodGuitarist implements Singer &#123; public void sing() &#123; System.out.println("I am GoodGuitarist"); &#125;&#125;public class GreatGuitarist implements Singer &#123; public void sing() &#123; System.out.println("I am GreatGuitarist"); &#125;&#125; 创建静态切入点： 1234567891011121314151617181920import org.springframework.aop.ClassFilter;import org.springframework.aop.support.StaticMethodMatcherPointcut;import java.lang.reflect.Method;public class SimpleStaticPointcut extends StaticMethodMatcherPointcut &#123; public boolean matches(Method method, Class&lt;?&gt; cls) &#123; return "sing".equals(method.getName()); // 只把通知应用到sing()方法 &#125; @Override public ClassFilter getClassFilter() &#123; return new ClassFilter() &#123; @Override public boolean matches(Class&lt;?&gt; clazz) &#123; return clazz == GoodGuitarist.class; // 只将通知应用于GoodGuitarist &#125; &#125;; &#125;&#125; 创建通知： 123456789101112import org.aopalliance.intercept.MethodInterceptor;import org.aopalliance.intercept.MethodInvocation;public class SimpleAdvice implements MethodInterceptor &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println("&gt;&gt; Invoke: " + invocation.getMethod().getName()); Object ret = invocation.proceed(); System.out.println("&lt;&lt; Invoke Done!"); return ret; &#125;&#125; 使用DefaultPointcutAdvisor组织Pointcut和Advice： 1234567891011121314151617181920212223242526272829303132import com.learn.aop.Singer;import org.aopalliance.aop.Advice;import org.springframework.aop.Advisor;import org.springframework.aop.Pointcut;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;public class StaticPointcutDemo &#123; public static void main(String[] args) &#123; GoodGuitarist good = new GoodGuitarist(); GreatGuitarist great = new GreatGuitarist(); Singer goodProxy, greatProxy; Pointcut pointcut = new SimpleStaticPointcut(); Advice advice = new SimpleAdvice(); Advisor advisor = new DefaultPointcutAdvisor(pointcut, advice); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.addAdvisor(advisor); proxyFactory.setTarget(good); goodProxy = (Singer) proxyFactory.getProxy(); proxyFactory = new ProxyFactory(); proxyFactory.addAdvisor(advisor); proxyFactory.setTarget(great); greatProxy = ((Singer) proxyFactory.getProxy()); goodProxy.sing(); System.out.println(); greatProxy.sing(); &#125;&#125; 运行结果： 12345&gt;&gt; Invoke: singI am GoodGuitarist&lt;&lt; Invoke Done!I am GreatGuitarist 可以看到只有 GoodGuitarist的sing方法被通知，GreatGuitarist没有被通知； 2、使用DynamicMethodMatcherPointcut创建动态切入点在本例中，只想通知foo()方法，而且 只有在 int参数 不等于 100 时，才通知该方法 创建目标对象： 12345678public class SampleBean &#123; public void foo(int x) &#123; System.out.println("invoke foo() with: " + x); &#125; public void bar() &#123; System.out.println("invoke bar() "); &#125;&#125; 创建针对foo的切入点： 1234567891011121314151617181920212223242526import org.springframework.aop.ClassFilter;import org.springframework.aop.support.DynamicMethodMatcherPointcut;import java.lang.reflect.Method;public class SimpleDynamicPointcut extends DynamicMethodMatcherPointcut &#123; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; System.out.println("static check for " + method.getName() + "(), calss: " + targetClass.getName()); return "foo".equals(method.getName()); // 针对foo()方法 &#125; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass, Object[] args) &#123; System.out.println("dynamic check for " + method.getName() + "(), calss: " + targetClass.getName()); int x = ((Integer) args[0]).intValue(); return x != 100; // 判断参数是否 等于100 &#125; @Override public ClassFilter getClassFilter() &#123; return new ClassFilter() &#123; @Override public boolean matches(Class&lt;?&gt; clazz) &#123; return clazz == SampleBean.class; // 判断类是否 SampleBean &#125; &#125;; &#125;&#125; 创建Advisor组织切入点和通知，调用代理对象： 123456789101112131415161718192021222324import com.learn.aop.pointcut.staticp.SimpleAdvice;import org.springframework.aop.Advisor;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;public class DynamicPointcutDemo &#123; public static void main(String[] args) &#123; SampleBean target = new SampleBean(); Advisor advisor = new DefaultPointcutAdvisor(new SimpleDynamicPointcut(), new SimpleAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); SampleBean proxy = ((SampleBean) proxyFactory.getProxy()); proxy.foo(1); proxy.foo(10); proxy.foo(100); proxy.bar(); proxy.bar(); proxy.bar(); &#125;&#125; 运行结果： 可以看到只有两个foo()被通知到，x=100时没有被通知；bar()没有被通知到； foo进行了两次静态检查，一次在初始阶段，一次在被调用时； 3、使用简单名称匹配无需考虑方法签名，只根据方法名匹配； 1234567891011121314151617181920public class Guitar &#123; public String play() &#123; return "G C GC AM D7"; &#125;&#125;public class GrammyGuitarist implements Singer&#123; @Override public void sing() &#123; System.out.println("sing()"); &#125; public void sing(Guitar guitar) &#123; System.out.println("play(): " + guitar.play()); &#125; public void rest() &#123; System.out.println("zzz~~"); &#125; public void talk() &#123; System.out.println("talk()"); &#125;&#125; 使用NameMatchMethodPointcut匹配： 1234567891011121314151617181920212223242526import com.learn.aop.pointcut.staticp.SimpleAdvice;import org.springframework.aop.Advisor;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;import org.springframework.aop.support.NameMatchMethodPointcut;public class NamePointcutDemo &#123; public static void main(String[] args) &#123; GrammyGuitarist johnMayer = new GrammyGuitarist(); NameMatchMethodPointcut pointcut = new NameMatchMethodPointcut(); pointcut.addMethodName("sing"); pointcut.addMethodName("rest"); Advisor advisor = new DefaultPointcutAdvisor(pointcut, new SimpleAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(johnMayer); proxyFactory.addAdvisor(advisor); GrammyGuitarist proxy = ((GrammyGuitarist) proxyFactory.getProxy()); proxy.sing(); proxy.sing(new Guitar()); proxy.rest(); proxy.talk(); &#125;&#125; 运行结果： 4、使用正则表达式创建切入点创建目标对象： 1234567891011121314import com.learn.aop.Singer;public class Guitarist implements Singer &#123; @Override public void sing() &#123; System.out.println("sing()"); &#125; public void sing2() &#123; System.out.println("sing2()"); &#125; public void rest() &#123; System.out.println("rest()"); &#125;&#125; 创建Advisor组织切入点和通知，并调用： 123456789101112131415161718192021222324import com.learn.aop.pointcut.staticp.SimpleAdvice;import org.springframework.aop.Advisor;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;import org.springframework.aop.support.JdkRegexpMethodPointcut;public class RegexPointcutDemo &#123; public static void main(String[] args) &#123; Guitarist target = new Guitarist(); JdkRegexpMethodPointcut pointcut = new JdkRegexpMethodPointcut(); pointcut.setPattern(".*sing.*"); Advisor advisor = new DefaultPointcutAdvisor(pointcut, new SimpleAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.addAdvisor(advisor); proxyFactory.setTarget(target); Guitarist proxy = (Guitarist) proxyFactory.getProxy(); proxy.sing(); proxy.sing2(); proxy.rest(); &#125;&#125; 运行结果： 5、使用 AspectJ切入点表达式 创建切入点AspectJ切入点表达式 在pom.xml中添加： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt; 使用 AspectJ表达式 实现 JDK正则表达式 相同的功能： 123456789101112131415161718192021222324import com.learn.aop.pointcut.regex.Guitarist;import com.learn.aop.pointcut.staticp.SimpleAdvice;import org.springframework.aop.Advisor;import org.springframework.aop.aspectj.AspectJExpressionPointcut;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;public class AspectjexpPointcutDemo &#123; public static void main(String[] args) &#123; Guitarist target = new Guitarist(); AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut(); pointcut.setExpression("execution(* sing*(..))"); Advisor advisor = new DefaultPointcutAdvisor(pointcut, new SimpleAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); Guitarist proxy = ((Guitarist) proxyFactory.getProxy()); proxy.sing(); proxy.sing2(); proxy.rest(); &#125;&#125; 运行结果： 6、创建注解匹配切入点基于注解定义切入点AnnotationMatchingPointcut； 定义注解： 123456789import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)public @interface AdviceRequired &#123;&#125; 在方法上使用注解： 123456789101112public class Guitarist implements Singer &#123; public void sing() &#123; System.out.println("sing()"); &#125; @AdviceRequired public void sing(Guitar guitar) &#123; System.out.println("play: " + guitar.play()); &#125; public void rest() &#123; System.out.println("rest()"); &#125;&#125; 对注解类织入切面： 123456789101112131415161718192021222324import com.learn.aop.pointcut.name.Guitar;import com.learn.aop.pointcut.staticp.SimpleAdvice;import org.springframework.aop.Advisor;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;import org.springframework.aop.support.annotation.AnnotationMatchingPointcut;public class AnnotationPointcutDemo &#123; public static void main(String[] args) &#123; Guitarist target = new Guitarist(); AnnotationMatchingPointcut pointcut = AnnotationMatchingPointcut .forMethodAnnotation(AdviceRequired.class); Advisor advisor = new DefaultPointcutAdvisor(pointcut, new SimpleAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); Guitarist proxy = ((Guitarist) proxyFactory.getProxy()); proxy.sing(new Guitar()); proxy.rest(); &#125;&#125; 运行结果： 了解代理ProxyFactory到目前为止，只是粗略的了解了一下 ProxyFactory的使用； 代理的核心：拦截方法调用，必要时 执行适用于特定方法的通知链； 通知的管理和调用 独立于代理，由Spring AOP框架管理。而代理主要负责拦截对方法的调用，并将它们根据需要传递给AOP框架，以便应用通知。 Spring有两种代理：JDK代理、CGLIB代理； JDK动态代理只能生成接口的代理，不能生成类的代理； 如何处理特定方法调用的决策 都在 运行时 做出； 无法区分 被通知方法 和 未被通知方法； 12345678910public static void runJdkTests(Advisor advisor, SimpleBean target) &#123; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setInterfaces(SimpleBean.class); SimpleBean proxy = ((SimpleBean) proxyFactory.getProxy()); System.out.println("&gt;&gt;&gt; Running JDK Tests"); test(proxy);&#125; CGLIB代理CGLIB会为每个代理 动态生成新类的字节码，并尽可能重用已生成的类； 所生成的类 是 目标对象target类的子类； 每次创建CGLIB代理时，CGLIB会询问Spring如何处理每个方法； 由于CGLIB生成实际的字节码，因此在处理方法上很灵活； CGLIB生成固定通知链以后，可以减少执行通知链的运行时间开销； 123456789101112131415161718192021public static void runCglibTests(Advisor advisor, SimpleBean target) &#123; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setProxyTargetClass(true); SimpleBean proxy = ((SimpleBean) proxyFactory.getProxy()); System.out.println("&gt;&gt;&gt; Running CGLIB (Standard) Tests"); test(proxy);&#125;public static void runCglibFrozenTests(Advisor advisor, SimpleBean target) &#123; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setProxyTargetClass(true); proxyFactory.setFrozen(true); SimpleBean proxy = ((SimpleBean) proxyFactory.getProxy()); System.out.println("&gt;&gt;&gt; Running CGLIB (Frozen) Tests"); test(proxy);&#125; 比较代理性能定义Target： 1234public interface SimpleBean &#123; void advised(); void unadvised();&#125; 123456789public class DefaultSimpleBean implements SimpleBean &#123; private long dummy = 0; public void advised() &#123; dummy = System.currentTimeMillis(); &#125; public void unadvised() &#123; dummy = System.currentTimeMillis(); &#125;&#125; 定义通知： 12345678import org.springframework.aop.MethodBeforeAdvice;import java.lang.reflect.Method;public class NoOpBeforeAdvice implements MethodBeforeAdvice &#123; public void before(Method method, Object[] args, Object target) throws Throwable &#123; // 什么都不做 &#125;&#125; 定义切入点： 12345678import org.springframework.aop.support.StaticMethodMatcherPointcut;import java.lang.reflect.Method;public class TestPointcut extends StaticMethodMatcherPointcut &#123; public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return "advise".equals(method.getName()); &#125;&#125; 分别调用JDK代理，CGLIB代理，CGLIB Frozen代理，比较他们的性能： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import org.springframework.aop.Advisor;import org.springframework.aop.framework.Advised;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.DefaultPointcutAdvisor;public class ProxyPerfTest &#123; public static void main(String[] args) &#123; SimpleBean target = new DefaultSimpleBean(); Advisor advisor = new DefaultPointcutAdvisor(new TestPointcut(), new NoOpBeforeAdvice()); runCglibTests(advisor, target); runCglibFrozenTests(advisor, target); runJdkTests(advisor, target); &#125; public static void runCglibTests(Advisor advisor, SimpleBean target) &#123; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setProxyTargetClass(true); SimpleBean proxy = ((SimpleBean) proxyFactory.getProxy()); System.out.println("&gt;&gt;&gt; Running CGLIB (Standard) Tests"); test(proxy); &#125; public static void runCglibFrozenTests(Advisor advisor, SimpleBean target) &#123; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setProxyTargetClass(true); proxyFactory.setFrozen(true); SimpleBean proxy = ((SimpleBean) proxyFactory.getProxy()); System.out.println("&gt;&gt;&gt; Running CGLIB (Frozen) Tests"); test(proxy); &#125; public static void runJdkTests(Advisor advisor, SimpleBean target) &#123; ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setInterfaces(SimpleBean.class); SimpleBean proxy = ((SimpleBean) proxyFactory.getProxy()); System.out.println("&gt;&gt;&gt; Running JDK Tests"); test(proxy); &#125; private static void test(SimpleBean bean) &#123; long before = 0; long after = 0; System.out.println("Testing advised() Method"); before = System.currentTimeMillis(); for (int i = 0; i &lt; 500000; i++) &#123; bean.advised(); &#125; after = System.currentTimeMillis(); System.out.println("Took " + (after-before) + " ms"); System.out.println("Testing unadvised() Method"); before = System.currentTimeMillis(); for (int i = 0; i &lt; 500000; i++) &#123; bean.unadvised(); &#125; after = System.currentTimeMillis(); System.out.println("Took " + (after-before) + " ms"); System.out.println("Test equals() method:"); before = System.currentTimeMillis(); for (int i = 0; i &lt; 500000; i++) &#123; bean.equals(bean); &#125; after = System.currentTimeMillis(); System.out.println("Took " + (after-before) + " ms"); System.out.println("Test hashCode() method:"); before = System.currentTimeMillis(); for (int i = 0; i &lt; 500000; i++) &#123; bean.hashCode(); &#125; after = System.currentTimeMillis(); System.out.println("Took " + (after-before) + " ms"); Advised advised = ((Advised) bean); System.out.println("Test advised.getTargetClass() method:"); before = System.currentTimeMillis(); for (int i = 0; i &lt; 500000; i++) &#123; advised.getTargetClass(); &#125; after = System.currentTimeMillis(); System.out.println("Took " + (after-before) + " ms"); System.out.println("&lt;&lt;&lt;\n"); &#125;&#125; 运行结果如下： 1234567891011121314151617181920212223242526272829303132333435363738&gt;&gt;&gt; Running CGLIB (Standard) TestsTesting advised() MethodTook 84 msTesting unadvised() MethodTook 57 msTest equals() method:Took 12 msTest hashCode() method:Took 21 msTest advised.getTargetClass() method:Took 9 ms&lt;&lt;&lt;&gt;&gt;&gt; Running CGLIB (Frozen) TestsTesting advised() MethodTook 16 msTesting unadvised() MethodTook 51 msTest equals() method:Took 10 msTest hashCode() method:Took 19 msTest advised.getTargetClass() method:Took 10 ms&lt;&lt;&lt;&gt;&gt;&gt; Running JDK TestsTesting advised() MethodTook 130 msTesting unadvised() MethodTook 110 msTest equals() method:Took 161 msTest hashCode() method:Took 94 msTest advised.getTargetClass() method:Took 68 ms&lt;&lt;&lt; 可以看到JDK代理的性能最低，其次，CGLIB Frozen的性能要明显好于 CGLIB； 选择要使用的代理CGLIB 可以代理类和接口，而JDK只能代理 接口； CGLIB Frozen模式性能明显好于 JDK代理和CGLIB代理； 如果要使用CGGLIB代理接口，必须使用proxyFactory.setOptimize(true);方法将ProxyFactory的optimize标志设置为true； 切入点的高级使用前面讲了6个Pointcut的使用，如果如下更大的灵活性，可以使用 ComposablePointcut 和 ControlFlowPointcut 使用控制流切入点ControlFlowPointcut定义target： 12345public class TestBean &#123; public void foo() &#123; System.out.println("foo()"); &#125;&#125; 定义通知： 12345678import org.springframework.aop.MethodBeforeAdvice;import java.lang.reflect.Method;public class SimpleBeforeAdvice implements MethodBeforeAdvice &#123; public void before(Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("Before method: " + method); &#125;&#125; 使用控制流ControlFlowPointcut： 123456789101112131415161718192021222324252627282930import org.springframework.aop.Advisor;import org.springframework.aop.Pointcut;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.ControlFlowPointcut;import org.springframework.aop.support.DefaultPointcutAdvisor;public class ControlFlowDemo &#123; public static void main(String[] args) &#123; ControlFlowDemo ex = new ControlFlowDemo(); ex.run(); &#125; public void run() &#123; TestBean target = new TestBean(); Pointcut pointcut = new ControlFlowPointcut(ControlFlowDemo.class, "test"); Advisor advisor = new DefaultPointcutAdvisor(pointcut, new SimpleBeforeAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); TestBean proxy = ((TestBean) proxyFactory.getProxy()); System.out.println("\tTrying normal invoke"); proxy.foo(); System.out.println("\n\tTrying under ControlFlowDemo.test()"); test(proxy); &#125; private void test(TestBean bean) &#123; bean.foo(); &#125;&#125; 运行结果： 123456 Trying normal invokefoo() Trying under ControlFlowDemo.test()Before method: public void com.learn.aop.pointcut.controlflow.TestBean.foo()foo() 关键代码： 1Pointcut pointcut = new ControlFlowPointcut(ControlFlowDemo.class, "test"); 表示 为ControlFlowDemo类的test()方法 创建了一个 ControlFlowPointcut实例， 切入从ControlFlowDemo.test() 调用 target 的所有方法； target 在 test()之外调用时，并不会被通知；只有在test()内调用时，才会被通知； 缺点：在其他切入点上使用 ControlFlowPointcut 会大大降低性能 使用组合切入点ComposablePointcut在前面的实例中，仅为每个Advisor使用了一个切入点。在大多数情况下，这通常是足够的，但有时需要多个Pointcut组合在一起 共同实现目标； ComposablePointcut 支持两种方法：union() 《等价于 “or”运算》和 intersection()《等价于 “and”运算》； 可以通过传入 ClassFilter、MethodMatcher、Pointcut 调用 union() 和 intersection()； 使用案例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import com.learn.aop.pointcut.controlflow.SimpleBeforeAdvice;import com.learn.aop.pointcut.name.GrammyGuitarist;import com.learn.aop.pointcut.name.Guitar;import org.springframework.aop.Advisor;import org.springframework.aop.ClassFilter;import org.springframework.aop.framework.ProxyFactory;import org.springframework.aop.support.ComposablePointcut;import org.springframework.aop.support.DefaultPointcutAdvisor;import org.springframework.aop.support.StaticMethodMatcher;import java.lang.reflect.Method;public class ComposablePointcutExample &#123; private static class SingMethodMatcher extends StaticMethodMatcher &#123; public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return method.getName().startsWith("si"); &#125; &#125; private static class TalkMethodMatcher extends StaticMethodMatcher &#123; public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return "talk".equals(method.getName()); &#125; &#125; private static class RestMethodMatcher extends StaticMethodMatcher &#123; public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return method.getName().endsWith("st"); &#125; &#125; private static GrammyGuitarist getProxy(ComposablePointcut pointcut, GrammyGuitarist target) &#123; Advisor advisor = new DefaultPointcutAdvisor(pointcut, new SimpleBeforeAdvice()); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); return ((GrammyGuitarist) proxyFactory.getProxy()); &#125; private static void testInvoke(GrammyGuitarist proxy) &#123; proxy.sing(); proxy.sing(new Guitar()); proxy.talk(); proxy.rest(); &#125; public static void main(String[] args) &#123; GrammyGuitarist target = new GrammyGuitarist(); ComposablePointcut pointcut = new ComposablePointcut(ClassFilter.TRUE, new SingMethodMatcher()); // 匹配si开头的方法 System.out.println("Test 1 &gt;&gt;&gt; "); GrammyGuitarist proxy = getProxy(pointcut, target); testInvoke(proxy); System.out.println(); // 匹配方法名为talk的方法 System.out.println("Test 2 &gt;&gt;&gt; "); pointcut.union(new TalkMethodMatcher()); proxy = getProxy(pointcut, target); testInvoke(proxy); System.out.println(); // 匹配st结尾的方法 System.out.println("Test 3 &gt;&gt;&gt; "); pointcut.intersection(new RestMethodMatcher()); proxy = getProxy(pointcut, target); testInvoke(proxy); System.out.println(); &#125;&#125; 运行结果： 12345678910111213141516171819202122Test 1 &gt;&gt;&gt; Before method: public void com.learn.aop.pointcut.name.GrammyGuitarist.sing()sing()Before method: public void com.learn.aop.pointcut.name.GrammyGuitarist.sing(com.learn.aop.pointcut.name.Guitar)play(): G C GC AM D7talk()zzz~~Test 2 &gt;&gt;&gt; Before method: public void com.learn.aop.pointcut.name.GrammyGuitarist.sing()sing()Before method: public void com.learn.aop.pointcut.name.GrammyGuitarist.sing(com.learn.aop.pointcut.name.Guitar)play(): G C GC AM D7Before method: public void com.learn.aop.pointcut.name.GrammyGuitarist.talk()talk()zzz~~Test 3 &gt;&gt;&gt; sing()play(): G C GC AM D7talk()zzz~~ “Test 1” 匹配si开头的方法，所以两个 sing() 方法都被通知； “Test 2”匹配si开头的方法 或 匹配方法名为talk的方法，所以两个 sing() 方法 和 talk() 都被通知； “Test 3” 匹配（ 以si 开头 || talk()） &amp;&amp; 以st 结尾， 没有方法符合，所以没有方法被通知；如果有一个方法名为 si***st() 那么，应该会被调用； 组合和切入点接口org.springframework.aop.support.Pointcuts 接口也能用来 组合两个Pointcut，但更多Pointcut的组合还是试用ComposablePointcut； 引入入门引入基础知识一种特殊类型的环绕通知； 适用于类级别，不能在使用引入时 使用切入点，两者语义不匹配； 实现 接口 IntroductionInterceptor来创建引入： 继承 类 DelegatingIntroductionInterceptor 来创建引入： 引入通知 构成了 被通知对象 状态的一部分；因此，每个被通知对象 都必须有一个独立的引入实例； 使用引入进行对象修改检测定义接口： 123public interface IsModified &#123; boolean isModified();&#125; 创建通知Advice类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.lang.reflect.Method;import java.util.HashMap;import java.util.Map;// 创建通知public class IsModifiedMixin extends DelegatingIntroductionInterceptor implements IsModified &#123; private boolean isModified = false; private Map&lt;Method, Method&gt; methodCache = new HashMap&lt;Method, Method&gt;(); public boolean isModified() &#123; return isModified; &#125; private Method getGetter(Method setter) &#123; Method getter = methodCache.get(setter); if (getter != null) &#123; return getter; &#125; String getterName = setter.getName().replace("set", "get"); try &#123; getter = setter.getDeclaringClass().getMethod(getterName, null); synchronized (methodCache) &#123; methodCache.put(setter, getter); &#125; return getter; &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); return null; &#125; &#125; @Override public Object invoke(MethodInvocation mi) throws Throwable &#123; if (!isModified) &#123; if (mi.getMethod().getName().startsWith("set") &amp;&amp; mi.getArguments().length == 1) &#123; Method getter = getGetter(mi.getMethod()); if (getter != null) &#123; Object newVal = mi.getArguments()[0]; Object oldVal = getter.invoke(mi.getThis(), null); System.out.printf("old:%s, new:%s, %s \n", String.valueOf(oldVal), String.valueOf(newVal), mi.getThis().toString()); if (newVal == null &amp;&amp; oldVal == null) &#123; isModified = false; &#125; else if (newVal == null &amp;&amp; oldVal != null) &#123; isModified = true; &#125; else if (newVal != null &amp;&amp; oldVal == null) &#123; isModified = true; &#125; else &#123; isModified = !newVal.equals(oldVal); &#125; &#125; &#125; &#125; return super.invoke(mi); &#125;&#125; 创建顾问Advisor, 关联通知IsModifiedMixin： 1234567import org.springframework.aop.support.DefaultIntroductionAdvisor;// 创建顾问public class IsModifiedAdvisor extends DefaultIntroductionAdvisor &#123; public IsModifiedAdvisor() &#123; super(new IsModifiedMixin()); &#125;&#125; 创建目标对象Target： 12345678910111213141516171819202122232425// 创建对象targetpublic class Contact &#123; private String name; private String phoneNumber; private String email; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPhoneNumber() &#123; return phoneNumber; &#125; public void setPhoneNumber(String phoneNumber) &#123; this.phoneNumber = phoneNumber; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125;&#125; 组装 Advisor 通知 和 target： 12345678910111213141516171819202122232425262728293031import org.springframework.aop.IntroductionAdvisor;import org.springframework.aop.framework.ProxyFactory;// 组装 Advisor 通知 和 targetpublic class IntroductionDemo &#123; public static void main(String[] args) &#123; Contact target = new Contact(); target.setName("John Mayer"); IntroductionAdvisor advisor = new IsModifiedAdvisor(); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(advisor); proxyFactory.setOptimize(true); // 使用CGLIB代理接口 Contact proxy = ((Contact) proxyFactory.getProxy()); IsModified proxyInterface = ((IsModified) proxy); System.out.println("Is Contact? : " + (proxy instanceof Contact)); System.out.println("Is IsModified? : " + (proxy instanceof IsModified)); System.out.println("\nHas Modified? : " + proxyInterface.isModified()); proxy.setName("John Mayer"); System.out.println("\nHas Modified? : " + proxyInterface.isModified()); proxy.setName("Eric Clapton"); System.out.println("\nHas Modified? : " + proxyInterface.isModified()); proxy.setName("Eric Clapton"); System.out.println("\nHas Modified? : " + proxyInterface.isModified()); proxy.setName("John Mayer"); System.out.println("\nHas Modified? : " + proxyInterface.isModified()); &#125;&#125; 运行结果： 12345678910111213Is Contact? : trueIs IsModified? : trueHas Modified? : falseold:John Mayer, new:John Mayer, com.learn.aop.inroduction.Contact@7d4793a8 Has Modified? : falseold:John Mayer, new:Eric Clapton, com.learn.aop.inroduction.Contact@7d4793a8 Has Modified? : trueHas Modified? : trueHas Modified? : true proxy对象 既是 target， 又是IsModified； proxyInterface.isModified() 第一次调用 查看初始状态John Mayer：未修改； proxyInterface.isModified() 第二次调用 设为同样的值John Mayer：未修改； proxyInterface.isModified() 第三次调用 设为新的值Eric Clapton：已经修改； proxyInterface.isModified() 第四次调用 设为新的值Eric Clapton：已经修改； proxyInterface.isModified() 第吴次调用 设为新的值Eric Clapton：已经修改； 目前只能检测一次变更；如果去掉 IsModifiedMixin 中的 if (!isModified) 就能检测多次变更； 123456789101112131415Is Contact? : trueIs IsModified? : trueHas Modified? : falseold:John Mayer, new:John Mayer, com.learn.aop.inroduction.Contact@7d4793a8 Has Modified? : falseold:John Mayer, new:Eric Clapton, com.learn.aop.inroduction.Contact@7d4793a8 Has Modified? : trueold:Eric Clapton, new:Eric Clapton, com.learn.aop.inroduction.Contact@7d4793a8 Has Modified? : falseold:Eric Clapton, new:John Mayer, com.learn.aop.inroduction.Contact@7d4793a8 Has Modified? : true 引入小结引入时Spring AOP最强大的 功能之一：不仅可以扩展现有方法的功能； 还可以动态扩展接口和对象实现。 AOP 框架事务我们不得不编写大量的代码 来生成代理； Spring提供了额外的框架，将代理注入到目标bean中； 以声明的方式配置AOP使用Spring AOP 声明式配置时，存在三个选项： 使用ProxyFactoryBean， ProxyFactoryBean提供了一种声明方式配置Spring的ApplicationContext（以及底层的BeanFactory）； 使用Spring AOP 命名空间： 使用@AspectJ 样式注解： 使用ProxyFactoryBeanProxyFactoryBean 允许 指定一个bean作为目标，并且为 该 bean 提供一组通知Advice 和 顾问Advisor（这些Advice 和 Advisor最终被合并到一个AOP代理中）； ProxyFactoryBean 将 拦截器 应用于现有的 目标bean； 使用AOP名称空间 使用 @AspectJ 注解 AspectJ集成]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>AOP</tag>
        <tag>面向切面编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 事务]]></title>
    <url>%2Fjava%2Fspring%2Fspring-transaction%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux网络流量监控]]></title>
    <url>%2Flinux%2Flinux-network-traffic-monitor%2F</url>
    <content type="text"><![CDATA[iftopcentos7： 1234yum install -y iftopifconfig -aiftop -i eth0 -n -P watch cat /proc/net/dev监控网卡流量 1watch cat /proc/net/dev iftop编译安装12345678yum install -y flex byacc libpcap ncurses-devel libpcap-devel #先要安装必需的软件 mkdir iftop cd iftop/ wget http://www.ex-parrot.com/pdw/iftop/download/iftop-1.0pre4.tar.gz #下载 tar zxvf iftop-1.0pre4.tar.gz #解压 cd iftop-1.0pre4 ./configure #配置 make &amp;&amp; make install #编译安装 命令说明 123456789101112131415161718192021222324252627语法: iftop -h | [-npblNBP] [-i interface] [-f filter code] [-F net/mask] [-G net6/mask6] -h 显示本帮助（Help）信息 -n 不进行主机名（hostName）查找 -N 不将端口号（port Number）转换成对应的服务 to services -p 混合（Promiscuous）模式（显示网络相关的其他主机信息） -b 不显示流量图形条（Bar） -B 以字节（Byte）为单位，显示带宽（Bandwidth）；默认以比特（bit）显示的 -i interface 监控的网卡接口（interface） -f filter code 包统计时，使用过滤码；默认：无，只统计IP包 -F net/mask 显示特定IPv4网段的进出流量（Flow）；如# iftop -F 10.10.1.0/24 -G net6/mask6 显示特定IPv6网段的进出流量（Flow） -l 显示并统计IPv6本地（Local）链接的流量（默认：关） -P 显示端口（Port） -m limit 设置显示界面上侧的带宽刻度（liMit） -c config file 指定配置（Config）文件 -t 使用不带窗口菜单的文本（text）接口 排序: -o 2s Sort by first column (2s traffic average) -o 10s Sort by second column (10s traffic average) [default] -o 40s Sort by third column (40s traffic average) -o source Sort by source address -o destination Sort by destination address The following options are only available in combination with -t -s num print one single text output afer num seconds, then quit -L num number of lines to print]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 tcp缓冲区和udp缓冲区大小设置_未完]]></title>
    <url>%2Flinux%2Flinux-tcp-and-udp-buffer%2F</url>
    <content type="text"><![CDATA[设置TCP缓冲区大小 设置UPD缓冲区大小鉴于tcp有重传机制，更多的时候udp对收发缓冲区的大小可能更加敏感一点。 udp缓冲区的大小主要和以下几个值有关： /proc/sys/net/core/rmem_max ——— udp缓冲区的最大值，单位字节，下同 /proc/sys/net/core/rmem_default ———- udp缓冲区的默认值，如果不更改的话程序的udp缓冲区默认值就是这个。 查看方法可以直接 cat 以上两个文件进行查看，也可以通过 sysctl 查看。 sysctl -a | grep rmem_max 其实sysctl信息来源就是 proc 下的文件。 程序中进行更改程序中可以使用setsockopt函数与SO_RCVBUF选项对udp缓冲区的值进行更改，但是要注意不管设置的值有多大，超过rmem_max的部分都会被无视。 int a = value_wanted; if (setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &amp;a, sizeof(int)) == - 1 ) { … } 更改系统值如果确实要把udp缓冲区改到一个比较大的值，那就需要更改rmem_max的值。编辑/etc/rc.local文件添加以下代码可使系统在每次启动的时候自动更改系统缓冲区的最大值。 echo value_wanted &gt; / proc /sys/net/core/rmem_default 或者在/etc/sysctl.conf添加以下代码即可在重启后永久生效。 rmem_max = MAX 不想重启的话使用命令 sysctl -p 即可。 可以顺便看下setsockopt在linux下的相关实现 1234567case SO_SNDBUF: if (val &gt; sysctl_wmem_max) val = sysctl_wmem_max; if ((val * 2 ) &lt; SOCK_MIN_SNDBUF) sk-&gt;sk_sndbuf = SOCK_MIN_SNDBUF; else sk-&gt;sk_sndbuf = val * 2 ; //当然缓冲区在系统中的实际值要大一点，因为udp报头以及IP报头等都是需要空间的。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tcp</tag>
        <tag>linux</tag>
        <tag>udp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux性能分析工具总纲_未完]]></title>
    <url>%2Flinux%2Flinux-performance-tools%2F</url>
    <content type="text"><![CDATA[linux 更详细的性能分析工具网址：http://www.brendangregg.com/ CSDN中文介绍：https://www.cnblogs.com/tcicy/p/8461807.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS解析过程]]></title>
    <url>%2Fcomputer-network%2Fdns-resolution-process%2F</url>
    <content type="text"><![CDATA[先说一下DNS的几个基本概念： 一． 根域就是所谓的“.”，其实我们的网址www.baidu.com在配置当中应该是www.baidu.com.（最后有一点），一般我们在浏览器里输入时会省略后面的点，而这也已经成为了习惯。 根域服务器我们知道有13台，但是这是错误的观点。 根域服务器只是具有13个IP地址，但机器数量却不是13台，因为这些IP地址借助了任播的技术，所以我们可以在全球设立这些IP的镜像站点，你访问到的这个IP并不是唯一的那台主机。 具体的镜像分布可以参考维基百科。这些主机的内容都是一样的 二． 域的划分根域下来就是顶级域或者叫一级域， 有两种划分方式，一种互联网刚兴起时的按照行业性质划分的com.，net.等，一种是按国家划分的如cn.，jp.，等。 具体多少你可以自己去查，我们这里不关心。 每个域都会有域名服务器，也叫权威域名服务器。 Baidu.com就是一个顶级域名，而www.baidu.com却不是顶级域名，他是在baidu.com 这个域里的一叫做www的主机。 一级域之后还有二级域，三级域，只要我买了一个顶级域，并且我搭建了自己BIND服务器（或者其他软件搭建的）注册到互联网中，那么我就可以随意在前面多加几个域了（当然长度是有限制的）。 比如a.www.baidu.com，在这个网址中，www.baidu.com变成了一个二级域而不是一台主机，主机名是a。 三． 域名服务器能提供域名解析的服务器，上面的记录类型可以是A(address)记录，NS记录（name server），MX（mail），CNAME等。 （详解参见博客：域名解析中A记录、CNAME、MX记录、NS记录的区别和联系） A记录是什么意思呢，就是记录一个IP地址和一个主机名字，比如我这个域名服务器所在的域test.baidu.com，我们知道这是一个二级的域名，然后我在里面有一条A记录,记录了主机为a的IP，查到了就返回给你了。 如果我现在要想baidu.com这个域名服务器查询a.test.baidu.com，那么这个顶级域名服务器就会发现你请求的这个网址在test.baidu.com这个域中，我这里记录了这个二级域的域名服务器test.baidu.com的NS的IP。我返回给你这个地址你再去查主机为a的主机把。 这些域内的域名服务器都称为权威服务器，直接提供DNS查询服务。（这些服务器可不会做递归哦） 四．解析过程那么我们的DNS是怎么解析一个域名的呢？ 1、现在我有一台计算机，通过ISP接入了互联网，那么ISP就会给我分配一个DNS服务器，这个DNS服务器不是权威服务器，而是相当于一个代理的dns解析服务器，他会帮你迭代权威服务器返回的应答，然后把最终查到IP返回给你。 2、现在的我计算机要向这台ISPDNS发起请求查询www.baidu.com这个域名了，(经网友提醒：这里其实准确来说不是ISPDNS，而应该是用户自己电脑网络设置里的DNS，并不一定是ISPDNS。比如也有可能你手工设置了8.8.8.8) 3、ISPDNS拿到请求后，先检查一下自己的缓存中有没有这个地址，有的话就直接返回。这个时候拿到的ip地址，会被标记为非权威服务器的应答。 4、如果缓存中没有的话，ISPDNS会从配置文件里面读取13个根域名服务器的地址（这些地址是不变的，直接在BIND的配置文件中）， 5、然后像其中一台发起请求。 6、根服务器拿到这个请求后，知道他是com.这个顶级域名下的，所以就会返回com域中的NS记录，一般来说是13台主机名和IP。 7、然后ISPDNS向其中一台再次发起请求，com域的服务器发现你这请求是baidu.com这个域的，我一查发现了这个域的NS，那我就返回给你，你再去查。 （目前百度有4台baidu.com的顶级域名服务器）。 8、ISPDNS不厌其烦的再次向baidu.com这个域的权威服务器发起请求，baidu.com收到之后，查了下有www的这台主机，就把这个IP返回给你了， 9、然后ISPDNS拿到了之后，将其返回给了客户端，并且把这个保存在高速缓存中。 下面我们来用 nslookup 这个工具详细来说一下解析步骤： 1yum install bind-utils -y 从上图我们可以看到: ​ 第一行Server是：DNS服务器的主机名–210.32.32.1 ​ 第二行Address是： 它的IP地址–210.32.32.1#53 ​ 下面的Name是：解析的URL– www.jsjzx.com ​ Address是：解析出来的IP–112.121.162.168 但是也有像百度这样的DNS比较复杂的解析: 你会发现百度有一个cname = www.a.shifen.com 的别名。 这是怎么一个过程呢？ 我们用dig工具来跟踪一下吧（linux系统自带有） -—————————————————————————————————————————————————————————————————————————– Dig工具会在本地计算机做迭代，然后记录查询的过程。 第一步是向我这台机器的ISPDNS获取到根域服务区的13个IP和主机名[b-j].root-servers.net.。 第二步是向其中的一台根域服务器（Servername就是末行小括号里面的）发送www.baidu.com的查询请求，他返回了com.顶级域的服务器IP（未显示）和名称 第三步，便向com.域的一台服务器192.43.172.30请求,www.baidu.com，他返回了baidu.com域的服务器IP（未显示）和名称，百度有四台顶级域的服务器 ​ 【此处可以用dig @192.43.172.30 www.baidu.com查看返回的百度顶级域名服务器IP地址】。 第四步呢，向百度的顶级域服务器（202.181.33.31）请求www.baidu.com，他发现这个www有个别名，而不是一台主机，别名是www.a.shifen.com。 -——————————————————————————————————————————————————————————————————————————– 按照一般的逻辑，当dns请求到别名的时候，查询会终止，然后重新发起查询别名的请求，所以此处应该返回的是www.a.shifen.com。 此处用dig +trace [www.a.shifen.com】跟踪一下 用一个图来说明一下 以下内容为在虚拟机中搭建local dns服务器得到的实验数据，纠正上述结论 在上面的分析中，我们用dig工具进行了追踪，但是dig没有继续追踪当我们从baidu.com拿到cname和ns2.a.shifen.com的IP之后的事情。 我们就所以然的下结论认为local dns会向ns2.a.shifen.com请求www.a.shifenc.om。 其实这个想法是错误，在自己的本地搭建一个local dns，抓取整个解析过程中是所有包，看看就明白拉。 实际的结果是虽然dns.baidu.com返回了a.shifen.com域的服务器地址和IP， 但是local dns并不是直接向上述返回的IP请求www.a.shifen.com，而是再一次去请求com域，得到shifen.com域的服务器（也就是baidu.com的那四台）， 然后又请求www.a.shifen.com，返回a.shifen.com的域的服务器，最后才是去请求www.a.shifen.com， 虽然上面已经返回了IP，但是实验的结果就是再走一遍shifen.com域的查询。 上图就是localdns在解析www.baidu.com的抓包全过程。蓝色那条就是在收到cname和响应的a.shifen.com的域名服务器IP地址之后，继续向com域请求shifen.com。 这个图充分说明了返回cname的同时也返回了ns2.a.shifen.com的IP。 因此总结一下便是 ​ ①本机向local dns请求www.baidu.com ​ ②local dns向根域请求www.baidu.com，根域返回com.域的服务器IP ​ ③向com.域请求www.baidu.com，com.域返回baidu.com域的服务器IP ​ ④向baidu.com请求www.baidu.com，返回cname www.a.shifen.com和a.shifen.com域的服务器IP ​ ⑤向root域请求www.a.shifen.com ​ ⑥向com.域请求www.a.shife.com ​ ⑦向shifen.com请求 ​ ⑧向a.shifen.com域请求 ​ ⑨拿到www.a.shifen.com的IP ​ ⑩localdns返回本机www.baidu.com cname www.a.shifen.com 以及 www.a.shifen.com的IP先说一下DNS的几个基本概念：]]></content>
      <categories>
        <category>computer-network</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1、lambda表达式入门：Java多核编程]]></title>
    <url>%2Fjava%2Flambda%2F1-java-lambda-introduce%2F</url>
    <content type="text"><![CDATA[第1章 新生代Java1、外部迭代 -&gt; 内部迭代外部迭代示例 123for (Point p : pointList) &#123; p.distance(0,0); // 点p 到 原点（0，0）的距离&#125; 内部迭代示例 1pointList.forEach(p -&gt; p.distance(0,0)); 1.1、内部迭代外部迭代 -&gt; 串行、有序 内部迭代 -&gt; 顺序不重要，重要的是行为； ​ -&gt; 延迟加载，乱序执行； 1.2、命令模式类似与命令模式，将 行为 抽象出来，作为Action； 123456789101112// 行为 接口public interface Consumer&lt;T&gt; &#123; void accept(T t);&#125;// 具体定义行为public Distance implements Consumer&lt;Point&gt; &#123; public void accept(Point p) &#123; p.distance(0, 0); &#125;&#125;// 为了调用 p.distance(0, 0); 不断创建对象，而且不够直观pointList.forEach(new Distance()); 为了调用 p.distance(0, 0); 不断创建对象，而且不够直观。 这是因为Java 方法只接受 对象引用 作为参数，因此要引入新语法 lambda 表达式 1.3、lambda 表达式12345pointList.forEach(new Consumer&lt;Point&gt; &#123; public void accept(Point p) &#123; p.distance(0, 0); &#125;&#125;); Consumer 是一种 ”单方法接口“，有利于 编译器直接 无歧义 的选择正确的方法执行； 引入一个额外的愈发（”-&gt;“）之后，将 参数与 表达式 分隔开，得到简单形式的 lambda表达式： 1pointList.forEach(p -&gt; p.distance(0,0)); 2、集合 -&gt; 流场景：一个Integer集合 —&gt; 转换成一个 Point实例 集合 —&gt; 寻找 距离原点(0, 0) 最远的点到原点的距离； 集合的写法 123456789List&lt;Integer&gt; intList = Arrays.asList(1,2,3,4,5);List&lt;Point&gt; pointList = new ArrayList&lt;&gt;();for (Integer i : intList) &#123; pointList.add(new Point(i%3, i/1));&#125;Double maxDistance = Double.MIN_VALUE;for (Point p : pointList) &#123; maxDistance = Math.max(maxDistance, p.distance(0, 0));&#125; 假如把 intList 看作一个无限流： 12345intStream = intList.stream();Stream&lt;Point&gt; pointStream = intStream.map(i -&gt; new Point(i%3, i/1));DoubleStream distanceStream = pointStream.mapToDouble(p -&gt; p.distance(0, 0));// 管道最后是终止操作maxOptionalDouble maxDistance = distances.max(); 完整的代码变成了如下： 12345OptionalDouble maxDistance = intList .stream() .map(i -&gt; new Point(i%3, i/1)) .mapToDouble(p -&gt; p.distance(0, 0)) .max(); 使用管道的好处是， 创建与管理中间集合所导致的性能消耗也随之消失； 3、串行 -&gt; 并行Java 8 支持对集合的并行处理，而 lambda表达式 是”提供这种支持“ 必备的一环； 原理： 将大任务 递归的 切分为 ”足够小“ 的 小任务（能串行执行为止）； 使用 cpu 的多核 分别执行完以后； 将结果 结合起来 返回； 实现方式：fork/join 框架 代码： 12345OptionalDouble maxDistance = intList .parallelStream() .map(i -&gt; new Point(i%3, i/1)) .mapToDouble(p -&gt; p.distance(0, 0)) .max(); 什么是 “足够小”的任务？ 取决于 可用核的数量； 4、组合行为Java 原版的排序器 12345Comparator&lt;Point&gt; byX = new Comparator&lt;Point&gt;() &#123; public int compare(Point p1, Point p2) &#123; return Double.compare(p1.getX(), p2.getX()); &#125;&#125; 创建一个通用的 排序器 Comparator 12345public static &lt;T,U extends Comparable&lt;U&gt;&gt; Compatable&lt;T&gt; comparing(Function&lt;T, U&gt; keyExtractor) &#123; return (c1, c2) -&gt; keyExtractor.apply(c1).compateTo(keyExtractor.apply(c2));&#125; 那么 1.3 中的问题，根据距离排序 可以实现为 1Comparator&lt;Point&gt; byDistance = comparing(p -&gt; p.distance(0, 0)); 新需求：按所有点到 原点的距离 升序排列，打印点： 12345intList .stream() .map(i -&gt; new Point(i%3, i/1)) .sorted(comparing(p -&gt; p.distance(0, 0))) .forEach(p -&gt; System.out.printf("(%f, %f)", p.getX(), p.getY()));]]></content>
      <categories>
        <category>java</category>
        <category>lambda</category>
      </categories>
      <tags>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Valid Parentheses]]></title>
    <url>%2Fleetcode%2Fvalid-parentheses%2F</url>
    <content type="text"><![CDATA[算法分类：Stackurl：https://leetcode.com/problems/valid-parentheses/ ​ 题目12345678910111213141516171819202120. Valid ParenthesesGiven a string containing just the characters '(', ')', '&#123;', '&#125;', '[' and ']', determine if the input string is valid.An input string is valid if:Open brackets must be closed by the same type of brackets.Open brackets must be closed in the correct order.Note that an empty string is also considered valid.Example 1:Input: "()"Output: trueExample 2:Input: "()[]&#123;&#125;"Output: trueExample 3:Input: "(]"Output: false ​ Java解法1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public boolean isValid(String s) &#123; Stack&lt;String&gt; stack = new Stack&lt;&gt;(); char[] chArr = s.toCharArray(); for (char ch : chArr) &#123; String top = stack.isEmpty() ? null : stack.peek(); String c = String.valueOf(ch); if (null == top) &#123; stack.push(c); &#125; else if (top.equals("&#123;")) &#123; if (c.equals("&#125;")) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; else if (top.equals("[")) &#123; if (c.equals("]")) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; else if (top.equals("(")) &#123; if (c.equals(")")) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; else &#123; stack.push(c); &#125; &#125; if (stack.isEmpty()) &#123; return true; &#125; else &#123; return false; &#125; &#125;&#125; 12345678910111213141516171819202122class Solution &#123; public boolean isValid(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;(); HashMap&lt;Character, Character&gt; map = new HashMap&lt;Character, Character&gt;() &#123;&#123; put('&#125;', '&#123;'); put(']', '['); put(')', '('); &#125;&#125;; char[] chArr = s.toCharArray(); for (char ch : chArr) &#123; Character top = stack.isEmpty() ? null : stack.peek(); if (null == top) &#123; stack.push(ch); &#125; else if (top == map.get(ch)) &#123; stack.pop(); &#125; else &#123; stack.push(ch); &#125; &#125; return stack.isEmpty(); &#125;&#125; ​ Python解法12345678910111213141516class Solution(object): def isValid(self, s): """ :type s: str :rtype: bool """ stack = [] map = &#123;u'&#125;':u'&#123;', u']':u'[', u')':u'('&#125; for ch in s: top = u'#' if (len(stack) == 0) else stack[-1] if map.has_key(ch) and top == map[ch]: stack.pop() else: stack.append(ch) return 0 == len(stack)]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unique Binary Search Trees ii]]></title>
    <url>%2Fleetcode%2Funique-binary-search-trees-ii%2F</url>
    <content type="text"><![CDATA[算法：二叉搜索树url：https://leetcode-cn.com/problems/unique-binary-search-trees-ii/ ​ 题目1234567891011121314151617181920给定一个整数 n，生成所有由 1 ... n 为节点所组成的二叉搜索树。示例:输入: 3输出:[ [1,null,3,2], [3,2,null,1], [3,1,null,null,2], [2,1,3], [1,null,2,null,3]]解释:以上的输出对应以下 5 种不同结构的二叉搜索树： 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 分析 首先来计数需要构建的二叉树数量。可能的二叉搜素数数量是一个 卡特兰数。 我们跟随上文的逻辑，只是这次是构建具体的树，而不是计数。 算法 我们从序列 1 ..n 中取出数字 i，作为当前树的树根。于是，剩余 i - 1 个元素可用于左子树，n - i 个元素用于右子树。如 前文所述，这样会产生 G(i - 1) 种左子树 和 G(n - i) 种右子树，其中 G 是卡特兰数。 现在，我们对序列 1 … i - 1 重复上述过程，以构建所有的左子树；然后对 i + 1 … n 重复，以构建所有的右子树。 这样，我们就有了树根 i 和可能的左子树、右子树的列表。 最后一步，对两个列表循环，将左子树和右子树连接在根上。 ​ Java解法123456789101112131415161718192021222324252627282930313233343536public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if (n &lt;= 0) &#123; return new LinkedList&lt;TreeNode&gt;(); &#125; return listTrees(1, n); &#125; public LinkedList&lt;TreeNode&gt; listTrees(int start, int end) &#123; LinkedList&lt;TreeNode&gt; rootList = new LinkedList&lt;TreeNode&gt;(); if(start &gt; end) &#123; rootList.add(null); return rootList; &#125; for(int i=start; i&lt;=end; ++i) &#123; LinkedList&lt;TreeNode&gt; leftList = listTrees(start, i-1); LinkedList&lt;TreeNode&gt; rightList = listTrees(i+1, end); for(TreeNode left : leftList) &#123; for(TreeNode right : rightList) &#123; TreeNode cur = new TreeNode(i); cur.left = left; cur.right = right; rootList.add(cur); &#125; &#125; &#125; return rootList; &#125;&#125; Python解法​]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Two Ssum]]></title>
    <url>%2Fleetcode%2Ftwo-sum%2F</url>
    <content type="text"><![CDATA[url：https://leetcode.com/problems/two-sum/submissions/ ​ 题目：Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. ​ Java 解法：12345678910111213class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException("No two sum solution"); &#125;&#125; ​ Python解法：12345678910111213# https://leetcode.com/problems/two-sum/submissions/class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for i in range(len(nums)): x = target-nums[i]; c = nums[0:i].count(x); if c &gt; 0 and nums.index(x) != i: return [nums.index(x), i]]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单链表反转 —— 变形]]></title>
    <url>%2Fleetcode%2Freverse-linked-list%2F</url>
    <content type="text"><![CDATA[算法：单链表反转题目123456这其实是一道变形的链表反转题，大致描述如下给定一个单链表的头节点 head,实现一个调整单链表的函数，使得每K个节点之间为一组进行逆序，并且从链表的尾部开始组起，头部剩余节点数量不够一组的不需要逆序。（不能使用队列或者栈作为辅助）例如：链表:1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;8-&gt;null, K = 3。那么 6-&gt;7-&gt;8，3-&gt;4-&gt;5，1-&gt;2各位一组。调整后：1-&gt;2-&gt;5-&gt;4-&gt;3-&gt;8-&gt;7-&gt;6-&gt;null。其中 1，2不调整，因为不够一组。 解析 这道题的难点在于，是从链表的尾部开始组起的，而不是从链表的头部，如果是头部的话，那我们还是比较容易做的，因为你可以遍历链表，每遍历 k 个就拆分为一组来逆序。但是从尾部的话就不一样了，因为是单链表，不能往后遍历组起； 思路1：整体反转，然后从头开始截取 K个节点 ，被后续截图的K个节点 的尾部连接；最后长度不足K的节点再局部反转 作为 队头 思路2：整体反转， ​ Java解法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Node &#123; int val; Node next; public Node(int d) &#123; this.val = d; &#125;&#125;class Solution &#123; public Node reverse(Node node) &#123; Node pre = node; Node cur = node.next; node.next = null; Node tmp; while(cur!=null) &#123; tmp = cur.next; cur.next = pre; pre = cur; cur = tmp; &#125; return pre; &#125; public Node reverseByK(Node node, int k) &#123; Node tmp = node; for (int i=1; i&lt;k &amp;&amp; tmp != null; i++) &#123; tmp = tmp.next; &#125; if (tmp == null) &#123; return node; &#125; Node childList = tmp.next; tmp.next = null; Node retNode = reverse(node); Node newChildNode = reverseByK(childList, k); node.next = newChildNode; return retNode; &#125; public static void main(String[] args) &#123; Node link = null; Node tmp = link; for(int i=1; i&lt;9; ++i) &#123; if(null == tmp) &#123; tmp = new Node(i); link = tmp; &#125; else &#123; tmp.next = new Node(i); tmp = tmp.next; &#125; &#125; Solution solution = new Solution(); link = solution.reverse(solution.reverseByK(solution.reverse(link), 3)); while(link!=null) &#123; System.out.print(link.val + " "); link = link.next; &#125; &#125;&#125; Python解法​]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recover Binary Search Tree]]></title>
    <url>%2Fleetcode%2Frecover-binary-search-tree%2F</url>
    <content type="text"><![CDATA[算法：二叉搜索树， 深度优先搜索url：https://leetcode.com/problems/recover-binary-search-tree/ ​ 题目123456789101112131415161718192021222324252627282930313233343599. Recover Binary Search TreeTwo elements of a binary search tree (BST) are swapped by mistake.Recover the tree without changing its structure.Example 1:Input: [1,3,null,null,2] 1 / 3 \ 2Output: [3,1,null,null,2] 3 / 1 \ 2Example 2:Input: [3,1,4,null,null,2] 3 / \1 4 / 2Output: [2,1,4,null,null,3] 2 / \1 4 / 3Follow up:A solution using O(n) space is pretty straight forward.Could you devise a constant space solution? 分析 要求，不改变现有树结构的前提，使数恢复为二叉搜索树； 使用DFS便利二叉树，凡是 不符合规则的节点都标记出来； 中序遍历，遍历结果有序，无序的节点就是 不合格的节点 Java解法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; TreeNode pre; TreeNode first, second; public void recoverTree(TreeNode root) &#123; pre = new TreeNode(Integer.MIN_VALUE); inorder(root); swap(first, second); &#125; public void inorder(TreeNode root) &#123; if(null == root) return; inorder(root.left); if(pre.val &gt;= root.val &amp;&amp; pre.val != Integer.MIN_VALUE) &#123; if(null == first) first = pre; second = root; &#125; pre = root; inorder(root.right); &#125; public void swap(TreeNode node1, TreeNode node2) &#123; int tmp = node1.val; node1.val = node2.val; node2.val = tmp; &#125;&#125;public class MainClass &#123; public static TreeNode stringToTreeNode(String input) &#123; input = input.trim(); input = input.substring(1, input.length() - 1); if (input.length() == 0) &#123; return null; &#125; String[] parts = input.split(","); String item = parts[0]; TreeNode root = new TreeNode(Integer.parseInt(item)); Queue&lt;TreeNode&gt; nodeQueue = new LinkedList&lt;&gt;(); nodeQueue.add(root); int index = 1; while(!nodeQueue.isEmpty()) &#123; TreeNode node = nodeQueue.remove(); if (index == parts.length) &#123; break; &#125; item = parts[index++]; item = item.trim(); if (!item.equals("null")) &#123; int leftNumber = Integer.parseInt(item); node.left = new TreeNode(leftNumber); nodeQueue.add(node.left); &#125; if (index == parts.length) &#123; break; &#125; item = parts[index++]; item = item.trim(); if (!item.equals("null")) &#123; int rightNumber = Integer.parseInt(item); node.right = new TreeNode(rightNumber); nodeQueue.add(node.right); &#125; &#125; return root; &#125; public static String treeNodeToString(TreeNode root) &#123; if (root == null) &#123; return "[]"; &#125; String output = ""; Queue&lt;TreeNode&gt; nodeQueue = new LinkedList&lt;&gt;(); nodeQueue.add(root); while(!nodeQueue.isEmpty()) &#123; TreeNode node = nodeQueue.remove(); if (node == null) &#123; output += "null, "; continue; &#125; output += String.valueOf(node.val) + ", "; nodeQueue.add(node.left); nodeQueue.add(node.right); &#125; return "[" + output.substring(0, output.length() - 2) + "]"; &#125; public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); String line; while ((line = in.readLine()) != null) &#123; TreeNode root = stringToTreeNode(line); new Solution().recoverTree(root); String out = treeNodeToString(root); System.out.print(out); &#125; &#125;&#125; Python解法​]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>深度优先搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Longest Palindromic Substring]]></title>
    <url>%2Fleetcode%2Flongest-palindromic-substring%2F</url>
    <content type="text"><![CDATA[算法：动态规划题目URL：https://leetcode-cn.com/problems/longest-palindromic-substring/ 1234567891011给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。示例 1：输入: "babad"输出: "bab"注意: "aba" 也是一个有效答案。示例 2：输入: "cbbd"输出: "bb" 分析 Java解法扩展中心算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.Stack;public class Solution &#123; public String longestPalindrome(String s) &#123; String longestS=null, curS=null; for(int i=0; i&lt;s.length(); ++i) &#123; curS = palindrome(s.toCharArray(), i); if (null == curS) continue; if (null == longestS) longestS = curS; else &#123; if (longestS.length() &lt;= curS.length()) longestS = curS; &#125; &#125; return (null == longestS) ? "" : longestS; &#125; public String palindrome(char[] chArr, int index) &#123; Stack&lt;Character&gt; stack1 = new Stack&lt;Character&gt;(); Stack&lt;Character&gt; stack2 = new Stack&lt;Character&gt;(); int len1=0, len2=0; if(index-1&gt;=0 &amp;&amp; chArr[index] == chArr[index-1]) &#123; for(int i=index-1; i&gt;=0; --i) &#123; int delta = index-1-i; if(index+delta&gt;=chArr.length) break; if(chArr[i] == chArr[index+delta]) &#123; stack1.push(chArr[i]); &#125; else &#123; break; &#125; &#125; len1 = stack1.size()*2; &#125; if(index-2&gt;=0 &amp;&amp; chArr[index] == chArr[index-2]) &#123; stack2.push(chArr[index-1]); for(int i=index-2; i&gt;=0; --i) &#123; int delta = index-2-i; if(index+delta&gt;=chArr.length) break; if(chArr[i] == chArr[index+delta]) &#123; stack2.push(chArr[i]); &#125; else &#123; break; &#125; &#125; len2 = stack2.size()*2 - 1; &#125; // System.out.printf("%d, %d %c \n", len1, len2, chArr[index]); if (0==len1 &amp;&amp; 0==len2) return String.valueOf(chArr[index]); if (len1&gt;len2) &#123; char[] res = new char[len1]; for(int i=0; !stack1.isEmpty(); ++i) &#123; char c = stack1.pop().charValue(); res[i] = res[len1-1-i] = c; &#125; return String.copyValueOf(res); &#125; else &#123; char[] res = new char[len2]; for(int i=0; !stack2.isEmpty(); ++i) &#123; char c = stack2.pop().charValue(); res[i] = res[len2-1-i] = c; &#125; return String.copyValueOf(res); &#125; &#125; public static void main(String[] args) &#123; String s = "ba"; String ret = new Solution().longestPalindrome(s); String out = (ret); System.out.print(out); &#125;&#125; 同样解法的代码优化 1234567891011121314151617181920212223242526class Solution &#123; public String longestPalindrome(String s) &#123; if (s == null || s.length() &lt; 1) return ""; int start = 0, end = 0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expandAroundCenter(s, i, i); int len2 = expandAroundCenter(s, i, i + 1); int len = Math.max(len1, len2); if (len &gt; end - start) &#123; start = i - (len - 1) / 2; end = i + len / 2; &#125; &#125; return s.substring(start, end + 1); &#125; private int expandAroundCenter(String s, int left, int right) &#123; int L = left, R = right; while (L &gt;= 0 &amp;&amp; R &lt; s.length() &amp;&amp; s.charAt(L) == s.charAt(R)) &#123; L--; R++; &#125; return R - L - 1; &#125;&#125; 动态规划的解法 12345678910111213141516171819202122232425262728293031323334353637383940public class App &#123; public static void main(String[] args) &#123; String s = "aba1ab"; String ret = new Solution().longestPalindrome(s); String out = (ret); System.out.print(out); &#125;&#125;class Solution &#123; public String longestPalindrome(String s) &#123; if(null == s || s.length() &lt;= 1) &#123; return s; &#125; String origin = s; String reverse = new StringBuffer(s).reverse().toString(); int length = s.length(); int[][] arr = new int[length][length]; int maxLen = 0; int maxEnd = 0; for(int i=0; i&lt;length; ++i) &#123; for(int j=0; j&lt;length; ++j) &#123; if(origin.charAt(i) == reverse.charAt(j)) &#123; if(0==i || 0==j) &#123; arr[i][j] = 1; &#125; else &#123; arr[i][j] = arr[i-1][j-1] + 1; &#125; &#125; if (arr[i][j]&gt;= maxLen) &#123; int beforeRev = length-1 - j; if(arr[i][j]-1+beforeRev == i) &#123; maxLen = arr[i][j]; maxEnd = i; &#125; &#125; &#125; &#125; return s.substring(maxEnd-(maxLen-1), maxEnd+1); &#125;&#125; Python解法​]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kth Largest Element in an Array]]></title>
    <url>%2Fleetcode%2Fkth-largest-element-in-an-array%2F</url>
    <content type="text"><![CDATA[算法：Heap 堆题目url:https://leetcode.com/problems/kth-largest-element-in-an-array/ 1234567891011121314215. Kth Largest Element in an ArrayFind the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element.Example 1:Input: [3,2,1,5,6,4] and k = 2Output: 5Example 2:Input: [3,2,3,1,2,4,5,5,6] and k = 4Output: 4Note: You may assume k is always valid, 1 ≤ k ≤ array's length. 分析用数据结构 Head（堆）来实现 堆：完全二叉树，常常用数组表示 用数组表示一棵树时，如果数组中节点的索引位x，则a、它的父节点的下标是：(x-1)/2；b、它的左子节点的下标为：2x + 1；c、它的右子节点的下标是：2x + 2； 堆的数组实现：https://www.cnblogs.com/g177w/p/8469399.html Java解法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133class Solution &#123; class Node &#123; private int data; public Node(int d) &#123; this.data = d; &#125; public int getValue() &#123; return this.data; &#125; public void setValue(int d) &#123; this.data = d; &#125; &#125; class Head &#123; private Node[] headArray; private int maxSize; private int currentSize; public Head(int maxSize) &#123; this.maxSize = maxSize; currentSize = 0; headArray = new Node[maxSize]; &#125; public boolean isEmpty() &#123; return 0 == currentSize; &#125; public boolean insert(int d) &#123; if (this.currentSize == this.maxSize) &#123; return false; &#125; Node node = new Node(d); headArray[currentSize] = node; trickleUp(currentSize++); return true; &#125; public void trickleUp(int index) &#123; int parentIndex = (index - 1) / 2; Node node = headArray[index]; while (index &gt; 0 &amp;&amp; headArray[parentIndex].getValue() &lt; node.getValue()) &#123; headArray[index] = headArray[parentIndex]; index = parentIndex; parentIndex = (index - 1) / 2; &#125; headArray[index] = node; &#125; public Node remove() &#123; Node root = headArray[0]; headArray[0] = headArray[--currentSize]; trickleDown(0); return root; &#125; public void trickleDown(int index) &#123; int largeChild; Node node = headArray[index]; while (index &lt; currentSize/2) &#123; int leftChild = 2 * index + 1; int rightChild = 2 * index +2; if (rightChild &lt; currentSize &amp;&amp; headArray[leftChild].getValue() &lt; headArray[rightChild].getValue()) &#123; largeChild = rightChild; &#125; else &#123; largeChild = leftChild; &#125; if (node.getValue() &gt;= headArray[largeChild].getValue()) &#123; break; &#125; headArray[index] = headArray[largeChild]; index = largeChild; &#125; headArray[index] = node; &#125; public boolean change(int index, int newValue) &#123; if (index &lt; 0 || index &gt; currentSize) &#123; return false; &#125; int oldValue = headArray[index].getValue(); headArray[index].setValue(newValue); if (oldValue &lt; newValue) &#123; trickleUp(index); &#125; else &#123; trickleDown(index); &#125; return true; &#125; public void displayHead() &#123; System.out.print("headArray:"); for (int i = 0; i &lt; currentSize; i++) &#123; if (headArray[i] != null) System.out.print(headArray[i].getValue()+" "); else System.out.print("--"); &#125; System.out.println(""); int nBlanks = 32; int itemsPerrow = 1; int column = 0; int j = 0; String dots = "........................"; System.out.println(dots + dots); while (currentSize &gt; 0)&#123; if (column == 0) for (int i = 0; i &lt; nBlanks; i++) &#123; System.out.print(" "); &#125; System.out.print(headArray[j].getValue()); if (++ j == currentSize) break; if (++ column == itemsPerrow)&#123; nBlanks /= 2; itemsPerrow *= 2; column = 0; System.out.println(); &#125; else for (int i = 0; i &lt; nBlanks * 2 - 2; i++) System.out.print(' '); &#125; System.out.println("\n"+dots + dots); &#125; &#125; public int findKthLargest(int[] nums, int k) &#123; Head head = new Solution().new Head(nums.length); for (int x : nums) &#123; head.insert(x); &#125; for (int i=0; i&lt;k-1; ++i) &#123; head.remove(); &#125; int ret = head.remove().getValue(); return ret; &#125;&#125; Python解法12]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[h index ii]]></title>
    <url>%2Fleetcode%2Fh-index-ii%2F</url>
    <content type="text"><![CDATA[算法：url：https://leetcode.com/problems/h-index-ii/ 题目1234567891011121314151617181920Given an array of citations sorted in ascending order (each citation is a non-negative integer) of a researcher, write a function to compute the researcher's h-index.According to the definition of h-index on Wikipedia: "A scientist has index h if h of his/her N papers have at least h citations each, and the other N − h papers have no more than h citations each."Example:Input: citations = [0,1,3,5,6]Output: 3 Explanation: [0,1,3,5,6] means the researcher has 5 papers in total and each of them had received 0, 1, 3, 5, 6 citations respectively. Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, her h-index is 3.Note:If there are several possible values for h, the maximum one is taken as the h-index.Follow up:This is a follow up problem to H-Index, where citations is now guaranteed to be sorted in ascending order.Could you solve it in logarithmic time complexity? 思路分析[0,1,3,5,6] 数组长度 n，存在一个元素 h， 使得 n 中有 h个元素 大于等于h， 其他 （n-h）个元素 &lt; h； 求h？ 数组是有序的 递增数组 遍历 arr，存在 arr[i], 使得 n-i == arr[i] Java解法12345678910class Solution &#123; public int hIndex(int[] citations) &#123; for(int i=citations.length-1; i&gt;=0; --i) &#123; if(citations[i] == citations.length-i) &#123; return citations[i]; &#125; &#125; return 0; &#125;&#125; 1234567891011121314151617181920public int hIndex(int[] citations) &#123; int l = 1, r = citations.length; int ans = 0; while(l &lt;= r)&#123; int m = l + ((r - l)&gt;&gt;1); int p = citations[citations.length - m]; if(m == p)&#123; return m; &#125; if(m &gt; p)&#123; r = m - 1; &#125; else&#123; ans = Math.max(ans, m); l = m + 1; &#125; &#125; return ans; &#125;&#125; Python解法]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Tree Right Side View]]></title>
    <url>%2Fleetcode%2Fbinary-tree-right-side-view%2F</url>
    <content type="text"><![CDATA[算法：二叉树广度优先遍历 BFSURL：https://leetcode.com/problems/binary-tree-right-side-view/ ​ 题目1234567891011121314151617181920212223242526Binary Tree Right Side ViewGiven a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.Example:Input: [1,2,3,null,5,null,4]Output: [1, 3, 4]Explanation: 1 &lt;--- / \2 3 &lt;--- \ \ 5 4 &lt;---Input: [1,2,3,null,5,null,4,null,null,8]Output: [1, 3, 4, 8]Explanation: 1 &lt;--- / \ 2 3 &lt;--- \ \ 5 4 &lt;--- / 8 &lt;--- 分析 使用层序遍历（level traversal），即广度优先搜索 —— 借助Queue 难点是 标注每一层，来标注 每层的最后一个节点 当前层节点总数：count1，当前访问到节点数 i 下一层几点总数：count2 ​ Java解法12345678910111213141516171819202122232425262728293031323334353637383940public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; return BFS(root); &#125; public List&lt;Integer&gt; BFS(TreeNode root) &#123; List&lt;Integer&gt; retList = new ArrayList&lt;Integer&gt;(); if(null == root) return retList; int count1=1, count2=0; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); while(!queue.isEmpty()) &#123; for(int i=0; i&lt;count1; ++i) &#123; TreeNode tn = queue.poll(); if(null != tn) &#123; if (i == count1-1) &#123; retList.add(tn.val); &#125; if(null != tn.left) &#123; queue.add(tn.left); count2++; &#125; if (null != tn.right) &#123; queue.add(tn.right); count2++; &#125; &#125; &#125; count1 = count2; count2 = 0; &#125; return retList; &#125;&#125; 结果 Runtime: 1 ms, faster than 98.22% of Java online submissions for Binary Tree Right Side View. Memory Usage: 36.3 MB, less than 100.00% of Java online submissions for Binary Tree Right Side View. 1次AC，Niubility Python解法12 ​]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>广度优先遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Tree Maximum Path Sum]]></title>
    <url>%2Fleetcode%2Fbinary-tree-maximum-path-sum%2F</url>
    <content type="text"><![CDATA[算法：二叉树 + 动态规划url：https://leetcode.com/problems/binary-tree-maximum-path-sum/ ​ 题目12345678910111213141516171819202122232425124. Binary Tree Maximum Path SumGiven a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root.Example 1:Input: [1,2,3] 1 / \ 2 3Output: 6Example 2:Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 分析： 不要求 最大值的路径，只要求最大值 ​ Java解法1234567891011121314151617181920public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125;&#125;class Solution &#123; int ret = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; return maxSum(root); &#125; public int maxSum(TreeNode root) &#123; if (null == root) return 0; int l = Math.max(0, maxSum(root.left)); int r = Math.max(0, maxSum(root.right)); int sum = l + r + root.val; ret = Math.max(ret, sum); return Math.max(l, r) + root.val; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Binary Tree</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Add Two Numbers]]></title>
    <url>%2Fleetcode%2Fadd-two-numbers%2F</url>
    <content type="text"><![CDATA[算法：链表 Linked list题目：Add Two Numbersurl：https://leetcode.com/problems/add-two-numbers/ 1234567You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself.Example:Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 分析 百位大数相加 Java解法 方法 定义 链表增加节点 单链表反转 不需要，只是为了练习算法 遍历链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110package com.test.demo;import java.io.IOException;class ListNode &#123; int val; ListNode next; public ListNode(int x) &#123;this.val = x;&#125;&#125;class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode retNode = null; int carryBit = 0; while(l1 != null || l2!= null) &#123; int d1 = null==l1 ? 0 : l1.val; int d2 = null==l2 ? 0 : l2.val; int d = d1 + d2 + carryBit; retNode = add(retNode, new ListNode(d%10)); carryBit = d/10; l1 = null==l1 ? l1 : l1.next; l2 = null==l2 ? l2 : l2.next; &#125; for (int i = carryBit; i&gt;0; i = i/10) &#123; retNode = add(retNode, new ListNode(i%10)); &#125; return retNode;// return reverse(retNode); &#125; public ListNode add(ListNode headNode, ListNode node) &#123; if (null == headNode) &#123; headNode = node; &#125; else &#123; ListNode index = headNode; while(index.next != null) &#123;index = index.next;&#125; index.next = node; &#125; return headNode; &#125; public ListNode reverse(ListNode headNode) &#123; if(headNode == null) return headNode; ListNode pre = headNode; ListNode cur; ListNode temp; for(cur = headNode.next; cur!=null; ) &#123; temp = cur.next; cur.next = pre; pre = cur; cur = temp; &#125; headNode.next = null; return pre; &#125;&#125;public class MainClass &#123; public static int[] stringToIntegerArray(String input) &#123; input = input.trim(); input = input.substring(1, input.length() - 1); if (input.length() == 0) &#123; return new int[0]; &#125; String[] parts = input.split(","); int[] output = new int[parts.length]; for(int index = 0; index &lt; parts.length; index++) &#123; String part = parts[index].trim(); output[index] = Integer.parseInt(part); &#125; return output; &#125; public static ListNode stringToListNode(String input) &#123; // Generate array from the input int[] nodeValues = stringToIntegerArray(input); // Now convert that list into linked list ListNode dummyRoot = new ListNode(0); ListNode ptr = dummyRoot; for(int item : nodeValues) &#123; ptr.next = new ListNode(item); ptr = ptr.next; &#125; return dummyRoot.next; &#125; public static String listNodeToString(ListNode node) &#123; if (node == null) &#123; return "[]"; &#125; String result = ""; while (node != null) &#123; result += Integer.toString(node.val) + ", "; node = node.next; &#125; return "[" + result.substring(0, result.length() - 2) + "]"; &#125; public static void main(String[] args) throws IOException &#123; ListNode l1 = stringToListNode("[9]"); ListNode l2 = stringToListNode("[1,9,9,9,9,9,9,9,9,9]"); ListNode ret = new Solution().addTwoNumbers(l1, l2); String out = listNodeToString(ret); System.out.print(out); &#125;&#125; Python解法​]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>Linked list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合类]]></title>
    <url>%2Fjava%2Fcollection%2Fjava-collection-class%2F</url>
    <content type="text"><![CDATA[参考：https://www.jianshu.com/p/32420c73941b?t=123 总纲 全貌 Set集合 List列表 Queue队列 Map键值对]]></content>
      <categories>
        <category>java</category>
        <category>collection</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Java集合类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux dstat 性能检测工具说明]]></title>
    <url>%2Flinux%2Flinux-monitor-dstat%2F</url>
    <content type="text"><![CDATA[dstat 介绍 dstat命令是一个用来替换vmstat、iostat、netstat、nfsstat和ifstat这些命令的工具，是一个全能系统信息统计工具。安装：yum install -y dstat dstat使用说明：直接使用dstat，默认使用的是-cdngy参数，分别显示cpu、disk、net、page、system信息 CPU状态: CPU的使用率。显示了用户占比，系统占比、空闲占比、等待占比、硬中断和软中断情况。 磁盘统计: 磁盘的读写，分别显示磁盘的读、写总数。 网络统计: 网络设备发送和接受的数据，分别显示的网络收、发数据总数。 分页统计: 系统的分页活动。分别显示换入（in）和换出（out）。 系统统计: 统计中断（int）和上下文切换（csw）。 dstat 是一个可以取代vmstat，iostat，netstat和ifstat这些命令的多功能产品。dstat克服了这些命令的局限并增加了一些另外的功能，增加了监控项，也变得更灵活了。dstat可以很方便监控系统运行状况并用于基准测试和排除故障。 dstat可以让你实时地看到所有系统资源，例如，你能够通过统计IDE控制器当前状态来比较磁盘利用率，或者直接通过网络带宽数值来比较磁盘的吞吐率（在相同的时间间隔内）。 dstat将以列表的形式为你提供选项信息并清晰地告诉你是在何种幅度和单位显示输出。这样更好地避免了信息混乱和误报。更重要的是，它可以让你更容易编写插件来收集你想要的数据信息，以从未有过的方式进行扩展。 dstat的默认输出是专门为人们实时查看而设计的，不过你也可以将详细信息通过CSV输出到一个文件，并导入到Gnumeric或者Excel生成表格中。 常见选项12345678910111213141516171819202122232425262728293031323334353637383940-c, --cpu：开启cpu统计-C：该选项跟cpu的编号（0~cpu核数-1,多个用都好隔开）如：0,3,total表示分别包含cpu0、cpu3和total-d, --disk：开启disk统计-D：改选跟具体的设备名（多个用逗号隔开）如：total,hda，hdb表示分别统计total、hda、hdb设备块-g, --page：开启分页统计-i, --int：开启中断统计-I 5,10：没弄懂呢~巴拉巴拉-l, --load：开启负载均衡统计，分别是1m，5m，15m-m, --mem：开启内存统计，包括used，buffers，cache，free-n, --net：开启net统计，包括接受和发送-N：该选项可以跟网络设备名多个用逗号隔开，如eth1,total-p, --proc：开启进程统计，包括runnable, uninterruptible, new-r, --io：io开启请求统计，包括read requests, write requests-s, --swap：开启swap统计，包括used, free-S：该选项可以跟具体的交换区，多个用逗号隔开如swap1,total-t, --time：启用时间和日期输出-T, --epoch：启用时间计数，从epoch到现在的秒数-y, --sys：开启系统统计，包括中断和上下文切换--aio：开启同步IO统计 (asynchronous I/O)--fs：开启文件系统统计，包括 (open files, inodes)--ipc：开启ipc统计，包括 (message queue, semaphores, shared memory)--lock：开启文件所统计，包括 (posix, flock, read, write)--raw：开启raw统计 (raw sockets)--socket：开启sockets统计，包括 (total, tcp, udp, raw, ip-fragments)--tcp：开启tcp统计，包括(listen, established, syn, time_wait, close)--udp：开启udp统计 (listen, active)--unix：开启unix统计(datagram, stream, listen, active)--vm：开启vm统计 (hard pagefaults, soft pagefaults, allocated, free)--stat：通过插件名称开启插件扩展，详见命令插件 ：可能的内置插件为aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock, mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time,udp, unix, vm--list：列举内置插件扩展的名称-a, --all：是默认值相当于 -cdngy (default)-f, --full：相当于 -C, -D, -I, -N and -S-v, --vmstat：相当于 -pmgdsc -D total-bw, --blackonwhite：在白色背景终端上改变显示颜色--float：在屏幕上的输出强制显示为浮点值（即带小数）(相反的选项设置为 --integer)--integer：在屏幕上的输出强制显示为整数值，此为默认值（相反的选项设置为--float）--nocolor：禁用颜色(意味着选项 --noupdate)--noheaders：禁止重复输出header，默认会打印一屏幕输出一次header--noupdate：当delay&gt;1时禁止在过程中更新（即在时间间隔内不允许更新）--output file：可以把状态信息以csv的格式重定向到指定的文件中，以便日后查看 命令参数 参数名称 参数描述 delay 两次输出之间的时间间隔，默认是1s count 报告输出的次数，默认是没有限制，一直输出知道ctrl+c 命令插件虽然anyone可以自由的为dstat编写插件，但dstat附带大量的插件已经大大扩展其功能，下面是dstat附带插件的一个概述 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152-battery：电池电池百分比(需要ACPI)-battery-remain：电池剩余小时、分钟(需要ACPI)-cpufreq：CPU频率百分比(需要ACPI)-dbus：dbus连接的数量(需要python-dbus)-disk-util：显示某一时间磁盘的忙碌状况-fan：风扇转速(需要ACPI)-freespace：每个文件系统的磁盘使用情况-gpfs：gpfs读/写 I/O(需要mmpmon)-gpfs-ops：GPFS文件系统操作(需要mmpmon)-helloworld：dstat插件Hello world示例-innodb-buffer：显示innodb缓冲区统计-innodb-io：显示innodb I/O统计数据-innodb-ops：显示innodb操作计数器-lustre：显示lustreI/O吞吐量-memcache-hits：显示memcache 的命中和未命中的数量-mysql5-cmds：显示MySQL5命令统计-mysql5-conn：显示MySQL5连接统计-mysql5-io：MySQL5 I/O统计数据-mysql5-keys：显示MySQL5关键字统计-mysql-io：显示MySQL I/O统计数据-mysql-keys：显示MySQL关键字统计-net-packets：显示接收和发送的数据包的数量-nfs3：显示NFS v3客户端操作-nfs3-ops：显示扩展NFS v3客户端操作-nfsd3：显示NFS v3服务器操作-nfsd3-ops：显示扩展NFS v3服务器操作-ntp：显示NTP服务器的ntp时间-postfix：显示后缀队列大小(需要后缀)-power：显示电源使用量-proc-count：显示处理器的总数-rpc：显示rpc客户端调用统计-rpcd：显示RPC服务器调用统计-sendmail：显示sendmail队列大小(需要sendmail)-snooze：显示每秒运算次数-test：显示插件输出-thermal：热系统的温度传感器-top-bio：显示消耗块I/O最大的进程-top-cpu：显示消耗CPU最大的进程-top-cputime：显示使用CPU时间最大的进程(单位ms)-top-cputime-avg：显示使用CPU时间平均最大的进程(单位ms)-top-io：显示消耗I/O最大进程-top-latency：显示总延迟最大的进程(单位ms)-top-latency-avg：显示平均延时最大的进程(单位ms)-top-mem：显示使用内存最大的进程-top-oom：显示第一个被OOM结束的进程-utmp：显示utmp连接的数量(需要python-utmp)-vmk-hba：显示VMware ESX内核vmhba统计数-vmk-int：显示VMware ESX内核中断数据-vmk-nic：显示VMware ESX内核端口统计-vz-io：显示每个OpenVZ请求CPU使用率-vz-ubc：显示OpenVZ用户统计-wifi：无线连接质量和信号噪声比 常用插件1234567--disk-util：显示某一时间磁盘的忙碌状况--freespace：显示当前磁盘空间使用率--proc-count：显示正在运行的程序数量--top-bio：显示块I/O最大的进程--top-cpu：显示CPU占用最大的进程--top-io：显示正常I/O最大的进程--top-mem：显示占用最多内存的进程 例如：监控磁盘 dstat -d –disk-util –disk-tps -d 分别显示磁盘的读、写总数–disk-tps #每秒每个磁盘事务(tps)统计–disk-util #磁盘利用率百分比 dstat -v-v, –vmstat：相当于 -pmgdsc -D total]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux strace命令详解_未完]]></title>
    <url>%2Flinux%2Flinux-strace%2F</url>
    <content type="text"><![CDATA[strace 简介 跟踪系统调用和信号 1234567NAME strace - trace system calls and signalsSYNOPSIS strace [-CdffhikqrtttTvVxxy] [-In] [-bexecve] [-eexpr]... [-acolumn] [-ofile] [-sstrsize] [-Ppath]... -ppid... / [-D] [-Evar[=val]]... [-uusername] command [args] strace -c[df] [-In] [-bexecve] [-eexpr]... [-Ooverhead] [-Ssortby] -ppid... / [-D] [-Evar[=val]]... [-uusername] command [args] 安装stracecentos7 安装：yum install strace -y 跟踪特定进程 strace -ppid 使用 strace -p32557 https://segmentfault.com/a/1190000005931147 strace &amp; ltrace &amp; phpstracestracestrace是Linux环境下的一款程序调试工具，用来监察一个应用程序所使用的系统调用及它所接收的系统信息。追踪程序运行时的整个生命周期，输出每一个系统调用的名字，参数，返回值和执行消耗的时间等。 常用参数：-p 跟踪指定的进程-f 跟踪由fork子进程系统调用-F 尝试跟踪vfork子进程系统调吸入，与-f同时出现时, vfork不被跟踪-o filename 默认strace将结果输出到stdout。通过-o可以将输出写入到filename文件中-ff 常与-o选项一起使用，不同进程(子进程)产生的系统调用输出到filename.PID文件-r 打印每一个系统调用的相对时间-t 在输出中的每一行前加上时间信息。 -tt 时间确定到微秒级。还可以使用-ttt打印相对时间-v 输出所有系统调用。默认情况下，一些频繁调用的系统调用不会输出-s 指定每一行输出字符串的长度,默认是32。文件名一直全部输出-c 统计每种系统调用所执行的时间，调用次数，出错次数。-e expr 输出过滤器，通过表达式，可以过滤出掉你不想要输出 这里特别说下strace的-e trace选项。 要跟踪某个具体的系统调用，-e trace=xxx即可。但有时候我们要跟踪一类系统调用，比如所有和文件名有关的调用、所有和内存分配有关的调用。 如果人工输入每一个具体的系统调用名称，可能容易遗漏。于是strace提供了几类常用的系统调用组合名字。 -e trace=file 跟踪和文件访问相关的调用(参数中有文件名)-e trace=process 和进程管理相关的调用，比如fork/exec/exit_group-e trace=network 和网络通信相关的调用，比如socket/sendto/connect-e trace=signal 信号发送和处理相关，比如kill/sigaction-e trace=desc 和文件描述符相关，比如write/read/select/epoll等-e trace=ipc 进程见同学相关，比如shmget等 绝大多数情况，我们使用上面的组合名字就够了。实在需要跟踪具体的系统调用时，可能需要注意C库实现的差异。 比如我们知道创建进程使用的是fork系统调用，但在glibc里面，fork的调用实际上映射到了更底层的clone系统调用。使用strace时，得指定-e trace=clone, 指定-e trace=fork什么也匹配不上。 当发现进程或服务异常时，我们可以通过strace来跟踪其系统调用，“看看它在干啥”，进而找到异常的原因。熟悉常用系统调用，能够更好地理解和使用strace。 当然，万能的strace也不是真正的万能。当目标进程卡死在用户态时，strace就没有输出了。 这个时候我们需要其他的跟踪手段，比如gdb/perf/SystemTap等。 备注： 1、perf原因kernel支持 2、ftrace kernel支持可编程 3、systemtap 功能强大，RedHat系统支持，对用户态，内核态逻辑都能探查，使用范围更广 追踪多个进程方法当有多个子进程的情况下，比如php-fpm、nginx等，用strace追踪显得很不方便。可以使用下面的方法来追踪所有的子进程。 12345678# vim /root/.bashrc //添加以下内容function straceall &#123; strace $(pidof &quot;$&#123;1&#125;&quot; | sed &apos;s/\([0-9]*\)/-p \1/g&apos;)&#125;# source /root/.bashrc# traceall php-fpm //监控phpfpmOR# strace -tt -T $(pidof &apos;php-fpm: pool www&apos; | sed &apos;s/\([0-9]*\)/\-p \1/g&apos;) 追踪web服务12# strace -f -F -s 1024 -o nginx-strace /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf# strace -f -F -o php-fpm-strace /usr/local/php/sbin/php-fpm -y /usr/local/php/etc/php-fpm.conf 追踪mysql12# strace -f -F -ff -o mysqld-strace -s 1024 -p mysql_pid# find ./ -name &quot;mysqld-strace*&quot; -type f -print |xargs grep -n &quot;SELECT.*FROM&quot; 查看程序做了什么1234567891011121314151617#!/bin/bash# This script is from http://poormansprofiler.org/nsamples=1sleeptime=0pid=$(pidof $1) for x in $(seq 1 $nsamples) do gdb -ex &quot;set pagination 0&quot; -ex &quot;thread apply all bt&quot; -batch -p $pid sleep $sleeptime done | \awk &apos; BEGIN &#123; s = &quot;&quot;; &#125; /^Thread/ &#123; print s; s = &quot;&quot;; &#125; /^\#/ &#123; if (s != &quot;&quot; ) &#123; s = s &quot;,&quot; $4&#125; else &#123; s = $4 &#125; &#125; END &#123; print s &#125;&apos; | \sort | uniq -c | sort -r -n -k 1,1 ltrace-a 对齐具体某个列的返回值-c 计算时间和调用，并在程序退出时打印摘要-C 解码低级别名称（内核级）为用户级名称-d 打印调试信息-e 改变跟踪的事件-f 跟踪子进-h 打印帮助信息-i 打印指令指针，当库调用时。-l 只打印某个库中的调用。-L 不打印库调用。-n, –indent=NR 对每个调用级别嵌套以NR个空格进行缩进输出。-o, –output=file 把输出定向到文件。-p PID 附着在值为PID的进程号上进行ltrace。-r 打印相对时间戳。-s STRLEN 设置打印的字符串最大长度。-S 显示系统调用。-t, -tt, -ttt 打印绝对时间戳。-T 输出每个调用过程的时间开销。-u USERNAME 使用某个用户id或组ID来运行命令。-V, –version 打印版本信息，然后退出。-x NAME treat the global NAME like a library subroutine. phpstracephpstrace追踪php进程 12345Usage: ./php-strace [ options ]-h|--help show this help-l|--lines &lt;integer&gt; output the last N lines of a stacktrace. Default: 100--process-name &lt;string&gt; name of running php processes. Default: autodetect--live search while running for new upcoming pid&apos;s 使用truss/strace/ltrace跟踪进程]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux vmstat命令详解]]></title>
    <url>%2Flinux%2Flinux-vmstat%2F</url>
    <content type="text"><![CDATA[一、前言​ 很显然，从名字我们就可以知道 vmstat 是一个查看虚拟内存（Virtual Memory）使用状况的工具，但是怎样通过vmstat 来发现系统中的瓶颈呢？在回答这个问题前，还是让我们回顾一下Linux中关于虚拟内存的相关内容。 二、虚拟内存​ 在系统中运行的每个进程都需要使用到内存，但不是每个进程都需要每时每刻使用系统分配的内存空间。当系统运行所需内存超过实际的物理内存，内核会释放某些进程所占用但未使用的部分或所有物理内存，将这部分资料存储在磁盘上直到下一次调用，将释放出的内存提供给有需要的进程使用。 ​ 在Linux内存管理中，主要通过”调页Paging“和”交换Swapping“来完成内存调度。调页算法是将内存中最近不常使用的页面换到磁盘上，把活动页面保留在内存中供进程使用。交换技术是将整个进程，而不是部分页面，全部交换到磁盘上。 ​ 分页（Page）写入磁盘的过程被称为Page-Out，分页（Page）从磁盘重新回到内存的过程叫Page-In。当内核需要一个分页时，但发现此分页不再物理内存中（因为已经被Page-Out了），此时就发生了分页错误（Page-Fault）。 ​ 当系统内核发现可运行内存变少时，会通过Page-Out来释放一部分物理内存。尽管Page-Out不是经常发生，但是如果Page-Out频繁不断的发生，直到当内核管理分页的时间超过运行程式的时间时，熊效能会急剧下降。这时系统已经运行非常慢或进入暂停状态，这种状态被称为 thrashing（颠簸）。 1234567891011121314151617181920212223241、page fault出现的原因： a). 页表中找不到对应虚拟地址的PTE(无效地址/有效地址但是没有载入主存); b). 对应虚拟地址的PTE拒绝访问。2、page fault在哪里进行处理 page fault被CPU捕获，跳转到 page_fault_handler 进行处理。3、page fault的处理方式 page fault -&gt; 访问地址是否合法 a. 无效地址：segment fault，返回(用户地址杀死进程、内核地址杀死内核) b. 有效地址： 1) page第一次被访问: demand_page_faults (demanding pages，请求调页) 检查页表中是否存在该PTE pmd_none, pte_none 分配新的页帧，初始化(从磁盘读入内存) 2) page被交换到swap分区 检查present标志位，如果该位为0表示不在主存中。 分配新的页帧，从磁盘重新读入内存。 3) COW(Copy-On-Write) vm_area_struct允许写，但是对应的PTE禁止写操作。4、如何判断访问地址是否合法？如果地址合法有什么操作？ 判断地址合法的方式： static int __do_page_fault()函数 vma = find_vma(mm, addr); 根据传入的地址addr查找对应的vm_area_struct，如果没有找到证明该地址访问无效，返回segment fault。 三、vmstat详解1、用法1234567vmstat [-a] [-n] [-S unit] [delay [ count]]vmstat [-s] [-n] [-S unit]vmstat [-m] [-n] [delay [ count]]vmstat [-d] [-n] [delay [ count]]vmstat [-p disk partition] [-n] [delay [ count]]vmstat [-f]vmstat [-V] -a：显示活跃和非活跃内存-f：显示从系统启动至今的fork数量 。-m：显示slabinfo-n：只在开始时显示一次各字段名称。-s：显示内存相关统计信息及多种系统活动数量。delay：刷新时间间隔。如果不指定，只显示一条结果。count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷。-d：显示磁盘相关统计信息。-p：显示指定磁盘分区统计信息-S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）。默认单位为K（1024 bytes）-V：显示vmstat版本信息。 2、使用说明vmstat 5 5 【在5秒时间内进行5次采样】 1vmstat 5 5 12345678910111213141516171819202122232425262728293031procs（进程）： r: 运行队列中进程数量，这个值也可以判断是否需要增加CPU。（长期大于1） b： 等待IO的进程数量 memory（内存）： swpd: 使用虚拟内存大小。注意：如果swpd的值不为0，但是si，so的值长期为0，这种情况不会影响系统性能。 free: 空闲物理内存大小 buff: 用作缓冲的内存大小 cache: 用作缓存的内存大小。注意：如果cache的值大的时候，说明cache处的文件数多，如果频繁访问到的文件都能被cache处，那么磁盘的读IO bi会非常小。 inact: 非活跃内存大小（当使用-a选项时显示） active: 活跃的内存大小（当使用-a选项时显示） swap： si: 每秒从交换区写到内存的大小，由磁盘调入内存 so: 每秒写入交换区的内存大小，由内存调入磁盘 注意：内存够用的时候，这2个值都是0，如果这2个值长期大于0时，系统性能会受到影响，磁盘IO和CPU资源都会被消耗。有些朋友看到空闲内存（free）很少的或接近于0时，就认为内存不够用了，不能光看这一点，还要结合si和so，如果free很少，但是si和so也很少（大多时候是0），那么不用担心，系统性能这时不会受到影响的。 io：（现在的Linux版本块的大小为1024bytes） bi: 每秒读取的块数 bo: 每秒写入的块数system： in: 每秒中断数，包括时钟中断。【interrupt】 cs: 每秒上下文切换数。【count/second】 注意：随机磁盘读写的时候，这2个值越大（如超出1024k)，能看到CPU在IO等待的值也会越大。cpu（以百分比表示）： us: 用户进程执行时间(user time)。注意： us的值比较高时，说明用户进程消耗的CPU时间多，但是如果长期超50%的使用，那么我们就该考虑优化程序算法或者进行加速。 sy: 系统进程执行时间(system time)。注意：sy的值高时，说明系统内核消耗的CPU资源多，这并不是良性表现，我们应该检查原因。 id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 。 wa: 等待IO时间。注意：wa的值高时，说明IO等待比较严重，这可能由于磁盘大量作随机访问造成，也有可能磁盘出现瓶颈（块操作）。 vmstat -a 2 5 【-a 显示活跃和非活跃内存,所显示的内容除增加inact和active 】 vmstat -f 【 linux下创建进程的系统调用是fork】 四、总结目前说来，对于服务器监控有用处的度量主要有： r（运行队列） pi（页导入） us（用户CPU） sy（系统CPU） id（空闲） 注意：如果r经常大于4 ，且id经常少于40，表示cpu的负荷很重。如果bi，bo 长期不等于0，表示内存不足。 通过VMSTAT识别CPU瓶颈：r（运行队列）展示了正在执行和等待CPU资源的任务个数。当这个值超过了CPU数目，就会出现CPU瓶颈了。 Linux下查看CPU核心数的命令：cat /proc/cpuinfo|grep processor|wc -l 当r值超过了CPU个数，就会出现CPU瓶颈，解决办法大体几种： 最简单的就是增加CPU个数和核数 通过调整任务执行时间，如大任务放到系统不繁忙的情况下进行执行，进尔平衡系统任务 调整已有任务的优先级 通过vmstat识别CPU满负荷： 首先需要声明一点的是，vmstat中CPU的度量是百分比的。当us＋sy的值接近100的时候，表示CPU正在接近满负荷工作。但要注意的是，CPU 满负荷工作并不能说明什么，Linux总是试图要CPU尽可能的繁忙，使得任务的吞吐量最大化。唯一能够确定CPU瓶颈的还是r（运行队列）的值。 通过vmstat识别RAM瓶颈： 数据库服务器都只有有限的RAM，出现内存争用现象是Oracle的常见问题。 首先用free查看RAM的数量：[oracle@oracle-db02 ~]$ freetotal used free shared buffers cachedMem: 2074924 2071112 3812 0 40616 1598656-/+ buffers/cache: 431840 1643084Swap: 3068404 195804 2872600 当内存的需求大于RAM的数量，服务器启动了虚拟内存机制，通过虚拟内存，可以将RAM段移到SWAP DISK的特殊磁盘段上，这样会 出现虚拟内存的页导出和页导入现象，页导出并不能说明RAM瓶颈，虚拟内存系统经常会对内存段进行页导出，但页导入操作就表明了服务器需要更多的内存了， 页导入需要从SWAP DISK上将内存段复制回RAM，导致服务器速度变慢。 解决的办法有几种： 最简单的，加大RAM； 改小SGA，使得对RAM需求减少； 减少RAM的需求。（如：减少PGA） 如果disk经常不等于0，且在b中的队列大于3，表示io性能不好。 如果在processes中运行的序列(process r)是连续的大于在系统中的CPU的个数表示系统现在运行比较慢,有多数的进程等待CPU。 如果r的输出数大于系统中可用CPU个数的4倍的话,则系统面临着CPU短缺的问题,或者是CPU的速率过低,系统中有多数的进程在等待CPU,造成系统中进程运行过慢。 如果空闲时间(cpu id)持续为0并且系统时间(cpu sy)是用户时间的两倍(cpu us)系统则面临着CPU资源的短缺。 解决办法:当发生以上问题的时候请先调整应用程序对CPU的占用情况.使得应用程序能够更有效的使用CPU.同时可以考虑增加更多的CPU. 关于CPU的使用情况还可以结合mpstat, ps aux top prstat –a等等一些相应的命令来综合考虑关于具体的CPU的使用情况,和那些进程在占用大量的CPU时间.一般情况下，应用程序的问题会比较大一些.比如一些sql语句不合理等等都会造成这样的现象. 内存问题现象:内存的瓶颈是由scan rate (sr)来决定的.scan rate是通过每秒的始终算法来进行页扫描的.如果scan rate(sr)连续的大于每秒200页则表示可能存在内存缺陷.同样的如果page项中的pi和po这两栏表示每秒页面的调入的页数和每秒调出的页数.如果该值经常为非零值,也有可能存在内存的瓶颈,当然,如果个别的时候不为0的话,属于正常的页面调度这个是虚拟内存的主要原理. 解决办法: 调节applications &amp; servers使得对内存和cache的使用更加有效. 增加系统的内存. Implement priority paging in s in pre solaris 8 versions by adding line “set priority paging=1” in /etc/system. Remove this line if upgrading from Solaris 7 to 8 &amp; retaining old /etc/system file. 关于内存的使用情况还可以结ps aux top prstat –a等等一些相应的命令来综合考虑关于具体的内存的使用情况,和那些进程在占用大量的内存.一般情况下，如果内存的占用率比较高,但是,CPU的占用很低的时候,可以考虑是有很多的应用程序占用了内存没有释放,但是,并没有占用CPU时间,可以考虑应用程序,对于未占用CPU时间和一些后台的程序,释放内存的占用。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx+keepalived双机热备（主从模式）]]></title>
    <url>%2Fnginx%2Fnginx-keepalived%2F</url>
    <content type="text"><![CDATA[参考：https://blog.csdn.net/l1028386804/article/details/80098334]]></content>
      <categories>
        <category>nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Nginx+keepalived双机热备（主从模式）]]></title>
    <url>%2Fnginx%2Fnginx-reverse-proxy-http%2F</url>
    <content type="text"><![CDATA[参考：https://blog.csdn.net/l1028386804/article/details/80098334]]></content>
      <categories>
        <category>nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx 转发 tcp/udp]]></title>
    <url>%2Fnginx%2Fnginx-reverse-proxy-tcp-and-udp%2F</url>
    <content type="text"><![CDATA[参考：https://blog.csdn.net/l1028386804/article/details/80098334 Nginx四层负载均衡]]></content>
      <categories>
        <category>nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx 永久跳转（301） 和 临时重定向（302）]]></title>
    <url>%2Fnginx%2Fnginx-301-and-302%2F</url>
    <content type="text"><![CDATA[科普301永久跳转：当用户或搜索引擎向网站服务器发出浏览请求时，服务器返回的HTTP数据流中头信息中的状态码的一种，表示本网页永久性转移到另一个地址。 302临时跳转：也是状态码的一种，意义是暂时转向到另外一个网址。 主要区别 一句话，302容易被搜索引擎视为spam，301则不会。 conf设置301跳转设置123456server &#123; listen 80; server_name 123.com; rewrite ^/(.*) http://456.com/$1 permanent; access_log off;&#125; 302跳转设置123456server &#123; listen 80; server_name 123.com; rewrite ^/(.*) http://456.com/$1 redirect; access_log off;&#125; 研究Nginx的重定向用到了Nginx的HttpRewriteModule rewrite命令nginx的rewrite相当于apache的rewriterule(大多数情况下可以把原有apache的rewrite规则加上引号就可以直接使用)，它可以用在server,location 和IF条件判断块中 rewrite 正则表达式 替换目标 flag标记，flag标记可以用以下几种格式： last – 基本上都用这个Flag。 break – 中止Rewirte，不在继续匹配 redirect – 返回临时重定向的HTTP状态302 permanent – 返回永久重定向的HTTP状态301 特别注意： last和break用来实现URL重写，浏览器地址栏的URL地址不变，但是在服务器端访问的路径发生了变化； redirect和permanent用来实现URL跳转，浏览器地址栏会显示跳转后的URL地址； 例如下面这段设定nginx将某个目录下面的文件重定向到另一个目录,$2对应第二个括号(.*)中对应的字符串： 123location /download/ &#123; rewrite ^(/download/.*)/m/(.*)\..*1 /nginx-rewrite/$2.gz break;&#125; 例如下面设定nginx在用户使用ie的使用重定向到/nginx-ie目录下： 12if (httpuseragent MSIE) rewrite (.∗)$ /nginx−ie/$1 break; 文件和目录判断： −f和!−f判断是否存在文件 −d和!−d判断是否存在目录 −e和!−e判断是否存在文件或目录 −x和!−x判断文件是否可执行 例如下面设定nginx在文件和目录不存在的时候重定向： 1234if(!−e request_filename) &#123; proxy_pass http://127.0.0.1;&#125;return nginx防盗链返回http代码，例如设置nginx防盗链： 123456location ~* \.(gif|jpg|png|swf|flv)$ &#123; valid_referers none blocked www.test.com www.test1.com; if ($invalid_referer) &#123; return 404; &#125;&#125;]]></content>
      <categories>
        <category>nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx 动静分离配置]]></title>
    <url>%2Fnginx%2Fnginx-synamic-and-static-separation%2F</url>
    <content type="text"><![CDATA[正则表达式12345678~ 或 ^： 表示 正则表达式的开始. ： 匹配除 \n 之外 的 一个任意字符( ：子表达式的开始) ：子表达式的结束| ：表示 在两个 或者 多个之间进行的选择\ ：转义字符$ 或 ^ ：表示 表达式的结尾* ：0个或多个匹配前面的 正则表达式]]></content>
      <categories>
        <category>nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统负载]]></title>
    <url>%2Flinux%2Flinux-cpu-load-average%2F</url>
    <content type="text"><![CDATA[一、查看系统负荷如果你的电脑很慢，你或许想查看一下，它的工作量是否太大了。 在Linux系统中，我们一般使用 uptime 命令查看（w命令和top命令也行）。（另外，它们在苹果公司的Mac电脑上也适用。） 你在终端窗口键入uptime，系统会返回一行信息。 这行信息的后半部分，显示 load average，它的意思是”系统的平均负荷”，里面有三个数字，我们可以从中判断系统负荷是大还是小。 为什么会有三个数字呢？你从手册中查到，它们的意思分别是1分钟、5分钟、15分钟内系统的平均负荷。 如果你继续看手册，它还会告诉你，当CPU完全空闲的时候，平均负荷为0；当CPU工作量饱和的时候，平均负荷为1。 那么很显然，load average 的值越低，比如等于0.2或0.3，就说明电脑的工作量越小，系统负荷比较轻。 但是，什么时候能看出系统负荷比较重呢？等于1的时候，还是等于0.5或等于1.5的时候？如果1分钟、5分钟、15分钟三个值不一样，怎么办？ 二、一个类比判断系统负荷是否过重，必须理解load average的真正含义。下面，我根据”Understanding Linux CPU Load“这篇文章，尝试用最通俗的语言，解释这个问题。 首先，假设最简单的情况，你的电脑只有一个CPU，所有的运算都必须由这个CPU来完成。 那么，我们不妨把这个CPU想象成一座大桥，桥上只有一根车道，所有车辆都必须从这根车道上通过。（很显然，这座桥只能单向通行。） 系统负荷为0，意味着大桥上一辆车也没有。 系统负荷为0.5，意味着大桥一半的路段有车。 系统负荷为1.0，意味着大桥的所有路段都有车，也就是说大桥已经”满”了。但是必须注意的是，直到此时大桥还是能顺畅通行的。 系统负荷为1.7，意味着车辆太多了，大桥已经被占满了（100%），后面等着上桥的车辆为桥面车辆的70%。以此类推，系统负荷2.0，意味着等待上桥的车辆与桥面的车辆一样多；系统负荷3.0，意味着等待上桥的车辆是桥面车辆的2倍。总之，当系统负荷大于1，后面的车辆就必须等待了；系统负荷越大，过桥就必须等得越久。 CPU的系统负荷，基本上等同于上面的类比。大桥的通行能力，就是CPU的最大工作量；桥梁上的车辆，就是一个个等待CPU处理的进程（process）。 如果CPU每分钟最多处理100个进程，那么系统负荷0.2，意味着CPU在这1分钟里只处理20个进程；系统负荷1.0，意味着CPU在这1分钟里正好处理100个进程；系统负荷1.7，意味着除了CPU正在处理的100个进程以外，还有70个进程正排队等着CPU处理。 为了电脑顺畅运行，系统负荷最好不要超过1.0，这样就没有进程需要等待了，所有进程都能第一时间得到处理。很显然，1.0是一个关键值，超过这个值，系统就不在最佳状态了，你要动手干预了。 三、系统负荷的经验法则1.0是系统负荷的理想值吗？ 不一定，系统管理员往往会留一点余地，当这个值达到0.7，就应当引起注意了。经验法则是这样的： 当系统负荷持续大于0.7，你必须开始调查了，问题出在哪里，防止情况恶化。 当系统负荷持续大于1.0，你必须动手寻找解决办法，把这个值降下来。 当系统负荷达到5.0，就表明你的系统有很严重的问题，长时间没有响应，或者接近死机了。你不应该让系统达到这个值。 四、多处理器上面，我们假设你的电脑只有1个CPU。如果你的电脑装了2个CPU，会发生什么情况呢？ 2个CPU，意味着电脑的处理能力翻了一倍，能够同时处理的进程数量也翻了一倍。 还是用大桥来类比，两个CPU就意味着大桥有两根车道了，通车能力翻倍了。 所以，2个CPU表明系统负荷可以达到2.0，此时每个CPU都达到100%的工作量。推广开来，n个CPU的电脑，可接受的系统负荷最大为n.0。 五、多核处理器芯片厂商往往在一个CPU内部，包含多个CPU核心，这被称为多核CPU。 在系统负荷方面，多核CPU与多CPU效果类似，所以考虑系统负荷的时候，必须考虑这台电脑有几个CPU、每个CPU有几个核心。然后，把系统负荷除以总的核心数，只要每个核心的负荷不超过1.0，就表明电脑正常运行。 怎么知道电脑有多少个CPU核心呢？ cat /proc/cpuinfo命令，可以查看CPU信息。grep -c &#39;model name&#39; /proc/cpuinfo命令，直接返回CPU的总核心数。 六、最佳观察时长最后一个问题，”load average”一共返回三个平均值—-1分钟系统负荷、5分钟系统负荷，15分钟系统负荷，—-应该参考哪个值？ 如果只有1分钟的系统负荷大于1.0，其他两个时间段都小于1.0，这表明只是暂时现象，问题不大。 如果15分钟内，平均系统负荷大于1.0（调整CPU核心数之后），表明问题持续存在，不是暂时现象。所以，你应该主要观察”15分钟系统负荷”，将它作为电脑正常运行的指标。]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux atop详解]]></title>
    <url>%2Flinux%2Flinux-monitor-atop%2F</url>
    <content type="text"><![CDATA[atop 是一个功能非常强大的linux服务器监控工具，它的数据采集主要包括：CPU、内存、磁盘、网络、进程等，并且内容非常的详细，特别是当那一部分存在压力它会以特殊的颜色进行展示，如果颜色是红色那么说明已经非常严重了。 注意：所有的信息都是反映过去10s的状态信息 安装12345678910111213141516171819202122232425262728293031323334#1.官网：http://www.atoptool.nl/downloadatop.php找下载链接#2.下载解压编译wget http://www.atoptool.nl/download/atop-2.2-3.tar.gztar xvf atop-2.2-3.tar.gzcd atop-2.2-3make prefix=/usr/local/atop install#3.（报错rawlog.c:154:18: fatal error: zlib.h: No such file or directory，yum -y install zlib未解决，因为只安装lib没有头文件，提示需要zlib头文件） #4.解决方案：标准安装zlib#官网: http://www.zlib.net/#找下载链接：http://zlib.net/zlib-1.2.8.tar.gz 或者：http://iweb.dl.sourceforge.net/project/libpng/zlib/1.2.8/zlib-1.2.8.tar.gzcd ../wget http://zlib.net/zlib-1.2.8.tar.gztar xzf zlib-1.2.8.tar.gzcd zlib-1.2.8#创建动态库./configure --sharedmake testmake installcp zutil.h /usr/local/includecp zutil.c /usr/local/include #5.重新编译cd atop-2.2-3make cleanmake install#提示Choose either 'make systemdinstall' or 'make sysvinstall' #关于采用哪种方式安装可以参考这里：http://www.linuxdiyf.com/linux/10711.htmlmake systemdimstall#6.运行atop#7.退出q参考：http://my.oschina.net/nox/blog/221014http://blog.sina.com.cn/s/blog_714dacd10102v6et.html 资源监控字段含义 上图中列出了不少字段以及数值，各字段的含义是什么？我们应该怎么看？以上每个字段的含义都是相对采样周期而言的，下面我们先来关注上图显示的上半部分。 ATOP列：该列显示了主机名、信息采样日期和时间点 PRC列：该列显示进程整体运行情况 sys：过去10s所有的进程在内核态运行的时间总和 usr：过去10s所有的进程在用户态的运行时间总和 #proc：进程总数 #trun：过去10s转换的进程数 #zombie：过去10s僵死进程的数量 #exit：在10s采样周期期间退出的进程数量 CPU列：该列显示CPU整体(即多核CPU作为一个整体CPU资源)的使用情况，我们知道CPU可被用于执行进程、处理中断，也可处于空闲状态(空闲状态分两种，一种是活动进程等待磁盘IO导致CPU空闲，另一种是完全空闲) sys：cpu在处理进程时处于内核态的时间所占的比例 usr：cpu在处理进程时处于用户态的时间所占的比例 irq：cpu在处理进程的中断请求所占的实际比例 idle：CPU处在完全空闲状态的时间比例 wait：CPU处在“进程等待磁盘IO导致CPU空闲”状态的时间比例 CPU列各个字段指示值相加结果为N00%，其中N为cpu核数。 cpu列：该列显示某一核cpu的使用情况，各字段含义可参照CPU列，各字段值相加结果为100% CPL列：该列显示CPU负载情况 avg1、avg5和avg15字段：过去1分钟、5分钟和15分钟内运行队列中的平均进程数量 csw(context swapping)：上下文交换次数 intr(interrupt)：中断发生次数 numcpu：cpu的核心数 MEM列：列主要展示内存的使用信息 tot：物理内存总量 free：空闲内存的大小 cache：用于页缓存的内存大小 buff：用于文件缓存的内存大小 slab：系统内核占用的内存大小 dirty：内存中的脏页大小 SWP列：该列指示交换空间的使用情况 tot：交换区总量 free：空闲交换空间大小 PAG列：该列指示虚拟内存分页情况 swin:换入内存页数 swout：换出内存页数 LVM/DSK列：该列指示磁盘使用情况，每一个磁盘设备对应一列，如果有sdb设备，那么增多一列DSK信息 sda字段：磁盘设备标识 busy：磁盘忙时所占比例 read、KiB/r 、MBr/s：每秒读的请求数和请求的kb、mb数 write、KiB/w 、MBr/w：每秒写的请求数和请求的kb、mb数 avq:磁盘平均队列长度（根据实际的监控该列好像是磁盘平均请求数avgrq） avio:磁盘的平均io时间 NET列：多列NET展示了传输层（TCP/UDP）、网络层（ip）、网络接口的网络传输信息。 XXXi 字段指示各层或活动网口收包数目 XXXo 字段指示各层或活动网口发包数目 transport：传输层（TCP/UDP）的数据输入输出的展示，例如在服务器的内部进程之间的数据传输就是在传输层展示，以为还不需要往下通过网络进行传输。 network：网络层（ip）的数据输入输出的展示； eth0：默认的网络接口的数据输入输出的展示，也就是通过etho的ip的数据传输的展示， sp:网卡的带宽（1000M） pcki：传入的数据包的大小 pcko：传出的数据包的大小 si:每秒传入的数据大小 so：每秒传出的数据大小 coll（collisions）：每秒的冲突数 mlti（MULTICAST）：每秒的多路广播的数量 erri/erro:每秒输入输出的错误数 drpi/drpo:每秒的输入输出的丢包数 lo:通过127.0.0.1网络接口的数据传输的数据展示，参数和上面的eth0是一样的 视图切换 g：默认视图 m：内存视图 c：命令视图 默认视图（g）按g键可以从其他视图跳到默认视图 从上图中，我们可以看到PID为3061的find进程在退出前在 内核模式下占用了3.43秒CPU时间， 在用户模式下占用了0.96秒CPU时间，共使用CPU时间为4.39秒，相对10分钟采样周期，CPU时间占用比例为1%， ST列表示进程状态，N表示该进程是前一个采样周期新生成的进程，E表示该进程已退出， EXC列指示进程的退出码。从进程名在“&lt;&gt;”符号中，我们亦可知该进程已退出。 内存视图（m）内存视图展示了进程使用内存情况，按m键可进入内存视图。 SYSCPU：过去10s内进程处于内核模式占用的CPU时间 USRCPU：过去10s进程处于用户模式占用的CPU时间 VSIZE：过去10s 进程占用的虚拟空间大小 RSIZE：过去10s 进程占用的内存空间大小 PSIZE：过去10s 进程占用的页大小 VGROW：过去10s 进程增长的虚拟空间大小 RGROW：过去10s 进程增长的内存大小 SWAPSZ：过去10s 进程使用交换空间的大小。 MEM：过去10s 进程占用内存百分比 磁盘状态模式（d）RDDSK:过去10S进程读磁盘的数据量 WRDSK:过去10S进程写磁盘的数据量 DSK:过去10S进程所占磁盘的百分比 CMD:进程名 进程状态模式（p）同一个名称的进程显示一列，根据进程名进行分组显示 NPROCS:相同名称的进程数量 其它的参数上面已经有列出 线程状态模式（v） 用户模式（u）根据用户进行分组显示 命令视图模式（c）按c键我们可以进入命令视图，该视图展示了与每个进程相对应的命令。 有时我们某位“马大哈”同事执行了某个脚本或命令，使得系统资源占用率异常飙高，这时，我们可以很容易地通过atop的命令视图找到导致异常的命令。 atop的相关文件/etc/atop：目录保存的是atop的配置文件/etc/rc.d/init.d/atop：atop的启动文件/etc/cron.d/atop：atop的定时任务文件，默认是每天0点开始/var/log/atop：atop日志文件，默认是每天0点开始会产生当天的一个日志文件，然后可以通过atop -r file 查看信息，但是没有找到自动播放的的功能，只能通过输入b显示一个指定的时间的信息，可以写个循环来实现/usr/bin/atop：atop命令目录 atop日志每个时间点采样页面组合起来就形成了一个atop日志文件，我们可以使用”atop -r XXX”命令对日志文件进行查看。那以什么形式保存atop日志文件呢？ 对于atop日志文件的保存方式，我们可以这样： 每天保存一个atop日志文件，该日志文件记录当天信息 日志文件以”atop_YYYYMMDD”的方式命名 设定日志失效期限，自动删除一段时间前的日志文件 其实atop开发者已经提供了以上日志保存方式，相应的atop.daily脚本可以在源码目录下找到。在atop.daily脚本中，我们可以通过修改INTERVAL变量改变atop信息采样周期(默认为10分钟)；通过修改以下命令中的数值改变日志保存天数(默认为28天)： 1(sleep 3; find $LOGPATH -name 'atop_*' -mtime +28 -exec rm &#123;&#125; \; )&amp;]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux htop详解]]></title>
    <url>%2Flinux%2Flinux-monitor-htop%2F</url>
    <content type="text"><![CDATA[htop是Linux系统下一个基本文本模式的、交互式的进程查看器，主要用于控制台或shell中，可以替代top； htop和top对比： 两者相比起来，top比较繁琐 默认支持图形界面的鼠标操作 可以横向或纵向滚动浏览进程列表，以便看到所有的进程和完整的命令行 杀进程时不需要输入进程号等 安装Ubuntu1apt-get install htop CentOS12yum -y install epel-release.noarchyum -y install htop 编译安装1234567891011121、安装gcc、内核库依赖库yum install -y gcc ncurses-deve2、下载源码wget http://sourceforge.net/projects/htop/files/latest/download3、解压并进入htop-1.0.2目录tar -zxf downloadcd htop-1.0.24、编译安装(执行./configure如果执行报错 configure: error: You may want to use --disable-unicode or install libncursesw，表明缺少lib 包，安装lib包：yum install ncurses-devel -y)./configure &amp;&amp; make &amp;&amp; make install5、验证htop 字段含义 PID：进行的标识号 USER：运行此进程的用户 PRI：进程的优先级 NI：进程的优先级别值，默认的为0，可以进行调整 VIRT：进程占用的虚拟内存值 RES：进程占用的物理内存值 SHR：进程占用的共享内存值 S：进程的运行状况，R表示正在运行、S表示休眠，等待唤醒、Z表示僵死状态 %CPU：该进程占用的CPU使用率 %MEM：该进程占用的物理内存和总内存的百分比 TIME+：该进程启动后占用的总的CPU时间 COMMAND：进程启动的启动命令名称 选项1htop [OPTIONS] htop常用功能键F1 : 查看htop使用说明F2 : 设置F3 : 搜索进程F4 : 过滤器，按关键字搜索F5 : 显示树形结构F6 : 选择排序方式F7 : 减少nice值，这样就可以提高对应进程的优先级F8 : 增加nice值，这样可以降低对应进程的优先级F9 : 杀掉选中的进程F10 : 退出htop / : 搜索字符h : 显示帮助l ：显示进程打开的文件: 如果安装了lsof，按此键可以显示进程所打开的文件u ：显示所有用户，并可以选择某一特定用户的进程s : 将调用strace追踪进程的系统调用t : 显示树形结构 H ：显示/隐藏用户线程I ：倒转排序顺序K ：显示/隐藏内核线程M ：按内存占用排序P ：按CPU排序T ：按运行时间排序 上下键或PgUP, PgDn : 移动选中进程左右键或Home, End : 移动列表Space(空格) : 标记/取消标记一个进程。命令可以作用于多个进程，例如 “kill”，将应用于所有已标记]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux iotop详解]]></title>
    <url>%2Flinux%2Flinux-monitor-iotop%2F</url>
    <content type="text"><![CDATA[iotop命令 是一个用来监视磁盘I/O使用状况的top类工具。iotop具有与top相似的UI，其中包括PID、用户、I/O、进程等相关信息。Linux下的IO统计工具如iostat，nmon等大多数是只能统计到per设备的读写情况，如果你想知道每个进程是如何使用IO的就比较麻烦，使用iotop命令可以很方便的查看。 iotop使用Python语言编写而成，要求Python2.5（及以上版本）和Linux kernel2.6.20（及以上版本）。iotop提供有源代码及rpm包，可从其官方主页下载。 安装Ubuntu 1apt-get install iotop CentOS 1yum install iotop -y 编译安装 1234wget http://guichaz.free.fr/iotop/files/iotop-0.4.4.tar.gz tar zxf iotop-0.4.4.tar.gz python setup.py build python setup.py install 选项123456789iotop [OPTIONS]-o：只显示有io操作的进程-p PID：监控的进程pid。-u USER：监控的进程用户。-b：批量显示，无交互，主要用作记录到文件。-n NUM：显示NUM次，主要用于非交互式模式。-d SEC：间隔SEC秒显示一次。 iotop常用快捷键 左右箭头：改变排序方式，默认是按IO排序。 r：改变排序顺序。 o：只显示有IO输出的进程。 p：进程/线程的显示方式的切换。 a：显示累积使用量。 q：退出。 实例直接执行iotop就可以看到效果了： 1iotop]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux iftop详解]]></title>
    <url>%2Flinux%2Flinux-monitor-iftop%2F</url>
    <content type="text"><![CDATA[iftop 主要用来显示本机网络流量情况及各相互通信的流量集合，如单独同那台机器间的流量大小，非常适合于代理服务器和iptables服务器使用； 安装Ubuntu 1apt-get install iotop CentOS 12yum install -y flex byacc libpcap ncurses ncurses-devel libpcap-develyum install -y iftop iftop界面相关说明 中间的&lt;= =&gt;这两个左右箭头，表示的是流量的方向。 TX：发送流量 RX：接收流量 TOTAL：总流量 Cumm：运行iftop到目前时间的总流量 peak：流量峰值 rates：分别表示过去 2s 10s 40s 的平均流量 参数选项1234567891011-i 设定监测的网卡，如：# iftop -i eth1-B 以bytes为单位显示流量(默认是bits)，如：# iftop -B-n 使host信息默认直接都显示IP，如：# iftop -n-N 使端口信息默认直接都显示端口号，如: # iftop -N-F 显示特定网段的进出流量，如# iftop -F 10.10.1.0/24或# iftop -F 10.10.1.0/255.255.255.0-h（display this message），帮助，显示参数信息-p 使用这个参数后，中间的列表显示的本地主机信息，出现了本机以外的IP信息;-b 使流量图形条默认就显示;-f 这个暂时还不太会用，过滤计算包用的;-P 使host信息及端口信息默认就都显示;-m 设置界面最上边的刻度的最大值，刻度分五个大段显示，例：# iftop -m 100M 操作命令1234567891011121314151617181920212223h：切换是否显示帮助;n：切换显示本机的IP或主机名;s：切换是否显示本机的host信息;d：切换是否显示远端目标主机的host信息;t：切换显示格式为2行/1行/只显示发送流量/只显示接收流量;N：切换显示端口号或端口服务名称;S：切换是否显示本机的端口信息;D：切换是否显示远端目标主机的端口信息;p：切换是否显示端口信息;P：切换暂停/继续显示;b：切换是否显示平均流量图形条;B：切换计算2秒或10秒或40秒内的平均流量;T：切换是否显示每个连接的总流量;l：打开屏幕过滤功能，输入要过滤的字符，比如ip,按回车后，屏幕就只显示这个IP相关的流量信息;L：切换显示画面上边的刻度;刻度不同，流量图形条会有变化;j：或按k可以向上或向下滚动屏幕显示的连接记录;1：或2或3可以根据右侧显示的三列流量数据进行排序;&lt;：根据左边的本机名或IP排序;&gt;：根据远端目标主机的主机名或IP排序;o：切换是否固定只显示当前的连接;f：可以编辑过滤代码；!：可以使用shell命令；q：退出监控；]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[cache设计]]></title>
    <url>%2Farchitecture%2Fcache-design%2F</url>
    <content type="text"><![CDATA[局部性原理 缓存策略 缓存淘汰策略]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ESB架构]]></title>
    <url>%2Farchitecture%2Fesb%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>ESB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SOA架构]]></title>
    <url>%2Farchitecture%2Fsoa%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>SOA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构]]></title>
    <url>%2Farchitecture%2Fmicro-service%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 安装配置]]></title>
    <url>%2Fnginx%2F1-nginx-setup-and-conf%2F</url>
    <content type="text"><![CDATA[1、概述nginx 能够支持 5W 并发连接，并且 cpu、内存 等资源消费非常低，运行非常稳定。 2、环境配置1、安装 gcc 环境 2、安装第三方开发包 PCRE ：解析 正则表达式 3、安装 zlib：提供压缩和解压，zlib用来对http包进行gzip 4、安装 OpenSSL：用来支持 https 3、安装nginx 启动nginx 1kill -9 master_pid 只会杀死 master进程，woker 进程仍然在运行 1./nginx -s reload 如果 woker进程 他爹 不是主进程（woker进程id不连续）， 要通过 reload 重启一下； 停止nginx 123456./nginx -g TERM|INT|QUIT# TERM|INT：用于快速停止# QUIT: 用于平缓停止kill TERM|INT|QUIT /nginx/logs/nginx.pid# 一般不建议 kill -9 Pid 去停止 4、conf 文件详解https://blog.csdn.net/tjiyu/article/details/53027619 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384user nginx; # 配置用户、工作组user nobody;worker_processes 1; # woker 进程数量error_log /var/log/nginx/error.log;error_log /var/log/nginx/error.log notice; # 日志切割error_log /var/log/nginx/error.log warn; # 日志等级pid /var/run/nginx.pid; # master进程 pid# 事件驱动相关events &#123; worker_connections 1024; # 线程数 use method; # 时间驱动模型 select，poll，epoll，kqueue &#125;http &#123; include /etc/nginx/mime.types; # 扫描外部的配置文件，包含了浏览器能识别的所有文件格式 default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; # 静群 keepalive_timeout 65; multi_accept off; # 默认off，一次只允许一个网络连接进来 #gzip on; # 是否使用http压缩 use method; include /etc/nginx/conf.d/*.conf;&#125;server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\.ht &#123; # deny all; #&#125;&#125; tcp_nopush： 静群效应，默认开启 accept_mutex：当一个新连接到达时，如果激活了accept_mutex，那么多个Worker将以串行方式来处理，其中有一个Worker会被唤醒，其他的Worker继续保持休眠状态；如果没有激活accept_mutex，那么所有的Worker都会被唤醒，不过只有一个Worker能获取新连接，其它的Worker会重新进入休眠状态，这就是「惊群问题」。 nginx缺省激活了accept_mutex，是一种保守的选择。如果关闭了它，可能会引起一定程度的惊群问题，表现为上下文切换增多（sar -w）或者负载上升，但是如果你的网站访问量比较大，为了系统的吞吐量，我还是建议大家关闭它。 multi_accept：告诉nginx收到一个新连接通知后接受尽可能多的连接，默认是on，设置为on后，多个worker按串行方式来处理连接，也就是一个连接只有一个worker被唤醒，其他的处于休眠状态，设置为off后，多个worker按并行方式来处理连接，也就是一个连接会唤醒所有的worker，直到连接分配完毕，没有取得连接的继续休眠。当你的服务器连接数不多时，开启这个参数会让负载有一定的降低，但是当服务器的吞吐量很大时，为了效率，可以关闭这个参数。 use epoll; ：nginx采用epoll事件模型，处理效率高 work_connections: 单个worker进程允许客户端最大连接数，这个数值一般根据服务器性能和内存来制定，实际最大值就是worker进程数乘以work_connections 5、功能1、 动静分离 2、负载均衡 3、反向代理]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 代理和负载均衡]]></title>
    <url>%2Fnginx%2F2-nginx-reverse-proxy-and-load-balance%2F</url>
    <content type="text"><![CDATA[反向代理 和 负载均衡七层负载均衡 四层负载均衡 正向代理]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx 架构设计]]></title>
    <url>%2Fnginx%2F3-nginx-architecture-design%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx+tomcat session复制]]></title>
    <url>%2Fnginx%2Fnginx-tomcat-session-copy%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos 常用资料]]></title>
    <url>%2Flinux%2Fcentos-docs%2F</url>
    <content type="text"><![CDATA[中文官网：https://www.centoschina.cn/ centos 文档安装12yum -y install kernel-doccd /usr/share/doc/kernel-doc-3.10.0/]]></content>
      <categories>
        <category>linux</category>
        <category>centos</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux磁盘占用分析]]></title>
    <url>%2Flinux%2Flinux-disk-space-sniffer%2F</url>
    <content type="text"><![CDATA[一，查看磁盘空间大小的命令：dfdf命令用于查看磁盘分区上的磁盘空间，包括使用了多少，还剩多少，默认单位是KB； 比如以下命令： 1df -hl 执行结果如下： 执行的结果每列的含义： 第一列Filesystem，磁盘分区第二列Size，磁盘分区的大小第三列Used，已使用的空间第四列Avail，可用的空间第五列Use%，已使用的百分比第六列Mounted on，挂载点 解释一下后面的h和l参数， h是把显示的单位改成容易辨认的单位，不再是默认的KB了； l参数表示只显示本地磁盘分区，不包含的分区比如其他服务器共享的磁盘； 下面附上df命令的全部参数使用说明： 12345678910111213141516-a或--all：包含全部的文件系统；--block-size=&lt;区块大小&gt;：以指定的区块大小来显示区块数目；-h或--human-readable：以可读性较高的方式来显示信息；-H或--si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes；-i或--inodes：显示inode的信息；-k或--kilobytes：指定区块大小为1024字节；-l或--local：仅显示本地端的文件系统；-m或--megabytes：指定区块大小为1048576字节；--no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值；-P或--portability：使用POSIX的输出格式；--sync：在取得磁盘使用信息前，先执行sync指令；-t&lt;文件系统类型&gt;或--type=&lt;文件系统类型&gt;：仅显示指定文件系统类型的磁盘信息；-T或--print-type：显示文件系统的类型；-x&lt;文件系统类型&gt;或--exclude-type=&lt;文件系统类型&gt;：不要显示指定文件系统类型的磁盘信息；--help：显示帮助；--version：显示版本信息 二，查看文件和目录大小的命令：dudu是用来查看文件和目录大小用的，和df略有区别 1，比如要看/data目录的总大小，可以用以下命令：1du -sh /data 执行果如下： 或者进到/data目录后直接执行： 1du -sh 其中 -s 参数就是查看总大小（区别于查看其中每个目录的大小）; -h 参数是把默认的单位KB改为比较好辨认的单位； 2，如果要看/data目录下各个子目录的大小包括子目录的子目录，但不包含/data下文件，可以用以下命令： 1du -h 执行结果如下： 注：该命令不包含/data目录下的文件大小 3，如果要看/data目录下各个子目录的大小包括子目录的子目录，且包含/data下文件，可以用以下命令： 1du –h * 4，如果要看/data目录下各个子目录的大小，不包括子目录的子目录可以用以下命令： 1du -sh * 执行结果如下： 5，如果要看/data目录下各个子目录和文件的大小，需要使用-a参数：1du -ah 命令执行结果如下： 下面附上du命令的参数使用说明：123456789101112131415-a或-all 显示目录中个别文件的大小。-b或-bytes 显示目录或文件大小时，以byte为单位。-c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。-k或--kilobytes 以KB(1024bytes)为单位输出。-m或--megabytes 以MB为单位输出。-s或--summarize 仅显示总计，只列出最后加总的值。-h或--human-readable 以K，M，G为单位，提高信息的可读性。-x或--one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。-L&lt;符号链接&gt;或--dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小。-S或--separate-dirs 显示个别目录的大小时，并不含其子目录的大小。-X&lt;文件&gt;或--exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件。--exclude=&lt;目录或文件&gt; 略过指定的目录或文件。-D或--dereference-args 显示指定符号链接的源文件大小。-H或--si 与-h参数相同，但是K，M，G是以1000为换算单位。-l或--count-links 重复计算硬件链接的文件。 三，排序命令，sortsort命令可以用于将文件内容排序并输出，也可以用于将某些查询命令的执行结果排序后输出 比如要将文件夹中的文件按大小排序，可以用以下命令： 1du -a | sort -rn 执行结果如下： 管道前面的 du –a就是列出目录下所有的文件和目录的大小，后面的sort命令就是排序。 其中: -r参数代表反向排序，因为sort默认是从小到大排序的，加-r是从大到小排序; -n代表按照数字排序，只认数字不认单位，本例中的数字就是文件大小，单位是默认的KB，所以这个命令不能用du -ah，这会使排序结果出现2M小于100K的情况。 附上sort命令各参数的使用说明： 123456789101112-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序；-d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符；-f：排序时，将小写字母视为大写字母；-i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并；-M：将前面3个字母依照月份的缩写进行排序；-n：依照数值的大小排序；-o&lt;输出文件&gt;：将排序后的结果存入制定的文件；-r：以相反的顺序来排序；-t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符；+&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 由参数可见sort命令主要还是用于文件内容输出的 四，只显示前几行的命令，headhead命令可以用于显示文件的前几行，也可以用于显示某些查询命令结果的前几行; 比如要将文件夹中的文件按大小排序，而且只看最大的几个，可以用以下命令： 1du -a | sort -rn | head -5 执行结果： head后面的-5表示显示前5行，不加数字则默认显示前10行 附上head命令各参数的使用说明： 1234-n&lt;数字&gt;：指定显示头部内容的行数；-c&lt;字符数&gt;：指定显示头部内容的字符数；-v：总是显示文件名的头信息；-q：不显示文件名的头信息。 以上。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>磁盘占用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi与vim修改tab为4个空格]]></title>
    <url>%2Ftools%2Fvi-or-vim-tab-2-4blankspace%2F</url>
    <content type="text"><![CDATA[本文主要给大家介绍了关于vi与vim修改tab为4个空格的相关内容，分享出来供大家参考学习，下面话不多说了，来一起看看详细的介绍吧 方法如下： 配置文件位置：/etc/virc 和 /etc/vimrc 配置文件中如果要添加注释，不能用#，要使用” 12345set ts=4set softtabstop=4set shiftwidth=4set expandtabset autoindent ts是tabstop的缩写，设TAB宽度为4个空格。 softtabstop 表示在编辑模式的时候按退格键的时候退回缩进的长度，当使用 expandtab 时特别有用。 shiftwidth 表示每一级缩进的长度，一般设置成跟 softtabstop 一样。 expandtab表示缩进用空格来表示，noexpandtab 则是用制表符表示一个缩进。 autoindent自动缩进 对以前的文件可以用下面的命令进行空格和TAB互换 TAB替换为空格 123:set ts=4:set expandtab:%retab! 空格替换为TAB 123:set ts=4:set noexpandtab:%retab! 加!是用于处理非空白字符之后的TAB，即所有的TAB，若不加!，则只处理行首的TAB。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java异步回调demo]]></title>
    <url>%2Fjava%2Fconcurrent-program%2Fasync-callback-demo%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.*;interface ICallback &#123; void getResult(int s);&#125;class Calc &#123; public void calc(int i, ICallback callback) throws Exception &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int s = i * i; Thread.sleep(5000); System.out.println("do in future task. " + Thread.currentThread().getName()); callback.getResult(s); return s; &#125; &#125;); new Thread(futureTask).start(); &#125;&#125;public class App &#123; public static void main(String[] args) throws Exception &#123; ICallback callback = new ICallback() &#123; @Override public void getResult(int s) &#123; System.out.println(s); &#125; &#125;; new Calc().calc(1000, callback); System.out.println("do in main"); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>java-concurrent-program</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Callable</tag>
        <tag>Callback</tag>
        <tag>FutureTask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to 0ms AC]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 修改swap虚拟内存大小]]></title>
    <url>%2Flinux%2Flinux-mem-swap%2F</url>
    <content type="text"><![CDATA[​ swap是内存的交换区；换句话说，如果内存不够用了，那么系统会在硬盘上存储一些内存中不常用的数据，之后将这部分数据在存储中析构掉；这样内存就又有剩余空间可以运行东东啦，这个过程也就是所谓的交换，存储数据的硬盘部分就是swap分区。 装系统的时候可能会将swap大小设置的不合适 但是swap的大小不正确，很影响服务器的性能。下面说说怎么修改swap分区的大小。 1、查看系统Swap空间使用 12345[root@localhost mapper]# free -m total used free shared buffers cachedMem: 1006 753 252 3 32 526-/+ buffers/cache: 195 810Swap: 100 0 100 2、创建swap文件 123456789[root@localhost mapper]# cd /usr[root@localhost usr]# mkdir swap[root@localhost usr]# cd swap[root@localhost swap]# ll总用量 0[root@localhost swap]# dd if=/dev/zero of=/usr/swap/swapfile1 bs=1024 count=1000000记录了1000000+0 的读入记录了1000000+0 的写出1024000000字节(1.0 GB)已复制，5.40277 秒，190 MB/秒 （注意：if 表示 infile，of 表示outfile，bs=1024 表示写入的每个块的大小为1024B=1KB(1024B字节=1024*8bit位) 3、查看创建文件的大小 12[root@localhost swap]# du -sh /usr/swap/swapfile1977M /usr/swap/swapfile1 4、将目标文件设置为swap分区文件 123[root@localhost swap]# mkswap /usr/swap/swapfile1Setting up swapspace version 1, size = 999996 KiBno label, UUID=7eec8e34-e5d9-48f7-aa71-028268a48e46 5、激活swap，立即启用交换分区文件 1[root@localhost swap]# swapon /usr/swap/swapfile1 6、若要想使开机时自启用，则需修改文件/etc/fstab中的swap行 123[root@localhost swap]# vi /etc/fstab/usr/swap/swapfile1 swap swap defaults 0 0#加入此行，重启系统。 7、回收swap空间 如果不再使用，可以卸载该swap空间 1swapoff /var/swap 8、从文件系统中回收 1rm -rf /var/swap 9、查看某个进程使用的虚拟内存大小 12cat /proc/26080/stat | awk -F" " '&#123;print "virt:"$23&#125;'cat /proc/26080/stat | awk -F" " '&#123;print "rss:"$24&#125;' ps aux： 其中 VSZ(或VSS)列 表示，程序占用了多少虚拟内存。 ​ RSS列 表示， 程序占用了多少物理内存。 ​ 虚拟内存可以不用考虑，它并不占用实际物理内存。 (2). top 命令也可以 其中 VIRT(或VSS)列 表示，程序占用了多少虚拟内存。 同 ps aux 中的 VSZ列 ​ RES列 表示， 程序占用了多少物理内存。同 ps aux 中的RSS列]]></content>
      <tags>
        <tag>linux</tag>
        <tag>虚拟内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单常用算法]]></title>
    <url>%2Falgorithm%2Fsimple_algorighm%2F</url>
    <content type="text"><![CDATA[单链表反转1234567891011121314151617181920212223242526272829303132class Node &#123; Node(int v, Node n) &#123; this.val = v; this.next = n; &#125; int val; Node next;&#125;public class App &#123; public static Node reverse(Node node) &#123; Node pre = node; Node cur = node.next; Node tmp; while (cur != null) &#123; tmp = cur.next; cur.next = pre; pre = cur; cur = tmp; &#125; node.next = null; return pre; &#125; public static void main(String []args) &#123; Node node = new Node(1, new Node(2, new Node(3, new Node(4, null)))); Node res = reverse(node); while (res != null) &#123; System.out.print(res.val + "-&gt;"); res = res.next; &#125; &#125;&#125; 结果： D:\B\A&gt;javac App.java D:\B\A&gt;java App4-&gt;3-&gt;2-&gt;1-&gt;D:\B\A&gt; 快速排序]]></content>
  </entry>
</search>
