<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  
    
      
    

    
  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=consolas:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="keywords" content="Java Kafka Docker JVM NIO Netty">
<meta property="og:type" content="website">
<meta property="og:title" content="Focus-1">
<meta property="og:url" content="https://carlo-z.com/page/14/index.html">
<meta property="og:site_name" content="Focus-1">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Focus-1">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://carlo-z.com/page/14/">





  <title>Focus-1</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Focus-1</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/java/interview/java-tech-stack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/java/interview/java-tech-stack/" itemprop="url">【置顶】Java 成神之路</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-24T00:00:00+08:00">
                2018-10-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191024161802.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/devops/jenkins/jenkins-deploy-springboot-to-k8s/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/devops/jenkins/jenkins-deploy-springboot-to-k8s/" itemprop="url">jenkins pipeline自动构建springboot并部署至k8s</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-11T00:00:00+08:00">
                2018-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/devops/" itemprop="url" rel="index">
                    <span itemprop="name">devops</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/devops/jenkins/" itemprop="url" rel="index">
                    <span itemprop="name">jenkins</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<blockquote>
</blockquote>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200731133049536.png" alt="image-20200731133049536"></p>
<h1 id="1、准备"><a href="#1、准备" class="headerlink" title="1、准备"></a>1、准备</h1><h3 id="1-1、安装k8s集群"><a href="#1-1、安装k8s集群" class="headerlink" title="1.1、安装k8s集群"></a>1.1、安装k8s集群</h3><p>每个docker机器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  "registry-mirrors" : [</span><br><span class="line">    "https://k8spv7nq.mirror.aliyuncs.com"</span><br><span class="line">  ],</span><br><span class="line">  "insecure-registries": ["czharbor.com"]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-2、harbor-启动"><a href="#1-2、harbor-启动" class="headerlink" title="1.2、harbor 启动"></a>1.2、harbor 启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f /data/tools/harbor/docker-compose.yml stop</span><br><span class="line">docker-compose -f /data/tools/harbor/docker-compose.yml start</span><br></pre></td></tr></table></figure>

<h1 id="2-k8s中部署jenkins"><a href="#2-k8s中部署jenkins" class="headerlink" title="2. k8s中部署jenkins"></a>2. k8s中部署jenkins</h1><h2 id="2-1、制作jenkins镜像"><a href="#2-1、制作jenkins镜像" class="headerlink" title="2.1、制作jenkins镜像"></a>2.1、制作jenkins镜像</h2><p><a href="https://hub.docker.com/r/jenkins/jenkins/tags" target="_blank" rel="noopener">https://hub.docker.com/r/jenkins/jenkins/tags</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull jenkins/jenkins:2.251-alpine</span><br><span class="line">mkdir -p /data/jenkins &amp;&amp; cd /data/jenkins</span><br><span class="line">vi Dockerfile</span><br></pre></td></tr></table></figure>

<p>dockerfile</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> jenkins/jenkins:<span class="number">2.251</span>-alpine</span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> sed -i <span class="string">'s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g'</span> /etc/apk/repositories \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apk update \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apk add -U tzdata \</span></span><br><span class="line"><span class="bash">    &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span></span><br><span class="line"><span class="bash">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"Asia/Shanghai"</span> &gt; /etc/timezone \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apk add git \</span></span><br><span class="line"><span class="bash"><span class="comment">#    &amp;&amp; apk add maven=3.3.9-r1 \</span></span></span><br><span class="line"><span class="bash">    &amp;&amp; apk add docker</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"jenkins ALL=NOPASSWD: ALL"</span> &gt;&gt; /etc/sudoers</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir -p /opt/maven/repository</span></span><br></pre></td></tr></table></figure>

<p>编译镜像</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t czharbor.com/devops/cz-jenkins:lts-alpine .</span><br><span class="line">docker push czharbor.com/devops/cz-jenkins:lts-alpine</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200805234423183.png" alt="image-20200805234423183"></p>
<h2 id="2-2、共享存储NFS部署"><a href="#2-2、共享存储NFS部署" class="headerlink" title="2.2、共享存储NFS部署"></a>2.2、共享存储NFS部署</h2><p>NFS服务部署：<a href="https://www.jianshu.com/p/26003390626e" target="_blank" rel="noopener">https://www.jianshu.com/p/26003390626e</a><br>创建NFS 动态供给参考：<a href="https://www.jianshu.com/p/092eb3aacefc" target="_blank" rel="noopener">https://www.jianshu.com/p/092eb3aacefc</a></p>
<blockquote>
<ul>
<li><p>centos7-hub(192.168.145.130)：磁盘所在机器</p>
</li>
<li><p>k8s-dn1()：使用centos7-hub共享的磁盘</p>
</li>
<li><p>k8s-dn2()：使用centos7-hub共享的磁盘</p>
</li>
<li><p>k8s-dn1()：使用centos7-hub共享的磁盘</p>
</li>
</ul>
</blockquote>
<h4 id="2-2-1、在-centos7-hub-上设置"><a href="#2-2-1、在-centos7-hub-上设置" class="headerlink" title="2.2.1、在 centos7-hub 上设置"></a>2.2.1、在 centos7-hub 上设置</h4><p>1、关闭防火墙</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop firewalld.service</span><br><span class="line">$ systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>

<p>2、安装配置 nfs</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install nfs-utils rpcbind</span><br></pre></td></tr></table></figure>

<p>3、共享目录设置权限：</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /<span class="keyword">data</span>/nfs/jenkins/</span><br><span class="line">$ chmod -R <span class="number">755</span> /<span class="keyword">data</span>/nfs/jenkins/</span><br></pre></td></tr></table></figure>

<p>4、配置 nfs，nfs 的默认配置文件在 /etc/exports 文件下，在该文件中添加下面的配置信息：</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/exports</span><br><span class="line">/data/nfs/jenkins/  *(rw,<span class="keyword">sync</span>,no_root_squash)</span><br></pre></td></tr></table></figure>

<p>5、配置说明：<br> /data/nfs/jenkins/：是共享的数据目录<br> *：表示任何人都有权限连接，当然也可以是一个网段，一个 IP，也可以是域名<br> rw：读写的权限<br> sync：表示文件同时写入硬盘和内存<br> no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份</p>
<p>启动服务 nfs 需要向 rpc 注册，rpc 一旦重启了，注册的文件都会丢失，向他注册的服务都需要重启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/netconfig</span></span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200726220714201.png" alt="image-20200726220714201"></p>
<p> 注意启动顺序，先启动 rpcbind</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl start rpcbind.service</span><br><span class="line">$ systemctl <span class="built_in">enable</span> rpcbind</span><br><span class="line">$ systemctl status rpcbind</span><br><span class="line">● rpcbind.service - RPC <span class="built_in">bind</span> service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Tue 2018-07-10 20:57:29 CST; 1min 54s ago</span><br><span class="line">  Process: 17696 ExecStart=/sbin/rpcbind -w <span class="variable">$RPCBIND_ARGS</span> (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 17697 (rpcbind)</span><br><span class="line">    Tasks: 1</span><br><span class="line">   Memory: 1.1M</span><br><span class="line">   CGroup: /system.slice/rpcbind.service</span><br><span class="line">           └─17697 /sbin/rpcbind -w</span><br><span class="line"></span><br><span class="line">Jul 10 20:57:29 master systemd[1]: Starting RPC <span class="built_in">bind</span> service...</span><br><span class="line">Jul 10 20:57:29 master systemd[1]: Started RPC <span class="built_in">bind</span> service.</span><br></pre></td></tr></table></figure>

<p>看到上面的 Started 证明启动成功了。</p>
<p>然后启动 nfs 服务：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl start nfs.service</span><br><span class="line">$ systemctl enable nfs</span><br><span class="line">$ systemctl status nfs</span><br><span class="line">● nfs-server.service - NFS server and services</span><br><span class="line">   Loaded: loaded (<span class="regexp">/usr/</span>lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: <span class="regexp">/run/</span>systemd/generator/nfs-server.service.d</span><br><span class="line">           └─order-<span class="keyword">with</span>-mounts.conf</span><br><span class="line">   Active: active (exited) since Tue <span class="number">2018</span><span class="number">-07</span><span class="number">-10</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">37</span> CST; <span class="number">14</span>s ago</span><br><span class="line"> Main PID: <span class="number">32067</span> (code=exited, status=<span class="number">0</span>/SUCCESS)</span><br><span class="line">   CGroup: <span class="regexp">/system.slice/</span>nfs-server.service</span><br><span class="line"></span><br><span class="line">Jul <span class="number">10</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">37</span> master systemd[<span class="number">1</span>]: Starting NFS server and services...</span><br><span class="line">Jul <span class="number">10</span> <span class="number">21</span>:<span class="number">35</span>:<span class="number">37</span> master systemd[<span class="number">1</span>]: Started NFS server and services.</span><br></pre></td></tr></table></figure>

<p>同样看到 Started 则证明 NFS Server 启动成功了。</p>
<p>另外还可以通过下面的命令确认下：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ rpcinfo -p<span class="params">|grep nfs</span></span><br><span class="line"><span class="params">    100003    3   tcp   2049  nfs</span></span><br><span class="line"><span class="params">    100003    4   tcp   2049  nfs</span></span><br><span class="line"><span class="params">    100227    3   tcp   2049  nfs_acl</span></span><br><span class="line"><span class="params">    100003    3   udp   2049  nfs</span></span><br><span class="line"><span class="params">    100003    4   udp   2049  nfs</span></span><br><span class="line"><span class="params">    100227    3   udp   2049  nfs_acl</span></span><br></pre></td></tr></table></figure>

<p>查看具体目录挂载权限：</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /<span class="keyword">var</span>/lib/nfs/etab</span><br><span class="line">/data/nfs/jenkins	*(rw,<span class="keyword">sync</span>,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=<span class="number">65534</span>,anongid=<span class="number">65534</span>,sec=sys,rw,secure,no_root_squash,no_all_squash)</span><br></pre></td></tr></table></figure>

<p>到这里nfs server就安装成功了，接下来在节点 <code>k8s-n1</code> 上来安装 nfs 的客户端来验证下 nfs</p>
<h4 id="2-2-2、在-k8s-n1上设置"><a href="#2-2-2、在-k8s-n1上设置" class="headerlink" title="2.2.2、在 k8s-n1上设置"></a>2.2.2、在 k8s-n1上设置</h4><p>安装 nfs 当前也需要先关闭防火墙：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop firewalld.service</span><br><span class="line">$ systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>

<p>然后安装 nfs</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install nfs-utils rpcbind</span><br></pre></td></tr></table></figure>

<p>禁用ipv6</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi /etc/netconfig</span></span><br></pre></td></tr></table></figure>

<p>安装完成后，和上面的方法一样，先启动 rpc、然后启动 nfs：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl start rpcbind.service </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> rpcbind.service</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl status rpcbind.service</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl start nfs-server</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl <span class="built_in">enable</span> nfs-server</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl status nfs-server</span></span><br></pre></td></tr></table></figure>

<p>挂载数据目录 客户端启动完成后，在客户端来挂载下 nfs 测试下：<br> 首先检查下 nfs 是否有共享目录：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ showmount -e <span class="number">192.168</span>.<span class="number">145.130</span></span><br><span class="line">Export list <span class="keyword">for</span> <span class="number">192.168</span>.<span class="number">145.130</span><span class="symbol">:</span></span><br><span class="line">/data/nfs/jenkins *</span><br></pre></td></tr></table></figure>

<p>然后我们在客户端上新建目录：</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /<span class="keyword">data</span></span><br></pre></td></tr></table></figure>

<p>将 nfs 共享目录挂载到上面的目录：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mount -t nfs <span class="number">192.168</span>.<span class="number">145.130</span><span class="symbol">:/data/nfs/jenkins</span> /data</span><br></pre></td></tr></table></figure>

<p>挂载成功后，在客户端上面的目录中新建一个文件，然后观察下 nfs 服务端的共享目录下面是否也会出现该文件：</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ touch /<span class="keyword">data</span>/test.txt</span><br></pre></td></tr></table></figure>

<p>然后在 nfs 服务端查看：</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls -ls /<span class="keyword">data</span>/nfs/jenkins</span><br><span class="line">total <span class="number">4</span></span><br><span class="line"><span class="number">4</span> -rw-r--r--. <span class="number">1</span> root root <span class="number">4</span> Jul <span class="number">10</span> <span class="number">21</span>:<span class="number">50</span> test.txt</span><br></pre></td></tr></table></figure>

<p>如果上面出现了 test.txt 的文件，那么证明 nfs 挂载成功了。</p>
<h2 id="2-3、部署jenkins到k8s"><a href="#2-3、部署jenkins到k8s" class="headerlink" title="2.3、部署jenkins到k8s"></a>2.3、部署jenkins到k8s</h2><h4 id="jenkins-pv-pvc-yaml"><a href="#jenkins-pv-pvc-yaml" class="headerlink" title="jenkins-pv-pvc.yaml"></a>jenkins-pv-pvc.yaml</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins-home-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">5</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteMany</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    server:</span> <span class="number">192.168</span><span class="number">.145</span><span class="number">.130</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">"/data/nfs/jenkins"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins-home-pvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">devops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteMany"]</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">5</span><span class="string">Gi</span></span><br></pre></td></tr></table></figure>

<h4 id="jenkins-service-account-yaml"><a href="#jenkins-service-account-yaml" class="headerlink" title="jenkins-service-account.yaml"></a>jenkins-service-account.yaml</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">devops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  resources:</span> <span class="string">["pods"]</span></span><br><span class="line"><span class="attr">  verbs:</span> <span class="string">["create","delete","get","list","patch","update","watch"]</span></span><br><span class="line"><span class="attr">- apiGroups:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  resources:</span> <span class="string">["pods/exec"]</span></span><br><span class="line"><span class="attr">  verbs:</span> <span class="string">["create","delete","get","list","patch","update","watch"]</span></span><br><span class="line"><span class="attr">- apiGroups:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  resources:</span> <span class="string">["pods/log"]</span></span><br><span class="line"><span class="attr">  verbs:</span> <span class="string">["get","list","watch"]</span></span><br><span class="line"><span class="attr">- apiGroups:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  resources:</span> <span class="string">["secrets"]</span></span><br><span class="line"><span class="attr">  verbs:</span> <span class="string">["get"]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">devops</span></span><br></pre></td></tr></table></figure>

<h4 id="jenkins-statefulset-yaml"><a href="#jenkins-statefulset-yaml" class="headerlink" title="jenkins-statefulset.yaml"></a>jenkins-statefulset.yaml</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">devops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">RollingUpdate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">czharbor.com/devops/cz-jenkins:lts-alpine</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">50000</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            limits:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">0.5</span></span><br><span class="line"><span class="attr">              memory:</span> <span class="number">500</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">LIMITS_MEMORY</span></span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                resourceFieldRef:</span></span><br><span class="line"><span class="attr">                  resource:</span> <span class="string">limits.memory</span></span><br><span class="line"><span class="attr">                  divisor:</span> <span class="number">1</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">JAVA_OPTS</span></span><br><span class="line"><span class="attr">              value:</span> <span class="bullet">-Xmx$(LIMITS_MEMORY)m</span> <span class="attr">-XshowSettings:vm</span> <span class="bullet">-Dhudson.slaves.NodeProvisioner.initialDelay=0</span> <span class="bullet">-Dhudson.slaves.NodeProvisioner.MARGIN=50</span> <span class="bullet">-Dhudson.slaves.NodeProvisioner.MARGIN0=0.85</span> <span class="bullet">-Duser.timezone=GMT+08</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">jenkins-home</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/jenkins_home</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">docker-sock</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> <span class="string">/login</span></span><br><span class="line"><span class="attr">              port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">12</span></span><br><span class="line"><span class="attr">          readinessProbe:</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> <span class="string">/login</span></span><br><span class="line"><span class="attr">              port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">12</span></span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        fsGroup:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jenkins-home</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">jenkins-home-pvc</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">docker-sock</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/var/run/docker.sock</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong><u>问题1</u></strong>：jenkins镜像里已经安装了docker，千万不能把 主机的 /usr/bin/docker 挂载到 docker 里去，否则会出错：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> bash-4.4<span class="comment"># docker ps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> bash: /usr/bin/docker: No such file or directory</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200805233316341.png" alt="image-20200805233316341"></p>
<p><u><strong>问题2</strong></u>：Jenkinsfile中运行docker 挂载 父容器的数据卷的时候，会继承父容器的网络，将 jenkins的网络模式指定为 <code>hostNetwork: true</code> 表示使用主机的网络和iptables，这样kubectl发布应用才能正常运行</p>
</blockquote>
<h4 id="jenkins-service-yaml"><a href="#jenkins-service-yaml" class="headerlink" title="jenkins-service.yaml"></a>jenkins-service.yaml</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">devops</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">prometheus.io/scrape:</span> <span class="string">'true'</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">jenkins-web</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">31442</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">jenkins-agent</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">50000</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">50000</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30005</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br></pre></td></tr></table></figure>

<h4 id="jenkins-ingress-yaml"><a href="#jenkins-ingress-yaml" class="headerlink" title="jenkins-ingress.yaml"></a>jenkins-ingress.yaml</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">devops</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">kubernetes.io/tls-acme:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-body-size:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-request-buffering:</span> <span class="string">"off"</span></span><br><span class="line">    <span class="string">ingress.kubernetes.io/ssl-redirect:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">ingress.kubernetes.io/proxy-body-size:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line">    <span class="string">ingress.kubernetes.io/proxy-request-buffering:</span> <span class="string">"off"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">cz-jenkins.dev</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure>

<h4 id="执行部署动作"><a href="#执行部署动作" class="headerlink" title="执行部署动作"></a>执行部署动作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-dn1 ~]# kubectl create ns devops</span><br><span class="line"></span><br><span class="line">kubectl apply -f /data/jenkins/jenkins-pv-pvc.yaml</span><br><span class="line">kubectl apply -f /data/jenkins/jenkins-service-account.yaml</span><br><span class="line">kubectl apply -f /data/jenkins/jenkins-statefulset.yaml</span><br><span class="line">kubectl apply -f /data/jenkins/jenkins-service.yaml</span><br><span class="line">kubectl apply -f /data/jenkins/jenkins-ingress.yaml</span><br><span class="line"></span><br><span class="line">kubectl delete -f /data/jenkins/jenkins-ingress.yaml</span><br><span class="line">kubectl delete -f /data/jenkins/jenkins-service.yaml</span><br><span class="line">kubectl delete -f /data/jenkins/jenkins-statefulset.yaml</span><br><span class="line">kubectl delete -f /data/jenkins/jenkins-service-account.yaml</span><br><span class="line">kubectl delete -f /data/jenkins/jenkins-pv-pvc.yaml</span><br><span class="line"></span><br><span class="line">kubectl get pv -n devops</span><br><span class="line">kubectl get sa -n devops</span><br><span class="line">kubectl get StatefulSet -n devops</span><br><span class="line">kubectl describe StatefulSet jenkins -n devops</span><br><span class="line">kubectl get Service -n devops</span><br><span class="line">kubectl describe Service jenkins -n devops</span><br><span class="line">kubectl get Ingress -n devops</span><br><span class="line">kubectl describe Ingress jenkins -n devops</span><br></pre></td></tr></table></figure>

<h4 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h4><p>在hosts文件中添加</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.145.151     cz-jenkins.dev</span><br></pre></td></tr></table></figure>

<p>访问jenkins</p>
<blockquote>
<p><a href="http://cz-jenkins.dev/" target="_blank" rel="noopener">http://cz-jenkins.dev/</a></p>
<p>admin/123456</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /data/nfs/jenkins/secrets/initialAdminPassword</span></span><br><span class="line">e8422ab2e6104faa8cf3a5033f1b0dd2</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200726222903171.png" alt="image-20200726222903171"></p>
<h2 id="2-4、Jenkins初始化"><a href="#2-4、Jenkins初始化" class="headerlink" title="2.4、Jenkins初始化"></a>2.4、Jenkins初始化</h2><blockquote>
<p><a href="http://focus-1.wiki/devops/jenkins/jenkins-centos7-setup/" target="_blank" rel="noopener">http://focus-1.wiki/devops/jenkins/jenkins-centos7-setup/</a></p>
</blockquote>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200726225212624.png" alt="image-20200726225212624"></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200726225409947.png" alt="image-20200726225409947"></p>
<h2 id="2-5、Jenkins配置"><a href="#2-5、Jenkins配置" class="headerlink" title="2.5、Jenkins配置"></a>2.5、Jenkins配置</h2><p>系统管理 –&gt; 插件管理 –&gt; available，安装需要的插件，有的插件下载不下来可以去官网下载之后上传安装。</p>
<p>系统管理——》插件管理——》可选插件——》安装</p>
<ul>
<li><p>Git</p>
</li>
<li><p>Git Parameter</p>
</li>
<li><p>Pipeline</p>
</li>
<li><p>Kubernetes</p>
</li>
<li><p>Kubernetes Continuous Deploy</p>
</li>
<li><p>Gitee</p>
</li>
</ul>
<h1 id="3、构建Jenkins-Slave镜像"><a href="#3、构建Jenkins-Slave镜像" class="headerlink" title="3、构建Jenkins Slave镜像"></a>3、构建Jenkins Slave镜像</h1><p>参考:<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fjenkinsci%2Fdocker-jnlp-slave" target="_blank" rel="noopener">https://github.com/jenkinsci/docker-jnlp-slave</a></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/7464841-42ee8f4af9caf7f7.png" alt="img"></p>
<h2 id="1-构建Jenkins-Slave镜像环境准备"><a href="#1-构建Jenkins-Slave镜像环境准备" class="headerlink" title="1. 构建Jenkins Slave镜像环境准备"></a>1. 构建Jenkins Slave镜像环境准备</h2><p><strong>构建Jenkins Slave镜像环境准备：</strong><br> 代码拉取：git，安装git命令<br> 单元测试：忽略，这不是我们擅长的,如果公司有可以写进来<br> 代码编译：maven，安装maven包<br> 构建镜像：Dockerfile文件、docker命令(通过挂载宿主机docker)<br> 推送镜像：docker命令(通过挂载宿主机docker)<br> 镜像启动后支持slave: 下载官方slave.jar包（获取：<a href="https://links.jianshu.com/go?to=http%3A%2F%2F10.40.6.213%3A30006%2FjnlpJars%2Fslave.jar" target="_blank" rel="noopener">http://10.40.6.213:30006/jnlpJars/slave.jar</a>）<br> 启动 slave.ja包：jenkins-slave启动脚步(通过参考文档URL)<br> maven配置文件：settings.xml （这里配置阿里云的仓库源）</p>
<p><strong>获取相关文件：</strong><br> Dockerfile<br> jenkins-slave 启动脚步<br> settings.xml<br> slave.jar</p>
<p>创建目录并进入：<br> <code>mkdir jenkins-slave &amp;&amp; cd jenkins-slave</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7cz jenkin]# pwd</span><br><span class="line">/data/jenkins/</span><br><span class="line">[root@centos7cz jenkins]# mkdir jenkins-slave &amp;&amp; cd jenkins-slave</span><br><span class="line">[root@centos7cz jenkins-slave]# wget http://192.168.145.151:31442/jnlpJars/slave.jar</span><br></pre></td></tr></table></figure>

<h2 id="2-jenkins-slave启动脚本"><a href="#2-jenkins-slave启动脚本" class="headerlink" title="2. jenkins-slave启动脚本"></a>2. jenkins-slave启动脚本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7cz jenkins-slave]# vi jenkins-slave</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat jenkins-slave</span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env sh</span></span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ]; then</span><br><span class="line"></span><br><span class="line">    # if `docker run` only has one arguments, we assume user is running alternate command like `bash` to inspect the image</span><br><span class="line">    exec "$@"</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">    # if -tunnel is not provided try env vars</span><br><span class="line">    case "$@" in</span><br><span class="line">        *"-tunnel "*) ;;</span><br><span class="line">        *)</span><br><span class="line">        if [ ! -z "$JENKINS_TUNNEL" ]; then</span><br><span class="line">            TUNNEL="-tunnel $JENKINS_TUNNEL"</span><br><span class="line">        fi ;;</span><br><span class="line">    esac</span><br><span class="line"></span><br><span class="line">    # if -workDir is not provided try env vars</span><br><span class="line">    if [ ! -z "$JENKINS_AGENT_WORKDIR" ]; then</span><br><span class="line">        case "$@" in</span><br><span class="line">            *"-workDir"*) echo "Warning: Work directory is defined twice in command-line arguments and the environment variable" ;;</span><br><span class="line">            *)</span><br><span class="line">            WORKDIR="-workDir $JENKINS_AGENT_WORKDIR" ;;</span><br><span class="line">        esac</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    if [ -n "$JENKINS_URL" ]; then</span><br><span class="line">        URL="-url $JENKINS_URL"</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    if [ -n "$JENKINS_NAME" ]; then</span><br><span class="line">        JENKINS_AGENT_NAME="$JENKINS_NAME"</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    if [ -z "$JNLP_PROTOCOL_OPTS" ]; then</span><br><span class="line">        echo "Warning: JnlpProtocol3 is disabled by default, use JNLP_PROTOCOL_OPTS to alter the behavior"</span><br><span class="line">        JNLP_PROTOCOL_OPTS="-Dorg.jenkinsci.remoting.engine.JnlpProtocol3.disabled=true"</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    # If both required options are defined, do not pass the parameters</span><br><span class="line">    OPT_JENKINS_SECRET=""</span><br><span class="line">    if [ -n "$JENKINS_SECRET" ]; then</span><br><span class="line">        case "$@" in</span><br><span class="line">            *"$&#123;JENKINS_SECRET&#125;"*) echo "Warning: SECRET is defined twice in command-line arguments and the environment variable" ;;</span><br><span class="line">            *)</span><br><span class="line">            OPT_JENKINS_SECRET="$&#123;JENKINS_SECRET&#125;" ;;</span><br><span class="line">        esac</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    OPT_JENKINS_AGENT_NAME=""</span><br><span class="line">    if [ -n "$JENKINS_AGENT_NAME" ]; then</span><br><span class="line">        case "$@" in</span><br><span class="line">            *"$&#123;JENKINS_AGENT_NAME&#125;"*) echo "Warning: AGENT_NAME is defined twice in command-line arguments and the environment variable" ;;</span><br><span class="line">            *)</span><br><span class="line">            OPT_JENKINS_AGENT_NAME="$&#123;JENKINS_AGENT_NAME&#125;" ;;</span><br><span class="line">        esac</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    #TODO: Handle the case when the command-line and Environment variable contain different values.</span><br><span class="line">    #It is fine it blows up for now since it should lead to an error anyway.</span><br><span class="line"></span><br><span class="line">    exec java $JAVA_OPTS $JNLP_PROTOCOL_OPTS -cp /usr/share/jenkins/slave.jar hudson.remoting.jnlp.Main -headless $TUNNEL $URL $WORKDIR $OPT_JENKINS_SECRET $OPT_JENKINS_AGENT_NAME "$@"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h2 id="3、maven源配置文件settings-xml"><a href="#3、maven源配置文件settings-xml" class="headerlink" title="3、maven源配置文件settings.xml"></a>3、maven源配置文件settings.xml</h2><p>maven源配置文件settings.xml，这里配置阿里云的源。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7cz jenkins-slave]# vi settings.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat settings.xml</span></span><br><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"></span><br><span class="line">&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"</span><br><span class="line">          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"</span><br><span class="line">          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt;</span><br><span class="line">  &lt;pluginGroups&gt;</span><br><span class="line">  &lt;/pluginGroups&gt;</span><br><span class="line"></span><br><span class="line">  &lt;proxies&gt;</span><br><span class="line">  &lt;/proxies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;servers&gt;</span><br><span class="line">  &lt;/servers&gt;</span><br><span class="line"></span><br><span class="line">  &lt;mirrors&gt;</span><br><span class="line">    &lt;mirror&gt;</span><br><span class="line">      &lt;id&gt;central&lt;/id&gt;</span><br><span class="line">      &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">      &lt;name&gt;aliyun maven&lt;/name&gt;</span><br><span class="line">      &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;</span><br><span class="line">    &lt;/mirror&gt;</span><br><span class="line">  &lt;/mirrors&gt;</span><br><span class="line"></span><br><span class="line">  &lt;profiles&gt;</span><br><span class="line">  &lt;/profiles&gt;</span><br><span class="line"></span><br><span class="line">&lt;/settings&gt;</span><br></pre></td></tr></table></figure>

<h2 id="4-Dockerfile配置文件"><a href="#4-Dockerfile配置文件" class="headerlink" title="4. Dockerfile配置文件"></a>4. Dockerfile配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7cz jenkins-slave]# vi Dockerfile</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest</span><br><span class="line">USER root</span><br><span class="line"></span><br><span class="line">RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \</span><br><span class="line">    &amp;&amp; apk update</span><br><span class="line">RUN apk add -U tzdata \</span><br><span class="line">    &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span><br><span class="line">    &amp;&amp; echo "Asia/Shanghai" &gt; /etc/timezone</span><br><span class="line">RUN wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \</span><br><span class="line">    &amp;&amp; wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-2.31-r0.apk \</span><br><span class="line">    &amp;&amp; apk add glibc-2.31-r0.apk</span><br><span class="line">RUN apk add openjdk8 \</span><br><span class="line">    &amp;&amp; apk add maven \</span><br><span class="line">    &amp;&amp; apk add protoc \</span><br><span class="line">    &amp;&amp; apk add grpc \</span><br><span class="line">    &amp;&amp; apk add git \</span><br><span class="line">    &amp;&amp; apk add docker\</span><br><span class="line">    &amp;&amp; apk add sshpass</span><br><span class="line"></span><br><span class="line">RUN echo "jenkins ALL=NOPASSWD: ALL" &gt;&gt; /etc/sudoers</span><br><span class="line"></span><br><span class="line">COPY slave.jar /usr/share/jenkins/slave.jar</span><br><span class="line">COPY jenkins-slave /usr/bin/jenkins-slave</span><br><span class="line">COPY settings.xml /etc/maven/settings.xml</span><br><span class="line">RUN chmod +x /usr/bin/jenkins-slave</span><br><span class="line"></span><br><span class="line">ENTRYPOINT ["jenkins-slave"]</span><br></pre></td></tr></table></figure>

<h2 id="5-构建镜像-并推送至私有镜像仓库"><a href="#5-构建镜像-并推送至私有镜像仓库" class="headerlink" title="5. 构建镜像, 并推送至私有镜像仓库"></a>5. 构建镜像, 并推送至私有镜像仓库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7cz jenkins-slave]# docker build -t czharbor.com/devops/jenkins-slave:2.249 .</span><br><span class="line"></span><br><span class="line">[root@centos7cz jenkins-slave]# docker push czharbor.com/devops/jenkins-slave:2.249</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200804211825096.png" alt="image-20200804211825096"></p>
<h1 id="4、构建maven镜像"><a href="#4、构建maven镜像" class="headerlink" title="4、构建maven镜像"></a>4、构建maven镜像</h1><h3 id="4-1、Dockerfile"><a href="#4-1、Dockerfile" class="headerlink" title="4.1、Dockerfile"></a>4.1、Dockerfile</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest</span><br><span class="line">USER root</span><br><span class="line"></span><br><span class="line">ENV LANG C.UTF-8</span><br><span class="line">ENV TZ Asia/Shanghai</span><br><span class="line"></span><br><span class="line">RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \</span><br><span class="line">    &amp;&amp; apk update</span><br><span class="line"></span><br><span class="line">RUN apk add -U tzdata \</span><br><span class="line">    &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span><br><span class="line">    &amp;&amp; echo "Asia/Shanghai" &gt; /etc/timezone</span><br><span class="line"></span><br><span class="line">RUN wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \</span><br><span class="line">    &amp;&amp; wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-2.31-r0.apk \</span><br><span class="line">    &amp;&amp; apk add glibc-2.31-r0.apk</span><br><span class="line"></span><br><span class="line">RUN apk add openjdk8 \</span><br><span class="line">    &amp;&amp; apk add maven \</span><br><span class="line">    &amp;&amp; apk add protoc \</span><br><span class="line">    &amp;&amp; apk add grpc \</span><br><span class="line">    &amp;&amp; apk add docker</span><br></pre></td></tr></table></figure>

<blockquote>
<p>为了调用protoc，一定要安装glibc</p>
<p><a href="https://github.com/sgerrand/alpine-pkg-glibc" target="_blank" rel="noopener">https://github.com/sgerrand/alpine-pkg-glibc</a></p>
<p>否则会碰到如下问题：</p>
<blockquote>
<p><a href="https://github.com/xolstice/protobuf-maven-plugin/issues/23" target="_blank" rel="noopener">https://github.com/xolstice/protobuf-maven-plugin/issues/23</a></p>
</blockquote>
<blockquote>
<p>用Alpine跑了JDK8的镜像结果发现,JDK还是无法执行.后来翻阅文档才发现<br>Java是基于GUN Standard C library(glibc)<br>Alpine是基于MUSL libc(mini libc)</p>
<p>所以Alpine需要安装glibc的库,以下是官方给出wiki<br><a href="https://wiki.alpinelinux.org/wiki/Running_glibc_programs" target="_blank" rel="noopener">https://wiki.alpinelinux.org/wiki/Running_glibc_programs</a></p>
</blockquote>
<blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.6.1:compile (default) on project nacos-grpc-iface: An error occurred <span class="keyword">while</span> invoking protoc: Error <span class="keyword">while</span> executing process.: Cannot run program <span class="string">"/var/jenkins_home/workspace/nacos-grpc-k8s@2/nacos-grpc-iface/target/protoc-plugins/protoc-3.12.2-linux-x86_64.exe"</span>: error=2, No such file or directory -&gt; [Help 1]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt;</span></span><br></pre></td></tr></table></figure>
</blockquote>
</blockquote>
<h3 id="4-2、构建运行"><a href="#4-2、构建运行" class="headerlink" title="4.2、构建运行"></a>4.2、构建运行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker build -t czharbor.com/devops/cz-maven:3.6.3-alpine .</span><br><span class="line"><span class="meta">#</span><span class="bash"> --net=host 表示使用主机iptables等网络环境，对docker <span class="keyword">in</span> docker 很重要</span></span><br><span class="line">docker --net=host run -it czharbor.com/devops/cz-maven:3.6.3-alpine</span><br><span class="line">java -version</span><br><span class="line">mvn -v</span><br><span class="line"></span><br><span class="line">docker push czharbor.com/devops/cz-maven:3.6.3-alpine</span><br></pre></td></tr></table></figure>

<h1 id="5、构建kubectl镜像"><a href="#5、构建kubectl镜像" class="headerlink" title="5、构建kubectl镜像"></a>5、构建kubectl镜像</h1><p>dockerfile</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> alpine:latest</span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> LANG C.UTF-<span class="number">8</span></span><br><span class="line"><span class="keyword">ENV</span> TZ Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> sed -i <span class="string">'s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g'</span> /etc/apk/repositories \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apk update</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk add -U tzdata \</span></span><br><span class="line"><span class="bash">    &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span></span><br><span class="line"><span class="bash">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"Asia/Shanghai"</span> &gt; /etc/timezone</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \</span></span><br><span class="line"><span class="bash">    &amp;&amp; wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.31-r0/glibc-2.31-r0.apk \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apk add glibc-2.31-r0.apk</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD kubectl /usr/local/bin/</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker build -t czharbor.com/devops/kubectl:1.18.6-alpine .</span><br><span class="line">docker push czharbor.com/devops/kubectl:1.18.6-alpine</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> --net=host 表示使用主机iptables等网络环境，对docker <span class="keyword">in</span> docker 很重要</span></span><br><span class="line">docker run -it --net=host \</span><br><span class="line">-v /root/.kube:/root/.kube \</span><br><span class="line">-v /usr/local/bin/kubectl:/usr/local/bin/kubectl \</span><br><span class="line">czharbor.com/devops/kubectl:1.18.6-alpine</span><br><span class="line"></span><br><span class="line">docker ps -a | grep kubectl</span><br></pre></td></tr></table></figure>

<h1 id="6、SpringBoot项目准备"><a href="#6、SpringBoot项目准备" class="headerlink" title="6、SpringBoot项目准备"></a>6、SpringBoot项目准备</h1><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200810233931007.png" alt="image-20200810233931007"></p>
<p> 关键文件： </p>
<ul>
<li><p>nacos-grpc-srv 本身的 Dockerfile</p>
</li>
<li><p>deployment.yaml的模板文件 k8s-deployment.tpl</p>
</li>
<li><p>jenkins Pipeline 文件 Jenkinsfile</p>
</li>
</ul>
<h3 id="deployment-yaml"><a href="#deployment-yaml" class="headerlink" title="deployment.yaml"></a>deployment.yaml</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">&#123;APP_NAME&#125;-deployment</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">&#123;APP_NAME&#125;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">&#123;APP_NAME&#125;</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">&#123;APP_NAME&#125;</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">&#123;APP_NAME&#125;</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">&#123;IMAGE_URL&#125;:&#123;IMAGE_TAG&#125;</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8010</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">SPRING_PROFILES_ACTIVE</span></span><br><span class="line"><span class="attr">            value:</span> <span class="string">&#123;SPRING_PROFILE&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="Jenkinsfile"><a href="#Jenkinsfile" class="headerlink" title="Jenkinsfile"></a>Jenkinsfile</h3><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要在jenkins的Credentials设置中配置jenkins-harbor-creds、jenkins-k8s-config参数</span></span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line">    parameters &#123;</span><br><span class="line">        string(<span class="string">name:</span> <span class="string">'K8S_NAMESPACE'</span>, <span class="string">defaultValue:</span> <span class="string">'default'</span>, <span class="string">description:</span> <span class="string">'k8s的namespace名称'</span>)</span><br><span class="line">        string(<span class="string">name:</span> <span class="string">'HARBOR_HOST'</span>, <span class="string">defaultValue:</span> <span class="string">'czharbor.com/devops'</span>, <span class="string">description:</span> <span class="string">'harbor仓库地址'</span>)</span><br><span class="line">        <span class="comment">// string(name: 'DOCKER_IMAGE', defaultValue: 'czharbor.com/nacos-grpc-srv', description: 'docker镜像名')</span></span><br><span class="line">        <span class="comment">// string(name: 'APP_NAME', defaultValue: 'nacos-grpc-srv', description: 'k8s中标签名')</span></span><br><span class="line">        <span class="comment">// choice(name: 'APP_NAME_LIST', choices: ['nacos-grpc-srv', 'nacos-grpc-cli'], description: 'app_name')</span></span><br><span class="line">    &#125;</span><br><span class="line">    environment &#123;</span><br><span class="line">        HARBOR_CREDS = credentials(<span class="string">'jenkins-harbor-creds'</span>)</span><br><span class="line">        K8S_CONFIG = credentials(<span class="string">'jenkins-kubeconfig'</span>)</span><br><span class="line">        GIT_TAG = sh(<span class="string">returnStdout:</span> <span class="literal">true</span>,<span class="string">script:</span> <span class="string">'git describe --tags'</span>).trim()</span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(<span class="string">'Maven Build'</span>) &#123;</span><br><span class="line">            agent &#123;</span><br><span class="line">                docker &#123;</span><br><span class="line">                    image <span class="string">'czharbor.com/devops/cz-maven:3.6.3-alpine'</span></span><br><span class="line">                    args <span class="string">'-v $HOME/.m2:/root/.m2 -v /var/run/docker.sock:/var/run/docker.sock'</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh <span class="string">'mvn clean package -Dfile.encoding=UTF-8 -DskipTests=true'</span></span><br><span class="line">                sh <span class="string">"cd nacos-grpc-srv &amp;&amp; mvn docker:build &amp;&amp; cd .."</span></span><br><span class="line">                sh <span class="string">"cd nacos-grpc-srv/target/docker &amp;&amp; docker build --build-arg JAR_FILE='nacos-grpc-srv-0.0.1-SNAPSHOT.jar' -t $&#123;params.HARBOR_HOST&#125;/nacos-grpc-srv:1.0 ."</span></span><br><span class="line">                sh <span class="string">"docker login -u $&#123;HARBOR_CREDS_USR&#125; -p $&#123;HARBOR_CREDS_PSW&#125; $&#123;params.HARBOR_HOST&#125;"</span></span><br><span class="line">                sh <span class="string">"docker push $&#123;params.HARBOR_HOST&#125;/nacos-grpc-srv:1.0"</span></span><br><span class="line">                sh <span class="string">"docker rmi -f $&#123;params.HARBOR_HOST&#125;/nacos-grpc-srv:1.0"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// stage('Docker Build') &#123;</span></span><br><span class="line">        <span class="comment">//     when &#123;</span></span><br><span class="line">        <span class="comment">//         allOf &#123;</span></span><br><span class="line">        <span class="comment">//             expression &#123; env.GIT_TAG != null &#125;</span></span><br><span class="line">        <span class="comment">//         &#125;</span></span><br><span class="line">        <span class="comment">//     &#125;</span></span><br><span class="line">        <span class="comment">//     agent any</span></span><br><span class="line">        <span class="comment">//     steps &#123;</span></span><br><span class="line">        <span class="comment">//         script &#123;</span></span><br><span class="line">        <span class="comment">//             def APP_NAMES = ['nacos-grpc-srv']</span></span><br><span class="line">        <span class="comment">//             for (int i = 0;i &lt; APP_NAMES.size(); ++i) &#123;</span></span><br><span class="line">        <span class="comment">//                 sh "cd $&#123;APP_NAMES[i]&#125;/target/docker"</span></span><br><span class="line">        <span class="comment">//                 sh "docker login -u $&#123;HARBOR_CREDS_USR&#125; -p $&#123;HARBOR_CREDS_PSW&#125; $&#123;params.HARBOR_HOST&#125;"</span></span><br><span class="line">        <span class="comment">//                 sh "docker build --build-arg JAR_FILE=`ls *.jar |cut -d '/' -f1` -t $&#123;params.HARBOR_HOST&#125;/$&#123;APP_NAMES[i]&#125;:1.0 ."</span></span><br><span class="line">        <span class="comment">//                 sh "docker push $&#123;params.HARBOR_HOST&#125;/$&#123;APP_NAMES[i]&#125;:1.0"</span></span><br><span class="line">        <span class="comment">//                 sh "docker rmi -f $&#123;params.HARBOR_HOST&#125;/$&#123;APP_NAMES[i]&#125;:1.0"</span></span><br><span class="line">        <span class="comment">//                 sh "cd .."</span></span><br><span class="line">        <span class="comment">//                 sh "cd .."</span></span><br><span class="line">        <span class="comment">//                 sh "cd .."</span></span><br><span class="line">        <span class="comment">//             &#125;</span></span><br><span class="line">        <span class="comment">//         &#125;</span></span><br><span class="line">        <span class="comment">//     &#125;</span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line">        stage(<span class="string">'Deploy'</span>) &#123;</span><br><span class="line">            when &#123;</span><br><span class="line">                allOf &#123;</span><br><span class="line">                    expression &#123; env.GIT_TAG != <span class="literal">null</span> &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            agent &#123;</span><br><span class="line">                docker &#123;</span><br><span class="line">                    image <span class="string">'czharbor.com/devops/kubectl:1.18.6-alpine'</span></span><br><span class="line">                    args <span class="string">'--net=host -v /root/.kube:/root/.kube -v /usr/local/bin/kubectl:/usr/local/bin/kubectl'</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            steps &#123;</span><br><span class="line">                script &#123;</span><br><span class="line">                    <span class="keyword">def</span> APP_NAMES = [<span class="string">'nacos-grpc-srv'</span>]</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; APP_NAMES.size(); ++i) &#123;</span><br><span class="line">                        <span class="comment">// sh "mkdir -p ~/.kube"</span></span><br><span class="line">                        <span class="comment">// sh "echo $&#123;K8S_CONFIG&#125; | base64 -d &gt; ~/.kube/config"</span></span><br><span class="line">                        sh <span class="string">"sed -e 's#&#123;IMAGE_URL&#125;#$&#123;params.HARBOR_HOST&#125;/$&#123;APP_NAMES[i]&#125;#g;s#&#123;IMAGE_TAG&#125;#1.0#g;s#&#123;APP_NAME&#125;#$&#123;APP_NAMES[i]&#125;#g;s#&#123;SPRING_PROFILE&#125;#k8s#g' k8s-deployment.tpl &gt; k8s-deployment.yml"</span></span><br><span class="line">                        sh <span class="string">"kubectl apply -f k8s-deployment.yml --namespace=$&#123;params.K8S_NAMESPACE&#125;"</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="7、使用jenkins部署项目到k8s"><a href="#7、使用jenkins部署项目到k8s" class="headerlink" title="7、使用jenkins部署项目到k8s"></a>7、使用jenkins部署项目到k8s</h1><h3 id="gitee秘钥"><a href="#gitee秘钥" class="headerlink" title="gitee秘钥"></a>gitee秘钥</h3><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200802204243247.png" alt="image-20200802204243247"></p>
<h3 id="kubeconfig配置"><a href="#kubeconfig配置" class="headerlink" title="kubeconfig配置"></a>kubeconfig配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-dn1 ~]# base64 ~/.kube/config &gt; kube-config.txt</span><br></pre></td></tr></table></figure>

<p>然后类似上一步，在jenkins凭据中增加配置文件内容。在凭据设置界面，类型选择为“Secret text”，ID设置为“jenkins-kubeconfig”（此处的ID必须与Jenkinsfile中的保持一致），Secret设置为上面经过base64编码后的配置文件内容。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200802204912224.png" alt="image-20200802204912224"></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200802205107574.png" alt="image-20200802205107574"></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200802204224110.png" alt="image-20200802204224110"></p>
<h3 id="开始构建"><a href="#开始构建" class="headerlink" title="开始构建"></a>开始构建</h3><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200810234404325.png" alt="image-20200810234404325"></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200810234326243.png" alt="image-20200810234326243"></p>
<h3 id="在k8s中查看"><a href="#在k8s中查看" class="headerlink" title="在k8s中查看"></a>在k8s中查看</h3><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200810234516285.png" alt="image-20200810234516285"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f pod/nacos-grpc-srv-deployment-66fbb6c749-vmm8s</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/image-20200810234702488.png" alt="image-20200810234702488"></p>
<h1 id="8、暴露nacos"><a href="#8、暴露nacos" class="headerlink" title="8、暴露nacos"></a>8、暴露nacos</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-dn1</span> <span class="string">nacos]#</span> <span class="string">vi</span> <span class="string">nacos-ingress.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nacos</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">nacos</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">kubernetes.io/tls-acme:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-body-size:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-request-buffering:</span> <span class="string">"off"</span></span><br><span class="line">    <span class="string">ingress.kubernetes.io/ssl-redirect:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">ingress.kubernetes.io/proxy-body-size:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line">    <span class="string">ingress.kubernetes.io/proxy-request-buffering:</span> <span class="string">"off"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">nacos.dev</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">nacos-headless</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8848</span></span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">kubectl</span> <span class="string">apply</span> <span class="bullet">-f</span> <span class="string">/data/nacos-grpc/nacos-ingress.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="string">kubectl</span> <span class="string">describe</span> <span class="string">ingress/nacos</span></span><br></pre></td></tr></table></figure>

<h1 id="9、问题总结"><a href="#9、问题总结" class="headerlink" title="9、问题总结"></a>9、问题总结</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unable to connect to the server: x509: certificate signed by unknown authority</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/lucene/4-lucene-srcode-analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/lucene/4-lucene-srcode-analysis/" itemprop="url">4、lucene源码阅读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-02T00:00:00+08:00">
                2018-10-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/lucene/" itemprop="url" rel="index">
                    <span itemprop="name">lucene</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<br>

<h1 id="Lucene写文档"><a href="#Lucene写文档" class="headerlink" title="Lucene写文档"></a>Lucene写文档</h1><p>主类：IndexWriter</p>
<blockquote>
<p>org.apache.lucene.index.IndexWriter</p>
</blockquote>
<p>API 调用过程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter.addDocument():</span><br><span class="line">-&gt; IndexWriter.updateDocument(): <span class="comment">// 局部删除</span></span><br><span class="line">		docWriter.updateDocument(doc, analyzer, delNode);</span><br><span class="line">-&gt; DocumentsWriter.updateDocument():</span><br><span class="line">		dwpt.updateDocument(doc, analyzer, delNode, flushNotifications);</span><br><span class="line">-&gt; DocumentsWriterPerThread.updateDocument():</span><br><span class="line">		consumer.processDocument();</span><br><span class="line">-&gt; DefaultIndexingChain.processDocument():</span><br><span class="line">    	fieldCount = processField(field, fieldGen, fieldCount);</span><br><span class="line">-&gt; DefaultIndexingChain.processField():</span><br><span class="line">		storedFieldsConsumer.writeField(fp.fieldInfo, field);</span><br><span class="line">-&gt; StoredFieldsConsumer.writeField(): </span><br><span class="line">		writer.writeField(info, field); <span class="comment">// writer来源于IndexWriterConfig中的Codec</span></span><br><span class="line">-&gt; SimpleTextStoredFieldsWriter.writeField(); <span class="comment">// 实际去写文档</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SimpleTextStoredFieldsWriter.writeField();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeField</span><span class="params">(FieldInfo info, IndexableField field)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  write(FIELD);</span><br><span class="line">  write(Integer.toString(info.number));</span><br><span class="line">  newLine();</span><br><span class="line">  </span><br><span class="line">  write(NAME);</span><br><span class="line">  write(field.name());</span><br><span class="line">  newLine();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">addDocument</span><span class="params">(Iterable&lt;? extends IndexableField&gt; doc)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> updateDocument((DocumentsWriterDeleteQueue.Node&lt;?&gt;) <span class="keyword">null</span>, doc);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">updateDocument</span><span class="params">(<span class="keyword">final</span> DocumentsWriterDeleteQueue.Node&lt;?&gt; delNode,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Iterable&lt;? extends IndexableField&gt; doc)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  ensureOpen();<span class="comment">// 确保索引能够打开，因为Lucene允许多线程，可能拿不到锁</span></span><br><span class="line">  <span class="keyword">boolean</span> success = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 关键：DocumentsWriter</span></span><br><span class="line">    <span class="keyword">long</span> seqNo = docWriter.updateDocument(doc, analyzer, delNode);</span><br><span class="line">    <span class="keyword">if</span> (seqNo &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      seqNo = -seqNo;</span><br><span class="line">      processEvents(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    success = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">return</span> seqNo;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (VirtualMachineError tragedy) &#123;</span><br><span class="line">    tragicEvent(tragedy, <span class="string">"updateDocument"</span>);</span><br><span class="line">    <span class="keyword">throw</span> tragedy;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (success == <span class="keyword">false</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (infoStream.isEnabled(<span class="string">"IW"</span>)) &#123;</span><br><span class="line">        infoStream.message(<span class="string">"IW"</span>, <span class="string">"hit exception updating document"</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    maybeCloseOnTragicEvent();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>org.apache.lucene.index.DocumentsWriter</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">updateDocument</span><span class="params">(<span class="keyword">final</span> Iterable&lt;? extends IndexableField&gt; doc, </span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">final</span> Analyzer analyzer,</span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">final</span> DocumentsWriterDeleteQueue.Node&lt;?&gt; delNode)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> hasEvents = preUpdate();</span><br><span class="line">  <span class="comment">// class DocumentsWriterPerThreadPool.ThreadState extends ReentrantLock</span></span><br><span class="line">  <span class="comment">// 本质是锁，配合DocumentsWriterPerThread完成Document的写操作</span></span><br><span class="line">  <span class="keyword">final</span> ThreadState perThread = flushControl.obtainAndLock();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> DocumentsWriterPerThread flushingDWPT;</span><br><span class="line">  <span class="keyword">long</span> seqNo;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// This must happen after we've pulled the ThreadState because IW.close</span></span><br><span class="line">    <span class="comment">// waits for all ThreadStates to be released:</span></span><br><span class="line">    ensureOpen();</span><br><span class="line">    ensureInitialized(perThread);</span><br><span class="line">    <span class="keyword">assert</span> perThread.isInitialized();</span><br><span class="line">    <span class="keyword">final</span> DocumentsWriterPerThread dwpt = perThread.dwpt;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> dwptNumDocs = dwpt.getNumDocsInRAM();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      seqNo = dwpt.updateDocument(doc, analyzer, delNode, flushNotifications);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (dwpt.isAborted()) &#123;</span><br><span class="line">        flushControl.doOnAbort(perThread);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// We don't know whether the document actually</span></span><br><span class="line">      <span class="comment">// counted as being indexed, so we must subtract here to</span></span><br><span class="line">      <span class="comment">// accumulate our separate counter:</span></span><br><span class="line">      numDocsInRAM.addAndGet(dwpt.getNumDocsInRAM() - dwptNumDocs);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">boolean</span> isUpdate = delNode != <span class="keyword">null</span> &amp;&amp; delNode.isDelete();</span><br><span class="line">    flushingDWPT = flushControl.doAfterDocument(perThread, isUpdate);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> seqNo &gt; perThread.lastSeqNo: <span class="string">"seqNo="</span> + seqNo + <span class="string">" lastSeqNo="</span> + perThread.lastSeqNo;</span><br><span class="line">    perThread.lastSeqNo = seqNo;</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    perThreadPool.release(perThread);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (postUpdate(flushingDWPT, hasEvents)) &#123;</span><br><span class="line">    seqNo = -seqNo;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> seqNo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">startStoredFields(docState.docID);  <span class="comment">// 开始存储Fields数据</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">for</span> (IndexableField field : docState.doc) &#123;</span><br><span class="line">    fieldCount = processField(field, fieldGen, fieldCount);</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (docWriter.hasHitAbortingException() == <span class="keyword">false</span>) &#123;</span><br><span class="line">    <span class="comment">// Finish each indexed field name seen in the document:</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;fieldCount;i++) &#123;</span><br><span class="line">      fields[i].finish();</span><br><span class="line">    &#125;</span><br><span class="line">    finishStoredFields();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h1 id="Lucene删除文档"><a href="#Lucene删除文档" class="headerlink" title="Lucene删除文档"></a>Lucene删除文档</h1><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191111204659.png" alt></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">IndexWriter.deleteDocuments(Term... terms);</span><br><span class="line">    -&gt; DocumentsWriter.deleteTerms(terms);</span><br><span class="line">	-&gt; DocumentsWriter.applyDeleteOrUpdate(q -&gt; q.addDelete(terms));</span><br><span class="line">	-&gt; DocumentsWriterFlushControl.doOnDelete();</span><br><span class="line">        -&gt; FlushByRamOrCountsPolicy.onDelete(<span class="keyword">this</span>, <span class="keyword">null</span>);</span><br><span class="line">        -&gt; FlushByRamOrCountsPolicy.setApplyAllDeletes();</span><br><span class="line">        -&gt; AtomicBoolean flushDeletes.set(<span class="keyword">true</span>);</span><br><span class="line"> 	-&gt; DocumentsWriter.applyAllDeletes(deleteQueue);</span><br><span class="line">		-&gt; DocumentsWriterFlushQueue.addDeletes(deleteQueue);</span><br><span class="line">			-&gt; org.apache.lucene.index.DocumentsWriterDeleteQueue#freezeGlobalBuffer</span><br><span class="line">			-&gt; org.apache.lucene.index.DocumentsWriterFlushQueue.FlushTicket#FlushTicket</span><br><span class="line">			-&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.lucene.index;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">DocumentsWriter</span> <span class="keyword">implements</span> <span class="title">Closeable</span>, <span class="title">Accountable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">applyDeleteOrUpdate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ToLongFunction&lt;DocumentsWriterDeleteQueue&gt; function)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// TODO why is this synchronized?</span></span><br><span class="line">    <span class="keyword">final</span> DocumentsWriterDeleteQueue deleteQueue = <span class="keyword">this</span>.deleteQueue;</span><br><span class="line">    <span class="keyword">long</span> seqNo = function.applyAsLong(deleteQueue);</span><br><span class="line">    flushControl.doOnDelete();</span><br><span class="line">    lastSeqNo = Math.max(lastSeqNo, seqNo);</span><br><span class="line">    <span class="keyword">if</span> (applyAllDeletes(deleteQueue)) &#123;</span><br><span class="line">      seqNo = -seqNo;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> seqNo;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DocumentsWriterDeleteQueue</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> BufferedUpdates globalBufferedUpdates; <span class="comment">// 全局索引删除</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">long</span> <span class="title">addDelete</span><span class="params">(Term... terms)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> seqNo = add(<span class="keyword">new</span> TermArrayNode(terms));</span><br><span class="line">    tryApplyGlobalSlice();</span><br><span class="line">    <span class="keyword">return</span> seqNo;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 便于内存控制</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">DocumentsWriterFlushControl</span> <span class="keyword">implements</span> <span class="title">Accountable</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">DocumentsWriterFlushQueue</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Queue&lt;FlushTicket&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">  <span class="comment">// we track tickets separately since count must be present even before the ticket is</span></span><br><span class="line">  <span class="comment">// constructed ie. queue.size would not reflect it.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger ticketCount = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ReentrantLock purgeLock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addDeletes</span><span class="params">(DocumentsWriterDeleteQueue deleteQueue)</span> </span>&#123;</span><br><span class="line">    incTickets();<span class="comment">// first inc the ticket count - freeze opens</span></span><br><span class="line">                 <span class="comment">// a window for #anyChanges to fail</span></span><br><span class="line">    <span class="keyword">boolean</span> success = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      queue.add(<span class="keyword">new</span> FlushTicket(deleteQueue.freezeGlobalBuffer(<span class="keyword">null</span>), <span class="keyword">false</span>));</span><br><span class="line">      success = <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!success) &#123;</span><br><span class="line">        decTickets();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191111212917.png" alt></p>
<p>globalBufferUpdates 是全局删除，删除Document</p>
<p>pendingUpdates 是局部删除， 添加或更新Document</p>
<p>局部指向最自己最后的节点， 全局永远指向 整个链表 的 最后一个节点；</p>
<br>

<h1 id="Lucene检索过程"><a href="#Lucene检索过程" class="headerlink" title="Lucene检索过程"></a>Lucene检索过程</h1><p>查询代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryParseTest</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ParseException </span>&#123;</span><br><span class="line">		String field = <span class="string">"title"</span>;</span><br><span class="line">		Path indexPath = Paths.get(<span class="string">"indexdir"</span>);</span><br><span class="line">		Directory directory = FSDirectory.open(indexPath);</span><br><span class="line">		IndexReader reader = DirectoryReader.open(directory);</span><br><span class="line">		IndexSearcher searcher = <span class="keyword">new</span> IndexSearcher(reader);</span><br><span class="line">		Analyzer analyzer = <span class="keyword">new</span> IKAnalyzer8x();</span><br><span class="line">		QueryParser parser = <span class="keyword">new</span> QueryParser(field, analyzer);</span><br><span class="line">		parser.setDefaultOperator(QueryParser.Operator.AND);</span><br><span class="line">		Query query = parser.parse(<span class="string">"农村学生"</span>);</span><br><span class="line">		System.out.println(<span class="string">"Query: "</span> + query.toString());  <span class="comment">// 查询关键词</span></span><br><span class="line">		<span class="comment">// 返回前10条</span></span><br><span class="line">		TopDocs topDocs = searcher.search(query, <span class="number">10</span>);</span><br><span class="line">		<span class="keyword">for</span> (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123;</span><br><span class="line">			Document doc = searcher.doc(scoreDoc.doc);</span><br><span class="line">			System.out.println(<span class="string">"DocID: "</span> + scoreDoc.doc);</span><br><span class="line">			System.out.println(<span class="string">"id: "</span> + doc.get(<span class="string">"id"</span>));</span><br><span class="line">			System.out.println(<span class="string">"title: "</span> + doc.get(<span class="string">"title"</span>));</span><br><span class="line">			System.out.println(<span class="string">"文档评分: "</span> + scoreDoc.score);</span><br><span class="line">		&#125;</span><br><span class="line">		directory.close();</span><br><span class="line">		reader.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>查询关键代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.lucene.search;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IndexSearcher</span> </span>&#123;</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Lower-level search API.</span></span><br><span class="line"><span class="comment">  * Search all leaves using the given &#123;<span class="doctag">@link</span> CollectorManager&#125;. In contrast</span></span><br><span class="line"><span class="comment">  * to &#123;<span class="doctag">@link</span> #search(Query, Collector)&#125;, this method will use the searcher's</span></span><br><span class="line"><span class="comment">  * &#123;<span class="doctag">@link</span> ExecutorService&#125; in order to parallelize execution of the collection</span></span><br><span class="line"><span class="comment">  * on the configured &#123;<span class="doctag">@link</span> #leafSlices&#125;.</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@see</span> CollectorManager</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@lucene</span>.experimental</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">public</span> &lt;C extends Collector, T&gt; <span class="function">T <span class="title">search</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      Query query, CollectorManager&lt;C, T&gt; collectorManager)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (executor == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">final</span> C collector = collectorManager.newCollector();</span><br><span class="line">      search(query, collector);</span><br><span class="line">      <span class="keyword">return</span> collectorManager.reduce(Collections.singletonList(collector));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">final</span> List&lt;C&gt; collectors = <span class="keyword">new</span> ArrayList&lt;&gt;(leafSlices.length);</span><br><span class="line">      <span class="keyword">boolean</span> needsScores = <span class="keyword">false</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; leafSlices.length; ++i) &#123;</span><br><span class="line">        <span class="keyword">final</span> C collector = collectorManager.newCollector();</span><br><span class="line">        collectors.add(collector);</span><br><span class="line">        needsScores |= collector.needsScores();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      query = rewrite(query);</span><br><span class="line">      <span class="comment">// 创建打分 Weight ，默认 1</span></span><br><span class="line">      <span class="keyword">final</span> Weight weight = createWeight(query, needsScores, <span class="number">1</span>);</span><br><span class="line">      <span class="comment">// 根据 leafSlices.length 创建 topDocsFutures，线程调度</span></span><br><span class="line">      <span class="keyword">final</span> List&lt;Future&lt;C&gt;&gt; topDocsFutures = <span class="keyword">new</span> ArrayList&lt;&gt;(leafSlices.length);</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; leafSlices.length; ++i) &#123;</span><br><span class="line">        <span class="keyword">final</span> LeafReaderContext[] leaves = leafSlices[i].leaves;</span><br><span class="line">        <span class="keyword">final</span> C collector = collectors.get(i);</span><br><span class="line">        topDocsFutures.add(executor.submit(<span class="keyword">new</span> Callable&lt;C&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> C <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 使用线程 去 遍历数据</span></span><br><span class="line">            search(Arrays.asList(leaves), weight, collector);</span><br><span class="line">            <span class="keyword">return</span> collector;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;));</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">final</span> List&lt;C&gt; collectedCollectors = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">      <span class="keyword">for</span> (Future&lt;C&gt; future : topDocsFutures) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          collectedCollectors.add(future.get());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> ThreadInterruptedException(e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> collectorManager.reduce(collectors);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<p>Analysyer 包含两个组件 </p>
<ul>
<li><p>Tokenizer 分词器（分词 token）</p>
</li>
<li><p>TokenFilter 分词过滤器（大小写转换，词根cats）</p>
</li>
</ul>
<p>Lucene70Codec =&gt; 实际去写Term</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lucene70Codec</span> <span class="keyword">extends</span> <span class="title">Codec</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TermVectorsFormat vectorsFormat = <span class="keyword">new</span> Lucene50TermVectorsFormat();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> FieldInfosFormat fieldInfosFormat = <span class="keyword">new</span> Lucene60FieldInfosFormat();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> SegmentInfoFormat segmentInfosFormat = <span class="keyword">new</span> Lucene70SegmentInfoFormat();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> LiveDocsFormat liveDocsFormat = <span class="keyword">new</span> Lucene50LiveDocsFormat();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CompoundFormat compoundFormat = <span class="keyword">new</span> Lucene50CompoundFormat();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/lucene/3-lucene-query/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/lucene/3-lucene-query/" itemprop="url">lucene索引查询</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-01T00:00:00+08:00">
                2018-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/lucene/" itemprop="url" rel="index">
                    <span itemprop="name">lucene</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<br>

<h1 id="一、Lucene-索引知识扩展"><a href="#一、Lucene-索引知识扩展" class="headerlink" title="一、Lucene 索引知识扩展"></a>一、Lucene 索引知识扩展</h1><br>

<h1 id="二、索引查询"><a href="#二、索引查询" class="headerlink" title="二、索引查询"></a>二、索引查询</h1><h3 id="Lucene-构建查询套路"><a href="#Lucene-构建查询套路" class="headerlink" title="Lucene 构建查询套路"></a>Lucene 构建查询套路</h3><br>

<h3 id="常用-Query-查询类"><a href="#常用-Query-查询类" class="headerlink" title="常用 Query 查询类"></a>常用 Query 查询类</h3><h4 id="TermQuery"><a href="#TermQuery" class="headerlink" title="TermQuery"></a>TermQuery</h4><h4 id="BooleanQuery"><a href="#BooleanQuery" class="headerlink" title="BooleanQuery"></a>BooleanQuery</h4><br>

<h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><br>

<h3 id="高亮显示"><a href="#高亮显示" class="headerlink" title="高亮显示"></a>高亮显示</h3><br>

<h3 id="Facets-分类索引"><a href="#Facets-分类索引" class="headerlink" title="Facets 分类索引"></a>Facets 分类索引</h3><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191108221536.png" alt></p>
<p>多级分类（类似：省-&gt;市-&gt;县）</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-facet --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.lucene<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lucene-facet<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;lucene.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191108215406.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191108220122.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191108220033.png" alt></p>
<p>下钻查询</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DrillDownQuery drillDownQuery = <span class="keyword">new</span> DrillDownQuery(config, query);</span><br><span class="line">drillDownQuery.add();</span><br><span class="line">FacetsCollector facetsCollector = <span class="keyword">new</span> FacetsConector();</span><br></pre></td></tr></table></figure>

<p>查询平行维度</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DrillSideways ds = <span class="keyword">new</span> DrillSideways(searcher, config, query);</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20191108221910.png" alt></p>
<br>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/tools/git-help-doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/tools/git-help-doc/" itemprop="url">珍藏多年的git问题和操作清单</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-04T00:00:00+08:00">
                2018-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="来源：微信公众号-猿天地"><a href="#来源：微信公众号-猿天地" class="headerlink" title="来源：微信公众号-猿天地"></a>来源：微信公众号-猿天地</h1><p><br><br></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本文整理自工作多年以来遇到的所有 Git 问题汇总，之前都是遗忘的时候去看一遍操作，这次重新整理了一下，发出来方便大家收藏以及需要的时候查找答案。</p>
<p><br><br></p>
<h1 id="一、必备知识点"><a href="#一、必备知识点" class="headerlink" title="一、必备知识点"></a>一、必备知识点</h1><p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905110527.png" alt></p>
<h4 id="仓库"><a href="#仓库" class="headerlink" title="仓库"></a>仓库</h4><ol>
<li><strong>Remote:</strong> 远程主仓库；</li>
<li><strong>Repository/History：</strong> 本地仓库；</li>
<li><strong>Stage/Index：</strong> Git追踪树,暂存区；</li>
<li><strong>workspace：</strong> 本地工作区（即你编辑器的代码）</li>
</ol>
<br>

<br>

<h1 id="二、git-add-提交到暂存区，出错怎么办"><a href="#二、git-add-提交到暂存区，出错怎么办" class="headerlink" title="二、git add 提交到暂存区，出错怎么办"></a>二、git add 提交到暂存区，出错怎么办</h1><p>一般代码提交流程为：<strong>工作区</strong> -&gt; <code>git status</code> 查看状态 -&gt; <code>git add .</code> 将所有修改加入<strong>暂存区</strong>-&gt; <code>git commit -m &quot;提交描述&quot;</code> 将代码提交到 <strong>本地仓库</strong> -&gt; <code>git push</code> 将本地仓库代码更新到 <strong>远程仓库</strong></p>
<h4 id="场景1："><a href="#场景1：" class="headerlink" title="场景1："></a>场景1：</h4><p>当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令<code>git checkout -- file</code>。</p>
<p><strong>git checkout</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 丢弃工作区的修改</span><br><span class="line">git checkout -- &lt;文件名&gt;</span><br></pre></td></tr></table></figure>

<h4 id="场景2："><a href="#场景2：" class="headerlink" title="场景2："></a>场景2：</h4><p>当你不但改乱了工作区某个文件的内容，还添加到了暂存时，想丢弃修改，分两步，第一步用命令 <code>git reset HEAD file</code>，就回到了场景1，第二步按场景1操作。</p>
<br>

<br>

<h1 id="三、git-commit-提交到本地仓库，出错怎么办？"><a href="#三、git-commit-提交到本地仓库，出错怎么办？" class="headerlink" title="三、git commit 提交到本地仓库，出错怎么办？"></a>三、git commit 提交到本地仓库，出错怎么办？</h1><h4 id="1-提交信息出错"><a href="#1-提交信息出错" class="headerlink" title="1.  提交信息出错"></a>1.  提交信息出错</h4><p>更改 commit 信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend -m“新提交消息”</span><br></pre></td></tr></table></figure>

<h4 id="2-漏提交"><a href="#2-漏提交" class="headerlink" title="2. 漏提交"></a>2. 漏提交</h4><p>commit 时，遗漏提交部分更新，有两种解决方案：</p>
<ul>
<li><p>方案一：再次 commit</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m“提交消息”</span><br></pre></td></tr></table></figure>

<p>此时，git 上会出现两次 commit</p>
</li>
<li><p>方案二：遗漏文件提交到之前 commit 上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add missed-file // missed-file 为遗漏提交文件</span><br><span class="line">git commit --amend --no-edit</span><br></pre></td></tr></table></figure>

<p><code>--no-edit</code> 表示提交消息不会更改，在 git 上仅为一次提交</p>
</li>
</ul>
<h4 id="3-提交错误文件，回退到上一个-commit-版本，再-commit"><a href="#3-提交错误文件，回退到上一个-commit-版本，再-commit" class="headerlink" title="3. 提交错误文件，回退到上一个 commit 版本，再 commit"></a>3. 提交错误文件，回退到上一个 commit 版本，再 commit</h4><h5 id="git-reset"><a href="#git-reset" class="headerlink" title="git reset"></a>git reset</h5><p>删除指定的 commit</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 修改版本库，修改暂存区，修改工作区</span><br><span class="line"></span><br><span class="line">git reset HEAD &lt;文件名&gt; // 把暂存区的修改撤销掉（unstage），重新放回工作区。</span><br><span class="line">// git版本回退，回退到特定的commit_id版本，可以通过git log查看提交历史，以便确定要回退到哪个版本(commit 之后的即为ID);</span><br><span class="line">git reset --hard commit_id </span><br><span class="line">//将版本库回退1个版本，不仅仅是将本地版本库的头指针全部重置到指定版本，也会重置暂存区，并且会将工作区代码也回退到这个版本</span><br><span class="line">git reset --hard HEAD~1</span><br><span class="line"></span><br><span class="line">// 修改版本库，保留暂存区，保留工作区</span><br><span class="line">// 将版本库软回退1个版本，软回退表示将本地版本库的头指针全部重置到指定版本，且将这次提交之后的所有变更都移动到暂存区。</span><br><span class="line">git reset --soft HEAD~1</span><br></pre></td></tr></table></figure>

<h5 id="git-revert"><a href="#git-revert" class="headerlink" title="git revert"></a>git revert</h5><p>撤销 某次操作，此次操作之前和之后的commit和history都会保留，并且把这次撤销</p>
<p>作为一次最新的提交</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 撤销前一次 commit</span><br><span class="line">git revert HEAD</span><br><span class="line">// 撤销前前一次 commit</span><br><span class="line">git revert HEAD^</span><br><span class="line">// (比如：fa042ce57ebbe5bb9c8db709f719cec2c58ee7ff）撤销指定的版本，撤销也会作为一次提交进行保存。</span><br><span class="line">git revert commit</span><br></pre></td></tr></table></figure>

<p><code>git revert</code>是提交一个新的版本，将需要<code>revert</code>的版本的内容再反向修改回去，<br>版本会递增，不影响之前提交的内容</p>
<h5 id="git-revert-和-git-reset-的区别"><a href="#git-revert-和-git-reset-的区别" class="headerlink" title="git revert 和 git reset 的区别"></a><code>git revert</code> 和 <code>git reset</code> 的区别</h5><ul>
<li><code>git revert</code>是用一次新的commit来回滚之前的commit，<code>git reset</code>是直接删除指定的commit。</li>
<li>在回滚这一操作上看，效果差不多。但是在日后继续merge以前的老版本时有区别。因为<code>git revert</code>是用一次逆向的commit“中和”之前的提交，因此日后合并老的branch时，导致这部分改变不会再次出现，但是<code>git reset</code>是之间把某些commit在某个branch上删除，因而和老的branch再次merge时，这些被回滚的commit应该还会被引入。</li>
<li><code>git reset</code> 是把HEAD向后移动了一下，而<code>git revert</code>是HEAD继续前进，只是新的commit的内容和要revert的内容正好相反，能够抵消要被revert的内容。</li>
</ul>
<p><br><br></p>
<h1 id="四、常用命令"><a href="#四、常用命令" class="headerlink" title="四、常用命令"></a>四、常用命令</h1><h4 id="1-初始开发-git-操作流程"><a href="#1-初始开发-git-操作流程" class="headerlink" title="1. 初始开发 git 操作流程"></a>1. 初始开发 git 操作流程</h4><ul>
<li>克隆最新主分支项目代码 <code>git clone 地址</code></li>
<li>创建本地分支 <code>git branch 分支名</code></li>
<li>查看本地分支 <code>git branch</code></li>
<li>查看远程分支 <code>git branch -a</code></li>
<li>切换分支  <code>git checkout 分支名</code> (一般修改未提交则无法切换，大小写问题经常会有，可强制切换  <code>git checkout 分支名 -f</code>  非必须慎用)</li>
<li>将本地分支推送到远程分支 <code>git push &lt;远程仓库&gt; &lt;本地分支&gt;:&lt;远程分支&gt;</code></li>
</ul>
<h4 id="2-git-fetch"><a href="#2-git-fetch" class="headerlink" title="2. git fetch"></a>2. git fetch</h4><p>将某个远程主机的更新，全部/分支 取回本地（此时之更新了Repository）它取回的代码对你本地的开发代码没有影响，如需彻底更新需合并或使用<code>git pull</code></p>
<h4 id="3-git-pull"><a href="#3-git-pull" class="headerlink" title="3. git pull"></a>3. git pull</h4><p>拉取远程主机某分支的更新，再与本地的指定分支合并（相当与fetch加上了合并分支功能的操作）</p>
<h4 id="4-git-push"><a href="#4-git-push" class="headerlink" title="4. git push"></a>4. git push</h4><p>将本地分支的更新，推送到远程主机，其命令格式与<code>git pull</code>相似</p>
<h4 id="5-分支操作"><a href="#5-分支操作" class="headerlink" title="5. 分支操作"></a>5. 分支操作</h4><ul>
<li>使用 Git 下载指定分支命令为：<code>git clone -b 分支名仓库地址</code></li>
<li>拉取远程新分支 <code>git checkout -b serverfix origin/serverfix</code></li>
<li>合并本地分支 <code>git merge hotfix</code>：(将 hotfix 分支合并到当前分支)</li>
<li>合并远程分支 <code>git merge origin/serverfix</code></li>
<li>删除本地分支 <code>git branch -d hotfix</code>：(删除本地 hotfix 分支)</li>
<li>删除远程分支 <code>git push origin --delete serverfix</code></li>
<li>上传新命名的本地分支：<code>git push origin newName</code>;</li>
<li>创建新分支：<code>git branch branchName</code>：(创建名为 branchName 的本地分支)</li>
<li>切换到新分支：<code>git checkout branchName</code>：(切换到 branchName 分支)</li>
<li>创建并切换分支：<code>git checkout -b branchName</code>：(相当于以上两条命令的合并)</li>
<li>查看本地分支：<code>git branch</code></li>
<li>查看远程仓库所有分支：<code>git branch -a</code></li>
<li>本地分支重命名：<code>git branch -m oldName newName</code></li>
<li>重命名远程分支对应的本地分支：<code>git branch -m oldName newName</code></li>
<li>把修改后的本地分支与远程分支关联：<code>git branch --set-upstream-to origin/newName</code></li>
</ul>
<p><br><br></p>
<h1 id="五、优化操作"><a href="#五、优化操作" class="headerlink" title="五、优化操作"></a>五、优化操作</h1><h4 id="1-拉取代码-pull-–rebase"><a href="#1-拉取代码-pull-–rebase" class="headerlink" title="1. 拉取代码 pull –rebase"></a>1. 拉取代码 pull –rebase</h4><p>在团队协作过程中，假设你和你的同伴在本地中分别有各自的新提交，而你的同伴先于你 <code>push</code>了代码到远程分支上，所以你必须先执行 <code>git pull</code> 来获取同伴的提交，然后才能<code>push</code> 自己的提交到远程分支。</p>
<p>而按照 Git 的默认策略，如果远程分支和本地分支之间的提交线图有分叉的话（即不是 fast-forwarded），Git 会执行一次 <code>merge</code> 操作，因此产生一次没意义的提交记录，从而造成了像上图那样的混乱。</p>
<p>其实在 pull 操作的时候，，使用 <code>git pull --rebase</code>选项即可很好地解决上述问题。加上 <code>--rebase</code> 参数的作用是，提交线图有分叉的话，Git 会 rebase 策略来代替默认的 merge 策略。</p>
<p>假设提交线图在执行 pull 前是这样的：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">      A---B---C  remotes/origin/master</span><br><span class="line">     /</span><br><span class="line">D---E---F---G  master</span><br></pre></td></tr></table></figure>

<p>如果是执行 <code>git pull</code> 后，提交线图会变成这样：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">      A---B---C remotes/origin/master</span><br><span class="line">     /         \</span><br><span class="line">D---E---F---G---H master</span><br></pre></td></tr></table></figure>

<p>结果多出了 <code>H</code> 这个没必要的提交记录。如果是执行 <code>git pull --rebase</code> 的话，提交线图就会变成这样：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">            remotes/origin/master</span><br><span class="line">                |</span><br><span class="line">D---E---A---B---C---F'---G'  master</span><br></pre></td></tr></table></figure>

<p><code>F</code> <code>G</code> 两个提交通过 <code>rebase</code> 方式重新拼接在 <code>C</code> 之后，多余的分叉去掉了，目的达到。</p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>大多数时候，使用 <code>git pull --rebase</code>是为了使提交线图更好看，从而方便 code review。</p>
<p>不过，如果你对使用 git 还不是十分熟练的话，我的建议是 <code>git pull --rebase</code>多练习几次之后再使用，因为 <strong>rebase 在 git 中，算得上是『危险行为』</strong>。</p>
<p>另外，还需注意的是，使用 <code>git pull --rebase</code>比直接 pull 容易导致冲突的产生，如果预期冲突比较多的话，建议还是直接 pull。</p>
<blockquote>
<p>注意：</p>
<p>git pull = git fetch + git merge</p>
<p>git pull –rebase = git fetch + git rebase</p>
</blockquote>
<h4 id="2-合代码-merge-–no-ff"><a href="#2-合代码-merge-–no-ff" class="headerlink" title="2. 合代码 merge –no-ff"></a>2. 合代码 merge –no-ff</h4><p>上述的 <code>git pull --rebase</code> 策略目的是修整提交线图，使其形成一条直线，而即将要用到的<code>git merge --no-ff &lt;branch-name&gt;</code> 策略偏偏是反行其道，刻意地弄出提交线图分叉出来。</p>
<p>假设你在本地准备合并两个分支，而刚好这两个分支是 fast-forwarded 的，那么直接合并后你得到一个直线的提交线图，当然这样没什么坏处，但如果你想更清晰地告诉你同伴：<strong>这一系列的提交都是为了实现同一个目的</strong>，那么你可以刻意地将这次提交内容弄成一次提交线图分叉。</p>
<p>执行 <code>git merge --no-ff &lt;branch-name&gt;</code> 的结果大概会是这样的：</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905111318.png" alt></p>
<p>中间的分叉线路图很清晰的显示这些提交都是为了实现 <strong>complete adjusting user domains and tags</strong></p>
<h5 id="更进一步"><a href="#更进一步" class="headerlink" title="更进一步"></a>更进一步</h5><p>往往我的习惯是，在合并分支之前（假设要在本地将 feature 分支合并到 dev 分支），会先检查 feature 分支是否『部分落后』于<strong>远程 dev 分支</strong>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout dev</span><br><span class="line">git pull # 更新 dev 分支</span><br><span class="line">git log feature..dev</span><br></pre></td></tr></table></figure>

<p>如果没有输出任何提交信息的话，即表示 feature 对于 dev 分支是 up-to-date 的。如果有输出的话而马上执行了 <code>git merge --no-ff</code> 的话，提交线图会变成这样：</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905111351.png" alt></p>
<p>所以这时在合并前，通常我会先执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout feature</span><br><span class="line">git rebase dev</span><br></pre></td></tr></table></figure>

<p>这样就可以将 feature 重新拼接到更新了的 dev 之后，然后就可以合并了，最终得到一个干净舒服的提交线图。</p>
<p><strong>再次提醒：像之前提到的，rebase 是『危险行为』，建议你足够熟悉 git 时才这么做，否则的话是得不偿失啊。</strong></p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>使用 <code>git pull --rebase</code> 和 <code>git merge --no-ff</code> 其实和直接使用 <code>git pull</code> <code>git merge</code> 得到的代码应该是一样。</p>
<p>使用 <code>git pull --rebase</code> 主要是为是将提交约线图平坦化，而 <code>git merge --no-ff</code> 则是刻意制造分叉。</p>
<br>

<br>

<h1 id="六、SSH"><a href="#六、SSH" class="headerlink" title="六、SSH"></a>六、SSH</h1><h4 id="1-查看是否生成了-SSH-公钥"><a href="#1-查看是否生成了-SSH-公钥" class="headerlink" title="1. 查看是否生成了 SSH 公钥"></a>1. 查看是否生成了 SSH 公钥</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/.ssh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">id_rsa      id_rsa.pub      known_hosts</span><br></pre></td></tr></table></figure>

<p>其中 id_rsa 是私钥，id_rsa.pub 是公钥。</p>
<h4 id="2-如果没有那就开始生成，设置全局的user-name与user-email"><a href="#2-如果没有那就开始生成，设置全局的user-name与user-email" class="headerlink" title="2. 如果没有那就开始生成，设置全局的user.name与user.email"></a>2. 如果没有那就开始生成，设置全局的user.name与user.email</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config --list // 查看是否设置了user.name与user.email，没有的话，去设置</span><br><span class="line">// 设置全局的user.name与user.email</span><br><span class="line">git config --global user.name "XX"</span><br><span class="line">git config --global user.email "XX"</span><br></pre></td></tr></table></figure>

<h4 id="3-输入-ssh-keygen-即可（或ssh-keygen-t-rsa-C-quot-email-quot-）"><a href="#3-输入-ssh-keygen-即可（或ssh-keygen-t-rsa-C-quot-email-quot-）" class="headerlink" title="3. 输入 ssh-keygen 即可（或ssh-keygen -t rsa -C &quot;email&quot;）"></a>3. 输入 ssh-keygen 即可（或<code>ssh-keygen -t rsa -C &quot;email&quot;</code>）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen</span></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/Users/schacon/.ssh/id_rsa):</span><br><span class="line">Enter passphrase (empty for no passphrase):</span><br><span class="line">Enter same passphrase again:</span><br><span class="line">Your identification has been saved in /Users/schacon/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /Users/schacon/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br></pre></td></tr></table></figure>

<h4 id="4-生成之后获取公钥内容，输入-cat-ssh-id-rsa-pub-即可，-复制-ssh-rsa-一直到-local这一整段内容"><a href="#4-生成之后获取公钥内容，输入-cat-ssh-id-rsa-pub-即可，-复制-ssh-rsa-一直到-local这一整段内容" class="headerlink" title="4. 生成之后获取公钥内容，输入 cat ~/.ssh/id_rsa.pub 即可， 复制 ssh-rsa 一直到 .local这一整段内容"></a>4. 生成之后获取公钥内容，输入 cat ~/.ssh/id_rsa.pub 即可， 复制 ssh-rsa 一直到 .local这一整段内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ~/.ssh/id_rsa.pub</span></span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSU</span><br><span class="line">GPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3</span><br><span class="line">Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XA</span><br><span class="line">t3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/En</span><br><span class="line">mZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbx</span><br><span class="line">NrRFi9wrf+M7Q== schacon@agadorlaptop.local</span><br></pre></td></tr></table></figure>

<h4 id="5-打开-GitLab-或者-GitHub，点击头像，找到设置页"><a href="#5-打开-GitLab-或者-GitHub，点击头像，找到设置页" class="headerlink" title="5. 打开 GitLab 或者 GitHub，点击头像，找到设置页"></a>5. 打开 GitLab 或者 GitHub，点击头像，找到设置页</h4><h4 id="6-左侧找到-SSH-keys-按钮并点击，输入刚刚复制的公钥即可"><a href="#6-左侧找到-SSH-keys-按钮并点击，输入刚刚复制的公钥即可" class="headerlink" title="6. 左侧找到 SSH keys 按钮并点击，输入刚刚复制的公钥即可"></a>6. 左侧找到 SSH keys 按钮并点击，输入刚刚复制的公钥即可</h4><p><br><br></p>
<h1 id="七、暂存"><a href="#七、暂存" class="headerlink" title="七、暂存"></a>七、暂存</h1><p><code>git stash</code> 可用来暂存当前正在进行的工作，比如想 pull 最新代码又不想 commit ， 或者另为了修改一个紧急的 bug ，先 stash，使返回到自己上一个 commit,，改完 bug 之后再 stash pop , 继续原来的工作；</p>
<ul>
<li>添加缓存栈：<code>git stash</code> ;</li>
<li>查看缓存栈：<code>git stash list</code> ;</li>
<li>推出缓存栈：<code>git stash pop</code> ;</li>
<li>取出特定缓存内容：<code>git stash apply stash@{1}</code> ;</li>
</ul>
<p><br><br></p>
<h1 id="八、文件名过长错误"><a href="#八、文件名过长错误" class="headerlink" title="八、文件名过长错误"></a>八、文件名过长错误</h1><p>Filename too long warning: Clone succeeded, but checkout failed.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --system core.longpaths true</span><br></pre></td></tr></table></figure>

<p><br><br></p>
<h1 id="九、邮箱和用户名"><a href="#九、邮箱和用户名" class="headerlink" title="九、邮箱和用户名"></a>九、邮箱和用户名</h1><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config user.name</span><br><span class="line">git config user.email</span><br></pre></td></tr></table></figure>

<h4 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name "username"</span><br><span class="line">git config --global user.email "email"</span><br></pre></td></tr></table></figure>

<p><br><br></p>
<h1 id="十、-gitignore-更新后生效："><a href="#十、-gitignore-更新后生效：" class="headerlink" title="十、.gitignore 更新后生效："></a>十、.gitignore 更新后生效：</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rm -r --cached .</span><br><span class="line">git add .</span><br><span class="line">git commit -m ".gitignore is now working”</span><br></pre></td></tr></table></figure>

<p><br><br></p>
<h1 id="十一、同步Github-fork-出来的分支"><a href="#十一、同步Github-fork-出来的分支" class="headerlink" title="十一、同步Github fork 出来的分支"></a>十一、同步Github fork 出来的分支</h1><p>1、配置remote，指向原始仓库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add upstream https://github.com/InterviewMap/InterviewMap.git</span><br></pre></td></tr></table></figure>

<p>2、上游仓库获取到分支，及相关的提交信息，它们将被保存在本地的 upstream/master 分支</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git fetch upstream</span><br><span class="line"><span class="meta">#</span><span class="bash"> remote: Counting objects: 75, <span class="keyword">done</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> remote: Compressing objects: 100% (53/53), <span class="keyword">done</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> remote: Total 62 (delta 27), reused 44 (delta 9)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unpacking objects: 100% (62/62), <span class="keyword">done</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> * [new branch] master -&gt; upstream/master</span></span><br></pre></td></tr></table></figure>

<p>3、切换到本地的 master 分支</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line"><span class="meta">#</span><span class="bash"> Switched to branch <span class="string">'master'</span></span></span><br></pre></td></tr></table></figure>

<p>4、把 upstream/master 分支合并到本地的 master 分支，本地的 master 分支便跟上游仓库保持同步了，并且没有丢失本地的修改。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git merge upstream/master</span><br><span class="line"><span class="meta">#</span><span class="bash"> Updating a422352..5fdff0f</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Fast-forward</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> README | 9 -------</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> README.md | 7 ++++++</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2 files changed, 7 insertions(+), 9 deletions(-)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> delete mode 100644 README</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> create mode 100644 README.md</span></span><br></pre></td></tr></table></figure>

<p>5、上传到自己的远程仓库中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/distribution/distributed-current-limiting/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/distribution/distributed-current-limiting/" itemprop="url">高并发 —— 限流算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-25T00:00:00+08:00">
                2018-08-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/distribution/" itemprop="url" rel="index">
                    <span itemprop="name">distribution</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="一、限流的作用"><a href="#一、限流的作用" class="headerlink" title="一、限流的作用"></a>一、限流的作用</h1><p>由于API接口无法控制调用方的行为，因此当遇到瞬时请求量激增时，会导致接口占用过多服务器资源，使得其他请求响应速度降低或是超时，更有甚者可能导致服务器宕机。 </p>
<p>限流(Rate limiting)指对应用服务的请求进行限制，例如某一接口的请求限制为100个每秒,对超过限制的请求则进行快速失败或丢弃。</p>
<p>限流可以应对：</p>
<ul>
<li>热点业务带来的突发请求；</li>
<li>调用方bug导致的突发请求；</li>
<li>恶意攻击请求。</li>
</ul>
<p>因此，对于公开的接口最好采取限流措施。</p>
<p>​      </p>
<h1 id="二、限流算法"><a href="#二、限流算法" class="headerlink" title="二、限流算法"></a>二、限流算法</h1><p>实现限流有很多办法，在程序中时通常是根据每秒处理的事务数(Transaction per second)来衡量接口的流量。 </p>
<p>本文介绍几种最常用的限流算法：</p>
<ul>
<li>固定窗口计数器；</li>
<li>滑动窗口计数器；</li>
<li>漏桶；</li>
<li>令牌桶；</li>
</ul>
<h3 id="1、-固定窗口计数器算法"><a href="#1、-固定窗口计数器算法" class="headerlink" title="1、 固定窗口计数器算法"></a>1、 固定窗口计数器算法</h3><p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825195957.png" alt></p>
<p>固定窗口计数器算法概念如下：</p>
<ul>
<li>将时间划分为多个窗口；</li>
<li>在每个窗口内每有一次请求就将计数器加一；</li>
<li>如果计数器超过了限制数量，则本窗口内所有的请求都被丢弃当时间到达下一个窗口时，计数器重置。</li>
</ul>
<p>固定窗口计数器是最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。考虑如下情况：限制1秒内最多通过5个请求，在第一个窗口的最后半秒内通过了5个请求，第二个窗口的前半秒内又通过了5个请求。这样看来就是在1秒内通过了10个请求。 </p>
<p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825200027.png" alt></p>
<p>​      </p>
<h3 id="2、滑动窗口计数器算法"><a href="#2、滑动窗口计数器算法" class="headerlink" title="2、滑动窗口计数器算法"></a>2、滑动窗口计数器算法</h3><p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825200106.png" alt></p>
<p>滑动窗口计数器算法概念如下：</p>
<ul>
<li>将时间划分为多个区间；</li>
<li>在每个区间内每有一次请求就将计数器加一维持一个时间窗口，占据多个区间；</li>
<li>每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间；</li>
<li>如果当前窗口内区间的请求计数总和超过了限制数量，则本窗口内所有的请求都被丢弃。</li>
</ul>
<p>滑动窗口计数器是通过将窗口再细分，并且按照时间”滑动”，这种算法避免了固定窗口计数器带来的双倍突发请求，但时间区间的精度越高，算法所需的空间容量就越大。</p>
<h3 id="3、漏桶算法"><a href="#3、漏桶算法" class="headerlink" title="3、漏桶算法"></a>3、漏桶算法</h3><p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825224036.png" alt></p>
<p>漏桶算法概念如下：</p>
<ul>
<li>将每个请求视作”水滴”放入”漏桶”进行存储；</li>
<li>“漏桶”以固定速率向外”漏”出请求来执行如果”漏桶”空了则停止”漏水”；</li>
<li>如果”漏桶”满了则多余的”水滴”会被直接丢弃。</li>
</ul>
<p>漏桶算法多使用队列实现，服务的请求会存到队列中，服务的提供方则按照固定的速率从队列中取出请求并执行，过多的请求则放在队列中排队或直接拒绝。</p>
<p>漏桶算法的缺陷也很明显，当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也都得在队列中等待一段时间才能被响应。</p>
<h3 id="4、令牌桶算法"><a href="#4、令牌桶算法" class="headerlink" title="4、令牌桶算法"></a>4、令牌桶算法</h3><p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825200242.png" alt></p>
<p>令牌桶算法概念如下：</p>
<ul>
<li>令牌以固定速率生成；</li>
<li>生成的令牌放入令牌桶中存放，如果令牌桶满了则多余的令牌会直接丢弃，当请求到达时，会尝试从令牌桶中取令牌，取到了令牌的请求可以执行；</li>
<li>如果桶空了，那么尝试取令牌的请求会被直接丢弃。</li>
</ul>
<p>令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。</p>
<p>​      </p>
<h1 id="三、单体应用限流"><a href="#三、单体应用限流" class="headerlink" title="三、单体应用限流"></a>三、单体应用限流</h1><h3 id="1、信号量Semaphore限流"><a href="#1、信号量Semaphore限流" class="headerlink" title="1、信号量Semaphore限流"></a>1、信号量Semaphore限流</h3><p>private final Semaphore permit = new Semaphore(10, true);</p>
<h3 id="2、Guava的RateLimiter实现限流"><a href="#2、Guava的RateLimiter实现限流" class="headerlink" title="2、Guava的RateLimiter实现限流"></a>2、Guava的RateLimiter实现限流</h3><p>RateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个</p>
<h3 id="3、利用Atomic类自己实现限流算法"><a href="#3、利用Atomic类自己实现限流算法" class="headerlink" title="3、利用Atomic类自己实现限流算法"></a>3、利用Atomic类自己实现限流算法</h3><p>​                     </p>
<h1 id="四、接入层限流"><a href="#四、接入层限流" class="headerlink" title="四、接入层限流"></a>四、接入层限流</h1><h3 id="1、nginx限流"><a href="#1、nginx限流" class="headerlink" title="1、nginx限流"></a>1、nginx限流</h3><h5 id="1、自带模块-limit-req-zone-and-limit-req"><a href="#1、自带模块-limit-req-zone-and-limit-req" class="headerlink" title="1、自带模块 limit_req_zone and limit_req"></a>1、自带模块 <code>limit_req_zone</code> and <code>limit_req</code></h5><p><a href="https://www.nginx.com/blog/rate-limiting-nginx/" target="_blank" rel="noopener">https://www.nginx.com/blog/rate-limiting-nginx/</a></p>
<h5 id="2、nginx-lua-redis-实现复杂的限流算法"><a href="#2、nginx-lua-redis-实现复杂的限流算法" class="headerlink" title="2、nginx+lua+redis 实现复杂的限流算法"></a>2、nginx+lua+redis 实现复杂的限流算法</h5><p>openresty了解一下</p>
<p>​             </p>
<h1 id="五、分布式限流"><a href="#五、分布式限流" class="headerlink" title="五、分布式限流"></a>五、分布式限流</h1><h3 id="1、为什么要分布式限流"><a href="#1、为什么要分布式限流" class="headerlink" title="1、为什么要分布式限流"></a>1、为什么要分布式限流</h3><p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825195441.png" alt></p>
<p>当应用为单点应用时，只要应用进行了限流，那么应用所依赖的各种服务也都得到了保护。</p>
<p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825222720.png" alt></p>
<p>但线上业务出于各种原因考虑，多是分布式系统，单节点的限流仅能保护自身节点，但无法保护应用依赖的各种服务，并且在进行节点扩容、缩容时也无法准确控制整个服务的请求限制。</p>
<p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190825222752.png" alt></p>
<p>而如果实现了分布式限流，那么就可以方便地控制整个服务集群的请求限制，且由于整个集群的请求数量得到了限制，因此服务依赖的各种资源也得到了限流的保护。</p>
<h3 id="2、现有方案"><a href="#2、现有方案" class="headerlink" title="2、现有方案"></a>2、现有方案</h3><p>而分布式限流常用的则有Hystrix、resilience4j、Sentinel等框架，但这些框架都需引入第三方的类库，对于国企等一些保守的企业，引入外部类库都需要经过层层审批，较为麻烦。 </p>
<h3 id="3、代码-redis-lua实现"><a href="#3、代码-redis-lua实现" class="headerlink" title="3、代码+redis+lua实现"></a>3、代码+redis+lua实现</h3><p>分布式限流本质上是一个集群并发问题，而Redis作为一个应用广泛的中间件，又拥有单进程单线程的特性，天然可以解决分布式集群的并发问题。本文简单介绍一个通过Redis实现单次请求判断限流的功能。</p>
<h5 id="1、脚本编写"><a href="#1、脚本编写" class="headerlink" title="1、脚本编写"></a>1、脚本编写</h5><p>经过上面的对比，最适合的限流算法就是令牌桶算法。而为实现限流算法，需要反复调用Redis查询与计算，一次限流判断需要多次请求较为耗时。因此我们采用编写Lua脚本运行的方式，将运算过程放在Redis端，使得对Redis进行一次请求就能完成限流的判断。 </p>
<p>令牌桶算法需要在Redis中存储桶的大小、当前令牌数量，并且实现每隔一段时间添加新的令牌。最简单的办法当然是每隔一段时间请求一次Redis，将存储的令牌数量递增。 </p>
<p>但实际上我们可以通过对限流两次请求之间的时间和令牌添加速度来计算得出上次请求之后到本次请求时，令牌桶应添加的令牌数量。因此我们在Redis中只需要存储上次请求的时间和令牌桶中的令牌数量，而桶的大小和令牌的添加速度可以通过参数传入实现动态修改。 </p>
<p>由于第一次运行脚本时默认令牌桶是满的，因此可以将数据的过期时间设置为令牌桶恢复到满所需的时间，及时释放资源。 </p>
<p>编写完成的Lua脚本如下：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">local</span> ratelimit_info = redis.<span class="built_in">pcall</span>(<span class="string">'HMGET'</span>,KEYS[<span class="number">1</span>],<span class="string">'last_time'</span>,<span class="string">'current_token'</span>)</span><br><span class="line"><span class="keyword">local</span> last_time = ratelimit_info[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">local</span> current_token = <span class="built_in">tonumber</span>(ratelimit_info[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">local</span> max_token = <span class="built_in">tonumber</span>(ARGV[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">local</span> token_rate = <span class="built_in">tonumber</span>(ARGV[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">local</span> current_time = <span class="built_in">tonumber</span>(ARGV[<span class="number">3</span>])</span><br><span class="line"><span class="keyword">local</span> reverse_time = <span class="number">1000</span>/token_rate</span><br><span class="line"><span class="keyword">if</span> current_token == <span class="literal">nil</span> <span class="keyword">then</span></span><br><span class="line">  current_token = max_token</span><br><span class="line">  last_time = current_time</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="keyword">local</span> past_time = current_time-last_time</span><br><span class="line">  <span class="keyword">local</span> reverse_token = <span class="built_in">math</span>.<span class="built_in">floor</span>(past_time/reverse_time)</span><br><span class="line">  current_token = current_token+reverse_token</span><br><span class="line">  last_time = reverse_time*reverse_token+last_time</span><br><span class="line">  <span class="keyword">if</span> current_token&gt;max_token <span class="keyword">then</span></span><br><span class="line">    current_token = max_token</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">local</span> result = <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span>(current_token&gt;<span class="number">0</span>) <span class="keyword">then</span></span><br><span class="line">  result = <span class="number">1</span></span><br><span class="line">  current_token = current_token<span class="number">-1</span></span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line">redis.call(<span class="string">'HMSET'</span>,KEYS[<span class="number">1</span>],<span class="string">'last_time'</span>,last_time,<span class="string">'current_token'</span>,current_token)</span><br><span class="line">redis.call(<span class="string">'pexpire'</span>,KEYS[<span class="number">1</span>],<span class="built_in">math</span>.<span class="built_in">ceil</span>(reverse_time*(max_token-current_token)+(current_time-last_time)))</span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h5 id="2、执行限流"><a href="#2、执行限流" class="headerlink" title="2、执行限流"></a>2、执行限流</h5><p>这里使用Spring Data Redis来进行Redis脚本的调用。</p>
<p>编写Redis脚本类:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisReteLimitScript</span> <span class="keyword">implements</span> <span class="title">RedisScript</span>&lt;<span class="title">String</span>&gt; </span>&#123; </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String SCRIPT = </span><br><span class="line">      <span class="string">"local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token') local last_time = ratelimit_info[1] local current_token = tonumber(ratelimit_info[2]) local max_token = tonumber(ARGV[1]) local token_rate = tonumber(ARGV[2]) local current_time = tonumber(ARGV[3]) local reverse_time = 1000/token_rate if current_token == nil then current_token = max_token last_time = current_time else local past_time = current_time-last_time; local reverse_token = math.floor(past_time/reverse_time) current_token = current_token+reverse_token; last_time = reverse_time*reverse_token+last_time if current_token&gt;max_token then current_token = max_token end end local result = '0' if(current_token&gt;0) then result = '1' current_token = current_token-1 end redis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_toke  redis.call('pexpire',KEYS[1],math.ceil(reverse_time*(max_tokencurrent_token)+(current_time-last_time))) return result"</span>; </span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getSha1</span><span class="params">()</span> </span>&#123; </span><br><span class="line">    <span class="keyword">return</span> DigestUtils.sha1Hex(SCRIPT); </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Class&lt;String&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> String.class; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getScriptAsString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> SCRIPT; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过RedisTemplate对象执行脚本：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">rateLimit</span><span class="params">(String key, <span class="keyword">int</span> max, <span class="keyword">int</span> rate)</span> </span>&#123;</span><br><span class="line">    List&lt;String&gt; keyList = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">1</span>);</span><br><span class="line">    keyList.add(key);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"1"</span>.equals(stringRedisTemplate</span><br><span class="line">        .execute(<span class="keyword">new</span> RedisReteLimitScript(), keyList, Integer.toString(max), Integer.toString(rate),</span><br><span class="line">            Long.toString(System.currentTimeMillis())));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>rateLimit方法传入的key为限流接口的ID，max为令牌桶的最大大小，rate为每秒钟恢复的令牌数量，返回的boolean即为此次请求是否通过了限流。为了测试Redis脚本限流是否可以正常工作，我们编写一个单元测试进行测试看看。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> RedisManager redisManager;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rateLimitTest</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">	String key = <span class="string">"test_rateLimit_key"</span>;</span><br><span class="line">	<span class="keyword">int</span> max = <span class="number">10</span>;  <span class="comment">//令牌桶大小</span></span><br><span class="line">	<span class="keyword">int</span> rate = <span class="number">10</span>; <span class="comment">//令牌每秒恢复速度</span></span><br><span class="line">	AtomicInteger successCount = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);</span><br><span class="line">	Executor executor = Executors.newFixedThreadPool(<span class="number">10</span>);</span><br><span class="line">	CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">30</span>);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">30</span>; i++) &#123;</span><br><span class="line">	  executor.execute(() -&gt; &#123;</span><br><span class="line">		<span class="keyword">boolean</span> isAllow = redisManager.rateLimit(key, max, rate);</span><br><span class="line">		<span class="keyword">if</span> (isAllow) &#123;</span><br><span class="line">		  successCount.addAndGet(<span class="number">1</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		log.info(Boolean.toString(isAllow));</span><br><span class="line">		countDownLatch.countDown();</span><br><span class="line">	  &#125;);</span><br><span class="line">	&#125;</span><br><span class="line">	countDownLatch.await();</span><br><span class="line">	log.info(<span class="string">"请求成功&#123;&#125;次"</span>, successCount.get());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>设置令牌桶大小为10，令牌桶每秒恢复10个，启动10个线程在短时间内进行30次请求，并输出每次限流查询的结果。日志输出：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[19:12:50,283]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,284]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,284]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,291]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,291]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,291]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,297]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,297]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,298]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,305]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,305]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,305]</span><span class="selector-tag">true</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,312]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,312]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,312]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,319]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,319]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,319]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,325]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,325]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,326]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,380]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,380]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,380]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,387]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,387]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,387]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,392]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,392]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,392]</span><span class="selector-tag">false</span> </span><br><span class="line"><span class="selector-attr">[19:12:50,393]</span>请求成功11次</span><br></pre></td></tr></table></figure>

<p>可以看到，在0.1秒内请求的30次请求中，除了初始的10个令牌以及随时间恢复的1个令牌外，剩下19个没有取得令牌的请求均返回了false，限流脚本正确的将超过限制的请求给判断出来了，业务中此时就可以直接返回系统繁忙或接口请求太过频繁等提示。</p>
<h5 id="3、开发中遇到的问题"><a href="#3、开发中遇到的问题" class="headerlink" title="3、开发中遇到的问题"></a>3、开发中遇到的问题</h5><p><strong>1）Lua变量格式</strong></p>
<p>Lua中的String和Number需要通过tonumber()和tostring()进行转换。</p>
<p><strong>2）Redis入参</strong></p>
<p>Redis的pexpire等命令不支持小数，但Lua的Number类型可以存放小数，因此Number类型传递给 Redis时最好通过math.ceil()等方式转换以避免存在小数导致命令失败。</p>
<p><strong>3）Time命令</strong></p>
<p>由于Redis在集群下是通过复制脚本及参数到所有节点上，因此无法在具有不确定性的命令后面执行写入命令，因此只能请求时传入时间而无法使用Redis的Time命令获取时间。</p>
<p>3.2版本之后的Redis脚本支持redis.replicate_commands()，可以改为使用Time命令获取当前时间。</p>
<p><strong>4）潜在的隐患</strong></p>
<p>由于此Lua脚本是通过请求时传入的时间做计算，因此务必保证分布式节点上获取的时间同步，如果时间不同步会导致限流无法正常运作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/mysql/sqlserver-mysql-migrate-tools/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/mysql/sqlserver-mysql-migrate-tools/" itemprop="url">各种主流 SQLServer 迁移到 MySQL 工具对比</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-15T00:00:00+08:00">
                2018-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/mysql/" itemprop="url" rel="index">
                    <span itemprop="name">mysql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<p>四种工具分别是：</p>
<p>● SQLyog（<a href="https://www.webyog.com/product/sqlyog）" target="_blank" rel="noopener">https://www.webyog.com/product/sqlyog）</a><br>● Navicat Premium（<a href="https://www.navicat.com/products/navicat-premium）" target="_blank" rel="noopener">https://www.navicat.com/products/navicat-premium）</a><br>● Mss2sql（<a href="http://www.convert-in.com/）" target="_blank" rel="noopener">http://www.convert-in.com/）</a><br>● DB2DB（<a href="http://www.szmesoft.com/DB2DB）" target="_blank" rel="noopener">http://www.szmesoft.com/DB2DB）</a></p>
<p>由于公司需要处理的是业务数据库，因此必须保证数据转换的准确率（不允许丢失数据，数据库字段、索引完整），并且需要保证数据库迁移后能立即使用。因 此在实施数据迁移前，对这几种 SQLServer 到 MySQL 的迁移工具进行一个全面测试。下面我们将基于以下需求为前提进行测试：</p>
<p>● 软件易用性<br>● 处理速度和内存占用<br>● 数据完整性<br>● 试用版限制<br>● 其它功能</p>
<h1 id="一、测试用的源数据库和系统"><a href="#一、测试用的源数据库和系统" class="headerlink" title="一、测试用的源数据库和系统"></a><strong>一、测试用的源数据库和系统</strong></h1><p>用于测试的源数据库名为 MesoftReportCenter。由于其中一个测试工具试用版限制只能处理两张数据表的原因，因此我们只选取了记录数最多的两张数据 表：HISOPChargeIntermediateResult 和 HISOPChargeItemIntermediateResult。两张数据表合计的记录数约为 328万，数据库不算大，但针对本次进行测试也基本上足够了。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201420964-1471216630-1000x719.png" alt="img">SQLServer 服务器和 MySQL 服务器分别运行在两台独立的虚拟机系统中，而所有的待测试程序都运行在 MySQL 所在的服务器上面。其中：</p>
<p>SQLServer 服务配置：</p>
<p>● 操作系统：Windows XP<br>● 内　　存：2GB<br>● 100MB 电信光纤</p>
<p>MySQL 服务配置：</p>
<p>● 操作系统：Windows XP<br>● 内　　存：1GB<br>● 100MB 电信光纤</p>
<p>同时为了测试的公平性，除 Mss2SQL 外，所有软件都是直接从官网下载最新的版本。 Mss2SQL 由于试用版的限制原因没有参与测试，而使用了网上唯一能找到的 5.3 破解版进行测试。</p>
<h1 id="二、软件易用性评测"><a href="#二、软件易用性评测" class="headerlink" title="二、软件易用性评测"></a><strong>二、软件易用性评测</strong></h1><p>软件易用性主要是指软件在导入前的配置是否容易。由于很多软件设计是面向程序员而非一般的数据库管理人员、甚至是普通的应用程序实施人员，而这一类人员很 多时候并没有数据源配置经验。因为一些使用 ODBC 或者 ADO 进行配置的程序往往会让这类用户造成困扰（主要是不知道应该选择什么类型的数据库驱动程序）。下面让我们看看四个工具的设计界面：</p>
<p>１、SQLyog</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201443089-45230517.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201450323-1147194668.png" alt="img"></p>
<p><img src="file:///E:/%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6/%E4%B8%BA%E7%9F%A5%E7%AC%94%E8%AE%B0%E6%95%B0%E6%8D%AE/temp/59245f09-eab9-4d80-bf4b-8df84010d0a3_4_files/d3e75e48-9f5c-47d8-b2dc-4f9f2bfd2a8c.png" alt="img">SQLyog 使用的是古老的 ODBC 连接，但对于新一代的程序来说，这种方式的非常的不熟悉并且不容易使用，并且必须要求本机安装好相应的数据库的 ODBC 驱动程序（SQL Server 一般自带好）。</p>
<p>２、Navicat Premium</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201457964-1270315284.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201505105-25089836.png" alt="img"></p>
<p>Navicat Premium 是四个应用工具中设计最不人性化的一个：从上图怎么也想像不到要点按那个小按钮来添加一个新的连接，并且这个连接设置不会保存，每次导入时都必须重新设 置。 Navicat Premium 使用的是比 ODBC 稍先进的 ADO 设置方式（199X年代的产物），但使用上依然是针对老一代的程序员。</p>
<p>３、Mss2sql</p>
<p>Mss2sql 是最容易在百度上搜索出来的工具，原因之一是它出现的时间较早。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201516026-223573122.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201523026-1763148340.png" alt="img"></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201530167-558836484.png" alt="img"></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201536698-2075088592.png" alt="img"></p>
<p>Mss2sql 由于是很有针对性的从 SQLServer 迁移到 MySQL，因为界面使用了操作向导设计，使用非常容易。同时在设置的过程中，有非常多的选项进行细节调整，可以感觉到软件经过了相当长一段时间的使用渐渐完善出来的。</p>
<p>４、DB2DB</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201546105-1005350006.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201552980-830484560.png" alt="img"></p>
<p>DB2DB 由于是由国人开发，因此无论是界面还是提示信息，都是全程汉字。另外，由于 DB2DB 在功能上很有针对性，因为界面设计一目了然和易使用。和 mss2sql 一样， DB2DB 提供了非常多的选项供用户进行选择和设置。</p>
<h1 id="三、处理速度和内存占用评测"><a href="#三、处理速度和内存占用评测" class="headerlink" title="三、处理速度和内存占用评测"></a><strong>三、处理速度和内存占用评测</strong></h1><p>在本评测前，本人的一位资深同事曾经从网上下载了某款迁移软件，把一个大约2500万记录数的数据表转送到阿里云 MySQL，结果经过了三天三夜（好在其中两天是星期六和星期日两个休息日）都未能迁移过来。因此这一次需要对这四个工具的处理速度作一个详细的测试。</p>
<p>考虑到从 SQL Server 迁移到 MySQL 会出现两种不同的场景：</p>
<p>● 从 SQL Server 迁移到本地 MySQL 进行代码测试和修改；<br>● 从 SQL Server 迁移到云端 MySQL 数据库正式上线使用；</p>
<p>因此我们的测试也会针对这两个场景分别进行评测，测试结果如下（记录数约为 328万）：</p>
<table>
<thead>
<tr>
<th><strong>工具名称</strong></th>
<th><strong>迁移到本地耗时</strong></th>
<th><strong>迁移到云端耗时</strong></th>
<th><strong>最高CPU占用</strong></th>
<th><strong>内存占用</strong></th>
</tr>
</thead>
<tbody><tr>
<td>SQLyog</td>
<td>2806秒</td>
<td>4438秒</td>
<td>08%</td>
<td>20MB</td>
</tr>
<tr>
<td>Navicat Premium</td>
<td>598秒</td>
<td>3166秒</td>
<td>52%</td>
<td>32MB</td>
</tr>
<tr>
<td>Mss2sql</td>
<td>726秒</td>
<td>1915秒</td>
<td>30%</td>
<td>12MB</td>
</tr>
<tr>
<td>DB2DB</td>
<td>164秒</td>
<td>1282秒</td>
<td>34%</td>
<td>40MB</td>
</tr>
</tbody></table>
<p>注：红色字体标识为胜出者。</p>
<p>以下为测试过程中的截图：<br>１、SQLyog</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201611964-1337456821.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201618026-1226073387.png" alt="img"></p>
<p>２、Navicat Premium</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201623980-436748096.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201629261-1319074770.png" alt="img"></p>
<p>注意：我们在测试 Navicat Premium 迁移到 MySQL 时发现，对于 SQL Server 的 Money 类型支持不好（不排除还有其它的数据类型支持不好）。Money 类型字段默认的小数位长度为 255，使得无法创建数据表导致整个测试无法成功，需要我们逐张表进行表结构修改才能完成测试过程。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201636308-566187496.png" alt="img">Navicat Premium 的处理速度属于中等，不算快也不算慢，但 CPU 占用还有内存占用都处于高位水平。不过以现在的电脑硬件水平来说，还是可以接受。但 CPU 占用率太高，将使得数据在导入的过程中，服务器不能用于其它用途。</p>
<p>３、Mss2sql</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201649761-1458638788.png" alt="img">Mss2sql 并没有提供计时器，因此我们使用人工计时的方法，整个过程处理完毕大于是 726 秒。Mss2sql 的 CPU 占用率相对其它工具来说较高，但仍属于可以接受的范围之内。</p>
<p>４、DB2DB</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201656776-50192047.png" alt="img"><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201703105-1920374288.png" alt="img"></p>
<p>DB2DB 同样迁移 300万数据时，仅仅使用了 2 分 44 秒，这个速度相当惊人。不过最后的结果出现一个 BUG，就是提示了转换成功，但后面的进度条却没有走完（在后面的数据完整性评测中，我们验证了数据其实是已经全部处理完毕了）。</p>
<h1 id="四、数据完整性评测"><a href="#四、数据完整性评测" class="headerlink" title="四、数据完整性评测"></a><strong>四、数据完整性评测</strong></h1><p>把数据准确无误地从 SQL Server 迁移到 MySQL 应该作为这些工具的一个基本要求，因此这里我们对四种工具转换之后的结果进行检查。<br><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201726120-1435879033.png" alt="img"><br><img src="file:///E:/%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6/%E4%B8%BA%E7%9F%A5%E7%AC%94%E8%AE%B0%E6%95%B0%E6%8D%AE/temp/59245f09-eab9-4d80-bf4b-8df84010d0a3_4_files/b1d137d1-6ce3-4d4e-aed3-39a7173e3a88.png" alt="img"><br>我们通过后台 SQL 对记录数进行检查，发现所有的工具都能把记录完整地迁移到新的数据库。如果仔细观察，可以发现上图中各个数据库的大小是不一致的，基本的判断是由于各种工 具在映射数据表字段时，字段长度取值可能不能而引起的。而 mesoftreportcenter2 数据库大小比起其它数据库差不多少了一半，这引起了我们的注意。通过分析，我们发现 Navicat Premium 在迁移数据库时，并不会为该数据库所有数据表创建索引和主键，缺少索引和主键的数据库大小显然比其它数据库要少得多。<br><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201759386-1443609950.png" alt="img"><br>为了解各工具迁移后的数据库能否立即应用于生产环境，我们对创建后的数据表进行了更深入的分析，发现各工具对字段默认值的支持程度也不尽相同。其中：</p>
<p>● SQLyog：完整支持 SQL Server 的默认值；<br>● Navicat Premium：完全不支持默认值，所有迁移后的数据表都没有默认值；<br>● Mss2sql：支持默认值但有严重错误；<br>● DB2DB：完整支持 SQL Server 的默认值。</p>
<p>Mss2sql 的默认值有一个严重的错误，在 SQL Server 中字段默认值为空字符串 ”，但迁移之后变成两个 ” 符号。Mss2sql 这个严重的错误会使得程序在正式环境运行后，数据库会产生错误的数据！<br><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/163208-20160822201815964-1392384812.png" alt="img"><br>在一些老旧的系统中，数据库还会存在 Text、二进制类型的字段数据，通过测试对比后，四种工具都完美支持 Text 和 二进制（Image）类型字段。</p>
<p>小结：</p>
<table>
<thead>
<tr>
<th><strong>测试项目</strong></th>
<th><strong>SQLyog</strong></th>
<th><strong>Navicat Premium</strong></th>
<th><strong>Mss2sql</strong></th>
<th><strong>DB2DB</strong></th>
</tr>
</thead>
<tbody><tr>
<td>表结构</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>字段长度</td>
<td>支持</td>
<td>部分支持（对Money等支持不好）</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>数据</td>
<td>完整</td>
<td>完整</td>
<td>完整</td>
<td>完整</td>
</tr>
<tr>
<td>索引</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>关键字</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>默认值</td>
<td>支持</td>
<td>不支持</td>
<td>支持，但有严重错误</td>
<td>支持</td>
</tr>
<tr>
<td>二进制数据</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody></table>
<h1 id="五、各工具其它功能及试用版限制"><a href="#五、各工具其它功能及试用版限制" class="headerlink" title="五、各工具其它功能及试用版限制"></a><strong>五、各工具其它功能及试用版限制</strong></h1><p>估计由于数据库同步会存在一些技术难题的原因，4 款工具目前都是只是提供试用版本，最后我们来看看四个工具的试用版本各自的限制是什么：</p>
<table>
<thead>
<tr>
<th><strong>工具名</strong></th>
<th><strong>价格</strong></th>
<th><strong>试用限制</strong></th>
<th><strong>其它功能</strong></th>
<th><strong>备注</strong></th>
</tr>
</thead>
<tbody><tr>
<td>SQyog</td>
<td>$199</td>
<td>30天试用，并且只允许转换两张数据表</td>
<td>无</td>
<td></td>
</tr>
<tr>
<td>Navicat Premium</td>
<td>$799</td>
<td></td>
<td>无</td>
<td></td>
</tr>
<tr>
<td>Mss2sql</td>
<td>$49</td>
<td>每张数据表只允许有50秒处理时间</td>
<td>支持导出为 SQL</td>
<td></td>
</tr>
<tr>
<td>DB2DB</td>
<td>￥199</td>
<td>10万记录限制</td>
<td>支持导出为 SQL</td>
<td></td>
</tr>
</tbody></table>
<p>四种工具中，由于 SQLyog 和 Navicat Premium 提供了额外的管理功能，所以价格相比其它两款工具的要高得多。特别是 Navicat，必须是 Premium 版本才提供数据转换的功能。而 Mss2sql 最新版本的试用版只提供了 50 秒处理时间，因为实用价值不大。而笔者与 DB2DB 作者联系时得知，DB2DB 设置 10万记录限制，主要是考虑国内很多小型软件记录数都是少于 10 万笔，而这一类人群一般都是小型创业团队。</p>
<h1 id="六、评测总结"><a href="#六、评测总结" class="headerlink" title="六、评测总结"></a><strong>六、评测总结</strong></h1><p>最后，对四款软件的测试结果作一个整体的总结：</p>
<table>
<thead>
<tr>
<th><strong>工具名</strong></th>
<th><strong>处理速度</strong></th>
<th><strong>数据完整性</strong></th>
<th><strong>价格</strong></th>
<th><strong>推荐度</strong></th>
</tr>
</thead>
<tbody><tr>
<td>SQLyog</td>
<td>★☆☆☆☆</td>
<td>★★★★★</td>
<td>★★☆☆☆</td>
<td>★★☆☆☆</td>
</tr>
<tr>
<td>Navicat Premium</td>
<td>★★★☆☆</td>
<td>★☆☆☆☆</td>
<td>★☆☆☆☆</td>
<td>★☆☆☆☆</td>
</tr>
<tr>
<td>Mss2sql</td>
<td>★★☆☆☆</td>
<td>★★★☆☆</td>
<td>★★★★☆</td>
<td>★★★☆☆</td>
</tr>
<tr>
<td>DB2DB</td>
<td>★★★★★</td>
<td>★★★★★</td>
<td>★★★★★</td>
<td>★★★★★</td>
</tr>
</tbody></table>
<p>以上四款软件中，最不推荐使用的是 Navicat Premium，主要原因是数据的完整性表现较差，转换后的数据不能立即用于生产环境，需要程序员仔细自行查找原因和分析。而 SQLyog 有较好的数据完整性，但整体处理速度非常的慢，如果数据较大的情况下，需要浪费非常多宝贵的时间。比较推荐的是 DB2DB，软件整体表现较好，对我来说最重要的是在不购买的情况下也够用了，而且全中文的傻瓜式界面操作起来实在方便。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/java/concurrent-program/4-3-线程间通信/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/java/concurrent-program/4-3-线程间通信/" itemprop="url">4.3、线程间通信</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-01T00:00:00+08:00">
                2018-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index">
                    <span itemprop="name">java</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/java-concurrent-program/" itemprop="url" rel="index">
                    <span itemprop="name">java-concurrent-program</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="4-3、线程间通信"><a href="#4-3、线程间通信" class="headerlink" title="4.3、线程间通信"></a>4.3、线程间通信</h1><p>通信方式：共享内存、消息传递</p>
<h3 id="4-3-1、volatile-和-synchronized关键字"><a href="#4-3-1、volatile-和-synchronized关键字" class="headerlink" title="4.3.1、volatile 和 synchronized关键字"></a>4.3.1、volatile 和 synchronized关键字</h3><ul>
<li><p>volatile修饰变量 —— 禁止指令重排 和 变量对 所有线程的可见；</p>
</li>
<li><p>synchronized修饰方法或同步块 —— 同一时刻，只有一个线程处于方法或同步块中，保证 线程对变量访问的可见性 和 排他性；synchronized本身没有禁止指令重排的功能，需要配合volatile使用；</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/distribution/distributed-computing/distributed-computing-docs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/distribution/distributed-computing/distributed-computing-docs/" itemprop="url">分布式计算文档汇总</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T00:00:00+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/distribution/" itemprop="url" rel="index">
                    <span itemprop="name">distribution</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/distribution/distributed-computing/" itemprop="url" rel="index">
                    <span itemprop="name">distributed-computing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<p>分布式计算总站：<a href="https://equn.com/wiki/首页" target="_blank" rel="noopener">https://equn.com/wiki/%E9%A6%96%E9%A1%B5</a></p>
<p>知乎·分布式计算：<a href="https://www.zhihu.com/topic/19552071/hot" target="_blank" rel="noopener">https://www.zhihu.com/topic/19552071/hot</a></p>
<h1 id="批计算"><a href="#批计算" class="headerlink" title="批计算"></a>批计算</h1><p>MapReduce 是一种 <u><strong>分而治之</strong></u> 的计算模式，在分布式领域中，除了典型的 Hadoop 的 MapReduce(Google MapReduce 的开源实现)，还有 Fork-Join，Fork-Join 是 Java 等语言或库提供的原生多线程并行处理框架，采用线程级的分而治之计算模式。它充分利用多核 CPU 的优势，以递归的方式把一个任务拆分成多个“小任务”，把多个“小任务”放到多个处理器上并行执行，即 Fork 操作。当多个“小任务”执行完成之后，再将这些执行结果合并起来即可得到原始任务的结果，即 Join 操作。</p>
<p>虽然 MapReduce 是进程级的分而治之计算模式，但与 Fork-Join 的核心思想是一致的。因此，Fork-Join 又被称为 Java 版的 MapReduce 框架。但，MapReduce 和 Fork-Join 之间有一个本质的区别：Fork-Join 不能大规模扩展，只适用于在单个 Java 虚拟机上运行，多个小任务虽然运行在不同的处理器上，但可以相互通信，甚至一个线程可以“窃取”其他线程上的子任务。</p>
<p>MapReduce 可以大规模扩展，适用于大型计算机集群。通过 MapReduce 拆分后的任务，可以跨多个计算机去执行，且各个小任务之间不会相互通信。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708160534.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708160729.png" alt></p>
<p>MapReduce 模式的核心思想是，将大任务拆分成多个小任务，针对这些小任务分别计算后，再合并各小任务的结果以得到大任务的计算结果，任务运行完成后整个任务进程就结束了，属于短任务模式。但任务进程的启动和停止是一件很耗时的事儿，因此 MapReduce 对处理实时性的任务就不太合适了</p>
<h1 id="流计算"><a href="#流计算" class="headerlink" title="流计算"></a>流计算</h1><p>实时性任务主要是针对流数据的处理，对处理时延要求很高，通常需要有常驻服务进程，等待数据的随时到来随时处理，以保证低时延。处理流数据任务的计算模式，在分布式领域中叫作 Stream。近年来，由于网络监控、传感监测、AR/VR 等实时性应用的兴起，一类需要处理流数据的业务发展了起来。比如各种直播平台中，我们需要处理直播产生的音视频数据流等。这种如流水般持续涌现，且需要实时处理的数据，我们称之为流数据。</p>
<p>总结来讲，流数据的特征主要包括以下 4 点：</p>
<ul>
<li>数据如流水般持续、快速地到达；</li>
<li>海量数据规模，数据量可达到 TB 级甚至 PB 级；</li>
<li>对实时性要求高，随着时间流逝，数据的价值会大幅降低；</li>
<li>数据顺序无法保证，系统无法控制将要处理的数据元素的顺序。</li>
</ul>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708160931.png" alt></p>
<p>第一步，提交流式计算作业</p>
<p>第二步，加载流式数据进行流计算</p>
<p>第三步，持续输出计算结果。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708161010.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/flink/flink-version-changelog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/flink/flink-version-changelog/" itemprop="url">Flink 各版本 changelog</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-11T00:00:00+08:00">
                2018-06-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/flink/" itemprop="url" rel="index">
                    <span itemprop="name">flink</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<blockquote>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/</a></p>
</blockquote>
<h1 id="Flink-1-10"><a href="#Flink-1-10" class="headerlink" title="Flink 1.10"></a>Flink 1.10</h1><h3 id="Clusters-amp-Deployment"><a href="#Clusters-amp-Deployment" class="headerlink" title="Clusters &amp; Deployment"></a>Clusters &amp; Deployment</h3><h4 id="FileSystems-should-be-loaded-via-Plugin-Architecture-FLINK-11956"><a href="#FileSystems-should-be-loaded-via-Plugin-Architecture-FLINK-11956" class="headerlink" title="FileSystems should be loaded via Plugin Architecture (FLINK-11956)"></a>FileSystems should be loaded via Plugin Architecture (<a href="https://issues.apache.org/jira/browse/FLINK-11956" target="_blank" rel="noopener">FLINK-11956</a>)</h4><p>s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/filesystems/#pluggable-file-systems" target="_blank" rel="noopener">plugins</a> but now seamlessly integrate with all credential providers. Other filesystems are strongly recommended to be only used as plugins as we will continue to remove relocations.</p>
<h4 id="Flink-Client-respects-Classloading-Policy-FLINK-13749"><a href="#Flink-Client-respects-Classloading-Policy-FLINK-13749" class="headerlink" title="Flink Client respects Classloading Policy (FLINK-13749)"></a>Flink Client respects Classloading Policy (<a href="https://issues.apache.org/jira/browse/FLINK-13749" target="_blank" rel="noopener">FLINK-13749</a>)</h4><p>The Flink client now also respects the configured classloading policy, i.e., <code>parent-first</code> or <code>child-first</code> classloading. Previously, only cluster components such as the job manager or task manager supported this setting. This does mean that users might get different behaviour in their programs, in which case they should configure the classloading policy explicitly to use <code>parent-first</code> classloading, which was the previous (hard-coded) behaviour.</p>
<h4 id="Enable-spreading-out-Tasks-evenly-across-all-TaskManagers-FLINK-12122"><a href="#Enable-spreading-out-Tasks-evenly-across-all-TaskManagers-FLINK-12122" class="headerlink" title="Enable spreading out Tasks evenly across all TaskManagers (FLINK-12122)"></a>Enable spreading out Tasks evenly across all TaskManagers (<a href="https://issues.apache.org/jira/browse/FLINK-12122" target="_blank" rel="noopener">FLINK-12122</a>)</h4><p>When <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6</a> was rolled out with Flink 1.5.0, we changed how slots are allocated from TaskManagers (TMs). Instead of evenly allocating the slots from all registered TMs, we had the tendency to exhaust a TM before using another one. To use a scheduling strategy that is more similar to the pre-FLIP-6 behaviour, where Flink tries to spread out the workload across all currently available TMs, one can set <code>cluster.evenly-spread-out-slots: true</code> in the <code>flink-conf.yaml</code>.</p>
<h4 id="Directory-Structure-Change-for-highly-available-Artifacts-FLINK-13633"><a href="#Directory-Structure-Change-for-highly-available-Artifacts-FLINK-13633" class="headerlink" title="Directory Structure Change for highly available Artifacts (FLINK-13633)"></a>Directory Structure Change for highly available Artifacts (<a href="https://issues.apache.org/jira/browse/FLINK-13633" target="_blank" rel="noopener">FLINK-13633</a>)</h4><p>All highly available artifacts stored by Flink will now be stored under <code>HA_STORAGE_DIR/HA_CLUSTER_ID</code> with <code>HA_STORAGE_DIR</code> configured by <code>high-availability.storageDir</code> and <code>HA_CLUSTER_ID</code> configured by <code>high-availability.cluster-id</code>.</p>
<h4 id="Resources-and-JARs-shipped-via-–yarnship-will-be-ordered-in-the-Classpath-FLINK-13127"><a href="#Resources-and-JARs-shipped-via-–yarnship-will-be-ordered-in-the-Classpath-FLINK-13127" class="headerlink" title="Resources and JARs shipped via –yarnship will be ordered in the Classpath (FLINK-13127)"></a>Resources and JARs shipped via –yarnship will be ordered in the Classpath (<a href="https://issues.apache.org/jira/browse/FLINK-13127" target="_blank" rel="noopener">FLINK-13127</a>)</h4><p>When using the <code>--yarnship</code> command line option, resource directories and jar files will be added to the classpath in lexicographical order with resources directories appearing first.</p>
<h4 id="Removal-of-–yn-–yarncontainer-Command-Line-Options-FLINK-12362"><a href="#Removal-of-–yn-–yarncontainer-Command-Line-Options-FLINK-12362" class="headerlink" title="Removal of –yn/–yarncontainer Command Line Options (FLINK-12362)"></a>Removal of –yn/–yarncontainer Command Line Options (<a href="https://issues.apache.org/jira/browse/FLINK-12362" target="_blank" rel="noopener">FLINK-12362</a>)</h4><p>The Flink CLI no longer supports the deprecated command line options <code>-yn/--yarncontainer</code>, which were used to specify the number of containers to start on YARN. This option has been deprecated since the introduction of <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6</a>. All Flink users are advised to remove this command line option.</p>
<h4 id="Removal-of-–yst-–yarnstreaming-Command-Line-Options-FLINK-14957"><a href="#Removal-of-–yst-–yarnstreaming-Command-Line-Options-FLINK-14957" class="headerlink" title="Removal of –yst/–yarnstreaming Command Line Options (FLINK-14957)"></a>Removal of –yst/–yarnstreaming Command Line Options (<a href="https://issues.apache.org/jira/browse/FLINK-14957" target="_blank" rel="noopener">FLINK-14957</a>)</h4><p>The Flink CLI no longer supports the deprecated command line options <code>-yst/--yarnstreaming</code>, which were used to disable eager pre-allocation of memory. All Flink users are advised to remove this command line option.</p>
<h4 id="Mesos-Integration-will-reject-expired-Offers-faster-FLINK-14029"><a href="#Mesos-Integration-will-reject-expired-Offers-faster-FLINK-14029" class="headerlink" title="Mesos Integration will reject expired Offers faster (FLINK-14029)"></a>Mesos Integration will reject expired Offers faster (<a href="https://issues.apache.org/jira/browse/FLINK-14029" target="_blank" rel="noopener">FLINK-14029</a>)</h4><p>Flink’s Mesos integration now rejects all expired offers instead of only 4. This improves the situation where Fenzo holds on to a lot of expired offers without giving them back to the Mesos resource manager.</p>
<h4 id="Scheduler-Rearchitecture-FLINK-14651"><a href="#Scheduler-Rearchitecture-FLINK-14651" class="headerlink" title="Scheduler Rearchitecture (FLINK-14651)"></a>Scheduler Rearchitecture (<a href="https://issues.apache.org/jira/browse/FLINK-14651" target="_blank" rel="noopener">FLINK-14651</a>)</h4><p>Flink’s scheduler was refactored with the goal of making scheduling strategies customizable in the future. Using the legacy scheduler is discouraged as it will be removed in a future release. However, users that experience issues related to scheduling can fallback to the legacy scheduler by setting <code>jobmanager.scheduler</code> to <code>legacy</code> in their <code>flink-conf.yaml</code> for the time being. Note, however, that using the legacy scheduler with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/task_failure_recovery.html#restart-pipelined-region-failover-strategy" target="_blank" rel="noopener">Pipelined Region Failover Strategy</a> enabled has the following caveats:</p>
<ul>
<li>Exceptions that caused a job to restart will not be shown on the job overview page of the Web UI (<a href="https://issues.apache.org/jira/browse/FLINK-15917" target="_blank" rel="noopener">FLINK-15917</a>). However, exceptions that cause a job to fail (e.g., when all restart attempts exhausted) will still be shown.</li>
<li>The <code>uptime</code> metric will not be reset after restarting a job due to task failure (<a href="https://issues.apache.org/jira/browse/FLINK-15918" target="_blank" rel="noopener">FLINK-15918</a>).</li>
</ul>
<p>Note that in the default <code>flink-conf.yaml</code>, the Pipelined Region Failover Strategy is already enabled. That is, users that want to use the legacy scheduler and cannot accept aforementioned caveats should make sure that <code>jobmanager.execution.failover-strategy</code> is set to <code>full</code> or not set at all.</p>
<h4 id="Java-11-Support-FLINK-10725"><a href="#Java-11-Support-FLINK-10725" class="headerlink" title="Java 11 Support (FLINK-10725)"></a>Java 11 Support (<a href="https://issues.apache.org/jira/browse/FLINK-10725" target="_blank" rel="noopener">FLINK-10725</a>)</h4><p>Beginning from this release, Flink can be compiled and run with Java 11. All Java 8 artifacts can be also used with Java 11. This means that users that want to run Flink with Java 11 do not have to compile Flink themselves.</p>
<p>When starting Flink with Java 11, the following warnings may be logged:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by org.apache.flink.core.memory.MemoryUtils (file:/opt/flink/flink-1.10.0/lib/flink-dist_2.11-1.10.0.jar) to constructor java.nio.DirectByteBuffer(long,int)</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of org.apache.flink.core.memory.MemoryUtils</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br><span class="line"></span><br><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/home/flinkuser/.m2/repository/org/apache/flink/flink-core/1.10.0/flink-core-1.10.0.jar) to field java.lang.String.value</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br><span class="line"></span><br><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/home/flinkuser/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar) to method java.nio.DirectByteBuffer.cleaner()</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br><span class="line"></span><br><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by com.esotericsoftware.kryo.util.UnsafeUtil (file:/home/flinkuser/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar) to constructor java.nio.DirectByteBuffer(long,int,java.lang.Object)</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of com.esotericsoftware.kryo.util.UnsafeUtil</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br></pre></td></tr></table></figure>

<p>These warnings are considered harmless and will be addressed in future Flink releases.</p>
<p>Lastly, note that the connectors for Cassandra, Hive, HBase, and Kafka 0.8–0.11 have not been tested with Java 11 because the respective projects did not provide Java 11 support at the time of the Flink 1.10.0 release.</p>
<h3 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h3><h4 id="New-Task-Executor-Memory-Model-FLINK-13980"><a href="#New-Task-Executor-Memory-Model-FLINK-13980" class="headerlink" title="New Task Executor Memory Model (FLINK-13980)"></a>New Task Executor Memory Model (<a href="https://issues.apache.org/jira/browse/FLINK-13980" target="_blank" rel="noopener">FLINK-13980</a>)</h4><p>With <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors" target="_blank" rel="noopener">FLIP-49</a>, a new memory model has been introduced for the task executor. New configuration options have been introduced to control the memory consumption of the task executor process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration. The memory model of the job manager process has not been changed yet but it is planned to be updated as well.</p>
<p>If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes.</p>
<p>Please, check <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_setup.html" target="_blank" rel="noopener">the user documentation</a> for more details.</p>
<h5 id="Deprecation-and-breaking-changes"><a href="#Deprecation-and-breaking-changes" class="headerlink" title="Deprecation and breaking changes"></a>Deprecation and breaking changes</h5><p>The following options have been removed and have no effect anymore:</p>
<table>
<thead>
<tr>
<th align="left">Deprecated/removed config option</th>
<th align="left">Note</th>
</tr>
</thead>
<tbody><tr>
<td align="left">taskmanager.memory.fraction</td>
<td align="left">Check also the description of the new option <code>taskmanager.memory.managed.fraction</code> but it has different semantics and the value of the deprecated option usually has to be adjusted</td>
</tr>
<tr>
<td align="left">taskmanager.memory.off-heap</td>
<td align="left">Support for on-heap managed memory has been removed, leaving off-heap managed memory as the only possibility</td>
</tr>
<tr>
<td align="left">taskmanager.memory.preallocate</td>
<td align="left">Pre-allocation is no longer supported, and managed memory is always allocated lazily</td>
</tr>
</tbody></table>
<p>The following options, if used, are interpreted as other new options in order to maintain backwards compatibility where it makes sense:</p>
<table>
<thead>
<tr>
<th align="left">Deprecated config option</th>
<th align="left">Interpreted as</th>
</tr>
</thead>
<tbody><tr>
<td align="left">taskmanager.heap.size</td>
<td align="left">taskmanager.memory.flink.size for standalone deploymenttaskmanager.memory.process.size for containerized deployments</td>
</tr>
<tr>
<td align="left">taskmanager.memory.size</td>
<td align="left">taskmanager.memory.managed.size</td>
</tr>
<tr>
<td align="left">taskmanager.network.memory.min</td>
<td align="left">taskmanager.memory.network.min</td>
</tr>
<tr>
<td align="left">taskmanager.network.memory.max</td>
<td align="left">taskmanager.memory.network.max</td>
</tr>
<tr>
<td align="left">taskmanager.network.memory.fraction</td>
<td align="left">taskmanager.memory.network.fraction</td>
</tr>
</tbody></table>
<p>The container cut-off configuration options, <code>containerized.heap-cutoff-ratio</code> and <code>containerized.heap-cutoff-min</code>, have no effect for task executor processes anymore but they still have the same semantics for the JobManager process.</p>
<h4 id="RocksDB-State-Backend-Memory-Control-FLINK-7289"><a href="#RocksDB-State-Backend-Memory-Control-FLINK-7289" class="headerlink" title="RocksDB State Backend Memory Control (FLINK-7289)"></a>RocksDB State Backend Memory Control (<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="noopener">FLINK-7289</a>)</h4><p>Together with the introduction of the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/flink-1.10.html#new-task-executor-memory-model-flink-13980" target="_blank" rel="noopener">new Task Executor Memory Model</a>, the memory consumption of the RocksDB state backend will be limited by the total amount of Flink Managed Memory, which can be configured via <code>taskmanager.memory.managed.size</code> or <code>taskmanager.memory.managed.fraction</code>. Furthermore, users can tune RocksDB’s write/read memory ratio (<code>state.backend.rocksdb.memory.write-buffer-ratio</code>, by default <code>0.5</code>) and the reserved memory fraction for indices/filters (<code>state.backend.rocksdb.memory.high-prio-pool-ratio</code>, by default <code>0.1</code>). More details and advanced configuration options can be found in the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/state/large_state_tuning.html#tuning-rocksdb-memory" target="_blank" rel="noopener">Flink user documentation</a>.</p>
<h4 id="Fine-grained-Operator-Resource-Management-FLINK-14058"><a href="#Fine-grained-Operator-Resource-Management-FLINK-14058" class="headerlink" title="Fine-grained Operator Resource Management (FLINK-14058)"></a>Fine-grained Operator Resource Management (<a href="https://issues.apache.org/jira/browse/FLINK-14058" target="_blank" rel="noopener">FLINK-14058</a>)</h4><p>Config options <code>table.exec.resource.external-buffer-memory</code>, <code>table.exec.resource.hash-agg.memory</code>, <code>table.exec.resource.hash-join.memory</code>, and <code>table.exec.resource.sort.memory</code> have been deprecated. Beginning from Flink 1.10, these config options are interpreted as weight hints instead of absolute memory requirements. Flink choses sensible default weight hints which should not be adjustment by users.</p>
<h3 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h3><h4 id="Rename-of-ANY-Type-to-RAW-Type-FLINK-14904"><a href="#Rename-of-ANY-Type-to-RAW-Type-FLINK-14904" class="headerlink" title="Rename of ANY Type to RAW Type (FLINK-14904)"></a>Rename of ANY Type to RAW Type (<a href="https://issues.apache.org/jira/browse/FLINK-14904" target="_blank" rel="noopener">FLINK-14904</a>)</h4><p>The identifier <code>raw</code> is a reserved keyword now and must be escaped with backticks when used as a SQL field or function name.</p>
<h4 id="Rename-of-Table-Connector-Properties-FLINK-14649"><a href="#Rename-of-Table-Connector-Properties-FLINK-14649" class="headerlink" title="Rename of Table Connector Properties (FLINK-14649)"></a>Rename of Table Connector Properties (<a href="https://issues.apache.org/jira/browse/FLINK-14649" target="_blank" rel="noopener">FLINK-14649</a>)</h4><p>Some indexed properties for table connectors have been flattened and renamed for a better user experience when writing DDL statements. This affects the Kafka Connector properties <code>connector.properties</code> and <code>connector.specific-offsets</code>. Furthermore, the Elasticsearch Connector property <code>connector.hosts</code> is affected. The aforementioned, old properties are deprecated and will be removed in future versions. Please consult the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/connect.html#table-connectors" target="_blank" rel="noopener">Table Connectors documentation</a> for the new property names.</p>
<h4 id="Methods-for-interacting-with-temporary-Tables-amp-Views-FLINK-14490"><a href="#Methods-for-interacting-with-temporary-Tables-amp-Views-FLINK-14490" class="headerlink" title="Methods for interacting with temporary Tables &amp; Views (FLINK-14490)"></a>Methods for interacting with temporary Tables &amp; Views (<a href="https://issues.apache.org/jira/browse/FLINK-14490" target="_blank" rel="noopener">FLINK-14490</a>)</h4><p>Methods <code>registerTable()</code>/<code>registerDataStream()</code>/<code>registerDataSet()</code> have been deprecated in favor of <code>createTemporaryView()</code>, which better adheres to the corresponding SQL term.</p>
<p>The <code>scan()</code> method has been deprecated in favor of the <code>from()</code> method.</p>
<p>Methods <code>registerTableSource()</code>/<code>registerTableSink()</code> become deprecated in favor of <code>ConnectTableDescriptor#createTemporaryTable()</code>. The <code>ConnectTableDescriptor</code> approach expects only a set of string properties as a description of a TableSource or TableSink instead of an instance of a class in case of the deprecated methods. This in return makes it possible to reliably store those definitions in catalogs.</p>
<p>Method <code>insertInto(String path, String... pathContinued)</code> has been removed in favor of in <code>insertInto(String path)</code>.</p>
<p>All the newly introduced methods accept a String identifier which will be parsed into a 3-part identifier. The parser supports quoting the identifier. It also requires escaping any reserved SQL keywords.</p>
<h4 id="Removal-of-ExternalCatalog-API-FLINK-13697"><a href="#Removal-of-ExternalCatalog-API-FLINK-13697" class="headerlink" title="Removal of ExternalCatalog API (FLINK-13697)"></a>Removal of ExternalCatalog API (<a href="https://issues.apache.org/jira/browse/FLINK-13697" target="_blank" rel="noopener">FLINK-13697</a>)</h4><p>The deprecated <code>ExternalCatalog</code> API has been dropped. This includes:</p>
<ul>
<li><code>ExternalCatalog</code> (and all dependent classes, e.g., <code>ExternalTable</code>)</li>
<li><code>SchematicDescriptor</code>, <code>MetadataDescriptor</code>, <code>StatisticsDescriptor</code></li>
</ul>
<p>Users are advised to use the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/catalogs.html#catalog-api" target="_blank" rel="noopener">new Catalog API</a>.</p>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><h4 id="Introduction-of-Type-Information-for-ConfigOptions-FLINK-14493"><a href="#Introduction-of-Type-Information-for-ConfigOptions-FLINK-14493" class="headerlink" title="Introduction of Type Information for ConfigOptions (FLINK-14493)"></a>Introduction of Type Information for ConfigOptions (<a href="https://issues.apache.org/jira/browse/FLINK-14493" target="_blank" rel="noopener">FLINK-14493</a>)</h4><p>Getters of <code>org.apache.flink.configuration.Configuration</code> throw <code>IllegalArgumentException</code> now if the configured value cannot be parsed into the required type. In previous Flink releases the default value was returned in such cases.</p>
<h4 id="Increase-of-default-Restart-Delay-FLINK-13884"><a href="#Increase-of-default-Restart-Delay-FLINK-13884" class="headerlink" title="Increase of default Restart Delay (FLINK-13884)"></a>Increase of default Restart Delay (<a href="https://issues.apache.org/jira/browse/FLINK-13884" target="_blank" rel="noopener">FLINK-13884</a>)</h4><p>The default restart delay for all shipped restart strategies, i.e., <code>fixed-delay</code> and <code>failure-rate</code>, has been raised to 1 s (from originally 0 s).</p>
<h4 id="Simplification-of-Cluster-Level-Restart-Strategy-Configuration-FLINK-13921"><a href="#Simplification-of-Cluster-Level-Restart-Strategy-Configuration-FLINK-13921" class="headerlink" title="Simplification of Cluster-Level Restart Strategy Configuration (FLINK-13921)"></a>Simplification of Cluster-Level Restart Strategy Configuration (<a href="https://issues.apache.org/jira/browse/FLINK-13921" target="_blank" rel="noopener">FLINK-13921</a>)</h4><p>Previously, if the user had set <code>restart-strategy.fixed-delay.attempts</code> or <code>restart-strategy.fixed-delay.delay</code> but had not configured the option <code>restart-strategy</code>, the cluster-level restart strategy would have been <code>fixed-delay</code>. Now the cluster-level restart strategy is only determined by the config option <code>restart-strategy</code> and whether checkpointing is enabled. See <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/task_failure_recovery.html" target="_blank" rel="noopener"><em>“Task Failure Recovery”</em></a> for details.</p>
<h4 id="Disable-memory-mapped-BoundedBlockingSubpartition-by-default-FLINK-14952"><a href="#Disable-memory-mapped-BoundedBlockingSubpartition-by-default-FLINK-14952" class="headerlink" title="Disable memory-mapped BoundedBlockingSubpartition by default (FLINK-14952)"></a>Disable memory-mapped BoundedBlockingSubpartition by default (<a href="https://issues.apache.org/jira/browse/FLINK-14952" target="_blank" rel="noopener">FLINK-14952</a>)</h4><p>The config option <code>taskmanager.network.bounded-blocking-subpartition-type</code> has been renamed to <code>taskmanager.network.blocking-shuffle.type</code>. Moreover, the default value of the aforementioned config option has been changed from <code>auto</code> to <code>file</code>. The reason is that TaskManagers running on YARN with <code>auto</code>, could easily exceed the memory budget of their container, due to incorrectly accounted memory-mapped files memory usage.</p>
<h4 id="Removal-of-non-credit-based-Network-Flow-Control-FLINK-14516"><a href="#Removal-of-non-credit-based-Network-Flow-Control-FLINK-14516" class="headerlink" title="Removal of non-credit-based Network Flow Control (FLINK-14516)"></a>Removal of non-credit-based Network Flow Control (<a href="https://issues.apache.org/jira/browse/FLINK-14516" target="_blank" rel="noopener">FLINK-14516</a>)</h4><p>The non-credit-based network flow control code was removed alongside of the configuration option <code>taskmanager.network.credit-model</code>. Flink will now always use credit-based flow control.</p>
<h4 id="Removal-of-HighAvailabilityOptions-HA-JOB-DELAY-FLINK-13885"><a href="#Removal-of-HighAvailabilityOptions-HA-JOB-DELAY-FLINK-13885" class="headerlink" title="Removal of HighAvailabilityOptions#HA_JOB_DELAY (FLINK-13885)"></a>Removal of HighAvailabilityOptions#HA_JOB_DELAY (<a href="https://issues.apache.org/jira/browse/FLINK-13885" target="_blank" rel="noopener">FLINK-13885</a>)</h4><p>The configuration option <code>high-availability.job.delay</code> has been removed since it is no longer used.</p>
<h3 id="State"><a href="#State" class="headerlink" title="State"></a>State</h3><h4 id="Enable-Background-Cleanup-of-State-with-TTL-by-default-FLINK-14898"><a href="#Enable-Background-Cleanup-of-State-with-TTL-by-default-FLINK-14898" class="headerlink" title="Enable Background Cleanup of State with TTL by default (FLINK-14898)"></a>Enable Background Cleanup of State with TTL by default (<a href="https://issues.apache.org/jira/browse/FLINK-14898" target="_blank" rel="noopener">FLINK-14898</a>)</h4><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/state/state.html#cleanup-of-expired-state" target="_blank" rel="noopener">Background cleanup of expired state with TTL</a> is activated by default now for all state backends shipped with Flink. Note that the RocksDB state backend implements background cleanup by employing a compaction filter. This has the caveat that even if a Flink job does not store state with TTL, a minor performance penalty during compaction is incurred. Users that experience noticeable performance degradation during RocksDB compaction can disable the TTL compaction filter by setting the config option <code>state.backend.rocksdb.ttl.compaction.filter.enabled</code> to <code>false</code>.</p>
<h4 id="Deprecation-of-StateTtlConfig-Builder-cleanupInBackground-FLINK-15606"><a href="#Deprecation-of-StateTtlConfig-Builder-cleanupInBackground-FLINK-15606" class="headerlink" title="Deprecation of StateTtlConfig#Builder#cleanupInBackground() (FLINK-15606)"></a>Deprecation of StateTtlConfig#Builder#cleanupInBackground() (<a href="https://issues.apache.org/jira/browse/FLINK-15606" target="_blank" rel="noopener">FLINK-15606</a>)</h4><p><code>StateTtlConfig#Builder#cleanupInBackground()</code> has been deprecated because the background cleanup of state with TTL is already enabled by default.</p>
<h4 id="Timers-are-stored-in-RocksDB-by-default-when-using-RocksDBStateBackend-FLINK-15637"><a href="#Timers-are-stored-in-RocksDB-by-default-when-using-RocksDBStateBackend-FLINK-15637" class="headerlink" title="Timers are stored in RocksDB by default when using RocksDBStateBackend (FLINK-15637)"></a>Timers are stored in RocksDB by default when using RocksDBStateBackend (<a href="https://issues.apache.org/jira/browse/FLINK-15637" target="_blank" rel="noopener">FLINK-15637</a>)</h4><p>The default timer store has been changed from Heap to RocksDB for the RocksDB state backend to support asynchronous snapshots for timer state and better scalability, with less than 5% performance cost. Users that find the performance decline critical can set <code>state.backend.rocksdb.timer-service.factory</code> to <code>HEAP</code> in <code>flink-conf.yaml</code> to restore the old behavior.</p>
<h4 id="Removal-of-StateTtlConfig-TimeCharacteristic-FLINK-15605"><a href="#Removal-of-StateTtlConfig-TimeCharacteristic-FLINK-15605" class="headerlink" title="Removal of StateTtlConfig#TimeCharacteristic (FLINK-15605)"></a>Removal of StateTtlConfig#TimeCharacteristic (<a href="https://issues.apache.org/jira/browse/FLINK-15605" target="_blank" rel="noopener">FLINK-15605</a>)</h4><p><code>StateTtlConfig#TimeCharacteristic</code> has been removed in favor of <code>StateTtlConfig#TtlTimeCharacteristic</code>.</p>
<h4 id="New-efficient-Method-to-check-if-MapState-is-empty-FLINK-13034"><a href="#New-efficient-Method-to-check-if-MapState-is-empty-FLINK-13034" class="headerlink" title="New efficient Method to check if MapState is empty (FLINK-13034)"></a>New efficient Method to check if MapState is empty (<a href="https://issues.apache.org/jira/browse/FLINK-13034" target="_blank" rel="noopener">FLINK-13034</a>)</h4><p>We have added a new method <code>MapState#isEmpty()</code> which enables users to check whether a map state is empty. The new method is 40% faster than <code>mapState.keys().iterator().hasNext()</code> when using the RocksDB state backend.</p>
<h4 id="RocksDB-Upgrade-FLINK-14483"><a href="#RocksDB-Upgrade-FLINK-14483" class="headerlink" title="RocksDB Upgrade (FLINK-14483)"></a>RocksDB Upgrade (<a href="https://issues.apache.org/jira/browse/FLINK-14483" target="_blank" rel="noopener">FLINK-14483</a>)</h4><p>We have again released our own RocksDB build (FRocksDB) which is based on RocksDB version 5.17.2 with several feature backports for the <a href="https://github.com/facebook/rocksdb/wiki/Write-Buffer-Manager" target="_blank" rel="noopener">Write Buffer Manager</a> to enable limiting RocksDB’s memory usage. The decision to release our own RocksDB build was made because later RocksDB versions suffer from a <a href="https://github.com/facebook/rocksdb/issues/5774" target="_blank" rel="noopener">performance regression under certain workloads</a>.</p>
<h4 id="RocksDB-Logging-disabled-by-default-FLINK-15068"><a href="#RocksDB-Logging-disabled-by-default-FLINK-15068" class="headerlink" title="RocksDB Logging disabled by default (FLINK-15068)"></a>RocksDB Logging disabled by default (<a href="https://issues.apache.org/jira/browse/FLINK-15068" target="_blank" rel="noopener">FLINK-15068</a>)</h4><p>Logging in RocksDB (e.g., logging related to flush, compaction, memtable creation, etc.) has been disabled by default to prevent disk space from being filled up unexpectedly. Users that need to enable logging should implement their own <code>RocksDBOptionsFactory</code> that creates <code>DBOptions</code> instances with <code>InfoLogLevel</code> set to <code>INFO_LEVEL</code>.</p>
<h4 id="Improved-RocksDB-Savepoint-Recovery-FLINK-12785"><a href="#Improved-RocksDB-Savepoint-Recovery-FLINK-12785" class="headerlink" title="Improved RocksDB Savepoint Recovery (FLINK-12785)"></a>Improved RocksDB Savepoint Recovery (<a href="https://issues.apache.org/jira/browse/FLINK-12785" target="_blank" rel="noopener">FLINK-12785</a>)</h4><p>In previous Flink releases users may encounter an <code>OutOfMemoryError</code> when restoring from a RocksDB savepoint containing large KV pairs. For that reason we introduced a configurable memory limit in the <code>RocksDBWriteBatchWrapper</code> with a default value of 2 MB. RocksDB’s WriteBatch will flush before the consumed memory limit is reached. If needed, the limit can be tuned via the <code>state.backend.rocksdb.write-batch-size</code> config option in <code>flink-conf.yaml</code>.</p>
<h3 id="PyFlink"><a href="#PyFlink" class="headerlink" title="PyFlink"></a>PyFlink</h3><h4 id="Python-2-Support-dropped-FLINK-14469"><a href="#Python-2-Support-dropped-FLINK-14469" class="headerlink" title="Python 2 Support dropped (FLINK-14469)"></a>Python 2 Support dropped (<a href="https://issues.apache.org/jira/browse/FLINK-14469" target="_blank" rel="noopener">FLINK-14469</a>)</h4><p>Beginning from this release, PyFlink does not support Python 2. This is because <a href="https://www.python.org/doc/sunset-python-2/" target="_blank" rel="noopener">Python 2 has reached end of life on January 1, 2020</a>, and several third-party projects that PyFlink depends on are also dropping Python 2 support.</p>
<h3 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h3><h4 id="InfluxdbReporter-skips-Inf-and-NaN-FLINK-12147"><a href="#InfluxdbReporter-skips-Inf-and-NaN-FLINK-12147" class="headerlink" title="InfluxdbReporter skips Inf and NaN (FLINK-12147)"></a>InfluxdbReporter skips Inf and NaN (<a href="https://issues.apache.org/jira/browse/FLINK-12147" target="_blank" rel="noopener">FLINK-12147</a>)</h4><p>The <code>InfluxdbReporter</code> now silently skips values that are unsupported by InfluxDB, such as <code>Double.POSITIVE_INFINITY</code>, <code>Double.NEGATIVE_INFINITY</code>, <code>Double.NaN</code>, etc.</p>
<h3 id="Connectors"><a href="#Connectors" class="headerlink" title="Connectors"></a>Connectors</h3><h4 id="Kinesis-Connector-License-Change-FLINK-12847"><a href="#Kinesis-Connector-License-Change-FLINK-12847" class="headerlink" title="Kinesis Connector License Change (FLINK-12847)"></a>Kinesis Connector License Change (<a href="https://issues.apache.org/jira/browse/FLINK-12847" target="_blank" rel="noopener">FLINK-12847</a>)</h4><p>flink-connector-kinesis is now licensed under the Apache License, Version 2.0, and its artifacts will be deployed to Maven central as part of the Flink releases. Users no longer need to build the Kinesis connector from source themselves.</p>
<h3 id="Miscellaneous-Interface-Changes"><a href="#Miscellaneous-Interface-Changes" class="headerlink" title="Miscellaneous Interface Changes"></a>Miscellaneous Interface Changes</h3><h4 id="ExecutionConfig-getGlobalJobParameters-cannot-return-null-anymore-FLINK-9787"><a href="#ExecutionConfig-getGlobalJobParameters-cannot-return-null-anymore-FLINK-9787" class="headerlink" title="ExecutionConfig#getGlobalJobParameters() cannot return null anymore (FLINK-9787)"></a>ExecutionConfig#getGlobalJobParameters() cannot return null anymore (<a href="https://issues.apache.org/jira/browse/FLINK-9787" target="_blank" rel="noopener">FLINK-9787</a>)</h4><p><code>ExecutionConfig#getGlobalJobParameters</code> has been changed to never return <code>null</code>. Conversely, <code>ExecutionConfig#setGlobalJobParameters(GlobalJobParameters)</code> will not accept <code>null</code> values anymore.</p>
<h4 id="Change-of-contract-in-MasterTriggerRestoreHook-interface-FLINK-14344"><a href="#Change-of-contract-in-MasterTriggerRestoreHook-interface-FLINK-14344" class="headerlink" title="Change of contract in MasterTriggerRestoreHook interface (FLINK-14344)"></a>Change of contract in MasterTriggerRestoreHook interface (<a href="https://issues.apache.org/jira/browse/FLINK-14344" target="_blank" rel="noopener">FLINK-14344</a>)</h4><p>Implementations of <code>MasterTriggerRestoreHook#triggerCheckpoint(long, long, Executor)</code> must be non-blocking now. Any blocking operation should be executed asynchronously, e.g., using the given executor.</p>
<h4 id="Client-and-Server-Side-Separation-of-HA-Services-FLINK-13750"><a href="#Client-and-Server-Side-Separation-of-HA-Services-FLINK-13750" class="headerlink" title="Client-/ and Server-Side Separation of HA Services (FLINK-13750)"></a>Client-/ and Server-Side Separation of HA Services (<a href="https://issues.apache.org/jira/browse/FLINK-13750" target="_blank" rel="noopener">FLINK-13750</a>)</h4><p>The <code>HighAvailabilityServices</code> have been split up into client-side <code>ClientHighAvailabilityServices</code> and cluster-side <code>HighAvailabilityServices</code>. When implementing custom high availability services, users should follow this separation by overriding the factory method <code>HighAvailabilityServicesFactory#createClientHAServices(Configuration)</code>. Moreover, <code>HighAvailabilityServices#getWebMonitorLeaderRetriever()</code> should no longer be implemented since it has been deprecated.</p>
<h4 id="Deprecation-of-HighAvailabilityServices-getWebMonitorLeaderElectionService-FLINK-13977"><a href="#Deprecation-of-HighAvailabilityServices-getWebMonitorLeaderElectionService-FLINK-13977" class="headerlink" title="Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() (FLINK-13977)"></a>Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() (<a href="https://issues.apache.org/jira/browse/FLINK-13977" target="_blank" rel="noopener">FLINK-13977</a>)</h4><p>Implementations of <code>HighAvailabilityServices</code> should implement <code>HighAvailabilityServices#getClusterRestEndpointLeaderElectionService()</code> instead of <code>HighAvailabilityServices#getWebMonitorLeaderElectionService()</code>.</p>
<h4 id="Interface-Change-in-LeaderElectionService-FLINK-14287"><a href="#Interface-Change-in-LeaderElectionService-FLINK-14287" class="headerlink" title="Interface Change in LeaderElectionService (FLINK-14287)"></a>Interface Change in LeaderElectionService (<a href="https://issues.apache.org/jira/browse/FLINK-14287" target="_blank" rel="noopener">FLINK-14287</a>)</h4><p><code>LeaderElectionService#confirmLeadership(UUID, String)</code> now takes an additional second argument, which is the address under which the leader will be reachable. All custom <code>LeaderElectionService</code> implementations will need to be updated accordingly.</p>
<h4 id="Deprecation-of-Checkpoint-Lock-FLINK-14857"><a href="#Deprecation-of-Checkpoint-Lock-FLINK-14857" class="headerlink" title="Deprecation of Checkpoint Lock (FLINK-14857)"></a>Deprecation of Checkpoint Lock (<a href="https://issues.apache.org/jira/browse/FLINK-14857" target="_blank" rel="noopener">FLINK-14857</a>)</h4><p>The method <code>org.apache.flink.streaming.runtime.tasks.StreamTask#getCheckpointLock()</code> is deprecated now. Users should use <code>MailboxExecutor</code> to run actions that require synchronization with the task’s thread (e.g. collecting output produced by an external thread). The methods <code>MailboxExecutor#yield()</code> or <code>MailboxExecutor#tryYield()</code> can be used for actions that need to give up control to other actions temporarily, e.g., if the current operator is blocked. The <code>MailboxExecutor</code> can be accessed by using <code>YieldingOperatorFactory</code> (see <code>AsyncWaitOperator</code> for an example usage).</p>
<h4 id="Deprecation-of-OptionsFactory-and-ConfigurableOptionsFactory-interfaces-FLINK-14926"><a href="#Deprecation-of-OptionsFactory-and-ConfigurableOptionsFactory-interfaces-FLINK-14926" class="headerlink" title="Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces (FLINK-14926)"></a>Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces (<a href="https://issues.apache.org/jira/browse/FLINK-14926" target="_blank" rel="noopener">FLINK-14926</a>)</h4><p>Interfaces <code>OptionsFactory</code> and <code>ConfigurableOptionsFactory</code> have been deprecated in favor of <code>RocksDBOptionsFactory</code> and <code>ConfigurableRocksDBOptionsFactory</code>, respectively.</p>
<h1 id="Flink-1-9"><a href="#Flink-1-9" class="headerlink" title="Flink 1.9"></a>Flink 1.9</h1><h2 id="Known-shortcomings-or-limitations-for-new-features"><a href="#Known-shortcomings-or-limitations-for-new-features" class="headerlink" title="Known shortcomings or limitations for new features"></a>Known shortcomings or limitations for new features</h2><h3 id="New-Table-SQL-Blink-planner"><a href="#New-Table-SQL-Blink-planner" class="headerlink" title="New Table / SQL Blink planner"></a>New Table / SQL Blink planner</h3><p>Flink 1.9.0 provides support for two planners for the Table API, namely Flink’s original planner and the new Blink planner. The original planner maintains same behaviour as previous releases, while the new Blink planner is still considered experimental and has the following limitations:</p>
<ul>
<li>The Blink planner can not be used with <code>BatchTableEnvironment</code>, and therefore Table programs ran with the planner can not be transformed to <code>DataSet</code> programs. This is by design and will also not be supported in the future. Therefore, if you want to run a batch job with the Blink planner, please use the new <code>TableEnvironment</code>. For streaming jobs, both <code>StreamTableEnvironment</code> and <code>TableEnvironment</code> works.</li>
<li>Implementations of <code>StreamTableSink</code> should implement the <code>consumeDataStream</code> method instead of <code>emitDataStream</code> if it is used with the Blink planner. Both methods work with the original planner. This is by design to make the returned <code>DataStreamSink</code> accessible for the planner.</li>
<li>Due to a bug with how transformations are not being cleared on execution, <code>TableEnvironment</code> instances should not be reused across multiple SQL statements when using the Blink planner.</li>
<li><code>Table.flatAggregate</code> is not supported</li>
<li>Session and count windows are not supported when running batch jobs.</li>
<li>The Blink planner only supports the new <code>Catalog</code> API, and does not support <code>ExternalCatalog</code> which is now deprecated.</li>
</ul>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13708" target="_blank" rel="noopener">FLINK-13708: Transformations should be cleared because a table environment could execute multiple job</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13473" target="_blank" rel="noopener">FLINK-13473: Add GroupWindowed FlatAggregate support to stream Table API (Blink planner), i.e, align with Flink planner</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13735" target="_blank" rel="noopener">FLINK-13735: Support session window with Blink planner in batch mode</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13736" target="_blank" rel="noopener">FLINK-13736: Support count window with Blink planner in batch mode</a></li>
</ul>
<h3 id="SQL-DDL"><a href="#SQL-DDL" class="headerlink" title="SQL DDL"></a>SQL DDL</h3><p>In Flink 1.9.0, the community also added a preview feature about SQL DDL, but only for batch style DDLs. Therefore, all streaming related concepts are not supported yet, for example watermarks.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13661" target="_blank" rel="noopener">FLINK-13661: Add a stream specific CREATE TABLE SQL DDL</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13568" target="_blank" rel="noopener">FLINK-13568: DDL create table doesn’t allow STRING data type</a></li>
</ul>
<h3 id="Java-9-support"><a href="#Java-9-support" class="headerlink" title="Java 9 support"></a>Java 9 support</h3><p>Since Flink 1.9.0, Flink can now be compiled and run on Java 9. Note that certain components interacting with external systems (connectors, filesystems, metric reporters, etc.) may not work since the respective projects may have skipped Java 9 support.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-8033" target="_blank" rel="noopener">FLINK-8033: JDK 9 support</a></li>
</ul>
<h3 id="Memory-management"><a href="#Memory-management" class="headerlink" title="Memory management"></a>Memory management</h3><p>In Fink 1.9.0 and prior version, the managed memory fraction of taskmanager is controlled by <code>taskmanager.memory.fraction</code>, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter <code>NewRatio</code> is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-14123" target="_blank" rel="noopener">FLINK-14123: Lower the default value of taskmanager.memory.fraction</a></li>
</ul>
<h2 id="Deprecations-and-breaking-changes"><a href="#Deprecations-and-breaking-changes" class="headerlink" title="Deprecations and breaking changes"></a>Deprecations and breaking changes</h2><h3 id="Scala-expression-DSL-for-Table-API-moved-to-flink-table-api-scala"><a href="#Scala-expression-DSL-for-Table-API-moved-to-flink-table-api-scala" class="headerlink" title="Scala expression DSL for Table API moved to flink-table-api-scala"></a>Scala expression DSL for Table API moved to <code>flink-table-api-scala</code></h3><p>Since 1.9.0, the implicit conversions for the Scala expression DSL for the Table API has been moved to <code>flink-table-api-scala</code>. This requires users to update the imports in their Table programs.</p>
<p>Users of pure Table programs should define their imports like:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.table.api._</span><br><span class="line"></span><br><span class="line">TableEnvironment.create(...)</span><br></pre></td></tr></table></figure>

<p>Users of the DataStream API should define their imports like:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.table.api._</span><br><span class="line">import org.apache.flink.table.api.scala._</span><br><span class="line"></span><br><span class="line">StreamTableEnvironment.create(...)</span><br></pre></td></tr></table></figure>

<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13045" target="_blank" rel="noopener">FLINK-13045: Move Scala expression DSL to flink-table-api-scala</a></li>
</ul>
<h3 id="Failover-strategies"><a href="#Failover-strategies" class="headerlink" title="Failover strategies"></a>Failover strategies</h3><p>As a result of completing fine-grained recovery (<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures" target="_blank" rel="noopener">FLIP-1</a>), Flink will now attempt to only restart tasks that are connected to failed tasks through a pipelined connection. By default, the <code>region</code> failover strategy is used.</p>
<p>Users who were not using a restart strategy or have already configured a failover strategy should not be affected. Moreover, users who already enabled the <code>region</code> failover strategy, along with a restart strategy that enforces a certain number of restarts or introduces a restart delay, will see changes in behavior. The <code>region</code> failover strategy now correctly respects constraints that are defined by the restart strategy.</p>
<p>Streaming users who were not using a failover strategy may be affected if their jobs are embarrassingly parallel or contain multiple independent jobs. In this case, only the failed parallel pipeline or affected jobs will be restarted.</p>
<p>Batch users may be affected if their job contains blocking exchanges (usually happens for shuffles) or the <code>ExecutionMode</code> was set to <code>BATCH</code> or <code>BATCH_FORCED</code> via the <code>ExecutionConfig</code>.</p>
<p>Overall, users should see an improvement in performance.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13223" target="_blank" rel="noopener">FLINK-13223: Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13060" target="_blank" rel="noopener">FLINK-13060: FailoverStrategies should respect restart constraints</a></li>
</ul>
<h3 id="Job-termination-via-CLI"><a href="#Job-termination-via-CLI" class="headerlink" title="Job termination via CLI"></a>Job termination via CLI</h3><p>With the support of graceful job termination with savepoints for semantic correctness (<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103090212" target="_blank" rel="noopener">FLIP-34</a>), a few changes related to job termination has been made to the CLI.</p>
<p>From now on, the <code>stop</code> command with no further arguments stops the job with a savepoint targeted at the default savepoint location (as configured via the <code>state.savepoints.dir</code> property in the job configuration), or a location explicitly specified using the <code>-p</code> option. Please make sure to configure the savepoint path using either one of these options.</p>
<p>Since job terminations are now always accompanied with a savepoint, stopping jobs is expected to take longer now.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13123" target="_blank" rel="noopener">FLINK-13123: Align Stop/Cancel Commands in CLI and REST Interface and Improve Documentation</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-11458" target="_blank" rel="noopener">FLINK-11458: Add TERMINATE/SUSPEND Job with Savepoint</a></li>
</ul>
<h3 id="Network-stack"><a href="#Network-stack" class="headerlink" title="Network stack"></a>Network stack</h3><p>A few changes in the network stack related to changes in the threading model of <code>StreamTask</code> to a mailbox-based approach requires close attention to some related configuration:</p>
<ul>
<li>Due to changes in the lifecycle management of result partitions, partition requests as well as re-triggers will now happen sooner. Therefore, it is possible that some jobs with long deployment times and large state might start failing more frequently with <code>PartitionNotFound</code> exceptions compared to previous versions. If that’s the case, users should increase the value of <code>taskmanager.network.request-backoff.max</code> in order to have the same effective partition request timeout as it was prior to 1.9.0.</li>
<li>To avoid a potential deadlock, a timeout has been added for how long a task will wait for assignment of exclusive memory segments. The default timeout is 30 seconds, and is configurable via <code>taskmanager.network.memory.exclusive-buffers-request-timeout-ms</code>. It is possible that for some previously working deployments this default timeout value is too low and might have to be increased.</li>
</ul>
<p>Please also notice that several network I/O metrics have had their scope changed. See the <a href="https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html" target="_blank" rel="noopener">1.9 metrics documentation</a> for which metrics are affected. In 1.9.0, these metrics will still be available under their previous scopes, but this may no longer be the case in future versions.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13013" target="_blank" rel="noopener">FLINK-13013: Make sure that SingleInputGate can always request partitions</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12852" target="_blank" rel="noopener">FLINK-12852: Deadlock occurs when requiring exclusive buffer for RemoteInputChannel</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12555" target="_blank" rel="noopener">FLINK-12555: Introduce an encapsulated metric group layout for shuffle API and deprecate old one</a></li>
</ul>
<h3 id="AsyncIO"><a href="#AsyncIO" class="headerlink" title="AsyncIO"></a>AsyncIO</h3><p>Due to a bug in the <code>AsyncWaitOperator</code>, in 1.9.0 the default chaining behaviour of the operator is now changed so that it is never chained after another operator. This should not be problematic for migrating from older version snapshots as long as an uid was assigned to the operator. If an uid was not assigned to the operator, please see the instructions <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/upgrading.html#matching-operator-state" target="_blank" rel="noopener">here</a> for a possible workaround.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13063" target="_blank" rel="noopener">FLINK-13063: AsyncWaitOperator shouldn’t be releasing checkpointingLock</a></li>
</ul>
<h3 id="Connectors-and-Libraries"><a href="#Connectors-and-Libraries" class="headerlink" title="Connectors and Libraries"></a>Connectors and Libraries</h3><h4 id="Introduced-KafkaSerializationSchema-to-fully-replace-KeyedSerializationSchema"><a href="#Introduced-KafkaSerializationSchema-to-fully-replace-KeyedSerializationSchema" class="headerlink" title="Introduced KafkaSerializationSchema to fully replace KeyedSerializationSchema"></a>Introduced <code>KafkaSerializationSchema</code> to fully replace <code>KeyedSerializationSchema</code></h4><p>The universal <code>FlinkKafkaProducer</code> (in <code>flink-connector-kafka</code>) supports a new <code>KafkaSerializationSchema</code> that will fully replace <code>KeyedSerializationSchema</code> in the long run. This new schema allows directly generating Kafka <code>ProducerRecord</code>s for sending to Kafka, therefore enabling the user to use all available Kafka features (in the context of Kafka records).</p>
<h4 id="Dropped-connectors-and-libraries"><a href="#Dropped-connectors-and-libraries" class="headerlink" title="Dropped connectors and libraries"></a>Dropped connectors and libraries</h4><ul>
<li>The Elasticsearch 1 connector has been dropped and will no longer receive patches. Users may continue to use the connector from a previous series (like 1.8) with newer versions of Flink. It is being dropped due to being used significantly less than more recent versions (Elasticsearch versions 2.x and 5.x are downloaded 4 to 5 times more), and hasn’t seen any development for over a year.</li>
<li>The older Python APIs for batch and streaming have been removed and will no longer receive new patches. A new API is being developed based on the Table API as part of <a href="https://issues.apache.org/jira/browse/FLINK-12308" target="_blank" rel="noopener">FLINK-12308: Support python language in Flink Table API</a>. Existing users may continue to use these older APIs with future versions of Flink by copying both the <code>flink-streaming-python</code> and <code>flink-python</code> jars into the <code>/lib</code> directory of the distribution and the corresponding start scripts <code>pyflink-stream.sh</code> and <code>pyflink.sh</code> into the <code>/bin</code> directory of the distribution.</li>
<li>The older machine learning libraries have been removed and will no longer receive new patches. This is due to efforts towards a new Table-based machine learning library (<a href="https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo/edit" target="_blank" rel="noopener">FLIP-39</a>). Users can still use the 1.8 version of the legacy library if their projects still rely on it.</li>
</ul>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-11693" target="_blank" rel="noopener">FLINK-11693: Add KafkaSerializationSchema that directly uses ProducerRecord</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12151" target="_blank" rel="noopener">FLINK-12151: Drop Elasticsearch 1 connector</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12903" target="_blank" rel="noopener">FLINK-12903: Remove legacy flink-python APIs</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12308" target="_blank" rel="noopener">FLINK-12308: Support python language in Flink Table API</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12597" target="_blank" rel="noopener">FLINK-12597: Remove the legacy flink-libraries/flink-ml</a></li>
</ul>
<h3 id="MapR-dependency-removed"><a href="#MapR-dependency-removed" class="headerlink" title="MapR dependency removed"></a>MapR dependency removed</h3><p>Dependency on MapR vendor-specific artifacts has been removed, by changing the MapR filesystem connector to work purely based on reflection. This does not introduce any regession in the support for the MapR filesystem. The decision to remove hard dependencies on the MapR artifacts was made due to very flaky access to the secure https endpoint of the MapR artifact repository, and affected build stability of Flink.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12578" target="_blank" rel="noopener">FLINK-12578: Use secure URLs for Maven repositories</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13499" target="_blank" rel="noopener">FLINK-13499: Remove dependency on MapR artifact repository</a></li>
</ul>
<h3 id="StateDescriptor-interface-change"><a href="#StateDescriptor-interface-change" class="headerlink" title="StateDescriptor interface change"></a>StateDescriptor interface change</h3><p>Access to the state serializer in <code>StateDescriptor</code> is now modified from protected to private access. Subclasses should use the <code>StateDescriptor#getSerializer()</code> method as the only means to obtain the wrapped state serializer.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12688" target="_blank" rel="noopener">FLINK-12688: Make serializer lazy initialization thread safe in StateDescriptor</a></li>
</ul>
<h3 id="Web-UI-dashboard"><a href="#Web-UI-dashboard" class="headerlink" title="Web UI dashboard"></a>Web UI dashboard</h3><p>The web frontend of Flink has been updated to use the latest Angular version (7.x). The old frontend remains available in Flink 1.9.x, but will be removed in a later Flink release once the new frontend is considered stable.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-10705" target="_blank" rel="noopener">FLINK-10705: Rework Flink Web Dashoard</a></li>
</ul>
<h1 id="Flink-1-8"><a href="#Flink-1-8" class="headerlink" title="Flink 1.8"></a>Flink 1.8</h1><h3 id="State-1"><a href="#State-1" class="headerlink" title="State"></a>State</h3><h4 id="Continuous-incremental-cleanup-of-old-Keyed-State-with-TTL"><a href="#Continuous-incremental-cleanup-of-old-Keyed-State-with-TTL" class="headerlink" title="Continuous incremental cleanup of old Keyed State with TTL"></a>Continuous incremental cleanup of old Keyed State with TTL</h4><p>We introduced TTL (time-to-live) for Keyed state in Flink 1.6 (<a href="https://issues.apache.org/jira/browse/FLINK-9510" target="_blank" rel="noopener">FLINK-9510</a>). This feature allowed to clean up and make inaccessible keyed state entries when accessing them. In addition state would now also being cleaned up when writing a savepoint/checkpoint.</p>
<p>Flink 1.8 introduces continous cleanup of old entries for both the RocksDB state backend (<a href="https://issues.apache.org/jira/browse/FLINK-10471" target="_blank" rel="noopener">FLINK-10471</a>) and the heap state backend (<a href="https://issues.apache.org/jira/browse/FLINK-10473" target="_blank" rel="noopener">FLINK-10473</a>). This means that old entries (according to the ttl setting) are continously being cleanup up.</p>
<h4 id="New-Support-for-Schema-Migration-when-restoring-Savepoints"><a href="#New-Support-for-Schema-Migration-when-restoring-Savepoints" class="headerlink" title="New Support for Schema Migration when restoring Savepoints"></a>New Support for Schema Migration when restoring Savepoints</h4><p>With Flink 1.7.0 we added support for changing the schema of state when using the <code>AvroSerializer</code> (<a href="https://issues.apache.org/jira/browse/FLINK-10605" target="_blank" rel="noopener">FLINK-10605</a>). With Flink 1.8.0 we made great progress migrating all built-in <code>TypeSerializers</code> to a new serializer snapshot abstraction that theoretically allows schema migration. Of the serializers that come with Flink, we now support schema migration for the <code>PojoSerializer</code> (<a href="https://issues.apache.org/jira/browse/FLINK-11485" target="_blank" rel="noopener">FLINK-11485</a>), and Java <code>EnumSerializer</code> (<a href="https://issues.apache.org/jira/browse/FLINK-11334" target="_blank" rel="noopener">FLINK-11334</a>), As well as for Kryo in limited cases (<a href="https://issues.apache.org/jira/browse/FLINK-11323" target="_blank" rel="noopener">FLINK-11323</a>).</p>
<h4 id="Savepoint-compatibility"><a href="#Savepoint-compatibility" class="headerlink" title="Savepoint compatibility"></a>Savepoint compatibility</h4><p>Savepoints from Flink 1.2 that contain a Scala <code>TraversableSerializer</code> are not compatible with Flink 1.8 anymore because of an update in this serializer (<a href="https://issues.apache.org/jira/browse/FLINK-11539" target="_blank" rel="noopener">FLINK-11539</a>). You can get around this restriction by first upgrading to a version between Flink 1.3 and Flink 1.7 and then updating to Flink 1.8.</p>
<h4 id="RocksDB-version-bump-and-switch-to-FRocksDB-FLINK-10471"><a href="#RocksDB-version-bump-and-switch-to-FRocksDB-FLINK-10471" class="headerlink" title="RocksDB version bump and switch to FRocksDB (FLINK-10471)"></a>RocksDB version bump and switch to FRocksDB (<a href="https://issues.apache.org/jira/browse/FLINK-10471" target="_blank" rel="noopener">FLINK-10471</a>)</h4><p>We needed to switch to a custom build of RocksDB called FRocksDB because we need certain changes in RocksDB for supporting continuous state cleanup with TTL. The used build of FRocksDB is based on the upgraded version 5.17.2 of RocksDB. For Mac OS X, RocksDB version 5.17.2 is supported only for OS X version &gt;= 10.13. See also: <a href="https://github.com/facebook/rocksdb/issues/4862" target="_blank" rel="noopener">https://github.com/facebook/rocksdb/issues/4862</a>.</p>
<h3 id="Maven-Dependencies"><a href="#Maven-Dependencies" class="headerlink" title="Maven Dependencies"></a>Maven Dependencies</h3><h4 id="Changes-to-bundling-of-Hadoop-libraries-with-Flink-FLINK-11266"><a href="#Changes-to-bundling-of-Hadoop-libraries-with-Flink-FLINK-11266" class="headerlink" title="Changes to bundling of Hadoop libraries with Flink (FLINK-11266)"></a>Changes to bundling of Hadoop libraries with Flink (<a href="https://issues.apache.org/jira/browse/FLINK-11266" target="_blank" rel="noopener">FLINK-11266</a>)</h4><p>Convenience binaries that include hadoop are no longer released.</p>
<p>If a deployment relies on <code>flink-shaded-hadoop2</code> being included in <code>flink-dist</code>, then you must manually download a pre-packaged Hadoop jar from the optional components section of the <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">download page</a> and copy it into the <code>/lib</code> directory. Alternatively, a Flink distribution that includes hadoop can be built by packaging <code>flink-dist</code> and activating the <code>include-hadoop</code> maven profile.</p>
<p>As hadoop is no longer included in <code>flink-dist</code> by default, specifying <code>-DwithoutHadoop</code> when packaging <code>flink-dist</code> no longer impacts the build.</p>
<h3 id="Configuration-1"><a href="#Configuration-1" class="headerlink" title="Configuration"></a>Configuration</h3><h4 id="TaskManager-configuration-FLINK-11716"><a href="#TaskManager-configuration-FLINK-11716" class="headerlink" title="TaskManager configuration (FLINK-11716)"></a>TaskManager configuration (<a href="https://issues.apache.org/jira/browse/FLINK-11716" target="_blank" rel="noopener">FLINK-11716</a>)</h4><p><code>TaskManagers</code> now bind to the host IP address instead of the hostname by default . This behaviour can be controlled by the configuration option <code>taskmanager.network.bind-policy</code>. If your Flink cluster should experience inexplicable connection problems after upgrading, try to set <code>taskmanager.network.bind-policy: name</code> in your <code>flink-conf.yaml</code> to return to the pre-1.8 behaviour.</p>
<h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><h4 id="Deprecation-of-direct-Table-constructor-usage-FLINK-11447"><a href="#Deprecation-of-direct-Table-constructor-usage-FLINK-11447" class="headerlink" title="Deprecation of direct Table constructor usage (FLINK-11447)"></a>Deprecation of direct <code>Table</code> constructor usage (<a href="https://issues.apache.org/jira/browse/FLINK-11447" target="_blank" rel="noopener">FLINK-11447</a>)</h4><p>Flink 1.8 deprecates direct usage of the constructor of the <code>Table</code> class in the Table API. This constructor would previously be used to perform a join with a <em>lateral table</em>. You should now use <code>table.joinLateral()</code> or <code>table.leftOuterJoinLateral()</code> instead.</p>
<p>This change is necessary for converting the Table class into an interface, which will make the API more maintainable and cleaner in the future.</p>
<h4 id="Introduction-of-new-CSV-format-descriptor-FLINK-9964"><a href="#Introduction-of-new-CSV-format-descriptor-FLINK-9964" class="headerlink" title="Introduction of new CSV format descriptor (FLINK-9964)"></a>Introduction of new CSV format descriptor (<a href="https://issues.apache.org/jira/browse/FLINK-9964" target="_blank" rel="noopener">FLINK-9964</a>)</h4><p>This release introduces a new format descriptor for CSV files that is compliant with RFC 4180. The new descriptor is available as <code>org.apache.flink.table.descriptors.Csv</code>. For now, this can only be used together with the Kafka connector. The old descriptor is available as <code>org.apache.flink.table.descriptors.OldCsv</code> for use with file system connectors.</p>
<h4 id="Deprecation-of-static-builder-methods-on-TableEnvironment-FLINK-11445"><a href="#Deprecation-of-static-builder-methods-on-TableEnvironment-FLINK-11445" class="headerlink" title="Deprecation of static builder methods on TableEnvironment (FLINK-11445)"></a>Deprecation of static builder methods on TableEnvironment (<a href="https://issues.apache.org/jira/browse/FLINK-11445" target="_blank" rel="noopener">FLINK-11445</a>)</h4><p>In order to separate API from actual implementation, the static methods <code>TableEnvironment.getTableEnvironment()</code> are deprecated. You should now use <code>Batch/StreamTableEnvironment.create()</code> instead.</p>
<h4 id="Change-in-the-Maven-modules-of-Table-API-FLINK-11064"><a href="#Change-in-the-Maven-modules-of-Table-API-FLINK-11064" class="headerlink" title="Change in the Maven modules of Table API (FLINK-11064)"></a>Change in the Maven modules of Table API (<a href="https://issues.apache.org/jira/browse/FLINK-11064" target="_blank" rel="noopener">FLINK-11064</a>)</h4><p>Users that had a <code>flink-table</code> dependency before, need to update their dependencies to <code>flink-table-planner</code> and the correct dependency of <code>flink-table-api-*</code>, depending on whether Java or Scala is used: one of <code>flink-table-api-java-bridge</code> or <code>flink-table-api-scala-bridge</code>.</p>
<h4 id="Change-to-External-Catalog-Table-Builders-FLINK-11522"><a href="#Change-to-External-Catalog-Table-Builders-FLINK-11522" class="headerlink" title="Change to External Catalog Table Builders (FLINK-11522)"></a>Change to External Catalog Table Builders (<a href="https://issues.apache.org/jira/browse/FLINK-11522" target="_blank" rel="noopener">FLINK-11522</a>)</h4><p><code>ExternalCatalogTable.builder()</code> is deprecated in favour of <code>ExternalCatalogTableBuilder()</code>.</p>
<h4 id="Change-to-naming-of-Table-API-connector-jars-FLINK-11026"><a href="#Change-to-naming-of-Table-API-connector-jars-FLINK-11026" class="headerlink" title="Change to naming of Table API connector jars (FLINK-11026)"></a>Change to naming of Table API connector jars (<a href="https://issues.apache.org/jira/browse/FLINK-11026" target="_blank" rel="noopener">FLINK-11026</a>)</h4><p>The naming scheme for kafka/elasticsearch6 sql-jars has been changed.</p>
<p>In maven terms, they no longer have the <code>sql-jar</code> qualifier and the artifactId is now prefixed with <code>flink-sql</code> instead of <code>flink</code>, e.g., <code>flink-sql-connector-kafka...</code>.</p>
<h4 id="Change-to-how-Null-Literals-are-specified-FLINK-11785"><a href="#Change-to-how-Null-Literals-are-specified-FLINK-11785" class="headerlink" title="Change to how Null Literals are specified (FLINK-11785)"></a>Change to how Null Literals are specified (<a href="https://issues.apache.org/jira/browse/FLINK-11785" target="_blank" rel="noopener">FLINK-11785</a>)</h4><p>Null literals in the Table API need to be defined with <code>nullOf(type)</code> instead of <code>Null(type)</code> from now on. The old approach is deprecated.</p>
<h3 id="Connectors-1"><a href="#Connectors-1" class="headerlink" title="Connectors"></a>Connectors</h3><h4 id="Introduction-of-a-new-KafkaDeserializationSchema-that-give-direct-access-to-ConsumerRecord-FLINK-8354"><a href="#Introduction-of-a-new-KafkaDeserializationSchema-that-give-direct-access-to-ConsumerRecord-FLINK-8354" class="headerlink" title="Introduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (FLINK-8354)"></a>Introduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (<a href="https://issues.apache.org/jira/browse/FLINK-8354" target="_blank" rel="noopener">FLINK-8354</a>)</h4><p>For the Flink <code>KafkaConsumers</code>, we introduced a new <code>KafkaDeserializationSchema</code> that gives direct access to the Kafka <code>ConsumerRecord</code>. This subsumes the <code>KeyedSerializationSchema</code> functionality, which is deprecated but still available for now.</p>
<h4 id="FlinkKafkaConsumer-will-now-filter-restored-partitions-based-on-topic-specification-FLINK-10342"><a href="#FlinkKafkaConsumer-will-now-filter-restored-partitions-based-on-topic-specification-FLINK-10342" class="headerlink" title="FlinkKafkaConsumer will now filter restored partitions based on topic specification (FLINK-10342)"></a>FlinkKafkaConsumer will now filter restored partitions based on topic specification (<a href="https://issues.apache.org/jira/browse/FLINK-10342" target="_blank" rel="noopener">FLINK-10342</a>)</h4><p>Starting from Flink 1.8.0, the <code>FlinkKafkaConsumer</code> now always filters out restored partitions that are no longer associated with a specified topic to subscribe to in the restored execution. This behaviour did not exist in previous versions of the <code>FlinkKafkaConsumer</code>. If you wish to retain the previous behaviour, please use the <code>disableFilterRestoredPartitionsWithSubscribedTopics()</code> configuration method on the <code>FlinkKafkaConsumer</code>.</p>
<p>Consider this example: if you had a Kafka Consumer that was consuming from topic <code>A</code>, you did a savepoint, then changed your Kafka consumer to instead consume from topic <code>B</code>, and then restarted your job from the savepoint. Before this change, your consumer would now consume from both topic <code>A</code> and <code>B</code> because it was stored in state that the consumer was consuming from topic <code>A</code>. With the change, your consumer would only consume from topic <code>B</code> after restore because we filter the topics that are stored in state using the configured topics.</p>
<h3 id="Miscellaneous-Interface-changes"><a href="#Miscellaneous-Interface-changes" class="headerlink" title="Miscellaneous Interface changes"></a>Miscellaneous Interface changes</h3><h4 id="The-canEqual-method-was-dropped-from-the-TypeSerializer-interface-FLINK-9803"><a href="#The-canEqual-method-was-dropped-from-the-TypeSerializer-interface-FLINK-9803" class="headerlink" title="The canEqual() method was dropped from the TypeSerializer interface (FLINK-9803)"></a>The canEqual() method was dropped from the TypeSerializer interface (<a href="https://issues.apache.org/jira/browse/FLINK-9803" target="_blank" rel="noopener">FLINK-9803</a>)</h4><p>The <code>canEqual()</code> methods are usually used to make proper equality checks across hierarchies of types. The <code>TypeSerializer</code> actually doesn’t require this property, so the method is now removed.</p>
<h4 id="Removal-of-the-CompositeSerializerSnapshot-utility-class-FLINK-11073"><a href="#Removal-of-the-CompositeSerializerSnapshot-utility-class-FLINK-11073" class="headerlink" title="Removal of the CompositeSerializerSnapshot utility class (FLINK-11073)"></a>Removal of the CompositeSerializerSnapshot utility class (<a href="https://issues.apache.org/jira/browse/FLINK-11073" target="_blank" rel="noopener">FLINK-11073</a>)</h4><p>The <code>CompositeSerializerSnapshot</code> utility class has been removed. You should now use <code>CompositeTypeSerializerSnapshot</code> instead, for snapshots of composite serializers that delegate serialization to multiple nested serializers. Please see <a href="http://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/state/custom_serialization.html#implementing-a-compositetypeserializersnapshot" target="_blank" rel="noopener">here</a> for instructions on using <code>CompositeTypeSerializerSnapshot</code>.</p>
<h3 id="Memory-management-1"><a href="#Memory-management-1" class="headerlink" title="Memory management"></a>Memory management</h3><p>In Fink 1.8.0 and prior version, the managed memory fraction of taskmanager is controlled by <code>taskmanager.memory.fraction</code>, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter <code>NewRatio</code> is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.</p>
<h1 id="Flink-1-7"><a href="#Flink-1-7" class="headerlink" title="Flink 1.7"></a>Flink 1.7</h1><h3 id="Scala-2-12-support"><a href="#Scala-2-12-support" class="headerlink" title="Scala 2.12 support"></a>Scala 2.12 support</h3><p>When using Scala <code>2.12</code> you might have to add explicit type annotations in places where they were not required when using Scala <code>2.11</code>. This is an excerpt from the <code>TransitiveClosureNaive.scala</code> example in the Flink code base that shows the changes that could be required.</p>
<p>Previous code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val terminate = prevPaths</span><br><span class="line"> .coGroup(nextPaths)</span><br><span class="line"> .where(0).equalTo(0) &#123;</span><br><span class="line">   (prev, next, out: Collector[(Long, Long)]) =&gt; &#123;</span><br><span class="line">     val prevPaths = prev.toSet</span><br><span class="line">     for (n &lt;- next)</span><br><span class="line">       if (!prevPaths.contains(n)) out.collect(n)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>With Scala <code>2.12</code> you have to change it to:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val terminate = prevPaths</span><br><span class="line"> .coGroup(nextPaths)</span><br><span class="line"> .where(0).equalTo(0) &#123;</span><br><span class="line">   (prev: Iterator[(Long, Long)], next: Iterator[(Long, Long)], out: Collector[(Long, Long)]) =&gt; &#123;</span><br><span class="line">       val prevPaths = prev.toSet</span><br><span class="line">       for (n &lt;- next)</span><br><span class="line">         if (!prevPaths.contains(n)) out.collect(n)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The reason for this is that Scala <code>2.12</code> changes how lambdas are implemented. They now use the lambda support using SAM interfaces introduced in Java 8. This makes some method calls ambiguous because now both Scala-style lambdas and SAMs are candidates for methods were it was previously clear which method would be invoked.</p>
<h3 id="State-evolution"><a href="#State-evolution" class="headerlink" title="State evolution"></a>State evolution</h3><p>Before Flink 1.7, serializer snapshots were implemented as a <code>TypeSerializerConfigSnapshot</code> (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new <code>TypeSerializerSnapshot</code> interface introduced in 1.7). Moreover, the responsibility of serializer schema compatibility checks lived within the <code>TypeSerializer</code>, implemented in the <code>TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)</code> method.</p>
<p>To be future-proof and to have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. Details and migration guides can be found <a href="https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/custom_serialization.html" target="_blank" rel="noopener">here</a>.</p>
<h3 id="Removal-of-the-legacy-mode"><a href="#Removal-of-the-legacy-mode" class="headerlink" title="Removal of the legacy mode"></a>Removal of the legacy mode</h3><p>Flink no longer supports the legacy mode. If you depend on this, then please use Flink <code>1.6.x</code>.</p>
<h3 id="Savepoints-being-used-for-recovery"><a href="#Savepoints-being-used-for-recovery" class="headerlink" title="Savepoints being used for recovery"></a>Savepoints being used for recovery</h3><p>Savepoints are now used while recovering. Previously when using exactly-once sink one could get into problems with duplicate output data when a failure occurred after a savepoint was taken but before the next checkpoint occurred. This results in the fact that savepoints are no longer exclusively under the control of the user. Savepoint should not be moved nor deleted if there was no newer checkpoint or savepoint taken.</p>
<h3 id="MetricQueryService-runs-in-separate-thread-pool"><a href="#MetricQueryService-runs-in-separate-thread-pool" class="headerlink" title="MetricQueryService runs in separate thread pool"></a>MetricQueryService runs in separate thread pool</h3><p>The metric query service runs now in its own <code>ActorSystem</code>. It needs consequently to open a new port for the query services to communicate with each other. The <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#metrics-internal-query-service-port" target="_blank" rel="noopener">query service port</a> can be configured in <code>flink-conf.yaml</code>.</p>
<h3 id="Granularity-of-latency-metrics"><a href="#Granularity-of-latency-metrics" class="headerlink" title="Granularity of latency metrics"></a>Granularity of latency metrics</h3><p>The default granularity for latency metrics has been modified. To restore the previous behavior users have to explicitly set the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#metrics-latency-granularity" target="_blank" rel="noopener">granularity</a> to <code>subtask</code>.</p>
<h3 id="Latency-marker-activation"><a href="#Latency-marker-activation" class="headerlink" title="Latency marker activation"></a>Latency marker activation</h3><p>Latency metrics are now disabled by default, which will affect all jobs that do not explicitly set the <code>latencyTrackingInterval</code> via <code>ExecutionConfig#setLatencyTrackingInterval</code>. To restore the previous default behavior users have to configure the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#metrics-latency-interval" target="_blank" rel="noopener">latency interval</a> in <code>flink-conf.yaml</code>.</p>
<h3 id="Relocation-of-Hadoop’s-Netty-dependency"><a href="#Relocation-of-Hadoop’s-Netty-dependency" class="headerlink" title="Relocation of Hadoop’s Netty dependency"></a>Relocation of Hadoop’s Netty dependency</h3><p>We now also relocate Hadoop’s Netty dependency from <code>io.netty</code> to <code>org.apache.flink.hadoop.shaded.io.netty</code>. You can now bundle your own version of Netty into your job but may no longer assume that <code>io.netty</code> is present in the <code>flink-shaded-hadoop2-uber-*.jar</code> file.</p>
<h3 id="Local-recovery-fixed"><a href="#Local-recovery-fixed" class="headerlink" title="Local recovery fixed"></a>Local recovery fixed</h3><p>With the improvements to Flink’s scheduling, it can no longer happen that recoveries require more slots than before if local recovery is enabled. Consequently, we encourage our users to enable <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#state-backend-local-recovery" target="_blank" rel="noopener">local recovery</a> in <code>flink-conf.yaml</code>.</p>
<h3 id="Support-for-multi-slot-TaskManagers"><a href="#Support-for-multi-slot-TaskManagers" class="headerlink" title="Support for multi slot TaskManagers"></a>Support for multi slot TaskManagers</h3><p>Flink now properly supports <code>TaskManagers</code> with multiple slots. Consequently, <code>TaskManagers</code> can now be started with an arbitrary number of slots and it is no longer recommended to start them with a single slot.</p>
<h3 id="StandaloneJobClusterEntrypoint-generates-JobGraph-with-fixed-JobID"><a href="#StandaloneJobClusterEntrypoint-generates-JobGraph-with-fixed-JobID" class="headerlink" title="StandaloneJobClusterEntrypoint generates JobGraph with fixed JobID"></a>StandaloneJobClusterEntrypoint generates JobGraph with fixed JobID</h3><p>The <code>StandaloneJobClusterEntrypoint</code>, which is launched by the script <code>standalone-job.sh</code> and used for the job-mode container images, now starts all jobs with a fixed <code>JobID</code>. Thus, in order to run a cluster in HA mode, one needs to set a different <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#high-availability-cluster-id" target="_blank" rel="noopener">cluster id</a> for each job/cluster.</p>
<h3 id="Scala-shell-does-not-work-with-Scala-2-12"><a href="#Scala-shell-does-not-work-with-Scala-2-12" class="headerlink" title="Scala shell does not work with Scala 2.12"></a>Scala shell does not work with Scala 2.12</h3><p>Flink’s Scala shell does not work with Scala 2.12. Therefore, the module <code>flink-scala-shell</code> is not being released for Scala 2.12.</p>
<p>See <a href="https://issues.apache.org/jira/browse/FLINK-10911" target="_blank" rel="noopener">FLINK-10911</a> for more details.</p>
<h3 id="Limitations-of-failover-strategies"><a href="#Limitations-of-failover-strategies" class="headerlink" title="Limitations of failover strategies"></a>Limitations of failover strategies</h3><p>Flink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option <code>jobmanager.execution.failover-strategy</code> from your <code>flink-conf.yaml</code> or set it to <code>&quot;full&quot;</code>.</p>
<p>In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See <a href="https://issues.apache.org/jira/browse/FLINK-10880" target="_blank" rel="noopener">FLINK-10880</a> for more details.</p>
<h3 id="SQL-over-window-preceding-clause"><a href="#SQL-over-window-preceding-clause" class="headerlink" title="SQL over window preceding clause"></a>SQL over window preceding clause</h3><p>The over window <code>preceding</code> clause is now optional. It defaults to <code>UNBOUNDED</code> if not specified.</p>
<h3 id="OperatorSnapshotUtil-writes-v2-snapshots"><a href="#OperatorSnapshotUtil-writes-v2-snapshots" class="headerlink" title="OperatorSnapshotUtil writes v2 snapshots"></a>OperatorSnapshotUtil writes v2 snapshots</h3><p>Snapshots created with <code>OperatorSnapshotUtil</code> are now written in the savepoint format <code>v2</code>.</p>
<h3 id="SBT-projects-and-the-MiniClusterResource"><a href="#SBT-projects-and-the-MiniClusterResource" class="headerlink" title="SBT projects and the MiniClusterResource"></a>SBT projects and the MiniClusterResource</h3><p>If you have a <code>sbt</code> project which uses the <code>MiniClusterResource</code>, you now have to add the <code>flink-runtime</code> test-jar dependency explicitly via:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libraryDependencies += &quot;org.apache.flink&quot; %% &quot;flink-runtime&quot; % flinkVersion % Test classifier &quot;tests&quot;</span><br></pre></td></tr></table></figure>

<p>The reason for this is that the <code>MiniClusterResource</code> has been moved from <code>flink-test-utils</code> to <code>flink-runtime</code>. The module <code>flink-test-utils</code> has correctly a <code>test-jar</code> dependency on <code>flink-runtime</code>. However, <code>sbt</code> does not properly pull in transitive <code>test-jar</code> dependencies as described in this <a href="https://github.com/sbt/sbt/issues/2964" target="_blank" rel="noopener">sbt issue</a>. Consequently, it is necessary to specify the <code>test-jar</code> dependency explicitly.</p>
<h1 id="Flink-1-6"><a href="#Flink-1-6" class="headerlink" title="Flink 1.6"></a>Flink 1.6</h1><h3 id="Changed-Configuration-Default-Values"><a href="#Changed-Configuration-Default-Values" class="headerlink" title="Changed Configuration Default Values"></a>Changed Configuration Default Values</h3><p>The default value of the slot idle timeout <code>slot.idle.timeout</code> is set to the default value of the heartbeat timeout (<code>50 s</code>).</p>
<h3 id="Changed-ElasticSearch-5-x-Sink-API"><a href="#Changed-ElasticSearch-5-x-Sink-API" class="headerlink" title="Changed ElasticSearch 5.x Sink API"></a>Changed ElasticSearch 5.x Sink API</h3><p>Previous APIs in the Flink ElasticSearch 5.x Sink’s <code>RequestIndexer</code> interface have been deprecated in favor of new signatures. When adding requests to the <code>RequestIndexer</code>, the requests now must be of type <code>IndexRequest</code>, <code>DeleteRequest</code>, or <code>UpdateRequest</code>, instead of the base <code>ActionRequest</code>.</p>
<h3 id="Limitations-of-failover-strategies-1"><a href="#Limitations-of-failover-strategies-1" class="headerlink" title="Limitations of failover strategies"></a>Limitations of failover strategies</h3><p>Flink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option <code>jobmanager.execution.failover-strategy</code> from your <code>flink-conf.yaml</code> or set it to <code>&quot;full&quot;</code>.</p>
<p>In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See <a href="https://issues.apache.org/jira/browse/FLINK-10880" target="_blank" rel="noopener">FLINK-10880</a> for more details.</p>
<h1 id="Flink-1-5"><a href="#Flink-1-5" class="headerlink" title="Flink 1.5"></a>Flink 1.5</h1><p>These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5.</p>
<h3 id="Update-Configuration-for-Reworked-Job-Deployment"><a href="#Update-Configuration-for-Reworked-Job-Deployment" class="headerlink" title="Update Configuration for Reworked Job Deployment"></a>Update Configuration for Reworked Job Deployment</h3><p>Flink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos. Flink will automatically determine the number of containers from the parallelism of the application.</p>
<p>Although the deployment logic was completely reworked, we aimed to not unnecessarily change the previous behavior to enable a smooth transition. Nonetheless, there are a few options that you should update in your <code>conf/flink-conf.yaml</code> or know about.</p>
<ul>
<li>The allocation of TaskManagers with multiple slots is not fully supported yet. Therefore, we recommend to configure TaskManagers with a single slot, i.e., set <code>taskmanager.numberOfTaskSlots: 1</code></li>
<li>If you observed any problems with the new deployment mode, you can always switch back to the pre-1.5 behavior by configuring <code>mode: legacy</code>.</li>
</ul>
<p>Please report any problems or possible improvements that you notice to the Flink community, either by posting to a mailing list or by opening a JIRA issue.</p>
<p><em>Note</em>: We plan to remove the legacy mode in the next release.</p>
<h3 id="Update-Configuration-for-Reworked-Network-Stack"><a href="#Update-Configuration-for-Reworked-Network-Stack" class="headerlink" title="Update Configuration for Reworked Network Stack"></a>Update Configuration for Reworked Network Stack</h3><p>The changes on the networking stack for credit-based flow control and improved latency affect the configuration of network buffers. In a nutshell, the networking stack can require more memory to run applications. Hence, you might need to adjust the network configuration of your Flink setup.</p>
<p>There are two ways to address problems of job submissions that fail due to lack of network buffers.</p>
<ul>
<li>Reduce the number of buffers per channel, i.e., <code>taskmanager.network.memory.buffers-per-channel</code> or</li>
<li>Increase the amount of TaskManager memory that is used by the network stack, i.e., increase <code>taskmanager.network.memory.fraction</code> and/or <code>taskmanager.network.memory.max</code>.</li>
</ul>
<p>Please consult the section about <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#configuring-the-network-buffers" target="_blank" rel="noopener">network buffer configuration</a> in the Flink documentation for details. In case you experience issues with the new credit-based flow control mode, you can disable flow control by setting <code>taskmanager.network.credit-model: false</code>.</p>
<p><em>Note</em>: We plan to remove the old model and this configuration in the next release.</p>
<h3 id="Hadoop-Classpath-Discovery"><a href="#Hadoop-Classpath-Discovery" class="headerlink" title="Hadoop Classpath Discovery"></a>Hadoop Classpath Discovery</h3><p>We removed the automatic Hadoop classpath discovery via the Hadoop binary. If you want Flink to pick up the Hadoop classpath you have to export <code>HADOOP_CLASSPATH</code>. On cloud environments and most Hadoop distributions you would do</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=`hadoop classpath`.</span><br></pre></td></tr></table></figure>

<h3 id="Breaking-Changes-of-the-REST-API"><a href="#Breaking-Changes-of-the-REST-API" class="headerlink" title="Breaking Changes of the REST API"></a>Breaking Changes of the REST API</h3><p>In an effort to harmonize, extend, and improve the REST API, a few handlers and return values were changed.</p>
<ul>
<li>The jobs overview handler is now registered under <code>/jobs/overview</code> (before <code>/joboverview</code>) and returns a list of job details instead of the pre-grouped view of running, finished, cancelled and failed jobs.</li>
<li>The REST API to cancel a job was changed.</li>
<li>The REST API to cancel a job with savepoint was changed.</li>
</ul>
<p>Please check the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/monitoring/rest_api.html#available-requests" target="_blank" rel="noopener">REST API documentation</a> for details.</p>
<h3 id="Kafka-Producer-Flushes-on-Checkpoint-by-Default"><a href="#Kafka-Producer-Flushes-on-Checkpoint-by-Default" class="headerlink" title="Kafka Producer Flushes on Checkpoint by Default"></a>Kafka Producer Flushes on Checkpoint by Default</h3><p>The Flink Kafka Producer now flushes on checkpoints by default. Prior to version 1.5, the behaviour was disabled by default and users had to explicitly call <code>setFlushOnCheckpoints(true)</code> on the producer to enable it.</p>
<h3 id="Updated-Kinesis-Dependency"><a href="#Updated-Kinesis-Dependency" class="headerlink" title="Updated Kinesis Dependency"></a>Updated Kinesis Dependency</h3><p>The Kinesis dependencies of Flink’s Kinesis connector have been updated to the following versions.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;aws.sdk.version&gt;1.11.319&lt;/aws.sdk.version&gt;</span><br><span class="line">&lt;aws.kinesis-kcl.version&gt;1.9.0&lt;/aws.kinesis-kcl.version&gt;</span><br><span class="line">&lt;aws.kinesis-kpl.version&gt;0.12.9&lt;/aws.kinesis-kcl.version&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Limitations-of-failover-strategies-2"><a href="#Limitations-of-failover-strategies-2" class="headerlink" title="Limitations of failover strategies"></a>Limitations of failover strategies</h3><p>Flink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option <code>jobmanager.execution.failover-strategy</code> from your <code>flink-conf.yaml</code> or set it to <code>&quot;full&quot;</code>.</p>
<p>In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See <a href="https://issues.apache.org/jira/browse/FLINK-10880" target="_blank" rel="noopener">FLINK-10880</a> for more details.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/13/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/15/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Focus-1</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">250</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">63</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">102</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://gitee.com/carloz" title="repository - https://gitee.com/carloz" target="_blank">repository - https://gitee.com/carloz</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Focus-1</span>

  
</div>








  <div class="footer-custom">Hosted by <a target="_blank" href="https://gitee.com/carloz">Gitee Repo</a></div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
