<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  
    
      
    

    
  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=consolas:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="keywords" content="Java Kafka Docker JVM NIO Netty">
<meta property="og:type" content="website">
<meta property="og:title" content="Focus-1">
<meta property="og:url" content="https://carlo-z.com/page/13/index.html">
<meta property="og:site_name" content="Focus-1">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Focus-1">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://carlo-z.com/page/13/">





  <title>Focus-1</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Focus-1</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/java/concurrent-program/4-3-线程间通信/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/java/concurrent-program/4-3-线程间通信/" itemprop="url">4.3、线程间通信</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-01T00:00:00+08:00">
                2018-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/" itemprop="url" rel="index">
                    <span itemprop="name">java</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/java/java-concurrent-program/" itemprop="url" rel="index">
                    <span itemprop="name">java-concurrent-program</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="4-3、线程间通信"><a href="#4-3、线程间通信" class="headerlink" title="4.3、线程间通信"></a>4.3、线程间通信</h1><p>通信方式：共享内存、消息传递</p>
<h3 id="4-3-1、volatile-和-synchronized关键字"><a href="#4-3-1、volatile-和-synchronized关键字" class="headerlink" title="4.3.1、volatile 和 synchronized关键字"></a>4.3.1、volatile 和 synchronized关键字</h3><ul>
<li><p>volatile修饰变量 —— 禁止指令重排 和 变量对 所有线程的可见；</p>
</li>
<li><p>synchronized修饰方法或同步块 —— 同一时刻，只有一个线程处于方法或同步块中，保证 线程对变量访问的可见性 和 排他性；synchronized本身没有禁止指令重排的功能，需要配合volatile使用；</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/distribution/distributed-computing/distributed-computing-docs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/distribution/distributed-computing/distributed-computing-docs/" itemprop="url">分布式计算文档汇总</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T00:00:00+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/distribution/" itemprop="url" rel="index">
                    <span itemprop="name">distribution</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/distribution/distributed-computing/" itemprop="url" rel="index">
                    <span itemprop="name">distributed-computing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<p>分布式计算总站：<a href="https://equn.com/wiki/首页" target="_blank" rel="noopener">https://equn.com/wiki/%E9%A6%96%E9%A1%B5</a></p>
<p>知乎·分布式计算：<a href="https://www.zhihu.com/topic/19552071/hot" target="_blank" rel="noopener">https://www.zhihu.com/topic/19552071/hot</a></p>
<h1 id="批计算"><a href="#批计算" class="headerlink" title="批计算"></a>批计算</h1><p>MapReduce 是一种 <u><strong>分而治之</strong></u> 的计算模式，在分布式领域中，除了典型的 Hadoop 的 MapReduce(Google MapReduce 的开源实现)，还有 Fork-Join，Fork-Join 是 Java 等语言或库提供的原生多线程并行处理框架，采用线程级的分而治之计算模式。它充分利用多核 CPU 的优势，以递归的方式把一个任务拆分成多个“小任务”，把多个“小任务”放到多个处理器上并行执行，即 Fork 操作。当多个“小任务”执行完成之后，再将这些执行结果合并起来即可得到原始任务的结果，即 Join 操作。</p>
<p>虽然 MapReduce 是进程级的分而治之计算模式，但与 Fork-Join 的核心思想是一致的。因此，Fork-Join 又被称为 Java 版的 MapReduce 框架。但，MapReduce 和 Fork-Join 之间有一个本质的区别：Fork-Join 不能大规模扩展，只适用于在单个 Java 虚拟机上运行，多个小任务虽然运行在不同的处理器上，但可以相互通信，甚至一个线程可以“窃取”其他线程上的子任务。</p>
<p>MapReduce 可以大规模扩展，适用于大型计算机集群。通过 MapReduce 拆分后的任务，可以跨多个计算机去执行，且各个小任务之间不会相互通信。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708160534.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708160729.png" alt></p>
<p>MapReduce 模式的核心思想是，将大任务拆分成多个小任务，针对这些小任务分别计算后，再合并各小任务的结果以得到大任务的计算结果，任务运行完成后整个任务进程就结束了，属于短任务模式。但任务进程的启动和停止是一件很耗时的事儿，因此 MapReduce 对处理实时性的任务就不太合适了</p>
<h1 id="流计算"><a href="#流计算" class="headerlink" title="流计算"></a>流计算</h1><p>实时性任务主要是针对流数据的处理，对处理时延要求很高，通常需要有常驻服务进程，等待数据的随时到来随时处理，以保证低时延。处理流数据任务的计算模式，在分布式领域中叫作 Stream。近年来，由于网络监控、传感监测、AR/VR 等实时性应用的兴起，一类需要处理流数据的业务发展了起来。比如各种直播平台中，我们需要处理直播产生的音视频数据流等。这种如流水般持续涌现，且需要实时处理的数据，我们称之为流数据。</p>
<p>总结来讲，流数据的特征主要包括以下 4 点：</p>
<ul>
<li>数据如流水般持续、快速地到达；</li>
<li>海量数据规模，数据量可达到 TB 级甚至 PB 级；</li>
<li>对实时性要求高，随着时间流逝，数据的价值会大幅降低；</li>
<li>数据顺序无法保证，系统无法控制将要处理的数据元素的顺序。</li>
</ul>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708160931.png" alt></p>
<p>第一步，提交流式计算作业</p>
<p>第二步，加载流式数据进行流计算</p>
<p>第三步，持续输出计算结果。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200708161010.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/flink/flink-version-changelog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/flink/flink-version-changelog/" itemprop="url">Flink 各版本 changelog</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-11T00:00:00+08:00">
                2018-06-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/flink/" itemprop="url" rel="index">
                    <span itemprop="name">flink</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<blockquote>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/</a></p>
</blockquote>
<h1 id="Flink-1-10"><a href="#Flink-1-10" class="headerlink" title="Flink 1.10"></a>Flink 1.10</h1><h3 id="Clusters-amp-Deployment"><a href="#Clusters-amp-Deployment" class="headerlink" title="Clusters &amp; Deployment"></a>Clusters &amp; Deployment</h3><h4 id="FileSystems-should-be-loaded-via-Plugin-Architecture-FLINK-11956"><a href="#FileSystems-should-be-loaded-via-Plugin-Architecture-FLINK-11956" class="headerlink" title="FileSystems should be loaded via Plugin Architecture (FLINK-11956)"></a>FileSystems should be loaded via Plugin Architecture (<a href="https://issues.apache.org/jira/browse/FLINK-11956" target="_blank" rel="noopener">FLINK-11956</a>)</h4><p>s3-hadoop and s3-presto filesystems do no longer use class relocations and need to be loaded through <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/filesystems/#pluggable-file-systems" target="_blank" rel="noopener">plugins</a> but now seamlessly integrate with all credential providers. Other filesystems are strongly recommended to be only used as plugins as we will continue to remove relocations.</p>
<h4 id="Flink-Client-respects-Classloading-Policy-FLINK-13749"><a href="#Flink-Client-respects-Classloading-Policy-FLINK-13749" class="headerlink" title="Flink Client respects Classloading Policy (FLINK-13749)"></a>Flink Client respects Classloading Policy (<a href="https://issues.apache.org/jira/browse/FLINK-13749" target="_blank" rel="noopener">FLINK-13749</a>)</h4><p>The Flink client now also respects the configured classloading policy, i.e., <code>parent-first</code> or <code>child-first</code> classloading. Previously, only cluster components such as the job manager or task manager supported this setting. This does mean that users might get different behaviour in their programs, in which case they should configure the classloading policy explicitly to use <code>parent-first</code> classloading, which was the previous (hard-coded) behaviour.</p>
<h4 id="Enable-spreading-out-Tasks-evenly-across-all-TaskManagers-FLINK-12122"><a href="#Enable-spreading-out-Tasks-evenly-across-all-TaskManagers-FLINK-12122" class="headerlink" title="Enable spreading out Tasks evenly across all TaskManagers (FLINK-12122)"></a>Enable spreading out Tasks evenly across all TaskManagers (<a href="https://issues.apache.org/jira/browse/FLINK-12122" target="_blank" rel="noopener">FLINK-12122</a>)</h4><p>When <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6</a> was rolled out with Flink 1.5.0, we changed how slots are allocated from TaskManagers (TMs). Instead of evenly allocating the slots from all registered TMs, we had the tendency to exhaust a TM before using another one. To use a scheduling strategy that is more similar to the pre-FLIP-6 behaviour, where Flink tries to spread out the workload across all currently available TMs, one can set <code>cluster.evenly-spread-out-slots: true</code> in the <code>flink-conf.yaml</code>.</p>
<h4 id="Directory-Structure-Change-for-highly-available-Artifacts-FLINK-13633"><a href="#Directory-Structure-Change-for-highly-available-Artifacts-FLINK-13633" class="headerlink" title="Directory Structure Change for highly available Artifacts (FLINK-13633)"></a>Directory Structure Change for highly available Artifacts (<a href="https://issues.apache.org/jira/browse/FLINK-13633" target="_blank" rel="noopener">FLINK-13633</a>)</h4><p>All highly available artifacts stored by Flink will now be stored under <code>HA_STORAGE_DIR/HA_CLUSTER_ID</code> with <code>HA_STORAGE_DIR</code> configured by <code>high-availability.storageDir</code> and <code>HA_CLUSTER_ID</code> configured by <code>high-availability.cluster-id</code>.</p>
<h4 id="Resources-and-JARs-shipped-via-–yarnship-will-be-ordered-in-the-Classpath-FLINK-13127"><a href="#Resources-and-JARs-shipped-via-–yarnship-will-be-ordered-in-the-Classpath-FLINK-13127" class="headerlink" title="Resources and JARs shipped via –yarnship will be ordered in the Classpath (FLINK-13127)"></a>Resources and JARs shipped via –yarnship will be ordered in the Classpath (<a href="https://issues.apache.org/jira/browse/FLINK-13127" target="_blank" rel="noopener">FLINK-13127</a>)</h4><p>When using the <code>--yarnship</code> command line option, resource directories and jar files will be added to the classpath in lexicographical order with resources directories appearing first.</p>
<h4 id="Removal-of-–yn-–yarncontainer-Command-Line-Options-FLINK-12362"><a href="#Removal-of-–yn-–yarncontainer-Command-Line-Options-FLINK-12362" class="headerlink" title="Removal of –yn/–yarncontainer Command Line Options (FLINK-12362)"></a>Removal of –yn/–yarncontainer Command Line Options (<a href="https://issues.apache.org/jira/browse/FLINK-12362" target="_blank" rel="noopener">FLINK-12362</a>)</h4><p>The Flink CLI no longer supports the deprecated command line options <code>-yn/--yarncontainer</code>, which were used to specify the number of containers to start on YARN. This option has been deprecated since the introduction of <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="noopener">FLIP-6</a>. All Flink users are advised to remove this command line option.</p>
<h4 id="Removal-of-–yst-–yarnstreaming-Command-Line-Options-FLINK-14957"><a href="#Removal-of-–yst-–yarnstreaming-Command-Line-Options-FLINK-14957" class="headerlink" title="Removal of –yst/–yarnstreaming Command Line Options (FLINK-14957)"></a>Removal of –yst/–yarnstreaming Command Line Options (<a href="https://issues.apache.org/jira/browse/FLINK-14957" target="_blank" rel="noopener">FLINK-14957</a>)</h4><p>The Flink CLI no longer supports the deprecated command line options <code>-yst/--yarnstreaming</code>, which were used to disable eager pre-allocation of memory. All Flink users are advised to remove this command line option.</p>
<h4 id="Mesos-Integration-will-reject-expired-Offers-faster-FLINK-14029"><a href="#Mesos-Integration-will-reject-expired-Offers-faster-FLINK-14029" class="headerlink" title="Mesos Integration will reject expired Offers faster (FLINK-14029)"></a>Mesos Integration will reject expired Offers faster (<a href="https://issues.apache.org/jira/browse/FLINK-14029" target="_blank" rel="noopener">FLINK-14029</a>)</h4><p>Flink’s Mesos integration now rejects all expired offers instead of only 4. This improves the situation where Fenzo holds on to a lot of expired offers without giving them back to the Mesos resource manager.</p>
<h4 id="Scheduler-Rearchitecture-FLINK-14651"><a href="#Scheduler-Rearchitecture-FLINK-14651" class="headerlink" title="Scheduler Rearchitecture (FLINK-14651)"></a>Scheduler Rearchitecture (<a href="https://issues.apache.org/jira/browse/FLINK-14651" target="_blank" rel="noopener">FLINK-14651</a>)</h4><p>Flink’s scheduler was refactored with the goal of making scheduling strategies customizable in the future. Using the legacy scheduler is discouraged as it will be removed in a future release. However, users that experience issues related to scheduling can fallback to the legacy scheduler by setting <code>jobmanager.scheduler</code> to <code>legacy</code> in their <code>flink-conf.yaml</code> for the time being. Note, however, that using the legacy scheduler with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/task_failure_recovery.html#restart-pipelined-region-failover-strategy" target="_blank" rel="noopener">Pipelined Region Failover Strategy</a> enabled has the following caveats:</p>
<ul>
<li>Exceptions that caused a job to restart will not be shown on the job overview page of the Web UI (<a href="https://issues.apache.org/jira/browse/FLINK-15917" target="_blank" rel="noopener">FLINK-15917</a>). However, exceptions that cause a job to fail (e.g., when all restart attempts exhausted) will still be shown.</li>
<li>The <code>uptime</code> metric will not be reset after restarting a job due to task failure (<a href="https://issues.apache.org/jira/browse/FLINK-15918" target="_blank" rel="noopener">FLINK-15918</a>).</li>
</ul>
<p>Note that in the default <code>flink-conf.yaml</code>, the Pipelined Region Failover Strategy is already enabled. That is, users that want to use the legacy scheduler and cannot accept aforementioned caveats should make sure that <code>jobmanager.execution.failover-strategy</code> is set to <code>full</code> or not set at all.</p>
<h4 id="Java-11-Support-FLINK-10725"><a href="#Java-11-Support-FLINK-10725" class="headerlink" title="Java 11 Support (FLINK-10725)"></a>Java 11 Support (<a href="https://issues.apache.org/jira/browse/FLINK-10725" target="_blank" rel="noopener">FLINK-10725</a>)</h4><p>Beginning from this release, Flink can be compiled and run with Java 11. All Java 8 artifacts can be also used with Java 11. This means that users that want to run Flink with Java 11 do not have to compile Flink themselves.</p>
<p>When starting Flink with Java 11, the following warnings may be logged:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by org.apache.flink.core.memory.MemoryUtils (file:/opt/flink/flink-1.10.0/lib/flink-dist_2.11-1.10.0.jar) to constructor java.nio.DirectByteBuffer(long,int)</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of org.apache.flink.core.memory.MemoryUtils</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br><span class="line"></span><br><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/home/flinkuser/.m2/repository/org/apache/flink/flink-core/1.10.0/flink-core-1.10.0.jar) to field java.lang.String.value</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br><span class="line"></span><br><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by org.jboss.netty.util.internal.ByteBufferUtil (file:/home/flinkuser/.m2/repository/io/netty/netty/3.10.6.Final/netty-3.10.6.Final.jar) to method java.nio.DirectByteBuffer.cleaner()</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of org.jboss.netty.util.internal.ByteBufferUtil</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br><span class="line"></span><br><span class="line">WARNING: An illegal reflective access operation has occurred</span><br><span class="line">WARNING: Illegal reflective access by com.esotericsoftware.kryo.util.UnsafeUtil (file:/home/flinkuser/.m2/repository/com/esotericsoftware/kryo/kryo/2.24.0/kryo-2.24.0.jar) to constructor java.nio.DirectByteBuffer(long,int,java.lang.Object)</span><br><span class="line">WARNING: Please consider reporting this to the maintainers of com.esotericsoftware.kryo.util.UnsafeUtil</span><br><span class="line">WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations</span><br><span class="line">WARNING: All illegal access operations will be denied in a future release</span><br></pre></td></tr></table></figure>

<p>These warnings are considered harmless and will be addressed in future Flink releases.</p>
<p>Lastly, note that the connectors for Cassandra, Hive, HBase, and Kafka 0.8–0.11 have not been tested with Java 11 because the respective projects did not provide Java 11 support at the time of the Flink 1.10.0 release.</p>
<h3 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h3><h4 id="New-Task-Executor-Memory-Model-FLINK-13980"><a href="#New-Task-Executor-Memory-Model-FLINK-13980" class="headerlink" title="New Task Executor Memory Model (FLINK-13980)"></a>New Task Executor Memory Model (<a href="https://issues.apache.org/jira/browse/FLINK-13980" target="_blank" rel="noopener">FLINK-13980</a>)</h4><p>With <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors" target="_blank" rel="noopener">FLIP-49</a>, a new memory model has been introduced for the task executor. New configuration options have been introduced to control the memory consumption of the task executor process. This affects all types of deployments: standalone, YARN, Mesos, and the new active Kubernetes integration. The memory model of the job manager process has not been changed yet but it is planned to be updated as well.</p>
<p>If you try to reuse your previous Flink configuration without any adjustments, the new memory model can result in differently computed memory parameters for the JVM and, thus, performance changes.</p>
<p>Please, check <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/memory/mem_setup.html" target="_blank" rel="noopener">the user documentation</a> for more details.</p>
<h5 id="Deprecation-and-breaking-changes"><a href="#Deprecation-and-breaking-changes" class="headerlink" title="Deprecation and breaking changes"></a>Deprecation and breaking changes</h5><p>The following options have been removed and have no effect anymore:</p>
<table>
<thead>
<tr>
<th align="left">Deprecated/removed config option</th>
<th align="left">Note</th>
</tr>
</thead>
<tbody><tr>
<td align="left">taskmanager.memory.fraction</td>
<td align="left">Check also the description of the new option <code>taskmanager.memory.managed.fraction</code> but it has different semantics and the value of the deprecated option usually has to be adjusted</td>
</tr>
<tr>
<td align="left">taskmanager.memory.off-heap</td>
<td align="left">Support for on-heap managed memory has been removed, leaving off-heap managed memory as the only possibility</td>
</tr>
<tr>
<td align="left">taskmanager.memory.preallocate</td>
<td align="left">Pre-allocation is no longer supported, and managed memory is always allocated lazily</td>
</tr>
</tbody></table>
<p>The following options, if used, are interpreted as other new options in order to maintain backwards compatibility where it makes sense:</p>
<table>
<thead>
<tr>
<th align="left">Deprecated config option</th>
<th align="left">Interpreted as</th>
</tr>
</thead>
<tbody><tr>
<td align="left">taskmanager.heap.size</td>
<td align="left">taskmanager.memory.flink.size for standalone deploymenttaskmanager.memory.process.size for containerized deployments</td>
</tr>
<tr>
<td align="left">taskmanager.memory.size</td>
<td align="left">taskmanager.memory.managed.size</td>
</tr>
<tr>
<td align="left">taskmanager.network.memory.min</td>
<td align="left">taskmanager.memory.network.min</td>
</tr>
<tr>
<td align="left">taskmanager.network.memory.max</td>
<td align="left">taskmanager.memory.network.max</td>
</tr>
<tr>
<td align="left">taskmanager.network.memory.fraction</td>
<td align="left">taskmanager.memory.network.fraction</td>
</tr>
</tbody></table>
<p>The container cut-off configuration options, <code>containerized.heap-cutoff-ratio</code> and <code>containerized.heap-cutoff-min</code>, have no effect for task executor processes anymore but they still have the same semantics for the JobManager process.</p>
<h4 id="RocksDB-State-Backend-Memory-Control-FLINK-7289"><a href="#RocksDB-State-Backend-Memory-Control-FLINK-7289" class="headerlink" title="RocksDB State Backend Memory Control (FLINK-7289)"></a>RocksDB State Backend Memory Control (<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="noopener">FLINK-7289</a>)</h4><p>Together with the introduction of the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/release-notes/flink-1.10.html#new-task-executor-memory-model-flink-13980" target="_blank" rel="noopener">new Task Executor Memory Model</a>, the memory consumption of the RocksDB state backend will be limited by the total amount of Flink Managed Memory, which can be configured via <code>taskmanager.memory.managed.size</code> or <code>taskmanager.memory.managed.fraction</code>. Furthermore, users can tune RocksDB’s write/read memory ratio (<code>state.backend.rocksdb.memory.write-buffer-ratio</code>, by default <code>0.5</code>) and the reserved memory fraction for indices/filters (<code>state.backend.rocksdb.memory.high-prio-pool-ratio</code>, by default <code>0.1</code>). More details and advanced configuration options can be found in the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/state/large_state_tuning.html#tuning-rocksdb-memory" target="_blank" rel="noopener">Flink user documentation</a>.</p>
<h4 id="Fine-grained-Operator-Resource-Management-FLINK-14058"><a href="#Fine-grained-Operator-Resource-Management-FLINK-14058" class="headerlink" title="Fine-grained Operator Resource Management (FLINK-14058)"></a>Fine-grained Operator Resource Management (<a href="https://issues.apache.org/jira/browse/FLINK-14058" target="_blank" rel="noopener">FLINK-14058</a>)</h4><p>Config options <code>table.exec.resource.external-buffer-memory</code>, <code>table.exec.resource.hash-agg.memory</code>, <code>table.exec.resource.hash-join.memory</code>, and <code>table.exec.resource.sort.memory</code> have been deprecated. Beginning from Flink 1.10, these config options are interpreted as weight hints instead of absolute memory requirements. Flink choses sensible default weight hints which should not be adjustment by users.</p>
<h3 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h3><h4 id="Rename-of-ANY-Type-to-RAW-Type-FLINK-14904"><a href="#Rename-of-ANY-Type-to-RAW-Type-FLINK-14904" class="headerlink" title="Rename of ANY Type to RAW Type (FLINK-14904)"></a>Rename of ANY Type to RAW Type (<a href="https://issues.apache.org/jira/browse/FLINK-14904" target="_blank" rel="noopener">FLINK-14904</a>)</h4><p>The identifier <code>raw</code> is a reserved keyword now and must be escaped with backticks when used as a SQL field or function name.</p>
<h4 id="Rename-of-Table-Connector-Properties-FLINK-14649"><a href="#Rename-of-Table-Connector-Properties-FLINK-14649" class="headerlink" title="Rename of Table Connector Properties (FLINK-14649)"></a>Rename of Table Connector Properties (<a href="https://issues.apache.org/jira/browse/FLINK-14649" target="_blank" rel="noopener">FLINK-14649</a>)</h4><p>Some indexed properties for table connectors have been flattened and renamed for a better user experience when writing DDL statements. This affects the Kafka Connector properties <code>connector.properties</code> and <code>connector.specific-offsets</code>. Furthermore, the Elasticsearch Connector property <code>connector.hosts</code> is affected. The aforementioned, old properties are deprecated and will be removed in future versions. Please consult the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/connect.html#table-connectors" target="_blank" rel="noopener">Table Connectors documentation</a> for the new property names.</p>
<h4 id="Methods-for-interacting-with-temporary-Tables-amp-Views-FLINK-14490"><a href="#Methods-for-interacting-with-temporary-Tables-amp-Views-FLINK-14490" class="headerlink" title="Methods for interacting with temporary Tables &amp; Views (FLINK-14490)"></a>Methods for interacting with temporary Tables &amp; Views (<a href="https://issues.apache.org/jira/browse/FLINK-14490" target="_blank" rel="noopener">FLINK-14490</a>)</h4><p>Methods <code>registerTable()</code>/<code>registerDataStream()</code>/<code>registerDataSet()</code> have been deprecated in favor of <code>createTemporaryView()</code>, which better adheres to the corresponding SQL term.</p>
<p>The <code>scan()</code> method has been deprecated in favor of the <code>from()</code> method.</p>
<p>Methods <code>registerTableSource()</code>/<code>registerTableSink()</code> become deprecated in favor of <code>ConnectTableDescriptor#createTemporaryTable()</code>. The <code>ConnectTableDescriptor</code> approach expects only a set of string properties as a description of a TableSource or TableSink instead of an instance of a class in case of the deprecated methods. This in return makes it possible to reliably store those definitions in catalogs.</p>
<p>Method <code>insertInto(String path, String... pathContinued)</code> has been removed in favor of in <code>insertInto(String path)</code>.</p>
<p>All the newly introduced methods accept a String identifier which will be parsed into a 3-part identifier. The parser supports quoting the identifier. It also requires escaping any reserved SQL keywords.</p>
<h4 id="Removal-of-ExternalCatalog-API-FLINK-13697"><a href="#Removal-of-ExternalCatalog-API-FLINK-13697" class="headerlink" title="Removal of ExternalCatalog API (FLINK-13697)"></a>Removal of ExternalCatalog API (<a href="https://issues.apache.org/jira/browse/FLINK-13697" target="_blank" rel="noopener">FLINK-13697</a>)</h4><p>The deprecated <code>ExternalCatalog</code> API has been dropped. This includes:</p>
<ul>
<li><code>ExternalCatalog</code> (and all dependent classes, e.g., <code>ExternalTable</code>)</li>
<li><code>SchematicDescriptor</code>, <code>MetadataDescriptor</code>, <code>StatisticsDescriptor</code></li>
</ul>
<p>Users are advised to use the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/catalogs.html#catalog-api" target="_blank" rel="noopener">new Catalog API</a>.</p>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><h4 id="Introduction-of-Type-Information-for-ConfigOptions-FLINK-14493"><a href="#Introduction-of-Type-Information-for-ConfigOptions-FLINK-14493" class="headerlink" title="Introduction of Type Information for ConfigOptions (FLINK-14493)"></a>Introduction of Type Information for ConfigOptions (<a href="https://issues.apache.org/jira/browse/FLINK-14493" target="_blank" rel="noopener">FLINK-14493</a>)</h4><p>Getters of <code>org.apache.flink.configuration.Configuration</code> throw <code>IllegalArgumentException</code> now if the configured value cannot be parsed into the required type. In previous Flink releases the default value was returned in such cases.</p>
<h4 id="Increase-of-default-Restart-Delay-FLINK-13884"><a href="#Increase-of-default-Restart-Delay-FLINK-13884" class="headerlink" title="Increase of default Restart Delay (FLINK-13884)"></a>Increase of default Restart Delay (<a href="https://issues.apache.org/jira/browse/FLINK-13884" target="_blank" rel="noopener">FLINK-13884</a>)</h4><p>The default restart delay for all shipped restart strategies, i.e., <code>fixed-delay</code> and <code>failure-rate</code>, has been raised to 1 s (from originally 0 s).</p>
<h4 id="Simplification-of-Cluster-Level-Restart-Strategy-Configuration-FLINK-13921"><a href="#Simplification-of-Cluster-Level-Restart-Strategy-Configuration-FLINK-13921" class="headerlink" title="Simplification of Cluster-Level Restart Strategy Configuration (FLINK-13921)"></a>Simplification of Cluster-Level Restart Strategy Configuration (<a href="https://issues.apache.org/jira/browse/FLINK-13921" target="_blank" rel="noopener">FLINK-13921</a>)</h4><p>Previously, if the user had set <code>restart-strategy.fixed-delay.attempts</code> or <code>restart-strategy.fixed-delay.delay</code> but had not configured the option <code>restart-strategy</code>, the cluster-level restart strategy would have been <code>fixed-delay</code>. Now the cluster-level restart strategy is only determined by the config option <code>restart-strategy</code> and whether checkpointing is enabled. See <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/task_failure_recovery.html" target="_blank" rel="noopener"><em>“Task Failure Recovery”</em></a> for details.</p>
<h4 id="Disable-memory-mapped-BoundedBlockingSubpartition-by-default-FLINK-14952"><a href="#Disable-memory-mapped-BoundedBlockingSubpartition-by-default-FLINK-14952" class="headerlink" title="Disable memory-mapped BoundedBlockingSubpartition by default (FLINK-14952)"></a>Disable memory-mapped BoundedBlockingSubpartition by default (<a href="https://issues.apache.org/jira/browse/FLINK-14952" target="_blank" rel="noopener">FLINK-14952</a>)</h4><p>The config option <code>taskmanager.network.bounded-blocking-subpartition-type</code> has been renamed to <code>taskmanager.network.blocking-shuffle.type</code>. Moreover, the default value of the aforementioned config option has been changed from <code>auto</code> to <code>file</code>. The reason is that TaskManagers running on YARN with <code>auto</code>, could easily exceed the memory budget of their container, due to incorrectly accounted memory-mapped files memory usage.</p>
<h4 id="Removal-of-non-credit-based-Network-Flow-Control-FLINK-14516"><a href="#Removal-of-non-credit-based-Network-Flow-Control-FLINK-14516" class="headerlink" title="Removal of non-credit-based Network Flow Control (FLINK-14516)"></a>Removal of non-credit-based Network Flow Control (<a href="https://issues.apache.org/jira/browse/FLINK-14516" target="_blank" rel="noopener">FLINK-14516</a>)</h4><p>The non-credit-based network flow control code was removed alongside of the configuration option <code>taskmanager.network.credit-model</code>. Flink will now always use credit-based flow control.</p>
<h4 id="Removal-of-HighAvailabilityOptions-HA-JOB-DELAY-FLINK-13885"><a href="#Removal-of-HighAvailabilityOptions-HA-JOB-DELAY-FLINK-13885" class="headerlink" title="Removal of HighAvailabilityOptions#HA_JOB_DELAY (FLINK-13885)"></a>Removal of HighAvailabilityOptions#HA_JOB_DELAY (<a href="https://issues.apache.org/jira/browse/FLINK-13885" target="_blank" rel="noopener">FLINK-13885</a>)</h4><p>The configuration option <code>high-availability.job.delay</code> has been removed since it is no longer used.</p>
<h3 id="State"><a href="#State" class="headerlink" title="State"></a>State</h3><h4 id="Enable-Background-Cleanup-of-State-with-TTL-by-default-FLINK-14898"><a href="#Enable-Background-Cleanup-of-State-with-TTL-by-default-FLINK-14898" class="headerlink" title="Enable Background Cleanup of State with TTL by default (FLINK-14898)"></a>Enable Background Cleanup of State with TTL by default (<a href="https://issues.apache.org/jira/browse/FLINK-14898" target="_blank" rel="noopener">FLINK-14898</a>)</h4><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/stream/state/state.html#cleanup-of-expired-state" target="_blank" rel="noopener">Background cleanup of expired state with TTL</a> is activated by default now for all state backends shipped with Flink. Note that the RocksDB state backend implements background cleanup by employing a compaction filter. This has the caveat that even if a Flink job does not store state with TTL, a minor performance penalty during compaction is incurred. Users that experience noticeable performance degradation during RocksDB compaction can disable the TTL compaction filter by setting the config option <code>state.backend.rocksdb.ttl.compaction.filter.enabled</code> to <code>false</code>.</p>
<h4 id="Deprecation-of-StateTtlConfig-Builder-cleanupInBackground-FLINK-15606"><a href="#Deprecation-of-StateTtlConfig-Builder-cleanupInBackground-FLINK-15606" class="headerlink" title="Deprecation of StateTtlConfig#Builder#cleanupInBackground() (FLINK-15606)"></a>Deprecation of StateTtlConfig#Builder#cleanupInBackground() (<a href="https://issues.apache.org/jira/browse/FLINK-15606" target="_blank" rel="noopener">FLINK-15606</a>)</h4><p><code>StateTtlConfig#Builder#cleanupInBackground()</code> has been deprecated because the background cleanup of state with TTL is already enabled by default.</p>
<h4 id="Timers-are-stored-in-RocksDB-by-default-when-using-RocksDBStateBackend-FLINK-15637"><a href="#Timers-are-stored-in-RocksDB-by-default-when-using-RocksDBStateBackend-FLINK-15637" class="headerlink" title="Timers are stored in RocksDB by default when using RocksDBStateBackend (FLINK-15637)"></a>Timers are stored in RocksDB by default when using RocksDBStateBackend (<a href="https://issues.apache.org/jira/browse/FLINK-15637" target="_blank" rel="noopener">FLINK-15637</a>)</h4><p>The default timer store has been changed from Heap to RocksDB for the RocksDB state backend to support asynchronous snapshots for timer state and better scalability, with less than 5% performance cost. Users that find the performance decline critical can set <code>state.backend.rocksdb.timer-service.factory</code> to <code>HEAP</code> in <code>flink-conf.yaml</code> to restore the old behavior.</p>
<h4 id="Removal-of-StateTtlConfig-TimeCharacteristic-FLINK-15605"><a href="#Removal-of-StateTtlConfig-TimeCharacteristic-FLINK-15605" class="headerlink" title="Removal of StateTtlConfig#TimeCharacteristic (FLINK-15605)"></a>Removal of StateTtlConfig#TimeCharacteristic (<a href="https://issues.apache.org/jira/browse/FLINK-15605" target="_blank" rel="noopener">FLINK-15605</a>)</h4><p><code>StateTtlConfig#TimeCharacteristic</code> has been removed in favor of <code>StateTtlConfig#TtlTimeCharacteristic</code>.</p>
<h4 id="New-efficient-Method-to-check-if-MapState-is-empty-FLINK-13034"><a href="#New-efficient-Method-to-check-if-MapState-is-empty-FLINK-13034" class="headerlink" title="New efficient Method to check if MapState is empty (FLINK-13034)"></a>New efficient Method to check if MapState is empty (<a href="https://issues.apache.org/jira/browse/FLINK-13034" target="_blank" rel="noopener">FLINK-13034</a>)</h4><p>We have added a new method <code>MapState#isEmpty()</code> which enables users to check whether a map state is empty. The new method is 40% faster than <code>mapState.keys().iterator().hasNext()</code> when using the RocksDB state backend.</p>
<h4 id="RocksDB-Upgrade-FLINK-14483"><a href="#RocksDB-Upgrade-FLINK-14483" class="headerlink" title="RocksDB Upgrade (FLINK-14483)"></a>RocksDB Upgrade (<a href="https://issues.apache.org/jira/browse/FLINK-14483" target="_blank" rel="noopener">FLINK-14483</a>)</h4><p>We have again released our own RocksDB build (FRocksDB) which is based on RocksDB version 5.17.2 with several feature backports for the <a href="https://github.com/facebook/rocksdb/wiki/Write-Buffer-Manager" target="_blank" rel="noopener">Write Buffer Manager</a> to enable limiting RocksDB’s memory usage. The decision to release our own RocksDB build was made because later RocksDB versions suffer from a <a href="https://github.com/facebook/rocksdb/issues/5774" target="_blank" rel="noopener">performance regression under certain workloads</a>.</p>
<h4 id="RocksDB-Logging-disabled-by-default-FLINK-15068"><a href="#RocksDB-Logging-disabled-by-default-FLINK-15068" class="headerlink" title="RocksDB Logging disabled by default (FLINK-15068)"></a>RocksDB Logging disabled by default (<a href="https://issues.apache.org/jira/browse/FLINK-15068" target="_blank" rel="noopener">FLINK-15068</a>)</h4><p>Logging in RocksDB (e.g., logging related to flush, compaction, memtable creation, etc.) has been disabled by default to prevent disk space from being filled up unexpectedly. Users that need to enable logging should implement their own <code>RocksDBOptionsFactory</code> that creates <code>DBOptions</code> instances with <code>InfoLogLevel</code> set to <code>INFO_LEVEL</code>.</p>
<h4 id="Improved-RocksDB-Savepoint-Recovery-FLINK-12785"><a href="#Improved-RocksDB-Savepoint-Recovery-FLINK-12785" class="headerlink" title="Improved RocksDB Savepoint Recovery (FLINK-12785)"></a>Improved RocksDB Savepoint Recovery (<a href="https://issues.apache.org/jira/browse/FLINK-12785" target="_blank" rel="noopener">FLINK-12785</a>)</h4><p>In previous Flink releases users may encounter an <code>OutOfMemoryError</code> when restoring from a RocksDB savepoint containing large KV pairs. For that reason we introduced a configurable memory limit in the <code>RocksDBWriteBatchWrapper</code> with a default value of 2 MB. RocksDB’s WriteBatch will flush before the consumed memory limit is reached. If needed, the limit can be tuned via the <code>state.backend.rocksdb.write-batch-size</code> config option in <code>flink-conf.yaml</code>.</p>
<h3 id="PyFlink"><a href="#PyFlink" class="headerlink" title="PyFlink"></a>PyFlink</h3><h4 id="Python-2-Support-dropped-FLINK-14469"><a href="#Python-2-Support-dropped-FLINK-14469" class="headerlink" title="Python 2 Support dropped (FLINK-14469)"></a>Python 2 Support dropped (<a href="https://issues.apache.org/jira/browse/FLINK-14469" target="_blank" rel="noopener">FLINK-14469</a>)</h4><p>Beginning from this release, PyFlink does not support Python 2. This is because <a href="https://www.python.org/doc/sunset-python-2/" target="_blank" rel="noopener">Python 2 has reached end of life on January 1, 2020</a>, and several third-party projects that PyFlink depends on are also dropping Python 2 support.</p>
<h3 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h3><h4 id="InfluxdbReporter-skips-Inf-and-NaN-FLINK-12147"><a href="#InfluxdbReporter-skips-Inf-and-NaN-FLINK-12147" class="headerlink" title="InfluxdbReporter skips Inf and NaN (FLINK-12147)"></a>InfluxdbReporter skips Inf and NaN (<a href="https://issues.apache.org/jira/browse/FLINK-12147" target="_blank" rel="noopener">FLINK-12147</a>)</h4><p>The <code>InfluxdbReporter</code> now silently skips values that are unsupported by InfluxDB, such as <code>Double.POSITIVE_INFINITY</code>, <code>Double.NEGATIVE_INFINITY</code>, <code>Double.NaN</code>, etc.</p>
<h3 id="Connectors"><a href="#Connectors" class="headerlink" title="Connectors"></a>Connectors</h3><h4 id="Kinesis-Connector-License-Change-FLINK-12847"><a href="#Kinesis-Connector-License-Change-FLINK-12847" class="headerlink" title="Kinesis Connector License Change (FLINK-12847)"></a>Kinesis Connector License Change (<a href="https://issues.apache.org/jira/browse/FLINK-12847" target="_blank" rel="noopener">FLINK-12847</a>)</h4><p>flink-connector-kinesis is now licensed under the Apache License, Version 2.0, and its artifacts will be deployed to Maven central as part of the Flink releases. Users no longer need to build the Kinesis connector from source themselves.</p>
<h3 id="Miscellaneous-Interface-Changes"><a href="#Miscellaneous-Interface-Changes" class="headerlink" title="Miscellaneous Interface Changes"></a>Miscellaneous Interface Changes</h3><h4 id="ExecutionConfig-getGlobalJobParameters-cannot-return-null-anymore-FLINK-9787"><a href="#ExecutionConfig-getGlobalJobParameters-cannot-return-null-anymore-FLINK-9787" class="headerlink" title="ExecutionConfig#getGlobalJobParameters() cannot return null anymore (FLINK-9787)"></a>ExecutionConfig#getGlobalJobParameters() cannot return null anymore (<a href="https://issues.apache.org/jira/browse/FLINK-9787" target="_blank" rel="noopener">FLINK-9787</a>)</h4><p><code>ExecutionConfig#getGlobalJobParameters</code> has been changed to never return <code>null</code>. Conversely, <code>ExecutionConfig#setGlobalJobParameters(GlobalJobParameters)</code> will not accept <code>null</code> values anymore.</p>
<h4 id="Change-of-contract-in-MasterTriggerRestoreHook-interface-FLINK-14344"><a href="#Change-of-contract-in-MasterTriggerRestoreHook-interface-FLINK-14344" class="headerlink" title="Change of contract in MasterTriggerRestoreHook interface (FLINK-14344)"></a>Change of contract in MasterTriggerRestoreHook interface (<a href="https://issues.apache.org/jira/browse/FLINK-14344" target="_blank" rel="noopener">FLINK-14344</a>)</h4><p>Implementations of <code>MasterTriggerRestoreHook#triggerCheckpoint(long, long, Executor)</code> must be non-blocking now. Any blocking operation should be executed asynchronously, e.g., using the given executor.</p>
<h4 id="Client-and-Server-Side-Separation-of-HA-Services-FLINK-13750"><a href="#Client-and-Server-Side-Separation-of-HA-Services-FLINK-13750" class="headerlink" title="Client-/ and Server-Side Separation of HA Services (FLINK-13750)"></a>Client-/ and Server-Side Separation of HA Services (<a href="https://issues.apache.org/jira/browse/FLINK-13750" target="_blank" rel="noopener">FLINK-13750</a>)</h4><p>The <code>HighAvailabilityServices</code> have been split up into client-side <code>ClientHighAvailabilityServices</code> and cluster-side <code>HighAvailabilityServices</code>. When implementing custom high availability services, users should follow this separation by overriding the factory method <code>HighAvailabilityServicesFactory#createClientHAServices(Configuration)</code>. Moreover, <code>HighAvailabilityServices#getWebMonitorLeaderRetriever()</code> should no longer be implemented since it has been deprecated.</p>
<h4 id="Deprecation-of-HighAvailabilityServices-getWebMonitorLeaderElectionService-FLINK-13977"><a href="#Deprecation-of-HighAvailabilityServices-getWebMonitorLeaderElectionService-FLINK-13977" class="headerlink" title="Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() (FLINK-13977)"></a>Deprecation of HighAvailabilityServices#getWebMonitorLeaderElectionService() (<a href="https://issues.apache.org/jira/browse/FLINK-13977" target="_blank" rel="noopener">FLINK-13977</a>)</h4><p>Implementations of <code>HighAvailabilityServices</code> should implement <code>HighAvailabilityServices#getClusterRestEndpointLeaderElectionService()</code> instead of <code>HighAvailabilityServices#getWebMonitorLeaderElectionService()</code>.</p>
<h4 id="Interface-Change-in-LeaderElectionService-FLINK-14287"><a href="#Interface-Change-in-LeaderElectionService-FLINK-14287" class="headerlink" title="Interface Change in LeaderElectionService (FLINK-14287)"></a>Interface Change in LeaderElectionService (<a href="https://issues.apache.org/jira/browse/FLINK-14287" target="_blank" rel="noopener">FLINK-14287</a>)</h4><p><code>LeaderElectionService#confirmLeadership(UUID, String)</code> now takes an additional second argument, which is the address under which the leader will be reachable. All custom <code>LeaderElectionService</code> implementations will need to be updated accordingly.</p>
<h4 id="Deprecation-of-Checkpoint-Lock-FLINK-14857"><a href="#Deprecation-of-Checkpoint-Lock-FLINK-14857" class="headerlink" title="Deprecation of Checkpoint Lock (FLINK-14857)"></a>Deprecation of Checkpoint Lock (<a href="https://issues.apache.org/jira/browse/FLINK-14857" target="_blank" rel="noopener">FLINK-14857</a>)</h4><p>The method <code>org.apache.flink.streaming.runtime.tasks.StreamTask#getCheckpointLock()</code> is deprecated now. Users should use <code>MailboxExecutor</code> to run actions that require synchronization with the task’s thread (e.g. collecting output produced by an external thread). The methods <code>MailboxExecutor#yield()</code> or <code>MailboxExecutor#tryYield()</code> can be used for actions that need to give up control to other actions temporarily, e.g., if the current operator is blocked. The <code>MailboxExecutor</code> can be accessed by using <code>YieldingOperatorFactory</code> (see <code>AsyncWaitOperator</code> for an example usage).</p>
<h4 id="Deprecation-of-OptionsFactory-and-ConfigurableOptionsFactory-interfaces-FLINK-14926"><a href="#Deprecation-of-OptionsFactory-and-ConfigurableOptionsFactory-interfaces-FLINK-14926" class="headerlink" title="Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces (FLINK-14926)"></a>Deprecation of OptionsFactory and ConfigurableOptionsFactory interfaces (<a href="https://issues.apache.org/jira/browse/FLINK-14926" target="_blank" rel="noopener">FLINK-14926</a>)</h4><p>Interfaces <code>OptionsFactory</code> and <code>ConfigurableOptionsFactory</code> have been deprecated in favor of <code>RocksDBOptionsFactory</code> and <code>ConfigurableRocksDBOptionsFactory</code>, respectively.</p>
<h1 id="Flink-1-9"><a href="#Flink-1-9" class="headerlink" title="Flink 1.9"></a>Flink 1.9</h1><h2 id="Known-shortcomings-or-limitations-for-new-features"><a href="#Known-shortcomings-or-limitations-for-new-features" class="headerlink" title="Known shortcomings or limitations for new features"></a>Known shortcomings or limitations for new features</h2><h3 id="New-Table-SQL-Blink-planner"><a href="#New-Table-SQL-Blink-planner" class="headerlink" title="New Table / SQL Blink planner"></a>New Table / SQL Blink planner</h3><p>Flink 1.9.0 provides support for two planners for the Table API, namely Flink’s original planner and the new Blink planner. The original planner maintains same behaviour as previous releases, while the new Blink planner is still considered experimental and has the following limitations:</p>
<ul>
<li>The Blink planner can not be used with <code>BatchTableEnvironment</code>, and therefore Table programs ran with the planner can not be transformed to <code>DataSet</code> programs. This is by design and will also not be supported in the future. Therefore, if you want to run a batch job with the Blink planner, please use the new <code>TableEnvironment</code>. For streaming jobs, both <code>StreamTableEnvironment</code> and <code>TableEnvironment</code> works.</li>
<li>Implementations of <code>StreamTableSink</code> should implement the <code>consumeDataStream</code> method instead of <code>emitDataStream</code> if it is used with the Blink planner. Both methods work with the original planner. This is by design to make the returned <code>DataStreamSink</code> accessible for the planner.</li>
<li>Due to a bug with how transformations are not being cleared on execution, <code>TableEnvironment</code> instances should not be reused across multiple SQL statements when using the Blink planner.</li>
<li><code>Table.flatAggregate</code> is not supported</li>
<li>Session and count windows are not supported when running batch jobs.</li>
<li>The Blink planner only supports the new <code>Catalog</code> API, and does not support <code>ExternalCatalog</code> which is now deprecated.</li>
</ul>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13708" target="_blank" rel="noopener">FLINK-13708: Transformations should be cleared because a table environment could execute multiple job</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13473" target="_blank" rel="noopener">FLINK-13473: Add GroupWindowed FlatAggregate support to stream Table API (Blink planner), i.e, align with Flink planner</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13735" target="_blank" rel="noopener">FLINK-13735: Support session window with Blink planner in batch mode</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13736" target="_blank" rel="noopener">FLINK-13736: Support count window with Blink planner in batch mode</a></li>
</ul>
<h3 id="SQL-DDL"><a href="#SQL-DDL" class="headerlink" title="SQL DDL"></a>SQL DDL</h3><p>In Flink 1.9.0, the community also added a preview feature about SQL DDL, but only for batch style DDLs. Therefore, all streaming related concepts are not supported yet, for example watermarks.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13661" target="_blank" rel="noopener">FLINK-13661: Add a stream specific CREATE TABLE SQL DDL</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13568" target="_blank" rel="noopener">FLINK-13568: DDL create table doesn’t allow STRING data type</a></li>
</ul>
<h3 id="Java-9-support"><a href="#Java-9-support" class="headerlink" title="Java 9 support"></a>Java 9 support</h3><p>Since Flink 1.9.0, Flink can now be compiled and run on Java 9. Note that certain components interacting with external systems (connectors, filesystems, metric reporters, etc.) may not work since the respective projects may have skipped Java 9 support.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-8033" target="_blank" rel="noopener">FLINK-8033: JDK 9 support</a></li>
</ul>
<h3 id="Memory-management"><a href="#Memory-management" class="headerlink" title="Memory management"></a>Memory management</h3><p>In Fink 1.9.0 and prior version, the managed memory fraction of taskmanager is controlled by <code>taskmanager.memory.fraction</code>, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter <code>NewRatio</code> is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-14123" target="_blank" rel="noopener">FLINK-14123: Lower the default value of taskmanager.memory.fraction</a></li>
</ul>
<h2 id="Deprecations-and-breaking-changes"><a href="#Deprecations-and-breaking-changes" class="headerlink" title="Deprecations and breaking changes"></a>Deprecations and breaking changes</h2><h3 id="Scala-expression-DSL-for-Table-API-moved-to-flink-table-api-scala"><a href="#Scala-expression-DSL-for-Table-API-moved-to-flink-table-api-scala" class="headerlink" title="Scala expression DSL for Table API moved to flink-table-api-scala"></a>Scala expression DSL for Table API moved to <code>flink-table-api-scala</code></h3><p>Since 1.9.0, the implicit conversions for the Scala expression DSL for the Table API has been moved to <code>flink-table-api-scala</code>. This requires users to update the imports in their Table programs.</p>
<p>Users of pure Table programs should define their imports like:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.table.api._</span><br><span class="line"></span><br><span class="line">TableEnvironment.create(...)</span><br></pre></td></tr></table></figure>

<p>Users of the DataStream API should define their imports like:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.table.api._</span><br><span class="line">import org.apache.flink.table.api.scala._</span><br><span class="line"></span><br><span class="line">StreamTableEnvironment.create(...)</span><br></pre></td></tr></table></figure>

<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13045" target="_blank" rel="noopener">FLINK-13045: Move Scala expression DSL to flink-table-api-scala</a></li>
</ul>
<h3 id="Failover-strategies"><a href="#Failover-strategies" class="headerlink" title="Failover strategies"></a>Failover strategies</h3><p>As a result of completing fine-grained recovery (<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures" target="_blank" rel="noopener">FLIP-1</a>), Flink will now attempt to only restart tasks that are connected to failed tasks through a pipelined connection. By default, the <code>region</code> failover strategy is used.</p>
<p>Users who were not using a restart strategy or have already configured a failover strategy should not be affected. Moreover, users who already enabled the <code>region</code> failover strategy, along with a restart strategy that enforces a certain number of restarts or introduces a restart delay, will see changes in behavior. The <code>region</code> failover strategy now correctly respects constraints that are defined by the restart strategy.</p>
<p>Streaming users who were not using a failover strategy may be affected if their jobs are embarrassingly parallel or contain multiple independent jobs. In this case, only the failed parallel pipeline or affected jobs will be restarted.</p>
<p>Batch users may be affected if their job contains blocking exchanges (usually happens for shuffles) or the <code>ExecutionMode</code> was set to <code>BATCH</code> or <code>BATCH_FORCED</code> via the <code>ExecutionConfig</code>.</p>
<p>Overall, users should see an improvement in performance.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13223" target="_blank" rel="noopener">FLINK-13223: Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13060" target="_blank" rel="noopener">FLINK-13060: FailoverStrategies should respect restart constraints</a></li>
</ul>
<h3 id="Job-termination-via-CLI"><a href="#Job-termination-via-CLI" class="headerlink" title="Job termination via CLI"></a>Job termination via CLI</h3><p>With the support of graceful job termination with savepoints for semantic correctness (<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103090212" target="_blank" rel="noopener">FLIP-34</a>), a few changes related to job termination has been made to the CLI.</p>
<p>From now on, the <code>stop</code> command with no further arguments stops the job with a savepoint targeted at the default savepoint location (as configured via the <code>state.savepoints.dir</code> property in the job configuration), or a location explicitly specified using the <code>-p</code> option. Please make sure to configure the savepoint path using either one of these options.</p>
<p>Since job terminations are now always accompanied with a savepoint, stopping jobs is expected to take longer now.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13123" target="_blank" rel="noopener">FLINK-13123: Align Stop/Cancel Commands in CLI and REST Interface and Improve Documentation</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-11458" target="_blank" rel="noopener">FLINK-11458: Add TERMINATE/SUSPEND Job with Savepoint</a></li>
</ul>
<h3 id="Network-stack"><a href="#Network-stack" class="headerlink" title="Network stack"></a>Network stack</h3><p>A few changes in the network stack related to changes in the threading model of <code>StreamTask</code> to a mailbox-based approach requires close attention to some related configuration:</p>
<ul>
<li>Due to changes in the lifecycle management of result partitions, partition requests as well as re-triggers will now happen sooner. Therefore, it is possible that some jobs with long deployment times and large state might start failing more frequently with <code>PartitionNotFound</code> exceptions compared to previous versions. If that’s the case, users should increase the value of <code>taskmanager.network.request-backoff.max</code> in order to have the same effective partition request timeout as it was prior to 1.9.0.</li>
<li>To avoid a potential deadlock, a timeout has been added for how long a task will wait for assignment of exclusive memory segments. The default timeout is 30 seconds, and is configurable via <code>taskmanager.network.memory.exclusive-buffers-request-timeout-ms</code>. It is possible that for some previously working deployments this default timeout value is too low and might have to be increased.</li>
</ul>
<p>Please also notice that several network I/O metrics have had their scope changed. See the <a href="https://ci.apache.org/projects/flink/flink-docs-master/monitoring/metrics.html" target="_blank" rel="noopener">1.9 metrics documentation</a> for which metrics are affected. In 1.9.0, these metrics will still be available under their previous scopes, but this may no longer be the case in future versions.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13013" target="_blank" rel="noopener">FLINK-13013: Make sure that SingleInputGate can always request partitions</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12852" target="_blank" rel="noopener">FLINK-12852: Deadlock occurs when requiring exclusive buffer for RemoteInputChannel</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12555" target="_blank" rel="noopener">FLINK-12555: Introduce an encapsulated metric group layout for shuffle API and deprecate old one</a></li>
</ul>
<h3 id="AsyncIO"><a href="#AsyncIO" class="headerlink" title="AsyncIO"></a>AsyncIO</h3><p>Due to a bug in the <code>AsyncWaitOperator</code>, in 1.9.0 the default chaining behaviour of the operator is now changed so that it is never chained after another operator. This should not be problematic for migrating from older version snapshots as long as an uid was assigned to the operator. If an uid was not assigned to the operator, please see the instructions <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/upgrading.html#matching-operator-state" target="_blank" rel="noopener">here</a> for a possible workaround.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13063" target="_blank" rel="noopener">FLINK-13063: AsyncWaitOperator shouldn’t be releasing checkpointingLock</a></li>
</ul>
<h3 id="Connectors-and-Libraries"><a href="#Connectors-and-Libraries" class="headerlink" title="Connectors and Libraries"></a>Connectors and Libraries</h3><h4 id="Introduced-KafkaSerializationSchema-to-fully-replace-KeyedSerializationSchema"><a href="#Introduced-KafkaSerializationSchema-to-fully-replace-KeyedSerializationSchema" class="headerlink" title="Introduced KafkaSerializationSchema to fully replace KeyedSerializationSchema"></a>Introduced <code>KafkaSerializationSchema</code> to fully replace <code>KeyedSerializationSchema</code></h4><p>The universal <code>FlinkKafkaProducer</code> (in <code>flink-connector-kafka</code>) supports a new <code>KafkaSerializationSchema</code> that will fully replace <code>KeyedSerializationSchema</code> in the long run. This new schema allows directly generating Kafka <code>ProducerRecord</code>s for sending to Kafka, therefore enabling the user to use all available Kafka features (in the context of Kafka records).</p>
<h4 id="Dropped-connectors-and-libraries"><a href="#Dropped-connectors-and-libraries" class="headerlink" title="Dropped connectors and libraries"></a>Dropped connectors and libraries</h4><ul>
<li>The Elasticsearch 1 connector has been dropped and will no longer receive patches. Users may continue to use the connector from a previous series (like 1.8) with newer versions of Flink. It is being dropped due to being used significantly less than more recent versions (Elasticsearch versions 2.x and 5.x are downloaded 4 to 5 times more), and hasn’t seen any development for over a year.</li>
<li>The older Python APIs for batch and streaming have been removed and will no longer receive new patches. A new API is being developed based on the Table API as part of <a href="https://issues.apache.org/jira/browse/FLINK-12308" target="_blank" rel="noopener">FLINK-12308: Support python language in Flink Table API</a>. Existing users may continue to use these older APIs with future versions of Flink by copying both the <code>flink-streaming-python</code> and <code>flink-python</code> jars into the <code>/lib</code> directory of the distribution and the corresponding start scripts <code>pyflink-stream.sh</code> and <code>pyflink.sh</code> into the <code>/bin</code> directory of the distribution.</li>
<li>The older machine learning libraries have been removed and will no longer receive new patches. This is due to efforts towards a new Table-based machine learning library (<a href="https://docs.google.com/document/d/1StObo1DLp8iiy0rbukx8kwAJb0BwDZrQrMWub3DzsEo/edit" target="_blank" rel="noopener">FLIP-39</a>). Users can still use the 1.8 version of the legacy library if their projects still rely on it.</li>
</ul>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-11693" target="_blank" rel="noopener">FLINK-11693: Add KafkaSerializationSchema that directly uses ProducerRecord</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12151" target="_blank" rel="noopener">FLINK-12151: Drop Elasticsearch 1 connector</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12903" target="_blank" rel="noopener">FLINK-12903: Remove legacy flink-python APIs</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12308" target="_blank" rel="noopener">FLINK-12308: Support python language in Flink Table API</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12597" target="_blank" rel="noopener">FLINK-12597: Remove the legacy flink-libraries/flink-ml</a></li>
</ul>
<h3 id="MapR-dependency-removed"><a href="#MapR-dependency-removed" class="headerlink" title="MapR dependency removed"></a>MapR dependency removed</h3><p>Dependency on MapR vendor-specific artifacts has been removed, by changing the MapR filesystem connector to work purely based on reflection. This does not introduce any regession in the support for the MapR filesystem. The decision to remove hard dependencies on the MapR artifacts was made due to very flaky access to the secure https endpoint of the MapR artifact repository, and affected build stability of Flink.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12578" target="_blank" rel="noopener">FLINK-12578: Use secure URLs for Maven repositories</a></li>
<li><a href="https://issues.apache.org/jira/browse/FLINK-13499" target="_blank" rel="noopener">FLINK-13499: Remove dependency on MapR artifact repository</a></li>
</ul>
<h3 id="StateDescriptor-interface-change"><a href="#StateDescriptor-interface-change" class="headerlink" title="StateDescriptor interface change"></a>StateDescriptor interface change</h3><p>Access to the state serializer in <code>StateDescriptor</code> is now modified from protected to private access. Subclasses should use the <code>StateDescriptor#getSerializer()</code> method as the only means to obtain the wrapped state serializer.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12688" target="_blank" rel="noopener">FLINK-12688: Make serializer lazy initialization thread safe in StateDescriptor</a></li>
</ul>
<h3 id="Web-UI-dashboard"><a href="#Web-UI-dashboard" class="headerlink" title="Web UI dashboard"></a>Web UI dashboard</h3><p>The web frontend of Flink has been updated to use the latest Angular version (7.x). The old frontend remains available in Flink 1.9.x, but will be removed in a later Flink release once the new frontend is considered stable.</p>
<p>Related issues:</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-10705" target="_blank" rel="noopener">FLINK-10705: Rework Flink Web Dashoard</a></li>
</ul>
<h1 id="Flink-1-8"><a href="#Flink-1-8" class="headerlink" title="Flink 1.8"></a>Flink 1.8</h1><h3 id="State-1"><a href="#State-1" class="headerlink" title="State"></a>State</h3><h4 id="Continuous-incremental-cleanup-of-old-Keyed-State-with-TTL"><a href="#Continuous-incremental-cleanup-of-old-Keyed-State-with-TTL" class="headerlink" title="Continuous incremental cleanup of old Keyed State with TTL"></a>Continuous incremental cleanup of old Keyed State with TTL</h4><p>We introduced TTL (time-to-live) for Keyed state in Flink 1.6 (<a href="https://issues.apache.org/jira/browse/FLINK-9510" target="_blank" rel="noopener">FLINK-9510</a>). This feature allowed to clean up and make inaccessible keyed state entries when accessing them. In addition state would now also being cleaned up when writing a savepoint/checkpoint.</p>
<p>Flink 1.8 introduces continous cleanup of old entries for both the RocksDB state backend (<a href="https://issues.apache.org/jira/browse/FLINK-10471" target="_blank" rel="noopener">FLINK-10471</a>) and the heap state backend (<a href="https://issues.apache.org/jira/browse/FLINK-10473" target="_blank" rel="noopener">FLINK-10473</a>). This means that old entries (according to the ttl setting) are continously being cleanup up.</p>
<h4 id="New-Support-for-Schema-Migration-when-restoring-Savepoints"><a href="#New-Support-for-Schema-Migration-when-restoring-Savepoints" class="headerlink" title="New Support for Schema Migration when restoring Savepoints"></a>New Support for Schema Migration when restoring Savepoints</h4><p>With Flink 1.7.0 we added support for changing the schema of state when using the <code>AvroSerializer</code> (<a href="https://issues.apache.org/jira/browse/FLINK-10605" target="_blank" rel="noopener">FLINK-10605</a>). With Flink 1.8.0 we made great progress migrating all built-in <code>TypeSerializers</code> to a new serializer snapshot abstraction that theoretically allows schema migration. Of the serializers that come with Flink, we now support schema migration for the <code>PojoSerializer</code> (<a href="https://issues.apache.org/jira/browse/FLINK-11485" target="_blank" rel="noopener">FLINK-11485</a>), and Java <code>EnumSerializer</code> (<a href="https://issues.apache.org/jira/browse/FLINK-11334" target="_blank" rel="noopener">FLINK-11334</a>), As well as for Kryo in limited cases (<a href="https://issues.apache.org/jira/browse/FLINK-11323" target="_blank" rel="noopener">FLINK-11323</a>).</p>
<h4 id="Savepoint-compatibility"><a href="#Savepoint-compatibility" class="headerlink" title="Savepoint compatibility"></a>Savepoint compatibility</h4><p>Savepoints from Flink 1.2 that contain a Scala <code>TraversableSerializer</code> are not compatible with Flink 1.8 anymore because of an update in this serializer (<a href="https://issues.apache.org/jira/browse/FLINK-11539" target="_blank" rel="noopener">FLINK-11539</a>). You can get around this restriction by first upgrading to a version between Flink 1.3 and Flink 1.7 and then updating to Flink 1.8.</p>
<h4 id="RocksDB-version-bump-and-switch-to-FRocksDB-FLINK-10471"><a href="#RocksDB-version-bump-and-switch-to-FRocksDB-FLINK-10471" class="headerlink" title="RocksDB version bump and switch to FRocksDB (FLINK-10471)"></a>RocksDB version bump and switch to FRocksDB (<a href="https://issues.apache.org/jira/browse/FLINK-10471" target="_blank" rel="noopener">FLINK-10471</a>)</h4><p>We needed to switch to a custom build of RocksDB called FRocksDB because we need certain changes in RocksDB for supporting continuous state cleanup with TTL. The used build of FRocksDB is based on the upgraded version 5.17.2 of RocksDB. For Mac OS X, RocksDB version 5.17.2 is supported only for OS X version &gt;= 10.13. See also: <a href="https://github.com/facebook/rocksdb/issues/4862" target="_blank" rel="noopener">https://github.com/facebook/rocksdb/issues/4862</a>.</p>
<h3 id="Maven-Dependencies"><a href="#Maven-Dependencies" class="headerlink" title="Maven Dependencies"></a>Maven Dependencies</h3><h4 id="Changes-to-bundling-of-Hadoop-libraries-with-Flink-FLINK-11266"><a href="#Changes-to-bundling-of-Hadoop-libraries-with-Flink-FLINK-11266" class="headerlink" title="Changes to bundling of Hadoop libraries with Flink (FLINK-11266)"></a>Changes to bundling of Hadoop libraries with Flink (<a href="https://issues.apache.org/jira/browse/FLINK-11266" target="_blank" rel="noopener">FLINK-11266</a>)</h4><p>Convenience binaries that include hadoop are no longer released.</p>
<p>If a deployment relies on <code>flink-shaded-hadoop2</code> being included in <code>flink-dist</code>, then you must manually download a pre-packaged Hadoop jar from the optional components section of the <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">download page</a> and copy it into the <code>/lib</code> directory. Alternatively, a Flink distribution that includes hadoop can be built by packaging <code>flink-dist</code> and activating the <code>include-hadoop</code> maven profile.</p>
<p>As hadoop is no longer included in <code>flink-dist</code> by default, specifying <code>-DwithoutHadoop</code> when packaging <code>flink-dist</code> no longer impacts the build.</p>
<h3 id="Configuration-1"><a href="#Configuration-1" class="headerlink" title="Configuration"></a>Configuration</h3><h4 id="TaskManager-configuration-FLINK-11716"><a href="#TaskManager-configuration-FLINK-11716" class="headerlink" title="TaskManager configuration (FLINK-11716)"></a>TaskManager configuration (<a href="https://issues.apache.org/jira/browse/FLINK-11716" target="_blank" rel="noopener">FLINK-11716</a>)</h4><p><code>TaskManagers</code> now bind to the host IP address instead of the hostname by default . This behaviour can be controlled by the configuration option <code>taskmanager.network.bind-policy</code>. If your Flink cluster should experience inexplicable connection problems after upgrading, try to set <code>taskmanager.network.bind-policy: name</code> in your <code>flink-conf.yaml</code> to return to the pre-1.8 behaviour.</p>
<h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><h4 id="Deprecation-of-direct-Table-constructor-usage-FLINK-11447"><a href="#Deprecation-of-direct-Table-constructor-usage-FLINK-11447" class="headerlink" title="Deprecation of direct Table constructor usage (FLINK-11447)"></a>Deprecation of direct <code>Table</code> constructor usage (<a href="https://issues.apache.org/jira/browse/FLINK-11447" target="_blank" rel="noopener">FLINK-11447</a>)</h4><p>Flink 1.8 deprecates direct usage of the constructor of the <code>Table</code> class in the Table API. This constructor would previously be used to perform a join with a <em>lateral table</em>. You should now use <code>table.joinLateral()</code> or <code>table.leftOuterJoinLateral()</code> instead.</p>
<p>This change is necessary for converting the Table class into an interface, which will make the API more maintainable and cleaner in the future.</p>
<h4 id="Introduction-of-new-CSV-format-descriptor-FLINK-9964"><a href="#Introduction-of-new-CSV-format-descriptor-FLINK-9964" class="headerlink" title="Introduction of new CSV format descriptor (FLINK-9964)"></a>Introduction of new CSV format descriptor (<a href="https://issues.apache.org/jira/browse/FLINK-9964" target="_blank" rel="noopener">FLINK-9964</a>)</h4><p>This release introduces a new format descriptor for CSV files that is compliant with RFC 4180. The new descriptor is available as <code>org.apache.flink.table.descriptors.Csv</code>. For now, this can only be used together with the Kafka connector. The old descriptor is available as <code>org.apache.flink.table.descriptors.OldCsv</code> for use with file system connectors.</p>
<h4 id="Deprecation-of-static-builder-methods-on-TableEnvironment-FLINK-11445"><a href="#Deprecation-of-static-builder-methods-on-TableEnvironment-FLINK-11445" class="headerlink" title="Deprecation of static builder methods on TableEnvironment (FLINK-11445)"></a>Deprecation of static builder methods on TableEnvironment (<a href="https://issues.apache.org/jira/browse/FLINK-11445" target="_blank" rel="noopener">FLINK-11445</a>)</h4><p>In order to separate API from actual implementation, the static methods <code>TableEnvironment.getTableEnvironment()</code> are deprecated. You should now use <code>Batch/StreamTableEnvironment.create()</code> instead.</p>
<h4 id="Change-in-the-Maven-modules-of-Table-API-FLINK-11064"><a href="#Change-in-the-Maven-modules-of-Table-API-FLINK-11064" class="headerlink" title="Change in the Maven modules of Table API (FLINK-11064)"></a>Change in the Maven modules of Table API (<a href="https://issues.apache.org/jira/browse/FLINK-11064" target="_blank" rel="noopener">FLINK-11064</a>)</h4><p>Users that had a <code>flink-table</code> dependency before, need to update their dependencies to <code>flink-table-planner</code> and the correct dependency of <code>flink-table-api-*</code>, depending on whether Java or Scala is used: one of <code>flink-table-api-java-bridge</code> or <code>flink-table-api-scala-bridge</code>.</p>
<h4 id="Change-to-External-Catalog-Table-Builders-FLINK-11522"><a href="#Change-to-External-Catalog-Table-Builders-FLINK-11522" class="headerlink" title="Change to External Catalog Table Builders (FLINK-11522)"></a>Change to External Catalog Table Builders (<a href="https://issues.apache.org/jira/browse/FLINK-11522" target="_blank" rel="noopener">FLINK-11522</a>)</h4><p><code>ExternalCatalogTable.builder()</code> is deprecated in favour of <code>ExternalCatalogTableBuilder()</code>.</p>
<h4 id="Change-to-naming-of-Table-API-connector-jars-FLINK-11026"><a href="#Change-to-naming-of-Table-API-connector-jars-FLINK-11026" class="headerlink" title="Change to naming of Table API connector jars (FLINK-11026)"></a>Change to naming of Table API connector jars (<a href="https://issues.apache.org/jira/browse/FLINK-11026" target="_blank" rel="noopener">FLINK-11026</a>)</h4><p>The naming scheme for kafka/elasticsearch6 sql-jars has been changed.</p>
<p>In maven terms, they no longer have the <code>sql-jar</code> qualifier and the artifactId is now prefixed with <code>flink-sql</code> instead of <code>flink</code>, e.g., <code>flink-sql-connector-kafka...</code>.</p>
<h4 id="Change-to-how-Null-Literals-are-specified-FLINK-11785"><a href="#Change-to-how-Null-Literals-are-specified-FLINK-11785" class="headerlink" title="Change to how Null Literals are specified (FLINK-11785)"></a>Change to how Null Literals are specified (<a href="https://issues.apache.org/jira/browse/FLINK-11785" target="_blank" rel="noopener">FLINK-11785</a>)</h4><p>Null literals in the Table API need to be defined with <code>nullOf(type)</code> instead of <code>Null(type)</code> from now on. The old approach is deprecated.</p>
<h3 id="Connectors-1"><a href="#Connectors-1" class="headerlink" title="Connectors"></a>Connectors</h3><h4 id="Introduction-of-a-new-KafkaDeserializationSchema-that-give-direct-access-to-ConsumerRecord-FLINK-8354"><a href="#Introduction-of-a-new-KafkaDeserializationSchema-that-give-direct-access-to-ConsumerRecord-FLINK-8354" class="headerlink" title="Introduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (FLINK-8354)"></a>Introduction of a new KafkaDeserializationSchema that give direct access to ConsumerRecord (<a href="https://issues.apache.org/jira/browse/FLINK-8354" target="_blank" rel="noopener">FLINK-8354</a>)</h4><p>For the Flink <code>KafkaConsumers</code>, we introduced a new <code>KafkaDeserializationSchema</code> that gives direct access to the Kafka <code>ConsumerRecord</code>. This subsumes the <code>KeyedSerializationSchema</code> functionality, which is deprecated but still available for now.</p>
<h4 id="FlinkKafkaConsumer-will-now-filter-restored-partitions-based-on-topic-specification-FLINK-10342"><a href="#FlinkKafkaConsumer-will-now-filter-restored-partitions-based-on-topic-specification-FLINK-10342" class="headerlink" title="FlinkKafkaConsumer will now filter restored partitions based on topic specification (FLINK-10342)"></a>FlinkKafkaConsumer will now filter restored partitions based on topic specification (<a href="https://issues.apache.org/jira/browse/FLINK-10342" target="_blank" rel="noopener">FLINK-10342</a>)</h4><p>Starting from Flink 1.8.0, the <code>FlinkKafkaConsumer</code> now always filters out restored partitions that are no longer associated with a specified topic to subscribe to in the restored execution. This behaviour did not exist in previous versions of the <code>FlinkKafkaConsumer</code>. If you wish to retain the previous behaviour, please use the <code>disableFilterRestoredPartitionsWithSubscribedTopics()</code> configuration method on the <code>FlinkKafkaConsumer</code>.</p>
<p>Consider this example: if you had a Kafka Consumer that was consuming from topic <code>A</code>, you did a savepoint, then changed your Kafka consumer to instead consume from topic <code>B</code>, and then restarted your job from the savepoint. Before this change, your consumer would now consume from both topic <code>A</code> and <code>B</code> because it was stored in state that the consumer was consuming from topic <code>A</code>. With the change, your consumer would only consume from topic <code>B</code> after restore because we filter the topics that are stored in state using the configured topics.</p>
<h3 id="Miscellaneous-Interface-changes"><a href="#Miscellaneous-Interface-changes" class="headerlink" title="Miscellaneous Interface changes"></a>Miscellaneous Interface changes</h3><h4 id="The-canEqual-method-was-dropped-from-the-TypeSerializer-interface-FLINK-9803"><a href="#The-canEqual-method-was-dropped-from-the-TypeSerializer-interface-FLINK-9803" class="headerlink" title="The canEqual() method was dropped from the TypeSerializer interface (FLINK-9803)"></a>The canEqual() method was dropped from the TypeSerializer interface (<a href="https://issues.apache.org/jira/browse/FLINK-9803" target="_blank" rel="noopener">FLINK-9803</a>)</h4><p>The <code>canEqual()</code> methods are usually used to make proper equality checks across hierarchies of types. The <code>TypeSerializer</code> actually doesn’t require this property, so the method is now removed.</p>
<h4 id="Removal-of-the-CompositeSerializerSnapshot-utility-class-FLINK-11073"><a href="#Removal-of-the-CompositeSerializerSnapshot-utility-class-FLINK-11073" class="headerlink" title="Removal of the CompositeSerializerSnapshot utility class (FLINK-11073)"></a>Removal of the CompositeSerializerSnapshot utility class (<a href="https://issues.apache.org/jira/browse/FLINK-11073" target="_blank" rel="noopener">FLINK-11073</a>)</h4><p>The <code>CompositeSerializerSnapshot</code> utility class has been removed. You should now use <code>CompositeTypeSerializerSnapshot</code> instead, for snapshots of composite serializers that delegate serialization to multiple nested serializers. Please see <a href="http://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/state/custom_serialization.html#implementing-a-compositetypeserializersnapshot" target="_blank" rel="noopener">here</a> for instructions on using <code>CompositeTypeSerializerSnapshot</code>.</p>
<h3 id="Memory-management-1"><a href="#Memory-management-1" class="headerlink" title="Memory management"></a>Memory management</h3><p>In Fink 1.8.0 and prior version, the managed memory fraction of taskmanager is controlled by <code>taskmanager.memory.fraction</code>, and with 0.7 as the default value. However, sometimes this will cause OOMs due to the fact that the default value of JVM parameter <code>NewRatio</code> is 2, which means the old generation occupied only 2/3 (0.66) of the heap memory. So if you run into this case, please manually change this value to a lower value.</p>
<h1 id="Flink-1-7"><a href="#Flink-1-7" class="headerlink" title="Flink 1.7"></a>Flink 1.7</h1><h3 id="Scala-2-12-support"><a href="#Scala-2-12-support" class="headerlink" title="Scala 2.12 support"></a>Scala 2.12 support</h3><p>When using Scala <code>2.12</code> you might have to add explicit type annotations in places where they were not required when using Scala <code>2.11</code>. This is an excerpt from the <code>TransitiveClosureNaive.scala</code> example in the Flink code base that shows the changes that could be required.</p>
<p>Previous code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val terminate = prevPaths</span><br><span class="line"> .coGroup(nextPaths)</span><br><span class="line"> .where(0).equalTo(0) &#123;</span><br><span class="line">   (prev, next, out: Collector[(Long, Long)]) =&gt; &#123;</span><br><span class="line">     val prevPaths = prev.toSet</span><br><span class="line">     for (n &lt;- next)</span><br><span class="line">       if (!prevPaths.contains(n)) out.collect(n)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>With Scala <code>2.12</code> you have to change it to:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val terminate = prevPaths</span><br><span class="line"> .coGroup(nextPaths)</span><br><span class="line"> .where(0).equalTo(0) &#123;</span><br><span class="line">   (prev: Iterator[(Long, Long)], next: Iterator[(Long, Long)], out: Collector[(Long, Long)]) =&gt; &#123;</span><br><span class="line">       val prevPaths = prev.toSet</span><br><span class="line">       for (n &lt;- next)</span><br><span class="line">         if (!prevPaths.contains(n)) out.collect(n)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The reason for this is that Scala <code>2.12</code> changes how lambdas are implemented. They now use the lambda support using SAM interfaces introduced in Java 8. This makes some method calls ambiguous because now both Scala-style lambdas and SAMs are candidates for methods were it was previously clear which method would be invoked.</p>
<h3 id="State-evolution"><a href="#State-evolution" class="headerlink" title="State evolution"></a>State evolution</h3><p>Before Flink 1.7, serializer snapshots were implemented as a <code>TypeSerializerConfigSnapshot</code> (which is now deprecated, and will eventually be removed in the future to be fully replaced by the new <code>TypeSerializerSnapshot</code> interface introduced in 1.7). Moreover, the responsibility of serializer schema compatibility checks lived within the <code>TypeSerializer</code>, implemented in the <code>TypeSerializer#ensureCompatibility(TypeSerializerConfigSnapshot)</code> method.</p>
<p>To be future-proof and to have flexibility to migrate your state serializers and schema, it is highly recommended to migrate from the old abstractions. Details and migration guides can be found <a href="https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/state/custom_serialization.html" target="_blank" rel="noopener">here</a>.</p>
<h3 id="Removal-of-the-legacy-mode"><a href="#Removal-of-the-legacy-mode" class="headerlink" title="Removal of the legacy mode"></a>Removal of the legacy mode</h3><p>Flink no longer supports the legacy mode. If you depend on this, then please use Flink <code>1.6.x</code>.</p>
<h3 id="Savepoints-being-used-for-recovery"><a href="#Savepoints-being-used-for-recovery" class="headerlink" title="Savepoints being used for recovery"></a>Savepoints being used for recovery</h3><p>Savepoints are now used while recovering. Previously when using exactly-once sink one could get into problems with duplicate output data when a failure occurred after a savepoint was taken but before the next checkpoint occurred. This results in the fact that savepoints are no longer exclusively under the control of the user. Savepoint should not be moved nor deleted if there was no newer checkpoint or savepoint taken.</p>
<h3 id="MetricQueryService-runs-in-separate-thread-pool"><a href="#MetricQueryService-runs-in-separate-thread-pool" class="headerlink" title="MetricQueryService runs in separate thread pool"></a>MetricQueryService runs in separate thread pool</h3><p>The metric query service runs now in its own <code>ActorSystem</code>. It needs consequently to open a new port for the query services to communicate with each other. The <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#metrics-internal-query-service-port" target="_blank" rel="noopener">query service port</a> can be configured in <code>flink-conf.yaml</code>.</p>
<h3 id="Granularity-of-latency-metrics"><a href="#Granularity-of-latency-metrics" class="headerlink" title="Granularity of latency metrics"></a>Granularity of latency metrics</h3><p>The default granularity for latency metrics has been modified. To restore the previous behavior users have to explicitly set the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#metrics-latency-granularity" target="_blank" rel="noopener">granularity</a> to <code>subtask</code>.</p>
<h3 id="Latency-marker-activation"><a href="#Latency-marker-activation" class="headerlink" title="Latency marker activation"></a>Latency marker activation</h3><p>Latency metrics are now disabled by default, which will affect all jobs that do not explicitly set the <code>latencyTrackingInterval</code> via <code>ExecutionConfig#setLatencyTrackingInterval</code>. To restore the previous default behavior users have to configure the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#metrics-latency-interval" target="_blank" rel="noopener">latency interval</a> in <code>flink-conf.yaml</code>.</p>
<h3 id="Relocation-of-Hadoop’s-Netty-dependency"><a href="#Relocation-of-Hadoop’s-Netty-dependency" class="headerlink" title="Relocation of Hadoop’s Netty dependency"></a>Relocation of Hadoop’s Netty dependency</h3><p>We now also relocate Hadoop’s Netty dependency from <code>io.netty</code> to <code>org.apache.flink.hadoop.shaded.io.netty</code>. You can now bundle your own version of Netty into your job but may no longer assume that <code>io.netty</code> is present in the <code>flink-shaded-hadoop2-uber-*.jar</code> file.</p>
<h3 id="Local-recovery-fixed"><a href="#Local-recovery-fixed" class="headerlink" title="Local recovery fixed"></a>Local recovery fixed</h3><p>With the improvements to Flink’s scheduling, it can no longer happen that recoveries require more slots than before if local recovery is enabled. Consequently, we encourage our users to enable <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#state-backend-local-recovery" target="_blank" rel="noopener">local recovery</a> in <code>flink-conf.yaml</code>.</p>
<h3 id="Support-for-multi-slot-TaskManagers"><a href="#Support-for-multi-slot-TaskManagers" class="headerlink" title="Support for multi slot TaskManagers"></a>Support for multi slot TaskManagers</h3><p>Flink now properly supports <code>TaskManagers</code> with multiple slots. Consequently, <code>TaskManagers</code> can now be started with an arbitrary number of slots and it is no longer recommended to start them with a single slot.</p>
<h3 id="StandaloneJobClusterEntrypoint-generates-JobGraph-with-fixed-JobID"><a href="#StandaloneJobClusterEntrypoint-generates-JobGraph-with-fixed-JobID" class="headerlink" title="StandaloneJobClusterEntrypoint generates JobGraph with fixed JobID"></a>StandaloneJobClusterEntrypoint generates JobGraph with fixed JobID</h3><p>The <code>StandaloneJobClusterEntrypoint</code>, which is launched by the script <code>standalone-job.sh</code> and used for the job-mode container images, now starts all jobs with a fixed <code>JobID</code>. Thus, in order to run a cluster in HA mode, one needs to set a different <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#high-availability-cluster-id" target="_blank" rel="noopener">cluster id</a> for each job/cluster.</p>
<h3 id="Scala-shell-does-not-work-with-Scala-2-12"><a href="#Scala-shell-does-not-work-with-Scala-2-12" class="headerlink" title="Scala shell does not work with Scala 2.12"></a>Scala shell does not work with Scala 2.12</h3><p>Flink’s Scala shell does not work with Scala 2.12. Therefore, the module <code>flink-scala-shell</code> is not being released for Scala 2.12.</p>
<p>See <a href="https://issues.apache.org/jira/browse/FLINK-10911" target="_blank" rel="noopener">FLINK-10911</a> for more details.</p>
<h3 id="Limitations-of-failover-strategies"><a href="#Limitations-of-failover-strategies" class="headerlink" title="Limitations of failover strategies"></a>Limitations of failover strategies</h3><p>Flink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option <code>jobmanager.execution.failover-strategy</code> from your <code>flink-conf.yaml</code> or set it to <code>&quot;full&quot;</code>.</p>
<p>In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See <a href="https://issues.apache.org/jira/browse/FLINK-10880" target="_blank" rel="noopener">FLINK-10880</a> for more details.</p>
<h3 id="SQL-over-window-preceding-clause"><a href="#SQL-over-window-preceding-clause" class="headerlink" title="SQL over window preceding clause"></a>SQL over window preceding clause</h3><p>The over window <code>preceding</code> clause is now optional. It defaults to <code>UNBOUNDED</code> if not specified.</p>
<h3 id="OperatorSnapshotUtil-writes-v2-snapshots"><a href="#OperatorSnapshotUtil-writes-v2-snapshots" class="headerlink" title="OperatorSnapshotUtil writes v2 snapshots"></a>OperatorSnapshotUtil writes v2 snapshots</h3><p>Snapshots created with <code>OperatorSnapshotUtil</code> are now written in the savepoint format <code>v2</code>.</p>
<h3 id="SBT-projects-and-the-MiniClusterResource"><a href="#SBT-projects-and-the-MiniClusterResource" class="headerlink" title="SBT projects and the MiniClusterResource"></a>SBT projects and the MiniClusterResource</h3><p>If you have a <code>sbt</code> project which uses the <code>MiniClusterResource</code>, you now have to add the <code>flink-runtime</code> test-jar dependency explicitly via:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libraryDependencies += &quot;org.apache.flink&quot; %% &quot;flink-runtime&quot; % flinkVersion % Test classifier &quot;tests&quot;</span><br></pre></td></tr></table></figure>

<p>The reason for this is that the <code>MiniClusterResource</code> has been moved from <code>flink-test-utils</code> to <code>flink-runtime</code>. The module <code>flink-test-utils</code> has correctly a <code>test-jar</code> dependency on <code>flink-runtime</code>. However, <code>sbt</code> does not properly pull in transitive <code>test-jar</code> dependencies as described in this <a href="https://github.com/sbt/sbt/issues/2964" target="_blank" rel="noopener">sbt issue</a>. Consequently, it is necessary to specify the <code>test-jar</code> dependency explicitly.</p>
<h1 id="Flink-1-6"><a href="#Flink-1-6" class="headerlink" title="Flink 1.6"></a>Flink 1.6</h1><h3 id="Changed-Configuration-Default-Values"><a href="#Changed-Configuration-Default-Values" class="headerlink" title="Changed Configuration Default Values"></a>Changed Configuration Default Values</h3><p>The default value of the slot idle timeout <code>slot.idle.timeout</code> is set to the default value of the heartbeat timeout (<code>50 s</code>).</p>
<h3 id="Changed-ElasticSearch-5-x-Sink-API"><a href="#Changed-ElasticSearch-5-x-Sink-API" class="headerlink" title="Changed ElasticSearch 5.x Sink API"></a>Changed ElasticSearch 5.x Sink API</h3><p>Previous APIs in the Flink ElasticSearch 5.x Sink’s <code>RequestIndexer</code> interface have been deprecated in favor of new signatures. When adding requests to the <code>RequestIndexer</code>, the requests now must be of type <code>IndexRequest</code>, <code>DeleteRequest</code>, or <code>UpdateRequest</code>, instead of the base <code>ActionRequest</code>.</p>
<h3 id="Limitations-of-failover-strategies-1"><a href="#Limitations-of-failover-strategies-1" class="headerlink" title="Limitations of failover strategies"></a>Limitations of failover strategies</h3><p>Flink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option <code>jobmanager.execution.failover-strategy</code> from your <code>flink-conf.yaml</code> or set it to <code>&quot;full&quot;</code>.</p>
<p>In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See <a href="https://issues.apache.org/jira/browse/FLINK-10880" target="_blank" rel="noopener">FLINK-10880</a> for more details.</p>
<h1 id="Flink-1-5"><a href="#Flink-1-5" class="headerlink" title="Flink 1.5"></a>Flink 1.5</h1><p>These release notes discuss important aspects, such as configuration, behavior, or dependencies, that changed between Flink 1.4 and Flink 1.5. Please read these notes carefully if you are planning to upgrade your Flink version to 1.5.</p>
<h3 id="Update-Configuration-for-Reworked-Job-Deployment"><a href="#Update-Configuration-for-Reworked-Job-Deployment" class="headerlink" title="Update Configuration for Reworked Job Deployment"></a>Update Configuration for Reworked Job Deployment</h3><p>Flink’s reworked cluster and job deployment component improves the integration with resource managers and enables dynamic resource allocation. One result of these changes is, that you no longer have to specify the number of containers when submitting applications to YARN and Mesos. Flink will automatically determine the number of containers from the parallelism of the application.</p>
<p>Although the deployment logic was completely reworked, we aimed to not unnecessarily change the previous behavior to enable a smooth transition. Nonetheless, there are a few options that you should update in your <code>conf/flink-conf.yaml</code> or know about.</p>
<ul>
<li>The allocation of TaskManagers with multiple slots is not fully supported yet. Therefore, we recommend to configure TaskManagers with a single slot, i.e., set <code>taskmanager.numberOfTaskSlots: 1</code></li>
<li>If you observed any problems with the new deployment mode, you can always switch back to the pre-1.5 behavior by configuring <code>mode: legacy</code>.</li>
</ul>
<p>Please report any problems or possible improvements that you notice to the Flink community, either by posting to a mailing list or by opening a JIRA issue.</p>
<p><em>Note</em>: We plan to remove the legacy mode in the next release.</p>
<h3 id="Update-Configuration-for-Reworked-Network-Stack"><a href="#Update-Configuration-for-Reworked-Network-Stack" class="headerlink" title="Update Configuration for Reworked Network Stack"></a>Update Configuration for Reworked Network Stack</h3><p>The changes on the networking stack for credit-based flow control and improved latency affect the configuration of network buffers. In a nutshell, the networking stack can require more memory to run applications. Hence, you might need to adjust the network configuration of your Flink setup.</p>
<p>There are two ways to address problems of job submissions that fail due to lack of network buffers.</p>
<ul>
<li>Reduce the number of buffers per channel, i.e., <code>taskmanager.network.memory.buffers-per-channel</code> or</li>
<li>Increase the amount of TaskManager memory that is used by the network stack, i.e., increase <code>taskmanager.network.memory.fraction</code> and/or <code>taskmanager.network.memory.max</code>.</li>
</ul>
<p>Please consult the section about <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#configuring-the-network-buffers" target="_blank" rel="noopener">network buffer configuration</a> in the Flink documentation for details. In case you experience issues with the new credit-based flow control mode, you can disable flow control by setting <code>taskmanager.network.credit-model: false</code>.</p>
<p><em>Note</em>: We plan to remove the old model and this configuration in the next release.</p>
<h3 id="Hadoop-Classpath-Discovery"><a href="#Hadoop-Classpath-Discovery" class="headerlink" title="Hadoop Classpath Discovery"></a>Hadoop Classpath Discovery</h3><p>We removed the automatic Hadoop classpath discovery via the Hadoop binary. If you want Flink to pick up the Hadoop classpath you have to export <code>HADOOP_CLASSPATH</code>. On cloud environments and most Hadoop distributions you would do</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=`hadoop classpath`.</span><br></pre></td></tr></table></figure>

<h3 id="Breaking-Changes-of-the-REST-API"><a href="#Breaking-Changes-of-the-REST-API" class="headerlink" title="Breaking Changes of the REST API"></a>Breaking Changes of the REST API</h3><p>In an effort to harmonize, extend, and improve the REST API, a few handlers and return values were changed.</p>
<ul>
<li>The jobs overview handler is now registered under <code>/jobs/overview</code> (before <code>/joboverview</code>) and returns a list of job details instead of the pre-grouped view of running, finished, cancelled and failed jobs.</li>
<li>The REST API to cancel a job was changed.</li>
<li>The REST API to cancel a job with savepoint was changed.</li>
</ul>
<p>Please check the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/monitoring/rest_api.html#available-requests" target="_blank" rel="noopener">REST API documentation</a> for details.</p>
<h3 id="Kafka-Producer-Flushes-on-Checkpoint-by-Default"><a href="#Kafka-Producer-Flushes-on-Checkpoint-by-Default" class="headerlink" title="Kafka Producer Flushes on Checkpoint by Default"></a>Kafka Producer Flushes on Checkpoint by Default</h3><p>The Flink Kafka Producer now flushes on checkpoints by default. Prior to version 1.5, the behaviour was disabled by default and users had to explicitly call <code>setFlushOnCheckpoints(true)</code> on the producer to enable it.</p>
<h3 id="Updated-Kinesis-Dependency"><a href="#Updated-Kinesis-Dependency" class="headerlink" title="Updated Kinesis Dependency"></a>Updated Kinesis Dependency</h3><p>The Kinesis dependencies of Flink’s Kinesis connector have been updated to the following versions.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;aws.sdk.version&gt;1.11.319&lt;/aws.sdk.version&gt;</span><br><span class="line">&lt;aws.kinesis-kcl.version&gt;1.9.0&lt;/aws.kinesis-kcl.version&gt;</span><br><span class="line">&lt;aws.kinesis-kpl.version&gt;0.12.9&lt;/aws.kinesis-kcl.version&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Limitations-of-failover-strategies-2"><a href="#Limitations-of-failover-strategies-2" class="headerlink" title="Limitations of failover strategies"></a>Limitations of failover strategies</h3><p>Flink’s non-default failover strategies are still a very experimental feature which come with a set of limitations. You should only use this feature if you are executing a stateless streaming job. In any other cases, it is highly recommended to remove the config option <code>jobmanager.execution.failover-strategy</code> from your <code>flink-conf.yaml</code> or set it to <code>&quot;full&quot;</code>.</p>
<p>In order to avoid future problems, this feature has been removed from the documentation until it will be fixed. See <a href="https://issues.apache.org/jira/browse/FLINK-10880" target="_blank" rel="noopener">FLINK-10880</a> for more details.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/java/sharding-sphere/antlr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/java/sharding-sphere/antlr/" itemprop="url">Antlr sql语法解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-31T00:00:00+08:00">
                2018-05-31
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/database/" itemprop="url" rel="index">
                    <span itemprop="name">database</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<blockquote>
<p>官网：<a href="https://www.antlr.org/" target="_blank" rel="noopener">https://www.antlr.org/</a></p>
<p>github：<a href="https://github.com/antlr/grammars-v4" target="_blank" rel="noopener">https://github.com/antlr/grammars-v4</a></p>
</blockquote>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20200531215656.png" alt></p>
<p>antlr是指可以根据输入自动生成语法树并可视化的显示出来的开源语法分析器。ANTLR—Another Tool for Language Recognition，其前身是PCCTS，它为包括Java，C++，C#在内的语言提供了一个通过语法描述来自动构造自定义语言的识别器（recognizer），<a href="https://baike.baidu.com/item/编译器/8853067" target="_blank" rel="noopener">编译器</a>（parser）和<a href="https://baike.baidu.com/item/解释器/10418965" target="_blank" rel="noopener">解释器</a>（translator）的框架。</p>
<h2 id="1-1-词法分析器（Lexer）"><a href="#1-1-词法分析器（Lexer）" class="headerlink" title="1.1 词法分析器（Lexer）"></a>1.1 词法分析器（Lexer）</h2><p><a href="https://baike.baidu.com/item/词法分析器" target="_blank" rel="noopener">词法分析器</a>又称为Scanner，Lexical analyser和Tokenizer。<a href="https://baike.baidu.com/item/程序设计语言" target="_blank" rel="noopener">程序设计语言</a>通常由<a href="https://baike.baidu.com/item/关键字" target="_blank" rel="noopener">关键字</a>和严格定义的语法结构组成。编译的最终目的是将程序设计语言的高层指令翻译成物理机器或<a href="https://baike.baidu.com/item/虚拟机" target="_blank" rel="noopener">虚拟机</a>可以执行的指令。词法分析器的工作是分析量化那些本来毫无意义的字符流，将他们翻译成离散的字符组（也就是一个一个的Token），包括关键字，标识符，符号（symbols）和操作符供<a href="https://baike.baidu.com/item/语法分析器" target="_blank" rel="noopener">语法分析器</a>使用。</p>
<h2 id="1-2-语法分析器（Parser）"><a href="#1-2-语法分析器（Parser）" class="headerlink" title="1.2 语法分析器（Parser）"></a>1.2 语法分析器（Parser）</h2><p><a href="https://baike.baidu.com/item/编译器" target="_blank" rel="noopener">编译器</a>又称为Syntactical analyser。在分析字符流的时候，Lexer不关心所生成的单个Token的语法意义及其与上下文之间的关系，而这就是Parser的工作。语法分析器将收到的Tokens组织起来，并转换成为目标语言语法定义所允许的序列。</p>
<p>无论是Lexer还是Parser都是一种识别器，Lexer是字符序列识别器而Parser是Token序列识别器。他们在本质上是类似的东西，而只是在分工上有所不同而已。如下图所示：</p>
<p><a href="https://baike.baidu.com/pic/antlr/9368750/0/7a899e510fb30f245f1f11a7ca95d143ac4b0388?fr=lemma&ct=single" target="_blank" rel="noopener"><img src="https://bkimg.cdn.bcebos.com/pic/7a899e510fb30f245f1f11a7ca95d143ac4b0388?x-bce-process=image/resize,m_lfit,w_220,h_220,limit_1" alt="字符输入流、tokens和AST之间的关系"></a>字符输入流、tokens和AST之间的关系</p>
<h2 id="1-3-树分析器-tree-parser"><a href="#1-3-树分析器-tree-parser" class="headerlink" title="1.3 树分析器 (tree parser)"></a>1.3 树分析器 (tree parser)</h2><p>树分析器可以用于对<a href="https://baike.baidu.com/item/语法分析" target="_blank" rel="noopener">语法分析</a>生成的<a href="https://baike.baidu.com/item/抽象语法树" target="_blank" rel="noopener">抽象语法树</a>进行遍历，并能执行一些相关的操作。</p>
<h2 id="1-4-ANTLR"><a href="#1-4-ANTLR" class="headerlink" title="1.4 ANTLR"></a>1.4 ANTLR</h2><p>ANTLR将上述结合起来，它允许我们定义识别字符流的词法规则和用于解释Token流的语法分析规则。然后，ANTLR将根据用户提供的语法文件自动生成相应的词法/<a href="https://baike.baidu.com/item/语法分析器" target="_blank" rel="noopener">语法分析器</a>。用户可以利用他们将输入的文本进行编译，并转换成其他形式（如AST—Abstract Syntax Tree，抽象的语法树）。</p>
<br>

<br>

<br>

<br>

<br>

<br>

<br>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/computer-network/tcp-half-open/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/computer-network/tcp-half-open/" itemprop="url">浅谈tcp的半打开连接</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T00:00:00+08:00">
                2018-05-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computer-network/" itemprop="url" rel="index">
                    <span itemprop="name">computer-network</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/computer-network/tcp/" itemprop="url" rel="index">
                    <span itemprop="name">tcp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <br>

<blockquote>
<p>原文： <a href="https://zhuanlan.zhihu.com/p/32081783" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32081783</a></p>
</blockquote>
<p>tcp连接一端在进行完三次握手以后进入ESTABLISHED状态，如果连接的对端在某一时刻在网络中消失，而本端没有感知到，还是处于ESTABLISHED状态，那么本端的连接就被称为半打开连接(Half Open)。</p>
<p>连接的对端在网络中消失的情况有好多：</p>
<ul>
<li><p>例如对端主机突然断电，tcp连接来不及发送任何信息就消失啦。</p>
</li>
<li><p>还有，连接路径上的某个nat设备aging-time过期，并且nat port被重用，虽然tcp连接的两端都还处于ESTABLISHED状态，可实际上两端的连接已经无法正常通信，此时这两端的连接都是半打开连接。(这种情况是我的猜测，还没有得到实践的检验。如果结论错误，会修改掉！)</p>
</li>
<li><p>还有，listen socket的accept调用缓慢导致积压队列满，client端连接会成为半打开连接。这种情况是本次讨论的主题。</p>
</li>
</ul>
<p>首先说下tcp的三次握手</p>
<p><img src="https://pic4.zhimg.com/80/v2-46e2a6c828b89c9b1554808f63e18ff7_720w.jpg" alt="img"></p>
<p>server端的tcp连接在三次握手阶段会经历SYN_RECV状态到ESTABLISHED状态的变迁，其中SYN_RECV状态到连接存放于listen socket积压队列的半连接队列中，当连接由SYN_RECV状态变为ESTABLISHED状态，连接会被从半连接队列中移到已连接队列中。系统调用accept的作用就是从listen socket的已连接队列中取走一个连接，然后将该连接与进程绑定。</p>
<p>但是，如果listen socket的积压队列(半连接队列与连接队列)全部满后，对于新来的client连接会如何处理呢。答案是，linux不同版本的实现不同。</p>
<p>当前的实验环境：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zuchunlei@ubuntu14:~$ uname -a</span><br><span class="line">Linux ubuntu14 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>

<p>服务端代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from socket import *</span><br><span class="line">In [2]: sock = socket(AF_INET,SOCK_STREAM)</span><br><span class="line">In [3]: sock.bind((&quot;&quot;,10000))</span><br><span class="line">In [4]: sock.listen(1)</span><br></pre></td></tr></table></figure>

<p>为了简单，我将listen的backlog设置为1，并且不调用sock.accept方法。这样所有的ESTABLISHED状态的连接都存在积压队列中，并且没有和进程绑定起来。</p>
<p>使用netstat查看10000端口的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p                                                                                                                                Sat Dec 16 20:23:03 2017</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name Timer</span><br><span class="line">tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      1578/python      off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>使用ss查看10000端口的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: ss -tnpoa|sed -n -e 1p -e /10000/p                                                                                                                                          Sat Dec 16 20:25:18 2017</span><br><span class="line"></span><br><span class="line">State      Recv-Q Send-Q        Local Address:Port          Peer Address:Port</span><br><span class="line">LISTEN     0      1                         *:10000                    *:*      users:((&quot;ipython&quot;,1578,6))</span><br></pre></td></tr></table></figure>

<p>解析一下，ss命令输出的State=Listen状态的数据时，其中Send-Q的大小表示该listen socket积压队列的长度，Recv-Q代表已完成三次握手，ESTABLISHED状态的连接个数。这样的连接存在于listen socket的已连接队列中。</p>
<p>用nc localhost 10000进行2次连接后，使用netstat查看10000端口的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p                                                                                                                                Sat Dec 16 20:32:45 2017</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name Timer</span><br><span class="line">tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      1578/python      off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59890         127.0.0.1:10000         ESTABLISHED 6301/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59890         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59892         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59892         127.0.0.1:10000         ESTABLISHED 6379/nc          off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>netstat显示当前客户端程序nc连接已经建立完成，服务端的2个连接也处于ESTABLISHED状态，但因为当前没有accept调用，所以服务端的两个连接的进程PID显示为-，表示当前连接没有和进程绑定起来。</p>
<p>使用ss查看10000端口的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: ss -tnpoa|sed -n -e 1p -e /10000/p                                                                                                                                          Sat Dec 16 20:36:10 2017</span><br><span class="line"></span><br><span class="line">State      Recv-Q Send-Q        Local Address:Port          Peer Address:Port</span><br><span class="line">LISTEN     2      1                         *:10000                    *:*      users:((&quot;ipython&quot;,1578,6))</span><br><span class="line">ESTAB      0      0                 127.0.0.1:59890            127.0.0.1:10000  users:((&quot;nc&quot;,6301,3))</span><br><span class="line">ESTAB      0      0                 127.0.0.1:10000            127.0.0.1:59890</span><br><span class="line">ESTAB      0      0                 127.0.0.1:10000            127.0.0.1:59892</span><br><span class="line">ESTAB      0      0                 127.0.0.1:59892            127.0.0.1:10000  users:((&quot;nc&quot;,6379,3))</span><br></pre></td></tr></table></figure>

<p>通过ss可以看到，当前LISTEN状态的RECV-Q值为2，表示有2个ESTABLISHED状态的连接在已连接队列中等待应用层调用accept取走。</p>
<p>用nc localhost 10000进行第三次连接后，netstat查看10000端口的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p                                                                                                                                Sat Dec 16 20:41:18 2017</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name Timer</span><br><span class="line">tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      1578/python      off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59890         127.0.0.1:10000         ESTABLISHED 6301/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59896         SYN_RECV    -                on (1.06/3/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59896         127.0.0.1:10000         ESTABLISHED 10989/nc         off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59890         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59892         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59892         127.0.0.1:10000         ESTABLISHED 6379/nc          off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>可以看到对于第三个客户端nc，连接状态为ESTABLISHED，表示3次握手已经正确完成。而对于服务端，当前的连接状态为SYN_RECV，表示半连接状态，因为当前积压队列已经满，没有空间再存放ESTABLISHED连接，所以该连接无法从SYN_RECV状态变为ESTABLISHED状态，虽然能正确接收到nc端的第三个ACK段。</p>
<p>此时使用tcpdump进行抓包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">zuchunlei@ubuntu14:~$ sudo tcpdump -i any tcp port 10000 -nn</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes</span><br><span class="line">20:50:15.739292 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1340001 ecr 1339751,nop,wscale 7], length 0</span><br><span class="line">20:50:15.739301 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1340001 ecr 1339751], length 0</span><br><span class="line">20:50:17.738724 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1340501 ecr 1340001,nop,wscale 7], length 0</span><br><span class="line">20:50:17.738772 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1340501 ecr 1339751], length 0</span><br><span class="line">20:50:21.739110 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1341501 ecr 1340501,nop,wscale 7], length 0</span><br><span class="line">20:50:21.739158 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1341501 ecr 1339751], length 0</span><br><span class="line">20:50:29.738975 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1343501 ecr 1341501,nop,wscale 7], length 0</span><br><span class="line">20:50:29.739022 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1343501 ecr 1339751], length 0</span><br><span class="line">20:50:45.739231 IP 127.0.0.1.10000 &gt; 127.0.0.1.59896: Flags [S.], seq 2458870060, ack 3925261891, win 43690, options [mss 65495,sackOK,TS val 1347501 ecr 1343501,nop,wscale 7], length 0</span><br><span class="line">20:50:45.739310 IP 127.0.0.1.59896 &gt; 127.0.0.1.10000: Flags [.], ack 1, win 342, options [nop,nop,TS val 1347501 ecr 1339751], length 0</span><br></pre></td></tr></table></figure>

<p>对于SYN_RECV状态的连接，linux会启动定时器进行重传三次握手的第二段[S.]，在4次重传后，如果当前listen socket已连接队列中依然没有空间，则将SYN_RECV状态的连接丢弃。</p>
<p>等待4次重传后，使用netstat查看10000端口状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p                                                                                                                                Sat Dec 16 20:58:20 2017</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name Timer</span><br><span class="line">tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      1578/python      off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59890         127.0.0.1:10000         ESTABLISHED 6301/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59890         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59896         127.0.0.1:10000         ESTABLISHED 15954/nc         off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:59892         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:59892         127.0.0.1:10000         ESTABLISHED 6379/nc          off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>server端将SYN_RECV状态的连接丢弃后，此时第三个nc客户端连接就已经成为了半打开连接。</p>
<p>对半打开连接进行send/recv操作时的影响：</p>
<p>如果此时，第三个nc客户端发送数据，则因为连接对对端不存在，对端会回复RST段，本端收到RST段后也会将连接重置。</p>
<p>如果第三个nc客户端只接收数据的话，则这个客户端永远阻塞在recv调用中无法返回。为了有效解决这种问题，客户端可以启动tcp的keepalive，因为默认tcp发送keepalive probe的间隔时间较长，应用可以通过设置socket option(TCP_KEEPDILE/TCP_KEEPINTVL/TCP_KEEPCNT)将发送keepalive probe的时间设短些。</p>
<p>今早我测试了一下最新版ubuntu16.04的实现，发现如果listen socket的积压队列满后，新来客户端的连接不再成为ESTABLISHED状态，而是在SYN_SENT状态进行进行SYN段的超时重传，而服务端不返回任何tcp段。</p>
<p>新版的测试环境：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zuchunlei@box:~$ uname -a</span><br><span class="line">Linux box 4.10.0-28-generic #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>

<p>与之前的测试场景一样，当前只关注第三个nc客户端连接的状态。</p>
<p>使用netstat查看10000端口的状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p                                                                                                                                Sat Dec 16 21:21:57 2017</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name Timer</span><br><span class="line">tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      2022/python      off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:36516         127.0.0.1:10000         ESTABLISHED 2347/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      1 127.0.0.1:36520         127.0.0.1:10000         SYN_SENT    2522/nc          on (5.18/3/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:36518         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:36518         127.0.0.1:10000         ESTABLISHED 2388/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:36516         ESTABLISHED -                off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>此时，第三个nc客户端连接状态为SYN_SENT，进行超时重传SYN段。</p>
<p>使用tcpdump抓去第三个nc客户端的tcp包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zuchunlei@box:~$ sudo tcpdump -i any tcp port 10000 -nn</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</span><br><span class="line">21:21:47.357226 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214107076 ecr 0,nop,wscale 7], length 0</span><br><span class="line">21:21:48.358267 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214107327 ecr 0,nop,wscale 7], length 0</span><br><span class="line">21:21:50.373837 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214107831 ecr 0,nop,wscale 7], length 0</span><br><span class="line">21:21:54.565832 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214108879 ecr 0,nop,wscale 7], length 0</span><br><span class="line">21:22:02.758111 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214110927 ecr 0,nop,wscale 7], length 0</span><br><span class="line">21:22:18.885934 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214114959 ecr 0,nop,wscale 7], length 0</span><br><span class="line">21:22:51.141643 IP 127.0.0.1.36520 &gt; 127.0.0.1.10000: Flags [S], seq 1445936074, win 43690, options [mss 65495,sackOK,TS val 4214123023 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure>

<p>可以看到客户端在进行超时重传SYN段的过程中，服务端没有发送一个包。</p>
<p>在客户端SYN_SENT超时后，使用netstat查看10000端口状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: sudo netstat -tnpoa|sed -n -e 2p -e /10000/p                                                                                                                       Sat Dec 16 21:27:36 2017</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name Timer</span><br><span class="line">tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      2022/python      off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:36516         127.0.0.1:10000         ESTABLISHED 2347/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:36518         ESTABLISHED -                off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:36518         127.0.0.1:10000         ESTABLISHED 2388/nc          off (0.00/0/0)</span><br><span class="line">tcp        0      0 127.0.0.1:10000         127.0.0.1:36516         ESTABLISHED -                off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>客户端连接消失。</p>
<p>在当前新版当linux实现中，由于listen socket积压队列满时，新的客户端连接并不会成为半打开连接，而是在connect调用时进行重传SYN段，如果达到了SYN_SENT状态的阈值后，tcp连接消失，应用层connect调用返回timeout异常！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/leetcode/maximum-product-subarray/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/leetcode/maximum-product-subarray/" itemprop="url">乘积最大子序列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-20T00:00:00+08:00">
                2018-05-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="算法：动态规划"><a href="#算法：动态规划" class="headerlink" title="算法：动态规划"></a>算法：动态规划</h1><br>

<br>

<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p><a href="https://leetcode-cn.com/problems/maximum-product-subarray/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/maximum-product-subarray/</a></p>
<p>给定一个整数数组 nums ，找出一个序列中乘积最大的连续子序列（该序列至少包含一个数）。</p>
<p>示例 1:</p>
<p>输入: [2,3,-2,4]<br>输出: 6<br>解释: 子数组 [2,3] 有最大乘积 6。<br>示例 2:</p>
<p>输入: [-2,0,-1]<br>输出: 0<br>解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。</p>
<br>

<br>

<h1 id="Java解法"><a href="#Java解法" class="headerlink" title="Java解法"></a>Java解法</h1><h3 id="暴力破解"><a href="#暴力破解" class="headerlink" title="暴力破解"></a>暴力破解</h3><p>申请空间储存所有的中间结果，再拿中间结果结算下一个位置的所有结果</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxProduct</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(<span class="keyword">null</span>==nums || nums.length==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">		ArrayList&lt;TreeSet&lt;Integer&gt;&gt; max = <span class="keyword">new</span> ArrayList&lt;TreeSet&lt;Integer&gt;&gt;();</span><br><span class="line">		max.add(<span class="keyword">new</span> TreeSet&lt;Integer&gt;());</span><br><span class="line">		Integer preMax = Integer.MIN_VALUE;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.length; ++i) &#123;</span><br><span class="line">			TreeSet&lt;Integer&gt; curList = <span class="keyword">new</span> TreeSet&lt;Integer&gt;();</span><br><span class="line">			TreeSet&lt;Integer&gt; preList = max.get(i);</span><br><span class="line">			<span class="keyword">if</span> (preList==<span class="keyword">null</span> || preList.size()&lt;=<span class="number">0</span>) &#123;</span><br><span class="line">				<span class="keyword">int</span> cur = nums[i];</span><br><span class="line">				curList.add(cur);</span><br><span class="line">				<span class="keyword">if</span> (preMax &lt; cur) preMax = cur;</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="keyword">for</span> (Integer x : preList) &#123;</span><br><span class="line">					<span class="keyword">int</span> cur = x * nums[i];</span><br><span class="line">					curList.add(cur);</span><br><span class="line">					<span class="keyword">int</span> curMax = cur&gt;nums[i]?cur:nums[i];</span><br><span class="line">					<span class="keyword">if</span> (cur != curMax) curList.add(curMax);</span><br><span class="line">					<span class="keyword">if</span> (preMax &lt; curMax) preMax = curMax;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			curList.add(nums[i]);</span><br><span class="line">			max.add(curList);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> preMax;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>,<span class="number">3</span>,-<span class="number">2</span>,<span class="number">4</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">2</span>,<span class="number">0</span>,-<span class="number">1</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">2</span>,<span class="number">3</span>,-<span class="number">4</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>,<span class="number">2</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>,-<span class="number">5</span>,-<span class="number">2</span>,-<span class="number">4</span>,<span class="number">3</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905105123.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905103706.png" alt></p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>遍历数组时计算当前最大值，不断更新<br>令imax为当前最大值，则当前最大值为 imax = max(imax * nums[i], nums[i])<br>由于存在负数，那么会导致最大的变最小的，最小的变最大的。因此还需要维护当前最小值imin，imin = min(imin * nums[i], nums[i])<br>当负数出现时则imax与imin进行交换再进行下一步计算<br>时间复杂度：O(n)O(n)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxProduct</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> max = Integer.MIN_VALUE, imax = <span class="number">1</span>, imin = <span class="number">1</span>;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.length; i++)&#123;</span><br><span class="line">			<span class="keyword">if</span>(nums[i] &lt; <span class="number">0</span>)&#123;</span><br><span class="line">				<span class="keyword">int</span> tmp = imax;</span><br><span class="line">				imax = imin;</span><br><span class="line">				imin = tmp;</span><br><span class="line">			&#125;</span><br><span class="line">			imax = Math.max(imax*nums[i], nums[i]);</span><br><span class="line">			imin = Math.min(imin*nums[i], nums[i]);</span><br><span class="line">			max = Math.max(max, imax);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> max;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>,<span class="number">3</span>,-<span class="number">2</span>,<span class="number">4</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">2</span>,<span class="number">0</span>,-<span class="number">1</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;-<span class="number">2</span>,<span class="number">3</span>,-<span class="number">4</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">0</span>,<span class="number">2</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">		x = <span class="keyword">new</span> Solution().maxProduct(<span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">2</span>,-<span class="number">5</span>,-<span class="number">2</span>,-<span class="number">4</span>,<span class="number">3</span>&#125;);</span><br><span class="line">		System.out.println(x);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905104722.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190905104742.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/leetcode/distinct-subsequences/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/leetcode/distinct-subsequences/" itemprop="url">不同的子序列统计</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-16T00:00:00+08:00">
                2018-05-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="算法：动态规划"><a href="#算法：动态规划" class="headerlink" title="算法：动态规划"></a>算法：动态规划</h1><br>

<br>

<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>url：<a href="https://leetcode-cn.com/problems/distinct-subsequences/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/distinct-subsequences/</a></p>
<p>给定一个字符串 S 和一个字符串 T，计算在 S 的子序列中 T 出现的个数。</p>
<p>一个字符串的一个子序列是指，通过删除一些（也可以不删除）字符且不干扰剩余字符相对位置所组成的新字符串。（例如，”ACE” 是 “ABCDE” 的一个子序列，而 “AEC” 不是）</p>
<p>示例 1：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">输入: S = "rabbbit", T = "rabbit"</span><br><span class="line">输出: 3</span><br><span class="line">解释:</span><br><span class="line"></span><br><span class="line">如下图所示, 有 3 种可以从 <span class="selector-tag">S</span> 中得到 "<span class="selector-tag">rabbit</span>" 的方案。</span><br><span class="line">(上箭头符号 ^ 表示选取的字母)</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">rabbbit</span></span><br><span class="line">^^^^ ^^</span><br><span class="line"><span class="selector-tag">rabbbit</span></span><br><span class="line">^^ ^^^^</span><br><span class="line"><span class="selector-tag">rabbbit</span></span><br><span class="line">^^^ ^^^</span><br></pre></td></tr></table></figure>

<p>示例 2：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">输入: S = "babgbag", T = "bag"</span><br><span class="line">输出: 5</span><br><span class="line">解释:</span><br><span class="line"></span><br><span class="line">如下图所示, 有 5 种可以从 <span class="selector-tag">S</span> 中得到 "<span class="selector-tag">bag</span>" 的方案。 </span><br><span class="line">(上箭头符号 ^ 表示选取的字母)</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">babgbag</span></span><br><span class="line">^^ ^</span><br><span class="line"><span class="selector-tag">babgbag</span></span><br><span class="line">^^    ^</span><br><span class="line"><span class="selector-tag">babgbag</span></span><br><span class="line">^    ^^</span><br><span class="line"><span class="selector-tag">babgbag</span></span><br><span class="line">  ^  ^^</span><br><span class="line"><span class="selector-tag">babgbag</span></span><br><span class="line">    ^^^</span><br></pre></td></tr></table></figure>

<br>

<br>

<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>动态规划的本质是穷举法</p>
<p>拿第二个示例分析，S = “babgbag”, T = “bag”</p>
<p>b 在S中出现的可能性，存；b后a出现的可能性，存储；在ba的基础上g出现的可能性储存；</p>
<p><img src="https://pic.leetcode-cn.com/7cbaac1d6171973f6e175d5ca6923029b8ebf0748f5c11526e74ff3da22b869c-Snipaste_2019-07-31_14-26-10.jpg" alt="Snipaste_2019-07-31_14-26-10.jpg"></p>
<p>二维数组的解法，转化为1位数组的解法</p>
<br>

<br>

<h1 id="Java解法"><a href="#Java解法" class="headerlink" title="Java解法"></a>Java解法</h1><p>s的长度为n，t的长度为m，空间复杂度为O(m)的解法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numDistinct</span><span class="params">(String s, String t)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> sl = s.length();</span><br><span class="line">		<span class="keyword">int</span> tl = t.length();</span><br><span class="line">		<span class="keyword">if</span>(sl==<span class="number">0</span>||tl==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">if</span>(sl&lt;tl)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">if</span>(sl==tl)<span class="keyword">return</span> s.equals(t)?<span class="number">1</span>:<span class="number">0</span>;</span><br><span class="line">		<span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[tl+<span class="number">1</span>];</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;sl; i++)&#123;</span><br><span class="line">			<span class="keyword">int</span> pre=<span class="number">0</span>;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;tl; j++)&#123;</span><br><span class="line">				<span class="keyword">if</span>(j==<span class="number">0</span>)pre=<span class="number">1</span>;</span><br><span class="line">				<span class="keyword">int</span> temp = res[j+<span class="number">1</span>];</span><br><span class="line">				<span class="keyword">if</span>(s.charAt(i)==t.charAt(j))&#123;</span><br><span class="line">					res[j+<span class="number">1</span>]=res[j+<span class="number">1</span>]+pre;</span><br><span class="line">				&#125;</span><br><span class="line">				pre = temp;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> res[tl];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> num = <span class="keyword">new</span> Solution().numDistinct(<span class="string">"babgbag"</span>, <span class="string">"bag"</span>);</span><br><span class="line">		System.out.println(num);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>pre和temp的作用是 记录本轮更改之前的值，用于计算，防止”rabbbit”, “rabbit”这样的数据，一个s中的b，影响多个t中的b的计算</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/algorithm/quick-sort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/algorithm/quick-sort/" itemprop="url">快速排序</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-15T00:00:00+08:00">
                2018-05-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p><a href="https://www.runoob.com/w3cnote/quick-sort-2.html" target="_blank" rel="noopener">https://www.runoob.com/w3cnote/quick-sort-2.html</a></p>
<p>快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要 Ο(nlogn) 次比较。在最坏状况下则需要 Ο(n2) 次比较，但这种状况并不常见。事实上，快速排序通常明显比其他 Ο(nlogn) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。</p>
<p>快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。</p>
<p>快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。</p>
<p>快速排序的名字起的是简单粗暴，因为一听到这个名字你就知道它存在的意义，就是快，而且效率高！它是处理大数据最快的排序算法之一了。虽然 Worst Case 的时间复杂度达到了 O(n²)，但是人家就是优秀，在大多数情况下都比平均时间复杂度为 O(n logn) 的排序算法表现要更好，可是这是为什么呢，我也不知道。好在我的强迫症又犯了，查了 N 多资料终于在《算法艺术与信息学竞赛》上找到了满意的答案：</p>
<blockquote>
<p><em>快速排序的最坏运行情况是 O(n²)，比如说顺序数列的快排。但它的平摊期望时间是 O(nlogn)，且 O(nlogn) 记号中隐含的常数因子很小，比复杂度稳定等于 O(nlogn) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。</em></p>
</blockquote>
<p><br><br></p>
<h3 id="1-算法步骤"><a href="#1-算法步骤" class="headerlink" title="1. 算法步骤"></a>1. 算法步骤</h3><ol>
<li>从数列中挑出一个元素，称为 “基准”（pivot）;</li>
<li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作；</li>
<li>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序；</li>
</ol>
<br>

<br>

<h3 id="2、图片演示"><a href="#2、图片演示" class="headerlink" title="2、图片演示"></a>2、图片演示</h3><p>方法其实很简单：分别从初始序列“6 1 2 7 9 3 4 5 10 8”两端开始“探测”。先从右往左找一个小于6的数，再从左往右找一个大于6的数，然后交换他们。这里可以用两个变量i和j，分别指向序列最左边和最右边。我们为这两个变量起个好听的名字“哨兵i”和“哨兵j”。刚开始的时候让哨兵i指向序列的最左边（即i=1），指向数字6。让哨兵j指向序列的最右边（即=10），指向数字。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133247.png" alt></p>
<p>首先哨兵j开始出动。因为此处设置的基准数是最左边的数，所以需要让哨兵j先出动，这一点非常重要（请自己想一想为什么）。哨兵j一步一步地向左挪动（即j–），直到找到一个小于6的数停下来。接下来哨兵i再一步一步向右挪动（即i++），直到找到一个数大于6的数停下来。最后哨兵j停在了数字5面前，哨兵i停在了数字7面前。</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133321.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133348.png" alt></p>
<p>现在交换哨兵i和哨兵j所指向的元素的值。交换之后的序列如下：</p>
<p>6 1 2 5 9 3 4 7 10 8</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133427.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133445.png" alt></p>
<p>到此，第一次交换结束。接下来开始哨兵j继续向左挪动（再友情提醒，每次必须是哨兵j先出发）。他发现了4（比基准数6要小，满足要求）之后停了下来。哨兵i也继续向右挪动的，他发现了9（比基准数6要大，满足要求）之后停了下来。此时再次进行交换，交换之后的序列如下：</p>
<p>6 1 2 5 4 3 9 7 10 8</p>
<p>第二次交换结束，“探测”继续。哨兵j继续向左挪动，他发现了3（比基准数6要小，满足要求）之后又停了下来。哨兵i继续向右移动，糟啦！此时哨兵i和哨兵j相遇了，哨兵i和哨兵j都走到3面前。说明此时“探测”结束。我们将基准数6和3进行交换。交换之后的序列如下：</p>
<p>3 1 2 5 4 6 9 7 10 8</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133543.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133556.png" alt></p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903133614.png" alt></p>
<p>到此第一轮“探测”真正结束。此时以基准数6为分界点，6左边的数都小于等于6，6右边的数都大于等于6。回顾一下刚才的过程，其实哨兵j的使命就是要找小于基准数的数，而哨兵i的使命就是要找大于基准数的数，直到i和j碰头为止。</p>
<p>下面上个霸气的图来描述下整个算法的处理过程：</p>
<p><img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190903132936.png" alt></p>
<p><br><br></p>
<h3 id="3、Java实现"><a href="#3、Java实现" class="headerlink" title="3、Java实现"></a>3、Java实现</h3><p>使用LinkedList实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class QuickSort &#123;</span><br><span class="line">	public void quickSort(LinkedList&lt;Integer&gt; list, int left, int right) &#123;</span><br><span class="line">		if(left&lt;right) &#123;</span><br><span class="line">			int partitionIndex = partition(list, left, right);</span><br><span class="line">			quickSort(list, left, partitionIndex-1);</span><br><span class="line">			quickSort(list, partitionIndex+1, right);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	public int partition(LinkedList&lt;Integer&gt; list, int left, int right) &#123;</span><br><span class="line">		int start = left;</span><br><span class="line">		int tmp = list.get(left);</span><br><span class="line">		while(left &lt; right) &#123;</span><br><span class="line">			while (left &lt; right &amp;&amp; tmp &lt;= list.get(right)) right--;</span><br><span class="line">			while (left &lt; right &amp;&amp; tmp &gt;= list.get(left)) left++;</span><br><span class="line">			if (left &lt; right) swap(list, left, right);</span><br><span class="line">		&#125;</span><br><span class="line">		swap(list, start, left);</span><br><span class="line">		return right;</span><br><span class="line">	&#125;</span><br><span class="line">	public void swap(LinkedList&lt;Integer&gt; list, int i, int j) &#123;</span><br><span class="line">		int tmp = list.get(i);</span><br><span class="line">		list.set(i, list.get(j));</span><br><span class="line">		list.set(j, tmp);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用链表实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> val;</span><br><span class="line">	ListNode next;</span><br><span class="line">	ListNode(<span class="keyword">int</span> x) &#123; val = x; &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> ListNode <span class="title">sortList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">		quickSort(head, <span class="keyword">null</span>);</span><br><span class="line">		<span class="keyword">return</span> head;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(ListNode head, ListNode tail)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (head == tail || head.next == tail) <span class="keyword">return</span>;</span><br><span class="line">		<span class="keyword">int</span> pivot = head.val;</span><br><span class="line">		ListNode left = head, cur = head.next;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (cur != tail) &#123;</span><br><span class="line">			<span class="keyword">if</span> (cur.val &lt; pivot) &#123;</span><br><span class="line">				left = left.next;</span><br><span class="line">				swap(left, cur);</span><br><span class="line">			&#125;</span><br><span class="line">			cur = cur.next;</span><br><span class="line">		&#125;</span><br><span class="line">		swap(head, left);</span><br><span class="line">		quickSort(head, left);</span><br><span class="line">		quickSort(left.next, tail);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(ListNode a, ListNode b)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> tmp = a.val;</span><br><span class="line">		a.val = b.val;</span><br><span class="line">		b.val = tmp;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/leetcode/sort-list/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/leetcode/sort-list/" itemprop="url">排序链表</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-15T00:00:00+08:00">
                2018-05-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h1 id="算法：排序"><a href="#算法：排序" class="headerlink" title="算法：排序"></a>算法：排序</h1><br>
<br>

<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。</p>
<p>示例 1:</p>
<p>输入: 4-&gt;2-&gt;1-&gt;3<br>输出: 1-&gt;2-&gt;3-&gt;4<br>示例 2:</p>
<p>输入: -1-&gt;5-&gt;3-&gt;4-&gt;0<br>输出: -1-&gt;0-&gt;3-&gt;4-&gt;5</p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>排序算法</p>
<h1 id="Java实现"><a href="#Java实现" class="headerlink" title="Java实现"></a>Java实现</h1><p>使用快速排序实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> val;</span><br><span class="line">	ListNode next;</span><br><span class="line">	ListNode(<span class="keyword">int</span> x) &#123; val = x; &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> ListNode <span class="title">sortList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">		quickSort(head, <span class="keyword">null</span>);</span><br><span class="line">		<span class="keyword">return</span> head;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(ListNode head, ListNode tail)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (head == tail || head.next == tail) <span class="keyword">return</span>;</span><br><span class="line">		<span class="keyword">int</span> pivot = head.val;</span><br><span class="line">		ListNode left = head, cur = head.next;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (cur != tail) &#123;</span><br><span class="line">			<span class="keyword">if</span> (cur.val &lt; pivot) &#123;</span><br><span class="line">				left = left.next;</span><br><span class="line">				swap(left, cur);</span><br><span class="line">			&#125;</span><br><span class="line">			cur = cur.next;</span><br><span class="line">		&#125;</span><br><span class="line">		swap(head, left);</span><br><span class="line">		quickSort(head, left);</span><br><span class="line">		quickSort(left.next, tail);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(ListNode a, ListNode b)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> tmp = a.val;</span><br><span class="line">		a.val = b.val;</span><br><span class="line">		b.val = tmp;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] stringToIntegerArray(String input) &#123;</span><br><span class="line">		input = input.trim();</span><br><span class="line">		input = input.substring(<span class="number">1</span>, input.length() - <span class="number">1</span>);</span><br><span class="line">		<span class="keyword">if</span> (input.length() == <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		String[] parts = input.split(<span class="string">","</span>);</span><br><span class="line">		<span class="keyword">int</span>[] output = <span class="keyword">new</span> <span class="keyword">int</span>[parts.length];</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; parts.length; index++) &#123;</span><br><span class="line">			String part = parts[index].trim();</span><br><span class="line">			output[index] = Integer.parseInt(part);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> output;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">stringToListNode</span><span class="params">(String input)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// Generate array from the input</span></span><br><span class="line">		<span class="keyword">int</span>[] nodeValues = stringToIntegerArray(input);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Now convert that list into linked list</span></span><br><span class="line">		ListNode dummyRoot = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">		ListNode ptr = dummyRoot;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> item : nodeValues) &#123;</span><br><span class="line">			ptr.next = <span class="keyword">new</span> ListNode(item);</span><br><span class="line">			ptr = ptr.next;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> dummyRoot.next;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">listNodeToString</span><span class="params">(ListNode node)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="string">"[]"</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		String result = <span class="string">""</span>;</span><br><span class="line">		<span class="keyword">while</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">			result += Integer.toString(node.val) + <span class="string">", "</span>;</span><br><span class="line">			node = node.next;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> <span class="string">"["</span> + result.substring(<span class="number">0</span>, result.length() - <span class="number">2</span>) + <span class="string">"]"</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		ListNode head = stringToListNode(<span class="string">"[8581,6131,9495,2797,105,3247,16943]"</span>);</span><br><span class="line">		ListNode ret = <span class="keyword">new</span> Solution().sortList(head);</span><br><span class="line">		String out = listNodeToString(ret);</span><br><span class="line">		System.out.println(out);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://carlo-z.com/leetcode/lfu-cache/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Focus-1">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Focus-1">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/leetcode/lfu-cache/" itemprop="url">LFU缓存机制</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-12T00:00:00+08:00">
                2018-05-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<br>

<h1 id="算法：LFU缓存机制-淘汰最近访问频率最小的元素"><a href="#算法：LFU缓存机制-淘汰最近访问频率最小的元素" class="headerlink" title="算法：LFU缓存机制-淘汰最近访问频率最小的元素"></a>算法：LFU缓存机制-淘汰最近访问频率最小的元素</h1><p><br><br></p>
<h1 id="题目——-LFU缓存机制"><a href="#题目——-LFU缓存机制" class="headerlink" title="题目—— LFU缓存机制"></a>题目—— LFU缓存机制</h1><p>url：<a href="https://leetcode-cn.com/problems/lfu-cache/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/lfu-cache/</a></p>
<p>设计并实现最不经常使用（LFU）缓存的数据结构。它应该支持以下操作：get 和 put。</p>
<p>get(key) - 如果键存在于缓存中，则获取键的值（总是正数），否则返回 -1。<br>put(key, value) - 如果键不存在，请设置或插入值。当缓存达到其容量时，它应该在插入新项目之前，使最不经常使用的项目无效。在此问题中，当存在平局（即两个或更多个键具有相同使用频率）时，最近最少使用的键将被去除。</p>
<p>进阶：<br>你是否可以在 O(1) 时间复杂度内执行两项操作？</p>
<p>示例:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">LFUCache cache = <span class="keyword">new</span> LFUCache( <span class="number">2</span> <span class="comment">/* capacity (缓存容量) */</span> );</span><br><span class="line"></span><br><span class="line">cache.put(<span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">cache.put(<span class="number">2</span>, <span class="number">2</span>);</span><br><span class="line">cache.get(<span class="number">1</span>);       <span class="comment">// 返回 1</span></span><br><span class="line">cache.put(<span class="number">3</span>, <span class="number">3</span>);    <span class="comment">// 去除 key 2</span></span><br><span class="line">cache.get(<span class="number">2</span>);       <span class="comment">// 返回 -1 (未找到key 2)</span></span><br><span class="line">cache.get(<span class="number">3</span>);       <span class="comment">// 返回 3</span></span><br><span class="line">cache.put(<span class="number">4</span>, <span class="number">4</span>);    <span class="comment">// 去除 key 1</span></span><br><span class="line">cache.get(<span class="number">1</span>);       <span class="comment">// 返回 -1 (未找到 key 1)</span></span><br><span class="line">cache.get(<span class="number">3</span>);       <span class="comment">// 返回 3</span></span><br><span class="line">cache.get(<span class="number">4</span>);       <span class="comment">// 返回 4</span></span><br></pre></td></tr></table></figure>

<br>

<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>LFU(Least Frequently Used)：淘汰最近访问频率最小的元素。</p>
<p>缺点：1. 最新加入的数据常常会被踢除，因为其起始方法次数少。 2. 如果频率时间度量是1小时，则平均一天每个小时内的访问频率1000的热点数据可能会被2个小时的一段时间内的访问频率是1001的数据剔除掉；</p>
<p><strong>实现方式</strong>：</p>
<p>一个k-v字典存储 数据，同时存储每个item的上次访问时间time 和 累计访问次数 cnt，当容量满时，淘汰最cnt最小的，cnt相同的情况下，淘汰time最早的；</p>
<br>

<h1 id="Java解法"><a href="#Java解法" class="headerlink" title="Java解法"></a>Java解法</h1><p>使用HashMap存储数据，使用优先级队列PriorityQueue存储累计访问次数 和 最近访问时间</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Comparator;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.PriorityQueue;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LfuObj</span> </span>&#123;</span><br><span class="line">	Integer key;</span><br><span class="line">	Integer val;</span><br><span class="line">	Integer cnt;</span><br><span class="line">	Long time;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">LfuObj</span><span class="params">(<span class="keyword">int</span> key, <span class="keyword">int</span> val, <span class="keyword">int</span> cnt, <span class="keyword">long</span> time)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.key = key;</span><br><span class="line">		<span class="keyword">this</span>.val = val;</span><br><span class="line">		<span class="keyword">this</span>.cnt = cnt;</span><br><span class="line">		<span class="keyword">this</span>.time = time;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>.key.hashCode();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(<span class="keyword">null</span> == obj) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		<span class="keyword">if</span>(!(obj <span class="keyword">instanceof</span> LfuObj)) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		<span class="keyword">return</span> key.equals(((LfuObj) obj).key);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LFUCache</span> </span>&#123;</span><br><span class="line">	HashMap&lt;Integer, LfuObj&gt; hashMap;</span><br><span class="line">	PriorityQueue&lt;LfuObj&gt; heap = <span class="keyword">new</span> PriorityQueue&lt;&gt;(<span class="keyword">new</span> Comparator&lt;LfuObj&gt;() &#123;</span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(LfuObj a, LfuObj b)</span> </span>&#123;</span><br><span class="line">			<span class="keyword">int</span> c = a.cnt - b.cnt;</span><br><span class="line">			<span class="keyword">return</span> (<span class="number">0</span>==c ? (<span class="keyword">int</span>)((a.time-b.time)%<span class="number">65536</span>):c);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;);</span><br><span class="line">	Integer capacity;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">LFUCache</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.hashMap = <span class="keyword">new</span> HashMap&lt;&gt;(capacity);</span><br><span class="line">		<span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</span><br><span class="line">		LfuObj lfuObj = hashMap.getOrDefault(key, <span class="keyword">null</span>);</span><br><span class="line">		<span class="keyword">if</span>(<span class="keyword">null</span> == lfuObj) &#123;</span><br><span class="line">			<span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		heap.remove(lfuObj);</span><br><span class="line">		lfuObj.cnt++;</span><br><span class="line">		lfuObj.time = System.nanoTime();</span><br><span class="line">		heap.offer(lfuObj);</span><br><span class="line">		hashMap.put(key, lfuObj);</span><br><span class="line">		<span class="keyword">return</span> lfuObj.val;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">int</span> key, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(capacity &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">		LfuObj lfuObj = hashMap.get(key);</span><br><span class="line">		<span class="keyword">boolean</span> isUpdate = <span class="keyword">true</span>;</span><br><span class="line">		<span class="keyword">if</span>(<span class="keyword">null</span> == lfuObj) &#123;</span><br><span class="line">			isUpdate = <span class="keyword">false</span>;</span><br><span class="line">			lfuObj = <span class="keyword">new</span> LfuObj(key, value, <span class="number">1</span>, System.nanoTime());</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			lfuObj.val = value;</span><br><span class="line">			lfuObj.cnt++;</span><br><span class="line">			lfuObj.time = System.nanoTime();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(hashMap.size() &lt; capacity || isUpdate) &#123;</span><br><span class="line">			hashMap.put(key, lfuObj);</span><br><span class="line">			heap.remove(lfuObj);</span><br><span class="line">			heap.offer(lfuObj);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			LfuObj tmp = heap.poll();</span><br><span class="line">			hashMap.remove(tmp.key);</span><br><span class="line">			heap.offer(lfuObj);</span><br><span class="line">			hashMap.put(key, lfuObj);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] agrs)</span> </span>&#123;</span><br><span class="line">		LFUCache cache = <span class="keyword">new</span> LFUCache(<span class="number">3</span>);</span><br><span class="line">		cache.put(<span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">		cache.put(<span class="number">2</span>, <span class="number">2</span>);</span><br><span class="line">		cache.put(<span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">		cache.put(<span class="number">4</span>, <span class="number">4</span>);</span><br><span class="line">		System.out.println(cache.get(<span class="number">4</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">3</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">2</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">1</span>));</span><br><span class="line">		cache.put(<span class="number">5</span>, <span class="number">5</span>);</span><br><span class="line">		System.out.println(cache.get(<span class="number">1</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">2</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">3</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">4</span>));</span><br><span class="line">		System.out.println(cache.get(<span class="number">5</span>));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> <img src="https://imagehome.oss-cn-beijing.aliyuncs.com/20190902111153.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Focus-1</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">228</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">57</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">98</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://gitee.com/carloz" title="repository - https://gitee.com/carloz" target="_blank">repository - https://gitee.com/carloz</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Focus-1</span>

  
</div>








  <div class="footer-custom">Hosted by <a target="_blank" href="https://gitee.com/carloz">Gitee Repo</a></div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
